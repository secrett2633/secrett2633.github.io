[{"id":"2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice","title":"[논문리뷰] VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice","excerpt":"이 [arXiv]에 게시한 'VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Understanding","Chain-of-Thought (CoT)","Reinforcement Learning (RL)","Adaptive Reasoning","Early Exit","Multimodal LLM","Video QA","Temporal Grounding"],"permalink":"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","title":"[논문리뷰] VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control","excerpt":"Ying Shan이 [arXiv]에 게시한 'VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video World Model","4D Geometric Control","Gaussian Trajectories","Video Generation","Diffusion Models","Camera Control","Object Motion Control","Data Engine"],"permalink":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset","title":"[논문리뷰] Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset","excerpt":"YuanFu Yang이 [arXiv]에 게시한 'Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Industrial Defect Detection","Multimodal Dataset","Vision-Language Model","Diffusion Model","Open-Vocabulary Learning","Quality Inspection","Data Efficiency","Foundation Model"],"permalink":"/ai/review/2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute","title":"[논문리뷰] Token-Level LLM Collaboration via FusionRoute","excerpt":"Furong Huang이 [arXiv]에 게시한 'Token-Level LLM Collaboration via FusionRoute' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Collaboration","Token-level Routing","Mixture-of-Experts","Complementary Logits","Preference Optimization","FusionRoute","Domain Adaptation"],"permalink":"/ai/review/2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models","title":"[논문리뷰] The Illusion of Specialization: Unveiling the Domain-Invariant 'Standing Committee' in Mixture-of-Experts Models","excerpt":"이 [arXiv]에 게시한 'The Illusion of Specialization: Unveiling the Domain-Invariant 'Standing Committee' in Mixture-of-Experts Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Sparse Routing","Domain Specialization","Load Balancing","Interpretability","Standing Committee","LLM"],"permalink":"/ai/review/2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","title":"[논문리뷰] RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation","excerpt":"Mingda Jia이 [arXiv]에 게시한 'RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robot Manipulation","Data Augmentation","Video Generation","Diffusion Models","Multi-View","Visual Identity Prompting","Action-Guided Segmentation","Visuomotor Policy"],"permalink":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[{"id":"comment-1","author":"로봇학습연구자","content":"멀티-뷰 비디오 생성이 로봇 조작에 이렇게 큰 영향을 미칠 수 있다니 흥미롭네요. 특히 시각적 ID 프롬프트를 사용한 부분이 인상적입니다.","date":"2026-01-10T01:30:00.000Z"},{"id":"comment-2","author":"AI엔지니어","content":"실세계 로봇 실험에서 9/10 성공률은 정말 인상적입니다. Diffusion Policy와의 결합이 효과적이었던 것 같습니다.","date":"2026-01-10T05:20:00.000Z"},{"id":"comment-3","author":"로봇학습연구자","content":"데이터 수집의 어려움을 해결하는 접근 방식이 실용적입니다. 실제로 적용해볼 만한 가치가 있어 보입니다.","date":"2026-01-10T07:45:00.000Z","parentId":"comment-1"}],"commentCount":3},{"id":"2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding","title":"[논문리뷰] RelayLLM: Efficient Reasoning via Collaborative Decoding","excerpt":"Haolin Liu이 [arXiv]에 게시한 'RelayLLM: Efficient Reasoning via Collaborative Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","SLM","Collaborative Decoding","Token-level Intervention","Reinforcement Learning","GRPO","Efficient Reasoning","Resource Efficiency"],"permalink":"/ai/review/2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","title":"[논문리뷰] Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing","excerpt":"Yu Xu이 [arXiv]에 게시한 'Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","In-Context Image Generation","Image Editing","Multimodal Models","Chain-of-Thought","Structured Reasoning","Reinforcement Learning","Alignment","Diffusion Models"],"permalink":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes","title":"[논문리뷰] RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes","excerpt":"Yu-Lun Liu이 [arXiv]에 게시한 'RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Auto White Balance (AWB)","Deep Reinforcement Learning (DRL)","Low-Light Imaging","Night-time Scenes","Color Constancy","Cross-Sensor Generalization","Statistical Methods","Curriculum Learning"],"permalink":"/ai/review/2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Plenoptic-Video-Generation","title":"[논문리뷰] Plenoptic Video Generation","excerpt":"이 [arXiv]에 게시한 'Plenoptic Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Video","Camera Control","Plenoptic Function","Autoregressive Model","Diffusion Transformer","3D FOV Retrieval","Spatio-Temporal Consistency"],"permalink":"/ai/review/2026-01-09-Plenoptic-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study","title":"[논문리뷰] Memorization in 3D Shape Generation: An Empirical Study","excerpt":"이 [arXiv]에 게시한 'Memorization in 3D Shape Generation: An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Shape Generation","Memorization","Generative Models","Diffusion Models","Evaluation Framework","Generalization","Data Augmentation"],"permalink":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers","title":"[논문리뷰] Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers","excerpt":"이 [arXiv]에 게시한 'Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Weight Decay","Learnable Multipliers","Scale Adaptation","Optimization","µP Parametrization","Adam","Muon"],"permalink":"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization","title":"[논문리뷰] GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization","excerpt":"이 [arXiv]에 게시한 'GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-reward RL","Policy Optimization","Reward Normalization","GRPO","GDPO","LLMs","Training Stability"],"permalink":"/ai/review/2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models","title":"[논문리뷰] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Adversarial Attacks","Entropy-Guided Attacks","Token Vulnerability","Harmful Content","Cross-Model Transferability","Autoregressive Generation"],"permalink":"/ai/review/2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach","title":"[논문리뷰] Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach","excerpt":"Carl James Debono이 [arXiv]에 게시한 'Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Object Detection","Privileged Information","Teacher-Student Learning","Knowledge Distillation","Model-Agnostic","Bounding Box Masks","UAV-based Detection"],"permalink":"/ai/review/2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking","title":"[논문리뷰] DocDancer: Towards Agentic Document-Grounded Information Seeking","excerpt":"이 [arXiv]에 게시한 'DocDancer: Towards Agentic Document-Grounded Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic AI","Document Question Answering","Tool-use","Information Seeking","Synthetic Data Generation","Long-context Understanding","Multimodal Documents"],"permalink":"/ai/review/2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","title":"[논문리뷰] DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs","excerpt":"Jing Ma이 [arXiv]에 게시한 'DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Chain-of-Thought","Diffusion Models","Large Language Models","Reasoning","Error Correction","Preference Optimization","Denoising"],"permalink":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering","title":"[논문리뷰] AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering","excerpt":"Di Zhang이 [arXiv]에 게시한 'AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Release Engineering","Self-Improvement","Regression Testing","Continuous Integration","Flip-Centered Gating","Auditable Development","Software Engineering"],"permalink":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-Agent-as-a-Judge","title":"[논문리뷰] Agent-as-a-Judge","excerpt":"Meng Liu이 [arXiv]에 게시한 'Agent-as-a-Judge' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agent-as-a-Judge","LLM Evaluation","Multi-Agent Systems","Tool Integration","AI Alignment","Automated Assessment","Survey"],"permalink":"/ai/review/2026-01-09-Agent-as-a-Judge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search","title":"[논문리뷰] AT^2PO: Agentic Turn-based Policy Optimization via Tree Search","excerpt":"이 [arXiv]에 게시한 'AT^2PO: Agentic Turn-based Policy Optimization via Tree Search' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","lastModifiedAt":"2026-01-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic RL","Multi-turn Tasks","Policy Optimization","Tree Search","Credit Assignment","Exploration Diversity","LLM Agents"],"permalink":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts","title":"[논문리뷰] Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts","excerpt":"이 [arXiv]에 게시한 'Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Machine Learning Research","Autonomous Research","LLM Agents","Scientific Workflow","Failure Modes","Experimental Design","AI Scientist","Agentic Systems"],"permalink":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing","title":"[논문리뷰] ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing","excerpt":"이 [arXiv]에 게시한 'ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Image Editing","Reasoning","Chain-of-Thought","Multimodal Generative Models","Reward Modeling","VLM"],"permalink":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization","title":"[논문리뷰] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization","excerpt":"이 [arXiv]에 게시한 'RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Gaussian Splatting","SLAM","Dense Initialization","Real-Time Tracking","Differentiable Rendering","DINOv3"],"permalink":"/ai/review/2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics","title":"[논문리뷰] MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics","excerpt":"이 [arXiv]에 게시한 'MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Molecular Dynamics","LAMMPS","Code Generation","Knowledge Q&A","Large Language Models","Reinforcement Learning","Multi-agent System","Domain Adaptation"],"permalink":"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents","title":"[논문리뷰] MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents","excerpt":"Bingzhe Li이 [arXiv]에 게시한 'MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Memory","Large Language Models","Retrieval-Augmented Generation","Knowledge Graphs","Multi-Graph Architecture","Long-Context Reasoning","Memory Evolution"],"permalink":"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning","title":"[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning","excerpt":"Guanchen Wu이 [arXiv]에 게시한 'EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Epidemiological Question Answering","Large Language Models","Benchmark","Multi-step Inference","Evidence Grounding","LLM Evaluation","Public Health AI","Chain-of-Thought"],"permalink":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting","title":"[논문리뷰] Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting","excerpt":"이 [arXiv]에 게시한 'Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Supervised Fine-Tuning (SFT)","Catastrophic Forgetting","Entropy-Adaptive Fine-Tuning (EAFT)","Large Language Models (LLMs)","Domain Adaptation","Reinforcement Learning (RL)","Confident Conflicts"],"permalink":"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models","title":"[논문리뷰] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models","excerpt":"이 [arXiv]에 게시한 'E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","lastModifiedAt":"2026-01-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Flow Models","Entropy-aware Sampling","Group Relative Policy Optimization","SDE","Human Preference Alignment","Image Generation"],"permalink":"/ai/review/2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework","title":"[논문리뷰] X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework","excerpt":"Shwetank Shekhar Singh이 [arXiv]에 게시한 'X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hate Speech Detection","Explainable AI (XAI)","Multilingual NLP","Large Language Models (LLMs)","Attention Mechanism","N-gram Explanations","Human Rationales","Benchmark Dataset"],"permalink":"/ai/review/2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision","title":"[논문리뷰] UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision","excerpt":"XinYu Sun이 [arXiv]에 게시한 'UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Self-Supervised Learning","Text-to-Image Generation","Multi-Agent Framework","Cognitive Pattern Reconstruction","Cycle-Consistency","Conduction Aphasia"],"permalink":"/ai/review/2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs","title":"[논문리뷰] Steerability of Instrumental-Convergence Tendencies in LLMs","excerpt":"j-hoscilowic이 [arXiv]에 게시한 'Steerability of Instrumental-Convergence Tendencies in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Steerability","Instrumental Convergence","AI Safety","AI Security","Open-Weight Models","Prompt Engineering","Model Control","Behavioral Alignment"],"permalink":"/ai/review/2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models","title":"[논문리뷰] SOP: A Scalable Online Post-Training System for Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'SOP: A Scalable Online Post-Training System for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Online Post-training","Scalable Robot Learning","Distributed Systems","Multi-task Learning","Imitation Learning","Reinforcement Learning"],"permalink":"/ai/review/2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation","title":"[논문리뷰] Parallel Latent Reasoning for Sequential Recommendation","excerpt":"Yuning Jiang이 [arXiv]에 게시한 'Parallel Latent Reasoning for Sequential Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Sequential Recommendation","Latent Reasoning","Parallel Processing","Computational Scaling","Mixture of Experts","Contrastive Learning","Transformer Architecture"],"permalink":"/ai/review/2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents","title":"[논문리뷰] NitroGen: An Open Foundation Model for Generalist Gaming Agents","excerpt":"이 [arXiv]에 게시한 'NitroGen: An Open Foundation Model for Generalist Gaming Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generalist Agents","Foundation Models","Behavior Cloning","Video Games","Action Extraction","Multi-game","Embodied AI"],"permalink":"/ai/review/2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-MiMo-V2-Flash-Technical-Report","title":"[논문리뷰] MiMo-V2-Flash Technical Report","excerpt":"이 [arXiv]에 게시한 'MiMo-V2-Flash Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts","Sliding Window Attention","Multi-Token Prediction","Multi-Teacher On-Policy Distillation","Reinforcement Learning","Long-Context Modeling","Agentic AI"],"permalink":"/ai/review/2026-01-07-MiMo-V2-Flash-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model","title":"[논문리뷰] LTX-2: Efficient Joint Audio-Visual Foundation Model","excerpt":"Andrew Kvochko이 [arXiv]에 게시한 'LTX-2: Efficient Joint Audio-Visual Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Text-to-Audio-Video","Diffusion Transformer","Cross-Modal Attention","Classifier-Free Guidance","Efficient Inference","Foundation Model"],"permalink":"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields","title":"[논문리뷰] InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields","excerpt":"이 [arXiv]에 게시한 'InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Depth Estimation","Neural Implicit Fields","Arbitrary Resolution","Fine-Grained","Novel View Synthesis","Vision Transformer","Synth4K Benchmark"],"permalink":"/ai/review/2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing","title":"[논문리뷰] FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing","excerpt":"Peng Tang이 [arXiv]에 게시한 'FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Editing","First-Frame Propagation (FFP)","Large-Scale Dataset","Generative Models","Temporal Consistency","Spatio-Temporal RoPE","Self-Distillation"],"permalink":"/ai/review/2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization","title":"[논문리뷰] DreamStyle: A Unified Framework for Video Stylization","excerpt":"이 [arXiv]에 게시한 'DreamStyle: A Unified Framework for Video Stylization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Stylization","Unified Framework","Diffusion Models","LoRA","Data Curation","Multi-modal Input","Image-to-Video"],"permalink":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving","title":"[논문리뷰] CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving","excerpt":"Tao Feng이 [arXiv]에 게시한 'CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","lastModifiedAt":"2026-01-07 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Visual Reasoning","Mathematical Problem Solving","Knowledge Internalization","Reinforcement Learning","Cognitive-Inspired AI","Perception-Reasoning Alignment"],"permalink":"/ai/review/2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context","title":"[논문리뷰] VINO: A Unified Visual Generator with Interleaved OmniModal Context","excerpt":"Kun Gai이 [arXiv]에 게시한 'VINO: A Unified Visual Generator with Interleaved OmniModal Context' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unified Generation","Multimodal Diffusion","Vision-Language Model","Image Editing","Video Editing","Interleaved Context","Progressive Training","Diffusion Transformer"],"permalink":"/ai/review/2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation","title":"[논문리뷰] VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation","excerpt":"이 [arXiv]에 게시한 'VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Autoregressive Models","Reinforcement Learning","Policy Conflicts","GRPO","Text-to-Image Generation","Credit Assignment","Multi-scale Generation"],"permalink":"/ai/review/2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion","title":"[논문리뷰] Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion","excerpt":"Shiying Wang이 [arXiv]에 게시한 'Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Semi-Supervised Learning","Semantic Segmentation","Remote Sensing","Vision Foundation Models","Pseudo-Label Drift","Co-Guidance","Feature Fusion"],"permalink":"/ai/review/2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes","title":"[논문리뷰] Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes","excerpt":"Shuo Yang이 [arXiv]에 게시한 'Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Text-Guided Image Editing","Object-Level Transformation","Geometric Transformation","Diffusion Models","GRPO","Scene Editing","Spatially Grounded Rewards"],"permalink":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving","title":"[논문리뷰] SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving","excerpt":"이 [arXiv]에 게시한 'SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Software Engineering","Issue Resolution","Supervised Fine-tuning (SFT)","Large Language Models (LLMs)","Hybrid Dataset","Error Masking","Curriculum Learning","Test-Time Scaling (TTS)","Generative Verifiers"],"permalink":"/ai/review/2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-Recursive-Language-Models","title":"[논문리뷰] Recursive Language Models","excerpt":"이 [arXiv]에 게시한 'Recursive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Recursive Language Models","Large Language Models","Long Context Processing","Inference Scaling","REPL Environment","Task Decomposition","Sub-LM Calls","Context Management"],"permalink":"/ai/review/2026-01-06-Recursive-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents","title":"[논문리뷰] Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents","excerpt":"이 [arXiv]에 게시한 'Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Faithfulness","XAI","Causal Inference","Structural Causal Models","Counterfactual Interventions","Reasoning Trace Auditing","Causal Decoupling"],"permalink":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment","title":"[논문리뷰] OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment","excerpt":"Chunchun Ma이 [arXiv]에 게시한 'OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM 에이전트 시스템","학술 독창성 평가","피어 리뷰 지원","증거 기반 검증","의미론적 검색","계층적 분류 체계","대규모 언어 모델"],"permalink":"/ai/review/2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation","title":"[논문리뷰] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation","excerpt":"이 [arXiv]에 게시한 'NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Decoder-only Transformer","Next-scale Prediction","Image Generation","Image Editing","Reinforcement Learning","Unified Modeling","TokenFlow"],"permalink":"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models","title":"[논문리뷰] M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models","excerpt":"Jun-Cheng Chen이 [arXiv]에 게시한 'M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Concept Erasure","Multimodal Evaluation","Adversarial Attacks","Robustness","Textual Inversion","Latent Inversion","Cross-Attention"],"permalink":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs","title":"[논문리뷰] KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs","excerpt":"Yi Yang이 [arXiv]에 게시한 'KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text Embedding","Decoder-only LLMs","Training-free","KV Re-routing","Causal Attention","Representation Learning","Intrinsic Dimensionality"],"permalink":"/ai/review/2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-K-EXAONE-Technical-Report","title":"[논문리뷰] K-EXAONE Technical Report","excerpt":"이 [arXiv]에 게시한 'K-EXAONE Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multilingual Language Model","Mixture-of-Experts (MoE)","Long Context","AI Safety","Korean AI","Foundation Model","Reinforcement Learning (RL)"],"permalink":"/ai/review/2026-01-06-K-EXAONE-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams","title":"[논문리뷰] InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams","excerpt":"이 [arXiv]에 게시한 'InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Transformer","Streaming Perception","Memory Management","KV Cache Pruning","Visual Geometry","Temporal Consistency","Continuous Learning"],"permalink":"/ai/review/2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset","title":"[논문리뷰] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset","excerpt":"이 [arXiv]에 게시한 'IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dermoscopy","Skin Lesion Segmentation","Multi-Annotator Dataset","Inter-Annotator Variability","ISIC Archive","Medical Image Analysis","Machine Learning","Data Annotation"],"permalink":"/ai/review/2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking","title":"[논문리뷰] GARDO: Reinforcing Diffusion Models without Reward Hacking","excerpt":"Zhiyong Wang이 [arXiv]에 게시한 'GARDO: Reinforcing Diffusion Models without Reward Hacking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Reinforcement Learning","Reward Hacking","KL Regularization","Adaptive Regularization","Diversity Optimization","Text-to-Image Generation"],"permalink":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling","title":"[논문리뷰] Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling","excerpt":"이 [arXiv]에 게시한 'Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reasoning","Small Language Models (SLMs)","Hybrid Architecture","Test-Time Scaling (TTS)","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","DeepConf","Computational Efficiency"],"permalink":"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer","title":"[논문리뷰] DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer","excerpt":"이 [arXiv]에 게시한 'DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Face Swapping","Diffusion Transformer","Identity Preservation","Temporal Consistency","Modality-Aware Conditioning","Reinforcement Learning","Data Synthesis"],"permalink":"/ai/review/2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits","title":"[논문리뷰] Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits","excerpt":"이 [arXiv]에 게시한 'Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Self-Awareness","Failure Prediction","Internal States","Attention Mechanisms","Neural Network Probes","Computational Efficiency","Zero-Shot Transfer"],"permalink":"/ai/review/2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs","title":"[논문리뷰] COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs","excerpt":"이 [arXiv]에 게시한 'COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","lastModifiedAt":"2026-01-06 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Policy Alignment","Organizational Policies","AI Safety","Adversarial Robustness","Refusal Behavior","Prompt Engineering","Fine-tuning"],"permalink":"/ai/review/2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization","title":"[논문리뷰] Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Automated Agent Generation","Reinforcement Learning","Hybrid Policy Optimization","Tool Synthesis","In-context Learning","Agent Framework","Scalability"],"permalink":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation","title":"[논문리뷰] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation","excerpt":"이 [arXiv]에 게시한 'Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","MLLMs","Video Understanding","Hallucinations","Counterfactual Generation","Diffusion Models","Reinforcement Learning","QA Dataset","DNA-Train"],"permalink":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning","title":"[논문리뷰] SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Agents","Reinforcement Learning","Vision-Language Models","Tool Use","Agentic Reasoning","Image Search","HR-MMSearch","BN-GSPO"],"permalink":"/ai/review/2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures","title":"[논문리뷰] Nested Learning: The Illusion of Deep Learning Architectures","excerpt":"Vahab Mirrokni이 [arXiv]에 게시한 'Nested Learning: The Illusion of Deep Learning Architectures' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Nested Learning","Continual Learning","In-context Learning","Associative Memory","Multi-Timescale Memory","Self-Modifying Models","Optimizers"],"permalink":"/ai/review/2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos","title":"[논문리뷰] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos","excerpt":"Feng Wang이 [arXiv]에 게시한 'NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","4D World Model","Gaussian Splatting","Monocular Video","Novel View Synthesis","Video Generation","Feed-Forward Reconstruction","Degradation Simulation"],"permalink":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing","title":"[논문리뷰] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing","excerpt":"Jian Yang이 [arXiv]에 게시한 'MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Morphing","Structured Latent (SLAT)","Generative Models","Attention Mechanisms","Training-Free Framework","Cross-Category Transitions","Temporal Coherence"],"permalink":"/ai/review/2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs","title":"[논문리뷰] InfoSynth: Information-Guided Benchmark Synthesis for LLMs","excerpt":"이 [arXiv]에 게시한 'InfoSynth: Information-Guided Benchmark Synthesis for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Benchmark Synthesis","LLM Evaluation","Code Generation","Information Theory","Genetic Algorithms","Novelty Metrics","Diversity Metrics"],"permalink":"/ai/review/2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-Fast-weight-Product-Key-Memory","title":"[논문리뷰] Fast-weight Product Key Memory","excerpt":"이 [arXiv]에 게시한 'Fast-weight Product Key Memory' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Fast-weight Memory","Product Key Memory","Episodic Memory","Language Models","Long-Context Modeling","Memory Augmented Networks","Continual Learning"],"permalink":"/ai/review/2026-01-05-Fast-weight-Product-Key-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction","title":"[논문리뷰] Diversity or Precision? A Deep Dive into Next Token Prediction","excerpt":"이 [arXiv]에 게시한 'Diversity or Precision? A Deep Dive into Next Token Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Next Token Prediction","Reinforcement Learning","Large Language Models","Reward Shaping","Pre-training Objective","Policy Gradient","Exploration-Exploitation"],"permalink":"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-Deep-Delta-Learning","title":"[논문리뷰] Deep Delta Learning","excerpt":"Quanquan Gu이 [arXiv]에 게시한 'Deep Delta Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Deep Residual Networks","Delta Operator","Geometric Transformation","Spectral Analysis","Gated Networks","Householder Reflection","Dynamical Systems","Identity Shortcut"],"permalink":"/ai/review/2026-01-05-Deep-Delta-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation","title":"[논문리뷰] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation","excerpt":"Sung Ju Hwang이 [arXiv]에 게시한 'Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Avatar Generation","Real-Time Interaction","Diffusion Models","Preference Optimization","Causal Inference","Multimodal Input","Head Avatar"],"permalink":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction","title":"[논문리뷰] AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction","excerpt":"Yu-Lun Liu이 [arXiv]에 게시한 'AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","lastModifiedAt":"2026-01-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dynamic Scene Reconstruction","Gabor Representation","Gaussian Splatting","Temporal Continuity","Cubic Hermite Splines","Frequency Adaptivity","Monocular Video"],"permalink":"/ai/review/2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs","title":"[논문리뷰] On the Role of Discreteness in Diffusion LLMs","excerpt":"이 [arXiv]에 게시한 'On the Role of Discreteness in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-02 00:00:00+0900+0900","lastModifiedAt":"2026-01-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Language Models","Discrete Text","Continuous Diffusion","Text Generation","Data Augmentation","Parallel Decoding","Structural Dependency"],"permalink":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space","title":"[논문리뷰] Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space","excerpt":"이 [arXiv]에 게시한 'Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-02 00:00:00+0900+0900","lastModifiedAt":"2026-01-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hierarchical Language Model","Concept-Level Reasoning","Dynamic Segmentation","Adaptive Computation","Scaling Laws","Maximal Update Parametrization","Next-Token Prediction","Flash Attention"],"permalink":"/ai/review/2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models","title":"[논문리뷰] DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models","excerpt":"Siyuan Huang이 [arXiv]에 게시한 'DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-02 00:00:00+0900+0900","lastModifiedAt":"2026-01-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Diffusion Models","Image-to-Image Generation","Vision-centric AI","Generative AI","Spatial Planning","Constraint Satisfaction"],"permalink":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-mHC-Manifold-Constrained-Hyper-Connections","title":"[논문리뷰] mHC: Manifold-Constrained Hyper-Connections","excerpt":"이 [arXiv]에 게시한 'mHC: Manifold-Constrained Hyper-Connections' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hyper-Connections","Residual Connections","Manifold Learning","Doubly Stochastic Matrices","Training Stability","Large Language Models","Infrastructure Optimization","Deep Learning Architecture"],"permalink":"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models","title":"[논문리뷰] Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models","excerpt":"Xinyi Dai이 [arXiv]에 게시한 'Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Lightweight LLM","Agentic AI","Pre-training","Multi-Latent Attention","Long-Context","Curriculum Learning","Agentic Mid-training","Instruction Tuning"],"permalink":"/ai/review/2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems","title":"[논문리뷰] Valori: A Deterministic Memory Substrate for AI Systems","excerpt":"varam17이 [arXiv]에 게시한 'Valori: A Deterministic Memory Substrate for AI Systems' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Deterministic AI","Reproducible Computation","Fixed-Point Arithmetic","Vector Databases","AI Memory","State Machine","Auditability"],"permalink":"/ai/review/2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time","title":"[논문리뷰] SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time","excerpt":"Tuanfeng Y. Wang이 [arXiv]에 게시한 'SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Diffusion Model","Generative Rendering","Novel View Synthesis","Space-Time Disentanglement","Temporal Control","Camera Control","Dynamic Scenes","Temporal Warping"],"permalink":"/ai/review/2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future","title":"[논문리뷰] Scaling Open-Ended Reasoning to Predict the Future","excerpt":"이 [arXiv]에 게시한 'Scaling Open-Ended Reasoning to Predict the Future' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Language Models","Forecasting","Open-Ended Reasoning","Reinforcement Learning (RL)","Data Generation","Calibration","Retrieval-Augmented Generation (RAG)","Future Prediction"],"permalink":"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","title":"[논문리뷰] Pretraining Frame Preservation in Autoregressive Video Memory Compression","excerpt":"Beijia Lu이 [arXiv]에 게시한 'Pretraining Frame Preservation in Autoregressive Video Memory Compression' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Compression","Autoregressive Models","Memory Compression","Frame Preservation","Pretraining","Video Generation","Diffusion Models","Long-Range Consistency"],"permalink":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation","title":"[논문리뷰] PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation","excerpt":"이 [arXiv]에 게시한 'PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Physics-Aware AI","Direct Preference Optimization","Groupwise Preference Learning","Vision-Language Model","LoRA"],"permalink":"/ai/review/2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem","title":"[논문리뷰] Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem","excerpt":"Wei Gao이 [arXiv]에 게시한 'Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Learning Ecosystem","Large Language Models","Reinforcement Learning","Agentic Crafting","Tool Use","ROME Model","Policy Optimization","Sandbox Environment"],"permalink":"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation","title":"[논문리뷰] JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation","excerpt":"이 [arXiv]에 게시한 'JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Sounding Video","Video Comprehension","Video Generation","Audio-Video Synchronization","Instruction Tuning","Diffusion Model","Encoder-Decoder"],"permalink":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself","title":"[논문리뷰] Guiding a Diffusion Transformer with the Internal Dynamics of Itself","excerpt":"이 [arXiv]에 게시한 'Guiding a Diffusion Transformer with the Internal Dynamics of Itself' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Transformer","Generative AI","Image Generation","Guidance Strategy","Internal Guidance","Auxiliary Loss","Classifier-Free Guidance"],"permalink":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers","title":"[논문리뷰] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers","excerpt":"Mahşuk Taylan이 [arXiv]에 게시한 'Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Respiratory Sound Classification","Audio Spectrogram Transformer","Sharpness-Aware Minimization","Loss Landscape","Imbalanced Learning","Transfer Learning","ICBHI 2017"],"permalink":"/ai/review/2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction","title":"[논문리뷰] GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction","excerpt":"Yu-Lun Liu이 [arXiv]에 게시한 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Sparse-View","Diffusion Models","Outpainting","Gaussian Splatting","Geometry-aware","Novel View Synthesis"],"permalink":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-GR-Dexter-Technical-Report","title":"[논문리뷰] GR-Dexter Technical Report","excerpt":"이 [arXiv]에 게시한 'GR-Dexter Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dexterous Manipulation","Bimanual Robotics","VLA Models","Robot Learning","Teleoperation","Cross-Embodiment Data","Robotic Hand Design"],"permalink":"/ai/review/2026-01-01-GR-Dexter-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems","title":"[논문리뷰] Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems","excerpt":"이 [arXiv]에 게시한 'Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-modal Pre-training","Autonomous Systems","Spatial Intelligence","Foundation Models","LiDAR-Camera Fusion","Self-Supervised Learning","Generative World Models","Embodied AI"],"permalink":"/ai/review/2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking","title":"[논문리뷰] Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking","excerpt":"Jie Zhou이 [arXiv]에 게시한 'Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Visual Thinking","Reinforcement Learning","Code Generation","Geometric Reasoning","Adaptive Reward Mechanism","Problem Solving"],"permalink":"/ai/review/2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process","title":"[논문리뷰] Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process","excerpt":"이 [arXiv]에 게시한 'Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Mechanistic Interpretability","Sparse Autoencoders (SAEs)","Activation Steering","Unsupervised Learning","Reasoning Behaviors","Chain-of-Thought","Feature Disentanglement"],"permalink":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models","title":"[논문리뷰] Factorized Learning for Temporally Grounded Video-Language Models","excerpt":"이 [arXiv]에 게시한 'Factorized Learning for Temporally Grounded Video-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video-Language Models","Temporal Grounding","Factorized Learning","Preference Optimization","Evidence Referencing","Video Understanding","Dense Captioning"],"permalink":"/ai/review/2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts","title":"[논문리뷰] BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts","excerpt":"Mengmeng Wang이 [arXiv]에 게시한 'BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Strategic Dialogue","Belief Estimation","Dialogue Acts","Probabilistic Constraints","Theory of Mind","Adversarial Dialogue","Alignment Dialogue"],"permalink":"/ai/review/2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents","title":"[논문리뷰] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents","excerpt":"Shixin Jiang이 [arXiv]에 게시한 'AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","lastModifiedAt":"2026-01-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autonomous Agents","Memory Systems","Cognitive Neuroscience","Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Memory Management","Multimodal Memory","Agent Skills"],"permalink":"/ai/review/2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement","title":"[논문리뷰] UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement","excerpt":"Kaiyi Zhang이 [arXiv]에 게시한 'UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","lastModifiedAt":"2025-12-31 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Shape Generation","Diffusion Models","Geometric Refinement","Data Curation","Watertight Mesh","Voxel-based","Scalability","High-Fidelity"],"permalink":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization","title":"[논문리뷰] GraphLocator: Graph-guided Causal Reasoning for Issue Localization","excerpt":"Wei Zhang이 [arXiv]에 게시한 'GraphLocator: Graph-guided Causal Reasoning for Issue Localization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","lastModifiedAt":"2025-12-31 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Issue Localization","Causal Reasoning","Graph-guided","Large Language Models","Software Engineering","Defect Analysis","Repository Mining"],"permalink":"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs","title":"[논문리뷰] GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs","excerpt":"이 [arXiv]에 게시한 'GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","lastModifiedAt":"2025-12-31 00:00:00+0900+0900","categories":["Review"],"tags":["Review","MoE LLM","Safety Alignment","Adversarial Attack","Neuron Pruning","Gate-level Profiling","Transfer Attack","Vision Language Model"],"permalink":"/ai/review/2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR","title":"[논문리뷰] Evaluating Parameter Efficient Methods for RLVR","excerpt":"이 [arXiv]에 게시한 'Evaluating Parameter Efficient Methods for RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","lastModifiedAt":"2025-12-31 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Parameter-Efficient Fine-Tuning (PEFT)","Reinforcement Learning with Verifiable Rewards (RLVR)","Low-Rank Adaptation (LoRA)","Mathematical Reasoning","LLM Adaptation","SVD Initialization"],"permalink":"/ai/review/2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-31-End-to-End-Test-Time-Training-for-Long-Context","title":"[논문리뷰] End-to-End Test-Time Training for Long Context","excerpt":"Marcel Rød이 [arXiv]에 게시한 'End-to-End Test-Time Training for Long Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","lastModifiedAt":"2025-12-31 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long-Context Language Modeling","Test-Time Training (TTT)","Meta-Learning","Continual Learning","Transformer","Sliding-Window Attention","Inference Efficiency","MLP Adaptation"],"permalink":"/ai/review/2025-12-31-End-to-End-Test-Time-Training-for-Long-Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation","title":"[논문리뷰] DreamOmni3: Scribble-based Editing and Generation","excerpt":"이 [arXiv]에 게시한 'DreamOmni3: Scribble-based Editing and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","lastModifiedAt":"2025-12-31 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Image Generation","Scribble-based Control","Multimodal AI","Diffusion Models","Data Synthesis","Human-Computer Interaction","Instruction-based Editing"],"permalink":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model","title":"[논문리뷰] Yume-1.5: A Text-Controlled Interactive World Generation Model","excerpt":"Kaining Ying이 [arXiv]에 게시한 'Yume-1.5: A Text-Controlled Interactive World Generation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Interactive World Generation","Video Diffusion Models","Text-to-Video","Image-to-Video","Real-time Generation","Temporal-Spatial-Channel Modeling","Self-Forcing"],"permalink":"/ai/review/2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Web-World-Models","title":"[논문리뷰] Web World Models","excerpt":"이 [arXiv]에 게시한 'Web World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Web World Model","LLM","Neuro-symbolic AI","Procedural Generation","Hybrid Architecture","Deterministic Generation","Persistent Environments","TypeScript"],"permalink":"/ai/review/2025-12-30-Web-World-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web","title":"[논문리뷰] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web","excerpt":"Kaixin Liang이 [arXiv]에 게시한 'Video-BrowseComp: Benchmarking Agentic Video Research on Open Web' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic AI","Video Understanding","Web Browsing","Benchmark","Multimodal LLMs","Temporal Grounding","Cross-Source Reasoning","Information Seeking"],"permalink":"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs","title":"[논문리뷰] VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs","excerpt":"Xihui Liu이 [arXiv]에 게시한 'VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Embodied AI","Vision and Language Navigation","Instance Object Navigation","Active Dialog","Large Language Models (LLMs)","Benchmark","Human-Robot Interaction"],"permalink":"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards","title":"[논문리뷰] Training AI Co-Scientists Using Rubric Rewards","excerpt":"이 [arXiv]에 게시한 'Training AI Co-Scientists Using Rubric Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Co-Scientists","Research Plan Generation","Reinforcement Learning (RL)","Self-Grading","Rubric Rewards","Language Models (LLMs)","Scientific Discovery"],"permalink":"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling","title":"[논문리뷰] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling","excerpt":"이 [arXiv]에 게시한 'SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Surgical Robotics","World Models","Video Generation","Imitation Learning","Inverse Dynamics Model","Synthetic Data","Vision-Language-Action Models","Data Scarcity"],"permalink":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion","title":"[논문리뷰] Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion","excerpt":"Po-Fan Yu이 [arXiv]에 게시한 'Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Super-Resolution","Diffusion Models","Low-Latency","Streamable","Auto-Regressive","Model Distillation","Temporal Consistency","Perceptual Quality"],"permalink":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers","title":"[논문리뷰] SpotEdit: Selective Region Editing in Diffusion Transformers","excerpt":"이 [arXiv]에 게시한 'SpotEdit: Selective Region Editing in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Transformer","Image Editing","Selective Editing","Computational Efficiency","Training-Free","Region-Aware","Perceptual Similarity"],"permalink":"/ai/review/2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents","title":"[논문리뷰] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents","excerpt":"이 [arXiv]에 게시한 'SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic RL","Self-Verifying Agents","GUI Automation","Evidence Curation","LLM-as-a-Judge","Reward Shaping","AndroidLab"],"permalink":"/ai/review/2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting","title":"[논문리뷰] Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting","excerpt":"이 [arXiv]에 게시한 'Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting","Open-vocabulary Segmentation","Neural Rendering","High-dimensional Features","Quantile Sampling","Real-time Rendering","Feature Distillation"],"permalink":"/ai/review/2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding","title":"[논문리뷰] OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding","excerpt":"Jian Liu이 [arXiv]에 게시한 'OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Omnimodal Understanding","Audio-Guided Perception","Active Learning Agents","Cross-Modal Alignment","Tool-Use","Video Understanding","Multimodal LLMs"],"permalink":"/ai/review/2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking","title":"[논문리뷰] Nested Browser-Use Learning for Agentic Information Seeking","excerpt":"이 [arXiv]에 게시한 'Nested Browser-Use Learning for Agentic Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Information Seeking","LLM Agents","Browser Automation","Nested Framework","Tool Learning","Context Efficiency","Deep Web"],"permalink":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Monadic-Context-Engineering","title":"[논문리뷰] Monadic Context Engineering","excerpt":"이 [arXiv]에 게시한 'Monadic Context Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Monads","Functional Programming","AI Agents","State Management","Error Handling","Concurrency","Monad Transformers","Meta-Agents"],"permalink":"/ai/review/2025-12-30-Monadic-Context-Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation","title":"[논문리뷰] LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation","excerpt":"Steffi Chern이 [arXiv]에 게시한 'LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Real-time Video Generation","Multimodal Diffusion","On-Policy Distillation","Interactive AI Avatars","Video Streaming","Anchor-Heavy Identity Sinks","Lip Synchronization"],"permalink":"/ai/review/2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models","title":"[논문리뷰] GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models","excerpt":"이 [arXiv]에 게시한 'GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text Encoder","Diffusion Models","Text Embedding","Evaluation Benchmark","MLLM Fine-tuning","Layer-wise Weighting","Text-to-Image Generation","Text-to-Video Generation"],"permalink":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone","title":"[논문리뷰] Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone","excerpt":"이 [arXiv]에 게시한 'Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Vision-Language Models","Vision-Language-Action Models","Robotics","Multimodal AI","Action Planning","Long-Horizon Planning","Bidirectional Attention"],"permalink":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation","title":"[논문리뷰] Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation","excerpt":"이 [arXiv]에 게시한 'Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Diffusion Model","Depth Estimation","Normal Estimation","Transparent Objects","Robotics","Data Generation","LoRA Fine-tuning"],"permalink":"/ai/review/2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models","title":"[논문리뷰] DiRL: An Efficient Post-Training Framework for Diffusion Language Models","excerpt":"이 [arXiv]에 게시한 'DiRL: An Efficient Post-Training Framework for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Post-Training","Reinforcement Learning","GRPO","FlexAttention","LMDeploy","Math Reasoning","SFT"],"permalink":"/ai/review/2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss","title":"[논문리뷰] Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss","excerpt":"이 [arXiv]에 게시한 'Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Router-Expert Coupling","Auxiliary Loss","Expert Specialization","Large Language Models (LLMs)","Computational Efficiency"],"permalink":"/ai/review/2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design","title":"[논문리뷰] An Information Theoretic Perspective on Agentic System Design","excerpt":"이 [arXiv]에 게시한 'An Information Theoretic Perspective on Agentic System Design' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Systems","Language Models","Mutual Information","Rate-Distortion Theory","Compute Efficiency","Scaling Laws","Compressor-Predictor Architecture","On-device AI"],"permalink":"/ai/review/2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy","title":"[논문리뷰] Act2Goal: From World Model To General Goal-conditioned Policy","excerpt":"이 [arXiv]에 게시한 'Act2Goal: From World Model To General Goal-conditioned Policy' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","lastModifiedAt":"2025-12-30 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Goal-Conditioned Policy","World Models","Robotic Manipulation","Multi-Scale Temporal Hashing","Online Adaptation","Hindsight Experience Replay","LoRA Finetuning","Zero-shot Generalization"],"permalink":"/ai/review/2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture","title":"[논문리뷰] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture","excerpt":"Kaiwen Zhu이 [arXiv]에 게시한 'UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Perceptual Understanding","Image Aesthetics","Image Quality","Image Structure","Image Texture","MLLM Benchmark","Visual Question Answering","Reward Model"],"permalink":"/ai/review/2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models","title":"[논문리뷰] TimeBill: Time-Budgeted Inference for Large Language Models","excerpt":"Yehan Ma이 [arXiv]에 게시한 'TimeBill: Time-Budgeted Inference for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Inference","Time Budgeting","KV Cache Eviction","Response Length Prediction","Execution Time Estimation","Real-time AI","Performance Optimization"],"permalink":"/ai/review/2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers","title":"[논문리뷰] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers","excerpt":"이 [arXiv]에 게시한 'SlideTailor: Personalized Presentation Slide Generation for Scientific Papers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Personalized Slide Generation","Preference Learning","Large Language Models","Multimodal AI","Chain-of-Speech","Agentic Framework","Document-to-Slides"],"permalink":"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning","title":"[논문리뷰] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning","excerpt":"이 [arXiv]에 게시한 'See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Vision-Language Models (VLMs)","Perceptual Shaping","KL-Divergence","Chart Understanding","Data Augmentation","Reinforcement Learning (RL)","GRPO"],"permalink":"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents","title":"[논문리뷰] SWE-RM: Execution-free Feedback For Software Engineering Agents","excerpt":"X. W.이 [arXiv]에 게시한 'SWE-RM: Execution-free Feedback For Software Engineering Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Software Engineering Agents","Execution-free Feedback","Reward Model","Reinforcement Learning","Test-Time Scaling","Calibration","AUC","SWE-Bench"],"permalink":"/ai/review/2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning","title":"[논문리뷰] SVBench: Evaluation of Video Generation Models on Social Reasoning","excerpt":"Xiaojie Xu이 [arXiv]에 게시한 'SVBench: Evaluation of Video Generation Models on Social Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Social Reasoning","Benchmark","Evaluation","Agent-based Pipeline","Vision-Language Models","Social Cognition"],"permalink":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right","title":"[논문리뷰] ProEdit: Inversion-based Editing From Prompts Done Right","excerpt":"Kun-Yu Lin이 [arXiv]에 게시한 'ProEdit: Inversion-based Editing From Prompts Done Right' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Inversion-based Editing","Text-to-Image Editing","Text-to-Video Editing","Diffusion Models","Flow-based Models","Attention Mechanism","Latent Space Manipulation","Plug-and-Play"],"permalink":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding","title":"[논문리뷰] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding","excerpt":"Yixin Chen이 [arXiv]에 게시한 'Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Foundation Model","Multimodal AI","Weather Nowcasting","Radar Inversion","Weather Understanding","Chain-of-Thought","Shared Attention"],"permalink":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding","title":"[논문리뷰] Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding","excerpt":"이 [arXiv]에 게시한 'Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Retrieval Augmented Generation","Long Context Understanding","Mindscape-Aware","Hierarchical Summarization","Context-Aware Embeddings","Integrative Reasoning","LLMs"],"permalink":"/ai/review/2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents","title":"[논문리뷰] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents","excerpt":"이 [arXiv]에 게시한 'MAI-UI Technical Report: Real-World Centric Foundation GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","GUI Agents","Foundation Models","Reinforcement Learning","Device-Cloud Collaboration","Mobile Navigation","Tool Augmentation","User Interaction"],"permalink":"/ai/review/2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion","title":"[논문리뷰] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion","excerpt":"이 [arXiv]에 게시한 'InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Object Insertion (VOI)","4D Scene Geometry","Diffusion Models","Mask Generation","Temporal Consistency","Occlusion Handling","Illumination Synthesis","ROSE++ Dataset"],"permalink":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search","title":"[논문리뷰] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search","excerpt":"Jierun Chen이 [arXiv]에 게시한 'InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Visual Search","Foundation Models","Multi-agent Systems","Reinforcement Learning","Benchmarking","Visual Reasoning"],"permalink":"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication","title":"[논문리뷰] A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication","excerpt":"A. I. Perminov이 [arXiv]에 게시한 'A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","lastModifiedAt":"2025-12-29 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Matrix Multiplication","Additive Complexity","Algorithm Optimization","Ternary Flip-Graph","Heuristic Search","Common Subexpression Elimination","BLAS"],"permalink":"/ai/review/2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation","title":"[논문리뷰] VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation","excerpt":"Yicong Li이 [arXiv]에 게시한 'VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","lastModifiedAt":"2025-12-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autoregressive Generation","Pixel-Aware Alignment","Variational Optimization","Reinforcement Learning","Visual Tokenizers","Image Quality","ELBO","Post-Training Framework"],"permalink":"/ai/review/2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","title":"[논문리뷰] Spatia: Video Generation with Updatable Spatial Memory","excerpt":"이 [arXiv]에 게시한 'Spatia: Video Generation with Updatable Spatial Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","lastModifiedAt":"2025-12-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Spatial Memory","3D Scene Point Cloud","Spatial Consistency","Camera Control","Interactive Editing","Diffusion Models","Visual SLAM"],"permalink":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models","title":"[논문리뷰] Schoenfeld's Anatomy of Mathematical Reasoning by Language Models","excerpt":"Tianyi Zhou이 [arXiv]에 게시한 'Schoenfeld's Anatomy of Mathematical Reasoning by Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","lastModifiedAt":"2025-12-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Cognitive Science","Schoenfeld's Episode Theory","Mathematical Problem Solving","Reasoning Dynamics","Interpretable AI","Behavioral Analysis"],"permalink":"/ai/review/2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-26-Latent-Implicit-Visual-Reasoning","title":"[논문리뷰] Latent Implicit Visual Reasoning","excerpt":"이 [arXiv]에 게시한 'Latent Implicit Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","lastModifiedAt":"2025-12-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Multimodal Models (LMMs)","Visual Reasoning","Latent Tokens","Visual Bottlenecking","Implicit Learning","Task-agnostic","Attention Mechanisms"],"permalink":"/ai/review/2025-12-26-Latent-Implicit-Visual-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode","title":"[논문리뷰] How Much 3D Do Video Foundation Models Encode?","excerpt":"이 [arXiv]에 게시한 'How Much 3D Do Video Foundation Models Encode?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","lastModifiedAt":"2025-12-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Foundation Models","3D Understanding","3D Reconstruction","Model Agnostic","Feature Probing","Diffusion Models","Temporal Reasoning"],"permalink":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training","title":"[논문리뷰] GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training","excerpt":"Yuanchun Shi이 [arXiv]에 게시한 'GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","lastModifiedAt":"2025-12-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-turn Reinforcement Learning","Vision-Language Models (VLMs)","Agentic AI","Knowledge Distillation","Model Merging","PPO","Thought Guidance","Cost Efficiency"],"permalink":"/ai/review/2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","title":"[논문리뷰] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times","excerpt":"이 [arXiv]에 게시한 'TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Acceleration","Quantization","Attention","Step Distillation","Performance Optimization","RTX 5090"],"permalink":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior","title":"[논문리뷰] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior","excerpt":"이 [arXiv]에 게시한 'TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Tokenizer","Language Models (LMs)","Robustness","Multilingual NLP","Benchmark","Subword Segmentation","Pre-training","Tokenization Impact"],"permalink":"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation","title":"[논문리뷰] T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation","excerpt":"이 [arXiv]에 게시한 'T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Audio-Video Generation","Multimodal Evaluation","Benchmark","MLLM-as-a-Judge","Cross-modal Alignment","Instruction Following","Perceptual Realism","Audio Realism"],"permalink":"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-Streaming-Video-Instruction-Tuning","title":"[논문리뷰] Streaming Video Instruction Tuning","excerpt":"Kaiyang Zhou이 [arXiv]에 게시한 'Streaming Video Instruction Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Streaming Video Understanding","Large Language Models (LLMs)","Instruction Tuning","Multi-task Learning","Real-time AI Assistant","Temporal Reasoning","Focal Loss","Video Question Answering"],"permalink":"/ai/review/2025-12-25-Streaming-Video-Instruction-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios","title":"[논문리뷰] SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios","excerpt":"Nghi D. Q. Bui이 [arXiv]에 게시한 'SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Coding Agents","Software Evolution","Benchmarking","Long-Horizon Tasks","Large Language Models (LLMs)","Software Engineering","Code Generation"],"permalink":"/ai/review/2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning","title":"[논문리뷰] Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning","excerpt":"이 [arXiv]에 게시한 'Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts","Mamba-Transformer","Agentic Reasoning","Long Context LLM","FP8 Quantization","Supervised Fine-Tuning","Reinforcement Learning"],"permalink":"/ai/review/2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence","title":"[논문리뷰] NVIDIA Nemotron 3: Efficient and Open Intelligence","excerpt":"이 [arXiv]에 게시한 'NVIDIA Nemotron 3: Efficient and Open Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hybrid Mamba-Transformer","Mixture-of-Experts","LatentMoE","NVFP4 Training","Multi-Token Prediction","Long Context","Reinforcement Learning","Open Models"],"permalink":"/ai/review/2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment","title":"[논문리뷰] Multi-hop Reasoning via Early Knowledge Alignment","excerpt":"Xuanjing Huang이 [arXiv]에 게시한 'Multi-hop Reasoning via Early Knowledge Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation (RAG)","Multi-hop Reasoning","Reinforcement Learning (RL)","Knowledge Alignment","Iterative RAG","Entropy Analysis","Plan Failure"],"permalink":"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models","title":"[논문리뷰] Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models","excerpt":"이 [arXiv]에 게시한 'Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dynamic Spatial Reasoning","Vision-Language Models","4D Understanding","Automated Data Generation","Geometry Selection Module","Video Analysis","Multimodal AI"],"permalink":"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations","title":"[논문리뷰] Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations","excerpt":"이 [arXiv]에 게시한 'Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autoregressive Model","Video Modeling","Generative Pretraining","Representation Learning","Flow-Matching Decoder","Context Isolation","Masked Next-Frame Prediction"],"permalink":"/ai/review/2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics","title":"[논문리뷰] LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics","excerpt":"이 [arXiv]에 게시한 'LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Competitive Ranking","Swiss-System","Monte Carlo Simulation","Failure Sensitivity Analysis","Robustness","Multi-Benchmark"],"permalink":"/ai/review/2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming","title":"[논문리뷰] HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming","excerpt":"이 [arXiv]에 게시한 'HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","High-Resolution Video Generation","Diffusion Models","Autoregressive","Efficiency","Caching","Attention Mechanisms","Video Streaming","Temporal Consistency"],"permalink":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation","title":"[논문리뷰] DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation","excerpt":"이 [arXiv]에 게시한 'DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","One-Shot Video","Diffusion Transformer (DiT)","Frame-Guided Generation","Auto-Regressive Generation","Supervised Fine-Tuning (SFT)","Direct Preference Optimization (DPO)"],"permalink":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models","title":"[논문리뷰] Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models","excerpt":"Yu-Lun Liu이 [arXiv]에 게시한 'Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","lastModifiedAt":"2025-12-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Popularity Bias","Ordinal Regression","Building Age Estimation","Multi-modal Learning","Benchmark Dataset","Explainable AI"],"permalink":"/ai/review/2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub","title":"[논문리뷰] Toxicity Ahead: Forecasting Conversational Derailment on GitHub","excerpt":"Kostadin Damevski이 [arXiv]에 게시한 'Toxicity Ahead: Forecasting Conversational Derailment on GitHub' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Conversational AI","Toxicity Detection","LLM","Prompt Engineering","Open Source Software","GitHub","Derailment Forecasting"],"permalink":"/ai/review/2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-Step-DeepResearch-Technical-Report","title":"[논문리뷰] Step-DeepResearch Technical Report","excerpt":"이 [arXiv]에 게시한 'Step-DeepResearch Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Deep Research Agents","LLMs","Reinforcement Learning","Supervised Fine-tuning","Agentic AI","Multi-hop Reasoning","Benchmarking","Cost-effectiveness"],"permalink":"/ai/review/2025-12-24-Step-DeepResearch-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs","title":"[논문리뷰] SpatialTree: How Spatial Abilities Branch Out in MLLMs","excerpt":"이 [arXiv]에 게시한 'SpatialTree: How Spatial Abilities Branch Out in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Spatial Intelligence","Multimodal LLMs","Cognitive Hierarchy","Benchmark","Reinforcement Learning","Supervised Fine-tuning","Spatial Reasoning"],"permalink":"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems","title":"[논문리뷰] Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems","excerpt":"Luisa Bentivogli이 [arXiv]에 게시한 'Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Streaming Speech-to-Text Translation","StreamST","Evaluation Toolkit","Open-Source Framework","Re-translation","Incremental Decoding","Latency Metrics","Quality Metrics","Real-time Demonstration"],"permalink":"/ai/review/2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","title":"[논문리뷰] SemanticGen: Video Generation in Semantic Space","excerpt":"이 [arXiv]에 게시한 'SemanticGen: Video Generation in Semantic Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Semantic Space","Diffusion Models","VAE Latents","Long Video Generation","Semantic Encoders","Generative AI"],"permalink":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-SAM-Audio-Segment-Anything-in-Audio","title":"[논문리뷰] SAM Audio: Segment Anything in Audio","excerpt":"이 [arXiv]에 게시한 'SAM Audio: Segment Anything in Audio' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio Source Separation","Foundation Models","Multimodal Prompting","Diffusion Transformers","Flow Matching","Self-Supervised Learning","Reference-Free Evaluation","Audio-Visual Learning"],"permalink":"/ai/review/2025-12-24-SAM-Audio-Segment-Anything-in-Audio/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library","title":"[논문리뷰] Reinforcement Learning for Self-Improving Agent with Skill Library","excerpt":"Soumya Smruti Mishra이 [arXiv]에 게시한 'Reinforcement Learning for Self-Improving Agent with Skill Library' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","LLM Agents","Skill Library","Self-Improvement","Sequential Rollout","AppWorld dataset","GRPO"],"permalink":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models","title":"[논문리뷰] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Physical Reasoning","Quantitative Benchmark","Kinematics","Mean Relative Accuracy","Video-Text","Embodied AI"],"permalink":"/ai/review/2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation","title":"[논문리뷰] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation","excerpt":"이 [arXiv]에 게시한 'Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Thematic Analysis","Large Language Models","Qualitative Research","Cohen's Kappa","Semantic Similarity","Reliability Metrics","Ensemble Validation","Prompt Engineering"],"permalink":"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems","title":"[논문리뷰] MemEvolve: Meta-Evolution of Agent Memory Systems","excerpt":"Junhao Wang이 [arXiv]에 게시한 'MemEvolve: Meta-Evolution of Agent Memory Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Memory Systems","Meta-Evolution","Self-Evolving AI","Memory Architecture","EvolveLab","Generalization"],"permalink":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos","title":"[논문리뷰] LongVideoAgent: Multi-Agent Reasoning with Long Videos","excerpt":"Renjie Pi이 [arXiv]에 게시한 'LongVideoAgent: Multi-Agent Reasoning with Long Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Long Video Understanding","Video Question Answering","Reinforcement Learning","Large Language Models","Temporal Grounding","Multimodal Reasoning","Tool-Augmented AI"],"permalink":"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-INTELLECT-3-Technical-Report","title":"[논문리뷰] INTELLECT-3: Technical Report","excerpt":"이 [arXiv]에 게시한 'INTELLECT-3: Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Mixture-of-Experts","Asynchronous Training","Distributed Systems","Agentic AI","Code Execution","Model Evaluation"],"permalink":"/ai/review/2025-12-24-INTELLECT-3-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination","title":"[논문리뷰] FaithLens: Detecting and Explaining Faithfulness Hallucination","excerpt":"이 [arXiv]에 게시한 'FaithLens: Detecting and Explaining Faithfulness Hallucination' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Hallucination Detection","Explainable AI","Faithfulness Evaluation","Data Augmentation","Reinforcement Learning","Fact-Checking"],"permalink":"/ai/review/2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies","title":"[논문리뷰] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies","excerpt":"이 [arXiv]에 게시한 'Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Policy Optimization","Interpretability","Transformer","Internal Policy","Entropy Analysis"],"permalink":"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling","title":"[논문리뷰] Active Intelligence in Video Avatars via Closed-loop World Modeling","excerpt":"Cheng Meng이 [arXiv]에 게시한 'Active Intelligence in Video Avatars via Closed-loop World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","lastModifiedAt":"2025-12-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Avatars","Active Intelligence","World Models","Closed-loop Reasoning","POMDP","Generative AI","Hierarchical Planning","Cognitive Architecture"],"permalink":"/ai/review/2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion","title":"[논문리뷰] WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion","excerpt":"이 [arXiv]에 게시한 'WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Novel View Synthesis","3D Geometry Propagation","Video Diffusion Models","Gaussian Splatting","Autoregressive Generation","Spatio-Temporal Noise","Geometric Consistency"],"permalink":"/ai/review/2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives","title":"[논문리뷰] Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives","excerpt":"Sujata Ghosh이 [arXiv]에 게시한 'Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Syllogistic Reasoning","Large Language Models (LLMs)","Belief Bias","Natural Language Understanding (NLU)","Formal Logic","Prompt Engineering","Self-Consistency","Cognitive Psychology"],"permalink":"/ai/review/2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models","title":"[논문리뷰] UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models","excerpt":"Yuqing Ma이 [arXiv]에 게시한 'UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unsupervised Learning","Code Generation","Large Language Models (LLMs)","Internal Probing","Self-Bootstrapping","Consensus Clustering","Code Intelligence"],"permalink":"/ai/review/2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding","title":"[논문리뷰] The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding","excerpt":"Ziwei Liu이 [arXiv]에 게시한 'The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unified Autoencoding","Prism Hypothesis","Semantic Representations","Pixel Representations","Frequency-Band Modulator","Foundation Models","Spectral Bias","Generative Models"],"permalink":"/ai/review/2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory","title":"[논문리뷰] StoryMem: Multi-shot Long Video Storytelling with Memory","excerpt":"이 [arXiv]에 게시한 'StoryMem: Multi-shot Long Video Storytelling with Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Storytelling","Multi-shot Video Generation","Memory Mechanism","Diffusion Models","Cross-shot Consistency","Latent Video Diffusion","ROPE Shift","Keyframe Selection"],"permalink":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing","title":"[논문리뷰] Region-Constraint In-Context Generation for Instructional Video Editing","excerpt":"이 [arXiv]에 게시한 'Region-Constraint In-Context Generation for Instructional Video Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Editing","In-Context Learning","Diffusion Models","Region-Constraint","Instruction-based Editing","Latent Space Regularization","Attention Space Regularization","Large-scale Dataset"],"permalink":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs","title":"[논문리뷰] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs","excerpt":"이 [arXiv]에 게시한 'Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Latent Variable Models","Variational Autoencoder (VAE)","Reinforcement Learning (RL)","Exploration","Large Language Models (LLMs)","Vision-Language Models (VLMs)","Controllable Generation","Reasoning Strategies"],"permalink":"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface","title":"[논문리뷰] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface","excerpt":"Liliang Chen이 [arXiv]에 게시한 'Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robotics","Demonstration Generation","3D Control Interface","Data Efficiency","Visuomotor Policy Learning","Spatial Generalization","Depth Map","Video Generation"],"permalink":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation","title":"[논문리뷰] QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation","excerpt":"Lu Cheng이 [arXiv]에 게시한 'QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dynamic RAG","Hallucination Detection","Corpus Statistics","Uncertainty Quantification","Pre-training Data","LLM Calibration","Infini-gram","Multi-hop QA"],"permalink":"/ai/review/2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming","title":"[논문리뷰] Name That Part: 3D Part Segmentation and Naming","excerpt":"Alan Yuille이 [arXiv]에 게시한 'Name That Part: 3D Part Segmentation and Naming' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Semantic Segmentation","Part Naming","Open-Vocabulary","LLM","Set Alignment","Geometric Deep Learning","Annotation Engine","Affordance Description"],"permalink":"/ai/review/2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments","title":"[논문리뷰] MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments","excerpt":"이 [arXiv]에 게시한 'MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mobile Agents","GUI Benchmarking","Agent-User Interaction","Tool-Augmented Agents","Model Context Protocol (MCP)","Long-Horizon Tasks","Reproducible Evaluation","Android Environment"],"permalink":"/ai/review/2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry","title":"[논문리뷰] MatSpray: Fusing 2D Material World Knowledge on 3D Geometry","excerpt":"이 [arXiv]에 게시한 'MatSpray: Fusing 2D Material World Knowledge on 3D Geometry' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Material Estimation","Diffusion Models","Gaussian Splatting","Inverse Rendering","PBR","Relighting","Neural Merger"],"permalink":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding","title":"[논문리뷰] LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding","excerpt":"이 [arXiv]에 게시한 'LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","dLLM","Parallel Decoding","Lookahead","Inference Acceleration","Token Filling Order","Branch Parallelism","Diffusion Models"],"permalink":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry","title":"[논문리뷰] LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry","excerpt":"Yuan Shen이 [arXiv]에 게시한 'LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autonomous Navigation","End-to-end Learning","Localization Grounded","Visual Geometry","Metric-aware Perception","Diffusion Policy","RGB-D"],"permalink":"/ai/review/2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","title":"[논문리뷰] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation","excerpt":"이 [arXiv]에 게시한 'Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Camera Control","Homography","Diffusion Models","Data Augmentation","Novel View Synthesis","Pose Fidelity"],"permalink":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators","title":"[논문리뷰] GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators","excerpt":"이 [arXiv]에 게시한 'GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Environment Simulation","Co-evolution","Curriculum Learning","Data Efficiency","Reinforcement Learning","Adaptive Simulation","Difficulty Alignment"],"permalink":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital","title":"[논문리뷰] Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital","excerpt":"이 [arXiv]에 게시한 'Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Legal AI","Venture Capital","Due Diligence","Capitalization Table","Multi-document Reasoning","Knowledge Graph","World Model","Neuro-Symbolic AI"],"permalink":"/ai/review/2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI","title":"[논문리뷰] DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI","excerpt":"이 [arXiv]에 게시한 'DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Data Preparation","Workflow Automation","Data-Centric AI","Synthetic Data","Multi-Agent System","Framework","Reproducibility"],"permalink":"/ai/review/2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction","title":"[논문리뷰] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction","excerpt":"Hong Jiao이 [arXiv]에 게시한 'Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Item Difficulty Prediction","Human-AI Alignment","Proficiency Simulation","Metacognition","Curse of Knowledge","Educational Assessment","Zero-shot Learning"],"permalink":"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States","title":"[논문리뷰] Brain-Grounded Axes for Reading and Steering LLM States","excerpt":"Sandro Andric이 [arXiv]에 게시한 'Brain-Grounded Axes for Reading and Steering LLM States' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","lastModifiedAt":"2025-12-23 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Interpretability","Brain-Grounded AI","MEG","Phase-Locking Value","ICA","LLM Steering","Neural Decoding","Latent Space"],"permalink":"/ai/review/2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-When-Reasoning-Meets-Its-Laws","title":"[논문리뷰] When Reasoning Meets Its Laws","excerpt":"Liu Ziyin이 [arXiv]에 게시한 'When Reasoning Meets Its Laws' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Reasoning Behaviors","Compute Law","Accuracy Law","Monotonicity","Compositionality","Fine-tuning","LORE-BENCH"],"permalink":"/ai/review/2025-12-22-When-Reasoning-Meets-Its-Laws/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs","title":"[논문리뷰] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs","excerpt":"Lihong Li이 [arXiv]에 게시한 'Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Turn Reinforcement Learning","LLM Agents","Proximal Policy Optimization (PPO)","Turn-Level MDP","Advantage Estimation","Generative AI","Deep Reinforcement Learning"],"permalink":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models","title":"[논문리뷰] StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models","excerpt":"이 [arXiv]에 게시한 'StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Autoregressive Models","Image Generation","Model Acceleration","Low-Rank Approximation","Semantic Irrelevance","Stage-Aware Optimization","Text-to-Image Synthesis"],"permalink":"/ai/review/2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience","title":"[논문리뷰] Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience","excerpt":"이 [arXiv]에 게시한 'Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Formal Theorem Proving","Large Language Models","Reinforcement Learning","Agentic Prover","Lean Theorem Prover","Mathematical Reasoning","Test-Time Scaling"],"permalink":"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories","title":"[논문리뷰] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories","excerpt":"이 [arXiv]에 게시한 'SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Software Engineering Benchmarks","Large Language Models (LLMs)","Code Generation","Automated Benchmark Generation","Multilingual","GitHub Pull Requests","Test Oracle","Fine-tuning"],"permalink":"/ai/review/2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding","title":"[논문리뷰] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding","excerpt":"Runtao Liu이 [arXiv]에 게시한 'Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Visual Degradation","Robustness","Reasoning Chains","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Degradation-Aware Reasoning","Interpretability"],"permalink":"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras","title":"[논문리뷰] RadarGen: Automotive Radar Point Cloud Generation from Cameras","excerpt":"Or Litany이 [arXiv]에 게시한 'RadarGen: Automotive Radar Point Cloud Generation from Cameras' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Radar Point Cloud Generation","Diffusion Models","Camera-to-Radar","BEV Representation","Autonomous Driving","Multi-modal Generative Models","Scene Editing"],"permalink":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows","title":"[논문리뷰] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows","excerpt":"Yuhao Zhou이 [arXiv]에 게시한 'Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Scientific General Intelligence (SGI)","LLMs","Benchmarking","Scientist-Aligned Workflows","Practical Inquiry Model","Multi-modal Reasoning","Code Generation","Test-Time Reinforcement Learning (TTRL)"],"permalink":"/ai/review/2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers","title":"[논문리뷰] Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers","excerpt":"이 [arXiv]에 게시한 'Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Language Models","Transformer Architecture","Canon Layers","Synthetic Pretraining","Reasoning Depth","Linear Attention","State-Space Models","NoPE"],"permalink":"/ai/review/2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence","title":"[논문리뷰] PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence","excerpt":"이 [arXiv]에 게시한 'PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Egocentric Data","Physical Intelligence","VLM","Robot Control","Embodied AI","VQA Supervision","Human-Robot Interaction","Zero-shot Transfer"],"permalink":"/ai/review/2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents","title":"[논문리뷰] Meta-RL Induces Exploration in Language Agents","excerpt":"Maria Brbic이 [arXiv]에 게시한 'Meta-RL Induces Exploration in Language Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Meta-RL","LLM Agents","Exploration","Reinforcement Learning","Policy Adaptation","In-context Learning","Self-reflection","Multi-episode tasks"],"permalink":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering","title":"[논문리뷰] HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering","excerpt":"이 [arXiv]에 게시한 'HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Question Answering","Multi-evidence Integration","Video-LLMs","Benchmark","Temporal Reasoning","Frame Selection","Evidential Requirement","MRFS"],"permalink":"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation","title":"[논문리뷰] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation","excerpt":"이 [arXiv]에 게시한 'GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Grounding","MLLMs","Benchmark","Multi-Dimensional Evaluation","Rejection Capability","Test-Time Scaling","Data Mixture Training"],"permalink":"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing","title":"[논문리뷰] Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing","excerpt":"이 [arXiv]에 게시한 'Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Image Editing","Representation Encoders","Latent Diffusion Models","Variational Autoencoder (VAE)","Semantic Reconstruction","Off-manifold Latents","DINOv2"],"permalink":"/ai/review/2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge","title":"[논문리뷰] Are We on the Right Way to Assessing LLM-as-a-Judge?","excerpt":"이 [arXiv]에 게시한 'Are We on the Right Way to Assessing LLM-as-a-Judge?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM-as-a-Judge","Evaluation Metrics","Consistency","Robustness","Positional Bias","Transitivity","Situational Preference","Multi-agent Systems"],"permalink":"/ai/review/2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","title":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges","excerpt":"이 [arXiv]에 게시한 'An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Embodied Intelligence","Robotics","Foundation Models","Multi-modal Learning","Reinforcement Learning","Sim-to-Real Transfer","Human-Robot Interaction"],"permalink":"/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation","title":"[논문리뷰] 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation","excerpt":"이 [arXiv]에 게시한 '4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","4D Understanding","Perceptual Distillation","Region-level VQA","Video Question Answering","Temporal Perception","Depth Perception"],"permalink":"/ai/review/2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework","title":"[논문리뷰] 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework","excerpt":"Hendrik P. A. Lensch이 [arXiv]에 게시한 '3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","lastModifiedAt":"2025-12-22 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Generative AI","Indoor Scenes","Compositional Framework","Differentiable Rendering","Image-to-3D","VFX","Game Development"],"permalink":"/ai/review/2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks","title":"[논문리뷰] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks","excerpt":"이 [arXiv]에 게시한 'VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","GUI Grounding","Multi-Platform","Benchmark","MLLM","Hierarchical Evaluation","Human-in-the-Loop Annotation","GUI Agents","Multilingual Dataset"],"permalink":"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","title":"[논문리뷰] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text","excerpt":"이 [arXiv]에 게시한 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","World Models","Video Generation","Multimodal Control","Trajectory Guidance","Reference Images","Promptable Events","Cross-Attention","Diffusion Models"],"permalink":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","title":"[논문리뷰] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors","excerpt":"이 [arXiv]에 게시한 'StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Monocular-to-Stereo Conversion","Video Generation","Diffusion Models","Feed-Forward Architecture","Domain Switcher","Cycle Consistency","Unified Dataset","Depth Ambiguity"],"permalink":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model","title":"[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model","excerpt":"이 [arXiv]에 게시한 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio-Visual Generation","Diffusion Transformer","Multimodal AI","Speech Synchronization","Video Generation","Reinforcement Learning from Human Feedback","Inference Acceleration"],"permalink":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing","title":"[논문리뷰] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing","excerpt":"Yuqi Liu이 [arXiv]에 게시한 'RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Vision-Language Models","Diffusion Models","Region-aligned Guidance","Reinforcement Learning","Instruction-Visual Complexity","Attention Mechanism"],"permalink":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion","title":"[논문리뷰] REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion","excerpt":"Giorgos Sfikas이 [arXiv]에 게시한 'REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Latent Diffusion Models","Vision Foundation Models","Semantic Compression","Global-Local Semantics","Image Generation","Representation Entanglement","Transformer Architecture"],"permalink":"/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners","title":"[논문리뷰] Next-Embedding Prediction Makes Strong Vision Learners","excerpt":"이 [arXiv]에 게시한 'Next-Embedding Prediction Makes Strong Vision Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Generative Pretraining","Vision Transformer","Next-Embedding Prediction","Autoregressive Model","Image Classification","Semantic Segmentation","Causal Masking"],"permalink":"/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models","title":"[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Grounding","Spatial Reasoning","Vision-Language Models","Depth Estimation","3D Object Detection","Chain-of-Thought","Data Generation","Multimodal AI"],"permalink":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image","title":"[논문리뷰] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image","excerpt":"이 [arXiv]에 게시한 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reward Models","Multimodal LLMs","Benchmark","Text-to-Image Generation","Image Editing","Interleaved Generation","Multimodal Reasoning","MLLM-as-a-judge"],"permalink":"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Kling-Omni-Technical-Report","title":"[논문리뷰] Kling-Omni Technical Report","excerpt":"이 [arXiv]에 게시한 'Kling-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Multimodal Visual Language","Generative AI","Video Editing","Reasoning-enhanced Generation","Diffusion Transformer","Multi-modal World Simulators"],"permalink":"/ai/review/2025-12-19-Kling-Omni-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language","title":"[논문리뷰] Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language","excerpt":"이 [arXiv]에 게시한 'Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Time Series Analysis","Multimodal Language Models","Natural Language Generation","Dataset Creation","Instruction Tuning","GPT-4","LLaVA","Cross-Domain Alignment"],"permalink":"/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs","title":"[논문리뷰] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs","excerpt":"Carlos Escolano이 [arXiv]에 게시한 'Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Speech-to-Text Translation","Multimodal LLMs","Speech Foundation Models","Cascaded Systems","Benchmarking","Speech Modality Integration","Robustness","Evaluation Metrics"],"permalink":"/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image","title":"[논문리뷰] Generative Refocusing: Flexible Defocus Control from a Single Image","excerpt":"Yu-Lun Liu이 [arXiv]에 게시한 'Generative Refocusing: Flexible Defocus Control from a Single Image' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative AI","Image Refocusing","Defocus Deblurring","Bokeh Synthesis","Depth of Field Control","Semi-Supervised Learning","Diffusion Models","Aperture Shape Control"],"permalink":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering","title":"[논문리뷰] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering","excerpt":"Hendrik P. A. Lensch이 [arXiv]에 게시한 'FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Neural Rendering","Diffusion Models","G-Buffer","Autoregressive Generation","Temporal Consistency","ControlNet","ControlLoRA","Interactive Applications"],"permalink":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","title":"[논문리뷰] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction","excerpt":"이 [arXiv]에 게시한 'FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Portrait Animation","Diffusion Models","Inference Acceleration","Identity Preservation","Video Generation","Latent Prediction","Sliding Window"],"permalink":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward","title":"[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward","excerpt":"이 [arXiv]에 게시한 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Exploration-Exploitation","Clipping","Policy Entropy","Spurious Rewards","Mathematical Reasoning","RLVR"],"permalink":"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification","title":"[논문리뷰] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification","excerpt":"이 [arXiv]에 게시한 'Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","MLLM","Model Auditing","Capability Gaps","Failure Mode Discovery","Reinforcement Learning","Data Rectification","Counterfactual Generation","VQA"],"permalink":"/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation","title":"[논문리뷰] Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation","excerpt":"Wenxuan Lu이 [arXiv]에 게시한 'Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Panoramic Depth Estimation","Foundation Model","Semi-Supervised Learning","Pseudo-Labeling","Data-in-the-Loop","DINOv3","Metric Depth","360-degree Vision"],"permalink":"/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers","title":"[논문리뷰] DeContext as Defense: Safe Image Editing in Diffusion Transformers","excerpt":"이 [arXiv]에 게시한 'DeContext as Defense: Safe Image Editing in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Transformers","Image Editing","Privacy Protection","Adversarial Attack","Attention Mechanism","Identity Preservation","Deepfake Defense","In-context Learning"],"permalink":"/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection","title":"[논문리뷰] Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection","excerpt":"Jiarong Ou이 [arXiv]에 게시한 'Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image","Data Selection","Meta-Learning","Meta-Gradient","Data Efficiency","Generative Models","Coreset Selection","Data Pruning"],"permalink":"/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-Adaptation-of-Agentic-AI","title":"[논문리뷰] Adaptation of Agentic AI","excerpt":"Zhiyi Shi이 [arXiv]에 게시한 'Adaptation of Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic AI","Adaptation","Agent Adaptation","Tool Adaptation","Reinforcement Learning","Fine-tuning","Modular AI"],"permalink":"/ai/review/2025-12-19-Adaptation-of-Agentic-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos","title":"[논문리뷰] AdaTooler-V: Adaptive Tool-Use for Images and Videos","excerpt":"Zhixun Li이 [arXiv]에 게시한 'AdaTooler-V: Adaptive Tool-Use for Images and Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","lastModifiedAt":"2025-12-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Adaptive Tool-Use","Reinforcement Learning","Chain-of-Thought","Vision-Language Models","Visual Reasoning","AT-GRPO"],"permalink":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory","title":"[논문리뷰] WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory","excerpt":"Sung Won Han이 [arXiv]에 게시한 'WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AIS data","vessel destination estimation","deep learning","transformer","channel attention","trajectory analysis","Gradient Dropout","maritime surveillance"],"permalink":"/ai/review/2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression","title":"[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?","excerpt":"이 [arXiv]에 게시한 'VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Text Compression (VTC)","Long Context Understanding","Vision-Language Models (VLMs)","Benchmark","Information Retrieval","Associative Reasoning","Multimodal AI"],"permalink":"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-Universal-Reasoning-Model","title":"[논문리뷰] Universal Reasoning Model","excerpt":"이 [arXiv]에 게시한 'Universal Reasoning Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Universal Transformer","Recurrent Neural Networks","ARC-AGI","Reasoning Tasks","Nonlinearity","Convolutional Gating","Truncated Backpropagation","Model Efficiency"],"permalink":"/ai/review/2025-12-18-Universal-Reasoning-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-Step-GUI-Technical-Report","title":"[논문리뷰] Step-GUI Technical Report","excerpt":"이 [arXiv]에 게시한 'Step-GUI Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","GUI Automation","Self-Evolving Pipeline","Reinforcement Learning","Multimodal LLMs","Privacy-Preserving AI","Human-Computer Interaction","Model Context Protocol","Benchmarking"],"permalink":"/ai/review/2025-12-18-Step-GUI-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning","title":"[논문리뷰] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning","excerpt":"이 [arXiv]에 게시한 'Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI-Generated Video Detection","Multimodal Large Language Model (MLLM)","Artifact Reasoning","Explainable AI","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Video Forensics"],"permalink":"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness","title":"[논문리뷰] SCOPE: Prompt Evolution for Enhancing Agent Effectiveness","excerpt":"Yunhe Wang이 [arXiv]에 게시한 'SCOPE: Prompt Evolution for Enhancing Agent Effectiveness' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Prompt Optimization","Context Management","Online Learning","Agent Effectiveness","Self-Evolving Prompts","Trace-Based Learning","Dual-Stream Routing"],"permalink":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning","title":"[논문리뷰] SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Reasoning","Reinforcement Learning","Multi-Turn Reasoning","Agent System","Long Videos","Synthetic Data","Any-Horizon Reasoning","Large Language Models"],"permalink":"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content","title":"[논문리뷰] Robust and Calibrated Detection of Authentic Multimedia Content","excerpt":"이 [arXiv]에 게시한 'Robust and Calibrated Detection of Authentic Multimedia Content' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Deepfake Detection","Content Authenticity","Generative Models","Adversarial Robustness","Image Inversion","Plausible Deniability","Diffusion Models","Multimedia Forensics"],"permalink":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition","title":"[논문리뷰] Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition","excerpt":"Xiao Xu이 [arXiv]에 게시한 'Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Layer Decomposition","RGBA Layers","Variational Autoencoder (VAE)","Multi-stage Training","Photoshop Documents (PSD)","Inherent Editability"],"permalink":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","title":"[논문리뷰] MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence","excerpt":"Peizhou Cao이 [arXiv]에 게시한 'MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video-Based Spatial Intelligence","MLLM Benchmark","Spatial Reasoning","Multi-Modal Learning","Perception","Planning","Prediction","Cross-Video Reasoning","Human-AI Gap"],"permalink":"/ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training","title":"[논문리뷰] In Pursuit of Pixel Supervision for Visual Pre-training","excerpt":"Dong Wang이 [arXiv]에 게시한 'In Pursuit of Pixel Supervision for Visual Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Pixel Supervision","Self-Supervised Learning","Masked Autoencoders (MAE)","Visual Pre-training","Foundation Models","Representation Learning","Web-Scale Data","Computer Vision"],"permalink":"/ai/review/2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning","title":"[논문리뷰] IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning","excerpt":"이 [arXiv]에 게시한 'IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video VFX Editing","In-Context Learning","Diffusion Transformers","Few-Shot Learning","LoRA","Spatiotemporal Tokenization","Instruction-Guided"],"permalink":"/ai/review/2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices","title":"[논문리뷰] HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices","excerpt":"Yuhang Dong이 [arXiv]에 게시한 'HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Model","Edge AI","Efficient Inference","Visual Resolution Compressor","Dual Consistency Learning","Vision Transformer","Quantization","Low-Latency"],"permalink":"/ai/review/2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing","title":"[논문리뷰] Fast and Accurate Causal Parallel Decoding using Jacobi Forcing","excerpt":"Tajana Rosing이 [arXiv]에 게시한 'Fast and Accurate Causal Parallel Decoding using Jacobi Forcing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Parallel Decoding","Causal LLM","Jacobi Decoding","Consistency Distillation","Transformer Inference","Latency Reduction","Rejection Recycling","Multi-block Decoding"],"permalink":"/ai/review/2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models","title":"[논문리뷰] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models","excerpt":"이 [arXiv]에 게시한 'DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Vision Language Models","Autoregressive Models","Diffusion Finetuning","Block Diffusion","Multimodal AI","KV Cache"],"permalink":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models","title":"[논문리뷰] DEER: Draft with Diffusion, Verify with Autoregressive Models","excerpt":"Zhijie Deng이 [arXiv]에 게시한 'DEER: Draft with Diffusion, Verify with Autoregressive Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Speculative Decoding","Diffusion LLM","Autoregressive Model","Inference Acceleration","Model Alignment","Code Generation","Block Regeneration"],"permalink":"/ai/review/2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning","title":"[논문리뷰] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning","excerpt":"이 [arXiv]에 게시한 'Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","lastModifiedAt":"2025-12-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Exploration Strategy","Gradient-Guided","Reward Shaping","Reasoning","PPO"],"permalink":"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans","title":"[논문리뷰] Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?","excerpt":"Ming Hu이 [arXiv]에 게시한 'Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AIGC Detection","ASMR Videos","VLM Evaluation","VGM Realism","Audio-Visual Consistency","Perceptual Fidelity","Adversarial Benchmark","Deepfake Detection"],"permalink":"/ai/review/2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models","title":"[논문리뷰] Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models","excerpt":"이 [arXiv]에 게시한 'Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Multimodal Models","Sparse Parameterization","KV Caching","Token Truncation","Image Generation","Image Editing","Visual Reasoning"],"permalink":"/ai/review/2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement","title":"[논문리뷰] ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement","excerpt":"Zhaohe Liao이 [arXiv]에 게시한 'ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Table Visualization","Infographic Generation","Multi-modal Large Language Models (MLLMs)","Diffusion Models","Self-Correction","Reinforcement Learning","Graphic Design","Data-to-Visual Mapping"],"permalink":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-RecGPT-V2-Technical-Report","title":"[논문리뷰] RecGPT-V2 Technical Report","excerpt":"Dian Chen이 [arXiv]에 게시한 'RecGPT-V2 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Recommender Systems","Large Language Models","Multi-Agent Systems","Reinforcement Learning","Dynamic Prompting","Hybrid Representation","Agentic Evaluation","Explanation Generation"],"permalink":"/ai/review/2025-12-17-RecGPT-V2-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-Olmo-3","title":"[논문리뷰] Olmo 3","excerpt":"이 [arXiv]에 게시한 'Olmo 3' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Open-Source AI","Model Flow","Long-Context Reasoning","Instruction Following","Function Calling","Thinking Models","Data Curation","Reinforcement Learning"],"permalink":"/ai/review/2025-12-17-Olmo-3/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-MMGR-Multi-Modal-Generative-Reasoning","title":"[논문리뷰] MMGR: Multi-Modal Generative Reasoning","excerpt":"Haozhe Zhao이 [arXiv]에 게시한 'MMGR: Multi-Modal Generative Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Modal Generative Models","Reasoning Evaluation","World Models","Physical Commonsense","Abstract Reasoning","Embodied Navigation","VLM-based Evaluation","Temporal Consistency"],"permalink":"/ai/review/2025-12-17-MMGR-Multi-Modal-Generative-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference","title":"[논문리뷰] Janus: Disaggregating Attention and Experts for Scalable MoE Inference","excerpt":"이 [arXiv]에 게시한 'Janus: Disaggregating Attention and Experts for Scalable MoE Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","MoE Inference","Disaggregated Architecture","Resource Management","Scalability","Load Balancing","GPU Utilization","Communication Optimization"],"permalink":"/ai/review/2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning","title":"[논문리뷰] A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning","excerpt":"Hongfei Zhang이 [arXiv]에 게시한 'A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","lastModifiedAt":"2025-12-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Affordance Prediction","Zero-Shot Learning","Agentic AI","Foundation Models","Multimodal Reasoning","Visual Grounding","Image Generation","Robotics"],"permalink":"/ai/review/2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions","title":"[논문리뷰] V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions","excerpt":"Kwesi Cobbina이 [arXiv]에 게시한 'V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Reasoning","Multi-step Exploration","Chain-of-Questions (CoQ)","Vision-Language Models (VLMs)","Benchmarking","Planning","Following"],"permalink":"/ai/review/2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation","title":"[논문리뷰] Towards Scalable Pre-training of Visual Tokenizers for Generation","excerpt":"이 [arXiv]에 게시한 'Towards Scalable Pre-training of Visual Tokenizers for Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Tokenizers","Pre-training","Latent Diffusion Models","Generative Models","Vision Transformer","Contrastive Learning","Self-Supervised Learning","Scaling Laws"],"permalink":"/ai/review/2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans","title":"[논문리뷰] Towards Interactive Intelligence for Digital Humans","excerpt":"Yifei Huang이 [arXiv]에 게시한 'Towards Interactive Intelligence for Digital Humans' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Digital Human","Interactive Intelligence","Multimodal Interaction","LLM Agent","Real-time Animation","Persona Fidelity","Diffusion Models"],"permalink":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection","title":"[논문리뷰] Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection","excerpt":"이 [arXiv]에 게시한 'Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Active Perception","Vision-Language Models (VLMs)","Embodied AI","View Selection","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Visual Question Answering (VQA)","3D Environments"],"permalink":"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge","title":"[논문리뷰] Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge","excerpt":"Jinwei Gu이 [arXiv]에 게시한 'Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Embodied AI","Long-horizon Tasks","Vision-Language-Action Models (VLA)","BEHAVIOR Challenge","Offline RL","Pre-training","Rejection Sampling Fine-Tuning (RFT)","Robotics"],"permalink":"/ai/review/2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents","title":"[논문리뷰] NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents","excerpt":"chongyang09이 [arXiv]에 게시한 'NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Coding Agents","LLMs","Software Engineering","Repository Generation","Long-Horizon Reasoning","Benchmark","Python Development","Autonomous Systems"],"permalink":"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-Memory-in-the-Age-of-AI-Agents","title":"[논문리뷰] Memory in the Age of AI Agents","excerpt":"Yanwei Yue이 [arXiv]에 게시한 'Memory in the Age of AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Agents","Memory Systems","LLMs","Taxonomy","Continual Learning","Self-Evolution","Multimodality","Reinforcement Learning"],"permalink":"/ai/review/2025-12-16-Memory-in-the-Age-of-AI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-KlingAvatar-2-0-Technical-Report","title":"[논문리뷰] KlingAvatar 2.0 Technical Report","excerpt":"이 [arXiv]에 게시한 'KlingAvatar 2.0 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Avatar Generation","Video Diffusion","Multi-modal LLM","Long-duration Video","High-resolution Video","Lip Synchronization","Multi-character Control","Spatio-temporal Cascade"],"permalink":"/ai/review/2025-12-16-KlingAvatar-2-0-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver","title":"[논문리뷰] Image Diffusion Preview with Consistency Solver","excerpt":"이 [arXiv]에 게시한 'Image Diffusion Preview with Consistency Solver' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","lastModifiedAt":"2025-12-16 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Efficient Sampling","Reinforcement Learning","ODE Solvers","Image Generation","Consistency","Diffusion Preview"],"permalink":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","title":"[논문리뷰] V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties","excerpt":"이 [arXiv]에 게시한 'V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Editing","Intrinsic Decomposition","Video Generation","Diffusion Models","Keyframe Editing","Inverse Rendering","Temporal Consistency","Physically Based Rendering"],"permalink":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge","title":"[논문리뷰] Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge","excerpt":"Akash Karnatak이 [arXiv]에 게시한 'Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA) models","Flow Matching","Embodied AI","Robot Manipulation","BEHAVIOR Challenge","Correlated Noise","Stage Tracking","Multi-Task Learning"],"permalink":"/ai/review/2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","title":"[논문리뷰] Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation","excerpt":"Qifeng Chen이 [arXiv]에 게시한 'Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Motion Tracking","Diffusion Models","Structure Preservation","SAM2","Feature Distillation","Local Gram Flow"],"permalink":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-Sliding-Window-Attention-Adaptation","title":"[논문리뷰] Sliding Window Attention Adaptation","excerpt":"이 [arXiv]에 게시한 'Sliding Window Attention Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Sliding Window Attention","Model Adaptation","Long Context","Inference Optimization","Fine-tuning","Chain-of-Thought","Sparse Attention"],"permalink":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second","title":"[논문리뷰] Sharp Monocular View Synthesis in Less Than a Second","excerpt":"이 [arXiv]에 게시한 'Sharp Monocular View Synthesis in Less Than a Second' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","View Synthesis","3D Gaussian Splatting","Single Image","Neural Rendering","Real-time","Feedforward Network","Monocular Depth Estimation","AR/VR"],"permalink":"/ai/review/2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models","title":"[논문리뷰] Scaling Behavior of Discrete Diffusion Language Models","excerpt":"이 [arXiv]에 게시한 'Scaling Behavior of Discrete Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Scaling Laws","Language Models","Masked Diffusion","Uniform Diffusion","Hyperparameter Tuning","Compute-Optimal Training"],"permalink":"/ai/review/2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder","title":"[논문리뷰] SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder","excerpt":"이 [arXiv]에 게시한 'SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Latent Diffusion Model","Visual Foundation Model","DINOv3","Flow Matching","High-Resolution Synthesis","VAE-free Generation"],"permalink":"/ai/review/2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming","title":"[논문리뷰] PersonaLive! Expressive Portrait Image Animation for Live Streaming","excerpt":"Jue Wang이 [arXiv]에 게시한 'PersonaLive! Expressive Portrait Image Animation for Live Streaming' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Live Streaming","Portrait Animation","Diffusion Models","Real-time AI","Appearance Distillation","Micro-chunk Streaming","Motion Control","Low Latency"],"permalink":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes","title":"[논문리뷰] MeshSplatting: Differentiable Rendering with Opaque Meshes","excerpt":"Matheus Gadelha이 [arXiv]에 게시한 'MeshSplatting: Differentiable Rendering with Opaque Meshes' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Differentiable Rendering","Novel View Synthesis","Mesh Reconstruction","3D Gaussian Splatting","Opaque Meshes","Real-time Rendering","Game Engines"],"permalink":"/ai/review/2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator","title":"[논문리뷰] LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator","excerpt":"이 [arXiv]에 게시한 'LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robotic Agent","Large Language Models (LLMs)","Embodied AI","Task Planning","Human-Robot Interaction","General-purpose Robotics","ROS"],"permalink":"/ai/review/2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","title":"[논문리뷰] Exploring MLLM-Diffusion Information Transfer with MetaCanvas","excerpt":"이 [arXiv]에 게시한 'Exploring MLLM-Diffusion Information Transfer with MetaCanvas' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Diffusion Models","Image Generation","Video Generation","Image Editing","Video Editing","Latent Space Planning","Canvas Tokens","Information Transfer"],"permalink":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video","title":"[논문리뷰] EgoX: Egocentric Video Generation from a Single Exocentric Video","excerpt":"이 [arXiv]에 게시한 'EgoX: Egocentric Video Generation from a Single Exocentric Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Egocentric Video Generation","Exocentric-to-Egocentric","Video Diffusion Models","3D Scene Reconstruction","Geometry-Guided Attention","View Synthesis","Camera Pose Estimation","LoRA Adaptation"],"permalink":"/ai/review/2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry","title":"[논문리뷰] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry","excerpt":"Yanchao Li이 [arXiv]에 게시한 'DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Model","Dental Imaging","Complex Reasoning","Domain Adaptation","Reinforcement Learning","Medical VQA","Dental Healthcare"],"permalink":"/ai/review/2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images","title":"[논문리뷰] CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images","excerpt":"Enzo Ferrante이 [arXiv]에 게시한 'CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","lastModifiedAt":"2025-12-15 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Uncertainty Quantification","Landmark Segmentation","Chest X-ray","VAE","Graph Neural Networks","Out-of-Distribution Detection","Medical Imaging"],"permalink":"/ai/review/2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction","title":"[논문리뷰] VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction","excerpt":"이 [arXiv]에 게시한 'VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Learning","Vector Quantization","Autoencoder","Unified Tokenizer","Image Generation","Image Reconstruction","Vision Transformers","Semantic Features"],"permalink":"/ai/review/2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task","title":"[논문리뷰] Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task","excerpt":"이 [arXiv]에 게시한 'Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","VideoQA","MLLMs","Tool Learning","Spatiotemporal Reasoning","Video Toolkit","Agentic AI"],"permalink":"/ai/review/2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Thinking-with-Images-via-Self-Calling-Agent","title":"[논문리뷰] Thinking with Images via Self-Calling Agent","excerpt":"Qixiang Ye이 [arXiv]에 게시한 'Thinking with Images via Self-Calling Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Self-Calling Chain-of-Thought","Reinforcement Learning","Visual Reasoning","Agentic AI","Tool Calling","Group Relative Policy Optimization"],"permalink":"/ai/review/2025-12-12-Thinking-with-Images-via-Self-Calling-Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality","title":"[논문리뷰] The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality","excerpt":"이 [arXiv]에 게시한 'The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Factuality Benchmark","Multimodal AI","Knowledge Grounding","Parametric Knowledge","Retrieval Augmented Generation","Automated Scoring"],"permalink":"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground","title":"[논문리뷰] T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground","excerpt":"이 [arXiv]에 게시한 'T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Russian LLM","Hybrid Reasoning","Speculative Decoding","Cyrillic Tokenizer","Instruction Tuning","Reward Modeling","T-Math Benchmark"],"permalink":"/ai/review/2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Stronger-Normalization-Free-Transformers","title":"[논문리뷰] Stronger Normalization-Free Transformers","excerpt":"Zhuang Liu이 [arXiv]에 게시한 'Stronger Normalization-Free Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Normalization-Free Transformers","Point-wise Functions","Error Function","Deep Learning","Transformer Architecture","Generalization","Normalization Layers"],"permalink":"/ai/review/2025-12-12-Stronger-Normalization-Free-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning","title":"[논문리뷰] ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning","excerpt":"Yujin Han이 [arXiv]에 게시한 'ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Editing","Reasoning","Unified Models","Self-Reflective Learning","Vision-Language Models (VLMs)","Diffusion Models","RVE-Bench"],"permalink":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification","title":"[논문리뷰] OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification","excerpt":"이 [arXiv]에 게시한 'OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Verification","Chain-of-Thought","Process-based Verifier","Outcome-based Verifier","Active Learning","Reinforcement Learning","Mathematical Reasoning","AI Alignment"],"permalink":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos","title":"[논문리뷰] MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos","excerpt":"Qi Wang이 [arXiv]에 게시한 'MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Motion Capture","Monocular Video","Arbitrary Skeletons","Motion Retargeting","Deep Learning","Inverse Kinematics","Transformer Architecture","Category-Agnostic"],"permalink":"/ai/review/2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents","title":"[논문리뷰] MOA: Multi-Objective Alignment for Role-Playing Agents","excerpt":"Yongbin Li이 [arXiv]에 게시한 'MOA: Multi-Objective Alignment for Role-Playing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Role-Playing Agents","Multi-Objective Reinforcement Learning","LLM Alignment","Persona Consistency","Dialogue Generation","Reward Shaping","Off-Policy Guidance"],"permalink":"/ai/review/2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving","title":"[논문리뷰] Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving","excerpt":"이 [arXiv]에 게시한 'Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Long-Horizon Reasoning","Multi-Agent System","Reinforcement Learning","Olympiad Problems","Lemma Memory","Context Length","OREAL-H"],"permalink":"/ai/review/2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos","title":"[논문리뷰] H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos","excerpt":"Mike Zheng Shou이 [arXiv]에 게시한 'H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video-to-Video Translation","Robot Learning","Human-Robot Transfer","Diffusion Models","Unpaired Data Learning","Pose-Guided Generation","Embodiment Gap Bridging"],"permalink":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models","title":"[논문리뷰] From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Microscopic Spatial Intelligence","Molecular Structures","Benchmarking","PDBbind Dataset","Spatial Reasoning","Drug Discovery"],"permalink":"/ai/review/2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents","title":"[논문리뷰] Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents","excerpt":"Xiaodong Gu이 [arXiv]에 게시한 'Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Federated Learning (FL)","LLM Agents","Self-Evolution","Privacy-Preserving","Multi-Environment","Parameter-Efficient Fine-Tuning","Low-Rank Aggregation","Reinforcement Learning"],"permalink":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator","title":"[논문리뷰] Evaluating Gemini Robotics Policies in a Veo World Simulator","excerpt":"이 [arXiv]에 게시한 'Evaluating Gemini Robotics Policies in a Veo World Simulator' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robotics","Policy Evaluation","World Model","Video Generation","Out-of-Distribution (OOD)","Safety","Gemini Robotics","Veo Simulator"],"permalink":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale","title":"[논문리뷰] Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale","excerpt":"이 [arXiv]에 게시한 'Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Agent","Software Engineering","Open-Source","LLM","Orchestrator","Context Management","Long-term Memory","Meta-agent"],"permalink":"/ai/review/2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation","title":"[논문리뷰] Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation","excerpt":"이 [arXiv]에 게시한 'Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Text-to-3D Generation","Autoregressive Models","Reward Modeling","Hierarchical RL","3D Benchmarking","ShapeLLM-Omni"],"permalink":"/ai/review/2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning","title":"[논문리뷰] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","lastModifiedAt":"2025-12-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Geometry Problem Solving","Reinforcement Learning","Curriculum Learning","Auxiliary Construction","Symbolic Reasoning","IMO"],"permalink":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation","title":"[논문리뷰] WonderZoom: Multi-Scale 3D World Generation","excerpt":"Jiajun Wu이 [arXiv]에 게시한 'WonderZoom: Multi-Scale 3D World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Scale 3D Generation","Gaussian Surfel","Progressive Synthesis","Neural Rendering","Scale-Adaptive","Content Creation","Zoom-in"],"permalink":"/ai/review/2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory","title":"[논문리뷰] VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory","excerpt":"이 [arXiv]에 게시한 'VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autoregressive Video Generation","Diffusion Models","Hybrid Memory","State-Space Models (SSM)","Long Video Synthesis","Temporal Consistency","Interactive AI"],"permalink":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","title":"[논문리뷰] UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving","excerpt":"이 [arXiv]에 게시한 'UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autonomous Driving","End-to-End Learning","Vision-Language Models","World Model","Chain-of-Thought","Video Generation","Trajectory Planning","Multimodal Learning"],"permalink":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression","title":"[논문리뷰] TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression","excerpt":"이 [arXiv]에 게시한 'TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","4D Gaussian Splatting","Dynamic Scene Compression","Rate-Distortion Optimization","Temporal Activation","Embedding-based Deformation","Neural Compression","3D Gaussian Splatting"],"permalink":"/ai/review/2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","title":"[논문리뷰] StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation","excerpt":"Guixun Luo이 [arXiv]에 게시한 'StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Monocular-to-Stereo","Video Generation","Diffusion Models","Geometry-Aware","XR","IPD-aligned Dataset","Novel View Synthesis"],"permalink":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication","title":"[논문리뷰] Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication","excerpt":"Hengshu Zhu이 [arXiv]에 게시한 'Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Clinical Dialogue","LLM Agents","Healthcare AI","Agentic Paradigm","Medical Decision Support","Knowledge Grounding","AI Safety","Workflow Automation"],"permalink":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models","title":"[논문리뷰] Pay Less Attention to Function Words for Free Robustness of Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Pay Less Attention to Function Words for Free Robustness of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Adversarial Robustness","Function Words","Cross-Attention","Adversarial Attacks","Differential Attention","Vision-Language Alignment"],"permalink":"/ai/review/2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer","title":"[논문리뷰] OmniPSD: Layered PSD Generation with Diffusion Transformer","excerpt":"Cheng Liu이 [arXiv]에 게시한 'OmniPSD: Layered PSD Generation with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Transformer","PSD Generation","Image Decomposition","RGBA-VAE","In-Context Learning","Text-to-PSD","Image-to-PSD"],"permalink":"/ai/review/2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models","title":"[논문리뷰] Learning Unmasking Policies for Diffusion Language Models","excerpt":"이 [arXiv]에 게시한 'Learning Unmasking Policies for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Reinforcement Learning","Masked Diffusion","Sampling Policy","Inference Optimization","Markov Decision Process","Generative AI","Text Generation"],"permalink":"/ai/review/2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models","title":"[논문리뷰] InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Linear Attention","Sliding Window Attention","Gated DeltaNet","Long-Context Understanding","Efficiency","Hybrid Architecture","Multimodal Learning"],"permalink":"/ai/review/2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting","title":"[논문리뷰] IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting","excerpt":"이 [arXiv]에 게시한 'IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Infrared Image Understanding","Benchmark Dataset","Visual Question Answering (VQA)","Generative Visual Prompting (GenViP)","Domain Adaptation","Image-to-Image Translation"],"permalink":"/ai/review/2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models","title":"[논문리뷰] HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action","Motion Representation","Temporal Reasoning","Long-Horizon Manipulation","Hindsight","Foresight","Robotics"],"permalink":"/ai/review/2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules","title":"[논문리뷰] Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules","excerpt":"Yang Zhang이 [arXiv]에 게시한 'Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Decoding Efficiency","Early Exit","Confidence Schedules","Training-free","Model-agnostic","Progress-aware"],"permalink":"/ai/review/2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing","title":"[논문리뷰] EtCon: Edit-then-Consolidate for Reliable Knowledge Editing","excerpt":"Chenglin Li이 [arXiv]에 게시한 'EtCon: Edit-then-Consolidate for Reliable Knowledge Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Knowledge Editing","Large Language Models","Lifelong Learning","Reinforcement Learning","Trust Region Policy Optimization","Chain-of-Thought","Catastrophic Forgetting"],"permalink":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding","title":"[논문리뷰] Composing Concepts from Images and Videos via Concept-prompt Binding","excerpt":"이 [arXiv]에 게시한 'Composing Concepts from Images and Videos via Concept-prompt Binding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Concept Composition","Diffusion Models","Text-to-Video Generation","Concept Binding","Hierarchical Binder","Diversify-and-Absorb Mechanism","Temporal Disentanglement","One-shot Learning"],"permalink":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain","title":"[논문리뷰] BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain","excerpt":"tamarott이 [arXiv]에 게시한 'BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","fMRI","Brain Mapping","Visual Representation","Interpretability","Sparse Autoencoders","Vision-Language Models","Unsupervised Learning","Neuroscience"],"permalink":"/ai/review/2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS","title":"[논문리뷰] Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS","excerpt":"Morteza Abolghasemi이 [arXiv]에 게시한 'Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","lastModifiedAt":"2025-12-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","TTS","Phonemization","G2P","Low Latency","Real-time","Service-Oriented Architecture","Context-Aware","Persian Language"],"permalink":"/ai/review/2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","title":"[논문리뷰] Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance","excerpt":"이 [arXiv]에 게시한 'Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Motion Control","Latent Trajectory Guidance","Image-to-Video","Diffusion Models","Neural Networks","MoveBench"],"permalink":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform","title":"[논문리뷰] Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform","excerpt":"Muyao Niu이 [arXiv]에 게시한 'Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Neural Rendering","3D Gaussian Splatting","WebGPU","ONNX Inference","World Models","Real-time Rendering","Browser-based","Dynamic Scenes"],"permalink":"/ai/review/2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models","title":"[논문리뷰] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models","excerpt":"Weirui Ye이 [arXiv]에 게시한 'TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Diffusion Models","Generative Models","Tree Search","Sample Efficiency","Credit Assignment","GRPO","Visual Generative Models"],"permalink":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels","title":"[논문리뷰] TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels","excerpt":"Tianyu Huang이 [arXiv]에 게시한 'TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Monocular 3D Tracking","World-centric Coordinates","Dense Tracking","Camera Pose Estimation","Dynamic Object Tracking","Optimization","2D Track Upsampling"],"permalink":"/ai/review/2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models","title":"[논문리뷰] ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models","excerpt":"Xiuyu Li이 [arXiv]에 게시한 'ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","Parallel Reasoning","Inference Latency","Chain-of-Thought","Reinforcement Learning","Adaptive Threading","Mathematical Reasoning","Speedup"],"permalink":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs","title":"[논문리뷰] Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs","excerpt":"이 [arXiv]에 게시한 'Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Cross-Modal Consistency","Reasoning Inconsistency","OCR Performance","Modality Gap","Benchmarking","Render Equivalence"],"permalink":"/ai/review/2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting","title":"[논문리뷰] SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting","excerpt":"Sung-Ho Bae이 [arXiv]에 게시한 'SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting (3DGS)","Gaussian Compression","Model Efficiency","Novel View Synthesis","Dynamic Scenes","Parameter Compression","Restructuring Compression","Real-time Rendering"],"permalink":"/ai/review/2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality","title":"[논문리뷰] Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality","excerpt":"이 [arXiv]에 게시한 'Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Face Swapping","Video Editing","Diffusion Models","Reference-guided Generation","Temporal Consistency","Keyframe Conditioning","Cinematic Quality","Dataset Construction"],"permalink":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks","title":"[논문리뷰] Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks","excerpt":"이 [arXiv]에 게시한 'Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Neural Operators","Time-Dependent Flow","Complex Geometries","DeepONet","Signed Distance Field","Autoregressive Prediction","Computational Fluid Dynamics","FlowBench"],"permalink":"/ai/review/2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory","title":"[논문리뷰] OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory","excerpt":"이 [arXiv]에 게시한 'OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Shot Video Generation","Adaptive Memory","Long-Range Context","Frame Selection","Diffusion Models","Image-to-Video","Autoregressive Generation","Narrative Coherence"],"permalink":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Modular-Neural-Image-Signal-Processing","title":"[논문리뷰] Modular Neural Image Signal Processing","excerpt":"Michael S. Brown이 [arXiv]에 게시한 'Modular Neural Image Signal Processing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Neural ISP","Modular Architecture","Raw Image Processing","Photo-Editing","Camera Agnostic","Generalization","Deep Learning","Image Enhancement"],"permalink":"/ai/review/2025-12-10-Modular-Neural-Image-Signal-Processing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","title":"[논문리뷰] MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment","excerpt":"이 [arXiv]에 게시한 'MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Robotic Manipulation","Hierarchical Framework","Reinforcement Learning","Diffusion Models","World Models","Cognitive Science","Physical Alignment"],"permalink":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning","title":"[논문리뷰] LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning","excerpt":"이 [arXiv]에 게시한 'LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Early Exit","Confidence Control","Reasoning Models","Conformal Prediction","LLM Optimization","Dynamic Exits","Hidden States","Chain-of-Thought"],"permalink":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation","title":"[논문리뷰] Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation","excerpt":"이 [arXiv]에 게시한 'Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Navigation","Dual-System Architecture","Foundation Models","Diffusion Policies","Robotics","Real-time Control","Generalization","Autonomous Navigation"],"permalink":"/ai/review/2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs","title":"[논문리뷰] From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs","excerpt":"이 [arXiv]에 게시한 'From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","LLM Adaptation","Block-Diffusion","Autoregressive Models","Attention Masks","Parallel Generation","Transfer Learning","Generative Models"],"permalink":"/ai/review/2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time","title":"[논문리뷰] Efficiently Reconstructing Dynamic Scenes One D4RT at a Time","excerpt":"이 [arXiv]에 게시한 'Efficiently Reconstructing Dynamic Scenes One D4RT at a Time' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dynamic Scene Reconstruction","4D Reconstruction","Point Tracking","Transformer Architecture","Feedforward Model","Query-based Inference","Computer Vision","Geometric Consistency"],"permalink":"/ai/review/2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce","title":"[논문리뷰] EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce","excerpt":"이 [arXiv]에 게시한 'EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","E-commerce","Foundation Agents","LLM Agents","Benchmark","Agent Evaluation","Tool Use","Multi-step Reasoning","Real-world Scenarios"],"permalink":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-DeepCode-Open-Agentic-Coding","title":"[논문리뷰] DeepCode: Open Agentic Coding","excerpt":"Chao Huang이 [arXiv]에 게시한 'DeepCode: Open Agentic Coding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Coding","LLM","Code Generation","Repository Synthesis","Information Flow Management","Code Memory","CodeRAG","Automated Verification","Scientific Reproduction"],"permalink":"/ai/review/2025-12-10-DeepCode-Open-Agentic-Coding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training","title":"[논문리뷰] Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training","excerpt":"Dim P. Papadopoulos이 [arXiv]에 게시한 'Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","lastModifiedAt":"2025-12-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unsupervised Video Instance Segmentation","Self-Training","Quality Assessment","Pseudo-labeling","Domain Adaptation","VideoMask2Former","YouTubeVIS"],"permalink":"/ai/review/2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering","title":"[논문리뷰] Voxify3D: Pixel Art Meets Volumetric Rendering","excerpt":"Yu-Lun Liu이 [arXiv]에 게시한 'Voxify3D: Pixel Art Meets Volumetric Rendering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Voxel Art","Volumetric Rendering","3D Stylization","Neural Radiance Fields","Discrete Optimization","Gumbel-Softmax","CLIP Loss"],"permalink":"/ai/review/2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators","title":"[논문리뷰] VideoVLA: Video Generators Can Be Generalizable Robot Manipulators","excerpt":"Yaobo Liang이 [arXiv]에 게시한 'VideoVLA: Video Generators Can Be Generalizable Robot Manipulators' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robot Manipulation","Video Generation Models","Vision-Language-Action (VLA)","Diffusion Transformer","Generalization","Action Prediction","Visual Imagination"],"permalink":"/ai/review/2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning","title":"[논문리뷰] VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning","excerpt":"Yansong Tang이 [arXiv]에 게시한 'VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Tool-integrated Visual Reasoning","Referring Grounded Reasoning","Agentic Reinforcement Learning","Self-Correction","Large Vision-Language Models","Chain-of-Thought","Tool Refinement"],"permalink":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","title":"[논문리뷰] UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation","excerpt":"이 [arXiv]에 게시한 'UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Multi-modal Learning","Multi-task Learning","Zero-shot Generalization","Diffusion Models","World Models","Video Understanding"],"permalink":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","title":"[논문리뷰] Unified Video Editing with Temporal Reasoner","excerpt":"이 [arXiv]에 게시한 'Unified Video Editing with Temporal Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Editing","Diffusion Models","Temporal Reasoning","Chain-of-Thought","In-Context Learning","ROPE","Multi-instance Editing"],"permalink":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation","title":"[논문리뷰] Scaling Zero-Shot Reference-to-Video Generation","excerpt":"이 [arXiv]에 게시한 'Scaling Zero-Shot Reference-to-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reference-to-Video Generation","Zero-Shot Learning","Diffusion Models","Masked Training","Video-Text Pairs","Identity Preservation","Scalability","Attention Mechanism"],"permalink":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation","title":"[논문리뷰] Rethinking Training Dynamics in Scale-wise Autoregressive Generation","excerpt":"이 [arXiv]에 게시한 'Rethinking Training Dynamics in Scale-wise Autoregressive Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autoregressive Generation","Visual Synthesis","Exposure Bias","Student Forcing","Self-Autoregressive Refinement","Scale-wise Prediction","Image Generation"],"permalink":"/ai/review/2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Relational-Visual-Similarity","title":"[논문리뷰] Relational Visual Similarity","excerpt":"Jing Shi이 [arXiv]에 게시한 'Relational Visual Similarity' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Relational Similarity","Visual Similarity","Vision-Language Models","Anonymous Captioning","Image Retrieval","Analogical Reasoning","Dataset Curation"],"permalink":"/ai/review/2025-12-09-Relational-Visual-Similarity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","title":"[논문리뷰] ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation","excerpt":"Taojun Ding이 [arXiv]에 게시한 'ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Camera Control","Novel Trajectory","3D Gaussian Splatting (3DGS)","LiDAR-Free","Diffusion Models","Autonomous Driving","Scene Synthesis"],"permalink":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","title":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models","excerpt":"이 [arXiv]에 게시한 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Pre-training","Mid-training","Reasoning LMs","Generalization","Synthetic Reasoning Tasks","Process-level Supervision"],"permalink":"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation","title":"[논문리뷰] OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation","excerpt":"Simeng Qin이 [arXiv]에 게시한 'OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Jailbreak Attack","Attack-Defense Evaluation","Benchmark","Safety Alignment","Vulnerability Analysis","Risk Taxonomy","Evaluation Metrics"],"permalink":"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning","title":"[논문리뷰] Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Parallel Reasoning","Self-Distilled Reinforcement Learning","Policy Optimization","Inference Acceleration","Structured Output","Agentic Reasoning"],"permalink":"/ai/review/2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader","title":"[논문리뷰] Multi-view Pyramid Transformer: Look Coarser to See Broader","excerpt":"Jungwoo Kim이 [arXiv]에 게시한 'Multi-view Pyramid Transformer: Look Coarser to See Broader' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-view Transformer","3D Reconstruction","Hierarchical Attention","Computational Efficiency","3D Gaussian Splatting","Novel View Synthesis","Scalability"],"permalink":"/ai/review/2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-LongCat-Image-Technical-Report","title":"[논문리뷰] LongCat-Image Technical Report","excerpt":"이 [arXiv]에 게시한 'LongCat-Image Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Generation","Text-to-Image","Image Editing","Diffusion Model","Multilingual Text Rendering","Photorealism","Efficiency","Open-Source"],"permalink":"/ai/review/2025-12-09-LongCat-Image-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Group-Representational-Position-Encoding","title":"[논문리뷰] Group Representational Position Encoding","excerpt":"이 [arXiv]에 게시한 'Group Representational Position Encoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Positional Encoding","Group Theory","Transformer","RoPE","ALiBi","Lie Groups","Multiplicative PE","Additive PE"],"permalink":"/ai/review/2025-12-09-Group-Representational-Position-Encoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","title":"[논문리뷰] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing","excerpt":"이 [arXiv]에 게시한 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Egocentric Video Editing","Real-Time Streaming","Augmented Reality","Video Generation","Dataset","Benchmark","Diffusion Models","Distillation"],"permalink":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems","title":"[논문리뷰] DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems","excerpt":"이 [arXiv]에 게시한 'DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Multi-Agent Systems","Debugging","Intervention-Driven","Failure Attribution","Automated Debugging","Verification","AI Agents","Reliability"],"permalink":"/ai/review/2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Distribution-Matching-Variational-AutoEncoder","title":"[논문리뷰] Distribution Matching Variational AutoEncoder","excerpt":"이 [arXiv]에 게시한 'Distribution Matching Variational AutoEncoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Variational Autoencoder (VAE)","Distribution Matching","Diffusion Models","Latent Space","Self-supervised Learning (SSL) Features","Generative Models","ImageNet","Tokenizer"],"permalink":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning","title":"[논문리뷰] Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning","excerpt":"이 [arXiv]에 게시한 'Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Reinforcement Learning","Self-Evolving Learning","Data-Scarce Domains","Context-First Learning","Reward Hacking Mitigation","Multimodal Reasoning","Curriculum Learning"],"permalink":"/ai/review/2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue","title":"[논문리뷰] DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue","excerpt":"YijunLiao이 [arXiv]에 게시한 'DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long-Context Dialogue","Mutable State Tracking","Temporal Alignment","Preference Optimization","Attention Mechanism","State Inertia","Non-Destructive Alignment"],"permalink":"/ai/review/2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning","title":"[논문리뷰] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning","excerpt":"Jiacheng Chen이 [arXiv]에 게시한 'Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Decoding-based Regression","Reinforcement Learning","Numerical Prediction","Large Language Models","Policy Gradient","Tokenization","Sequence Generation"],"permalink":"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs","title":"[논문리뷰] Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs","excerpt":"이 [arXiv]에 게시한 'Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","lastModifiedAt":"2025-12-09 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Rotary Position Embedding","Long-Context LLMs","Complex-Valued Neural Networks","Self-Attention","Positional Encoding","Information Loss","Length Extrapolation"],"permalink":"/ai/review/2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty","title":"[논문리뷰] World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty","excerpt":"Anirudha Majumdar이 [arXiv]에 게시한 'World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Controllable Video Generation","Uncertainty Quantification","Video Models","Calibration","Out-of-Distribution Detection","Proper Scoring Rules","Latent Space"],"permalink":"/ai/review/2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows","title":"[논문리뷰] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows","excerpt":"이 [arXiv]에 게시한 'TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Models","One-step Generation","Self-Adversarial Learning","Flow Matching","Large Language Models","Text-to-Image","Efficient Inference","Diffusion Models"],"permalink":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation","title":"[논문리뷰] TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation","excerpt":"Salih Tileylioglu이 [arXiv]에 게시한 'TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Strong Motion Generation","Deep Learning","TimesNet","Conditional Generation","Site Effects","Seismology","HVSR","Time Series"],"permalink":"/ai/review/2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling","title":"[논문리뷰] SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling","excerpt":"Marc Pollefeys이 [arXiv]에 게시한 'SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Generative Models","Spatial Control","Test-Time Guidance","Rectified Flow","Superquadrics","Training-Free","Trellis"],"permalink":"/ai/review/2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations","title":"[논문리뷰] Self-Improving VLM Judges Without Human Annotations","excerpt":"이 [arXiv]에 게시한 'Self-Improving VLM Judges Without Human Annotations' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Self-Improvement","Judge Models","Synthetic Data Generation","Iterative Refinement","Reward Modeling","Human-free Alignment"],"permalink":"/ai/review/2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs","title":"[논문리뷰] SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs","excerpt":"Minghui Yu이 [arXiv]에 게시한 'SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Quantization","Sparsification","Hardware Acceleration","Mixed-Precision","Post-Training Quantization","Data Format","GPU Optimization","AI Accelerator"],"permalink":"/ai/review/2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations","title":"[논문리뷰] SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations","excerpt":"이 [arXiv]에 게시한 'SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Character Animation","3D Pose Representation","In-Context Learning","Diffusion Transformer","Studio-Grade Animation","Spatio-Temporal Reasoning","Video Generation"],"permalink":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards","title":"[논문리뷰] RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards","excerpt":"Zilong Huang이 [arXiv]에 게시한 'RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Photorealism","Reinforcement Learning","Diffusion Models","Adversarial Learning","Detector-Guided Rewards","LLM Prompt Optimization","Image Quality Assessment"],"permalink":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning","title":"[논문리뷰] ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning","excerpt":"Shengju Qian이 [arXiv]에 게시한 'ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Object Segmentation","Reinforcement Learning","Vision-Language Models","Reasoning Chain","Explainable AI","Multi-step Reasoning"],"permalink":"/ai/review/2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation","title":"[논문리뷰] ProPhy: Progressive Physical Alignment for Dynamic World Simulation","excerpt":"Yuhao Cheng이 [arXiv]에 게시한 'ProPhy: Progressive Physical Alignment for Dynamic World Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Physics-aware","World Simulation","Progressive Alignment","Mixture-of-Experts","Vision-Language Models","Token-level Routing"],"permalink":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image","title":"[논문리뷰] Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image","excerpt":"이 [arXiv]에 게시한 'Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","4D Synthesis","3D Reconstruction","Motion Generation","Single Image","Diffusion Model","Point Cloud","Dataset Curation","View Synthesis"],"permalink":"/ai/review/2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks","title":"[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks","excerpt":"Yang Li이 [arXiv]에 게시한 'From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Curriculum Learning","Advantage Function","Reasoning Tasks","Multimodal AI","Policy Optimization","Generalization"],"permalink":"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning","title":"[논문리뷰] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning","excerpt":"Zijia Lin이 [arXiv]에 게시한 'Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Policy Optimization","Trust Region","Entropy Clipping","Large Language Models","Training Stability","Distributional Shift"],"permalink":"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor","title":"[논문리뷰] EditThinker: Unlocking Iterative Reasoning for Any Image Editor","excerpt":"Ziyu Guo이 [arXiv]에 게시한 'EditThinker: Unlocking Iterative Reasoning for Any Image Editor' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Iterative Reasoning","Multimodal Large Language Model (MLLM)","Reinforcement Learning (RL)","Instruction Following","Critique-Refine-Repeat Cycle","Think-while-Edit"],"permalink":"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence","title":"[논문리뷰] COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence","excerpt":"Jiawei Sheng이 [arXiv]에 게시한 'COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Spatial Reasoning","Perception Enhancement","Auxiliary Modalities","Adaptive Interleaved Reasoning","Reinforcement Learning","Chain-of-Thought"],"permalink":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence","title":"[논문리뷰] AI & Human Co-Improvement for Safer Co-Superintelligence","excerpt":"이 [arXiv]에 게시한 'AI & Human Co-Improvement for Safer Co-Superintelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","lastModifiedAt":"2025-12-08 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Safety","Superintelligence","Human-AI Collaboration","Self-Improving AI","Co-Improvement","Alignment","AI Research Agents"],"permalink":"/ai/review/2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers","title":"[논문리뷰] UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers","excerpt":"이 [arXiv]에 게시한 'UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Transformers","Resolution Extrapolation","Positional Encoding","Frequency Analysis","Adaptive Attention","High-Resolution Image Generation","Image Quality","Content Repetition"],"permalink":"/ai/review/2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation","title":"[논문리뷰] TV2TV: A Unified Framework for Interleaved Language and Video Generation","excerpt":"이 [arXiv]에 게시한 'TV2TV: A Unified Framework for Interleaved Language and Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Language Modeling","Multimodal AI","Interleaved Generation","Flow Matching","Transformer","Controllability","World Models"],"permalink":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting","title":"[논문리뷰] Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting","excerpt":"Yu-Lun Liu이 [arXiv]에 게시한 'Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Monocular 3D Reconstruction","Mannequin Challenge","Dynamic Gaussian Splatting","Freeze-Time Video","Temporal Consistency","Artifact Suppression","Regularization"],"permalink":"/ai/review/2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs","title":"[논문리뷰] SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs","excerpt":"이 [arXiv]에 게시한 'SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Post-Training Quantization (PTQ)","Large Language Models (LLMs)","Low-Bit Quantization","Mixed-Precision Quantization","Sensitivity Metric","Quantization Scale Initialization","Accuracy Preservation"],"permalink":"/ai/review/2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion","title":"[논문리뷰] Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion","excerpt":"이 [arXiv]에 게시한 'Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Latent Diffusion Models","Asynchronous Denoising","Semantic Modeling","Texture Modeling","Image Generation","Vision Transformer","VAE","Fast Convergence"],"permalink":"/ai/review/2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization","title":"[논문리뷰] SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization","excerpt":"이 [arXiv]에 게시한 'SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Navigation","Large Vision-Language Models","Visual Prompt","Reinforcement Fine-Tuning","Policy Optimization","Embodied AI","Spatial Reasoning","Perception Errors"],"permalink":"/ai/review/2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds","title":"[논문리뷰] SIMA 2: A Generalist Embodied Agent for Virtual Worlds","excerpt":"이 [arXiv]에 게시한 'SIMA 2: A Generalist Embodied Agent for Virtual Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Embodied AI","Generalist Agent","Virtual Worlds","Foundation Models","Gemini","Self-Improvement","Dialogue","Reasoning","Reinforcement Learning"],"permalink":"/ai/review/2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation","title":"[논문리뷰] Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation","excerpt":"Hao Ouyang이 [arXiv]에 게시한 'Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Streaming Video Generation","Video Diffusion Models","Distribution Matching Distillation","Reinforcement Learning","Autoregressive Models","Attention Sink","Real-time"],"permalink":"/ai/review/2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance","title":"[논문리뷰] REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance","excerpt":"Yaxin Fan이 [arXiv]에 게시한 'REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Fact-Checking","Explainable AI (XAI)","Large Language Models (LLMs)","Self-Refinement","Latent Space","Disentanglement","Steering Vectors","Misinformation"],"permalink":"/ai/review/2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory","title":"[논문리뷰] QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory","excerpt":"Nan-Yow Chen이 [arXiv]에 게시한 'QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Quantum Machine Learning","Kolmogorov-Arnold Networks","Long Short-Term Memory (LSTM)","Time Series Forecasting","Hybrid Quantum-Classical Learning","Quantum-inspired","Recurrent Neural Networks"],"permalink":"/ai/review/2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing","title":"[논문리뷰] PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing","excerpt":"이 [arXiv]에 게시한 'PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Academic Writing","In-editor Assistant","Multi-agent System","Overleaf Integration","Chrome Extension","Kubernetes","XtraMCP"],"permalink":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral","title":"[논문리뷰] On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral","excerpt":"Christos Thrampoulidis이 [arXiv]에 게시한 'On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Tool-Integrated Reasoning (TIR)","GRPO","Training Stability","Lazy Likelihood Displacement (LLD)","Regularization","Search-R1"],"permalink":"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction","title":"[논문리뷰] Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction","excerpt":"이 [arXiv]에 게시한 'Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Models","Large Language Models (LLMs)","Agentic Scaling","Environment Construction","NexAU","NexA4A","NexGAP","Interactive Environments"],"permalink":"/ai/review/2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation","title":"[논문리뷰] NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation","excerpt":"Vitor Guizilini이 [arXiv]에 게시한 'NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Phase Preservation","Frequency Domain","Structure-Aligned Generation","Image-to-Image Translation","Sim-to-Real","Generative AI"],"permalink":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing","title":"[논문리뷰] Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing","excerpt":"Jun Wang이 [arXiv]에 게시한 'Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Sphere Packing","Mathematical Discovery","Semidefinite Programming (SDP)","Bayesian Optimization (BO)","Monte Carlo Tree Search (MCTS)","Sample-Efficient AI","Model-Based Learning","Geometric Constraints"],"permalink":"/ai/review/2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment","title":"[논문리뷰] Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment","excerpt":"이 [arXiv]에 게시한 'Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Video Understanding","Hallucination Mitigation","Object Hallucination","Action Hallucination","Contrastive Learning","Self-Augmentation","Tracklet-Phrase Alignment"],"permalink":"/ai/review/2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates","title":"[논문리뷰] Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates","excerpt":"Nikolaos Aletras이 [arXiv]에 게시한 'Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Catastrophic Forgetting","Language Adaptation","Continual Pre-training","Parameter Freezing","Low-Resource Languages","Source Knowledge Preservation"],"permalink":"/ai/review/2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length","title":"[논문리뷰] Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length","excerpt":"Shifeng Zhang이 [arXiv]에 게시한 'Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio-Driven Avatar Generation","Real-time Streaming","Diffusion Models","Infinite Length","Pipeline Parallelism","Temporal Consistency","Model Distillation"],"permalink":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale","title":"[논문리뷰] LATTICE: Democratize High-Fidelity 3D Generation at Scale","excerpt":"Qingxiang Lin이 [arXiv]에 게시한 'LATTICE: Democratize High-Fidelity 3D Generation at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Generation","High-Fidelity","Latent Representation","Voxel Grid","Diffusion Models","Transformer","Scalable AI","Asset Creation"],"permalink":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior","title":"[논문리뷰] Generative Neural Video Compression via Video Diffusion Prior","excerpt":"이 [arXiv]에 게시한 'Generative Neural Video Compression via Video Diffusion Prior' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Neural Video Compression","Diffusion Models","Generative Models","Video Compression","Temporal Coherence","Perceptual Quality","Flow Matching","Video Diffusion Transformer (VideoDiT)"],"permalink":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces","title":"[논문리뷰] GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces","excerpt":"Sezer Karaoglu이 [arXiv]에 게시한 'GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting","Text-to-3D Stylization","Latent Diffusion Models","Disentangled Latent Spaces","Feed-forward Editing","Geometry Preservation","Multi-view Consistency"],"permalink":"/ai/review/2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring","title":"[논문리뷰] FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring","excerpt":"Munchurl Kim이 [arXiv]에 게시한 'FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Super-Resolution","Video Deblurring","Joint Restoration","Exposure-Aware","Motion Compensation","Transformer Architecture","Dynamic Filtering","Real-World Degradations"],"permalink":"/ai/review/2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion","title":"[논문리뷰] EgoLCD: Egocentric Video Generation with Long Context Diffusion","excerpt":"이 [arXiv]에 게시한 'EgoLCD: Egocentric Video Generation with Long Context Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Egocentric Video Generation","Long-Context Diffusion","Long-Short Memory","Sparse KV Cache","Memory Regulation Loss","Structured Narrative Prompting","World Models","Embodied AI"],"permalink":"/ai/review/2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling","title":"[논문리뷰] DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling","excerpt":"이 [arXiv]에 게시한 'DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","4D World Modeling","Multimodal Data","Dynamic Scenes","Metric-Scale","Bundle Adjustment","Foundation Models","Video Analysis","Data Curation"],"permalink":"/ai/review/2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation","title":"[논문리뷰] DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation","excerpt":"Ziyu Guo이 [arXiv]에 게시한 'DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Chain-of-Thought (CoT)","Multimodal Large Language Models (MLLMs)","Visual Planning","Rare Concept Generation","Drafting","Classifier-Free Guidance (CFG)","Image Refinement"],"permalink":"/ai/review/2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle","title":"[논문리뷰] DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle","excerpt":"이 [arXiv]에 게시한 'DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Data Agents","Benchmarking","Data Engineering","Data Analysis","LLM-as-Judge","Full Data Intelligence Lifecycle","Repository-Level","Open-Ended Tasks"],"permalink":"/ai/review/2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","title":"[논문리뷰] BulletTime: Decoupled Control of Time and Camera Pose for Video Generation","excerpt":"Jan Ackermann이 [arXiv]에 게시한 'BulletTime: Decoupled Control of Time and Camera Pose for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","4D Control","Camera Pose Control","Time Control","Positional Encoding","Adaptive Normalization","Synthetic Dataset"],"permalink":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models","title":"[논문리뷰] Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models","excerpt":"이 [arXiv]에 게시한 'Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image","LVLM","Social Bias","System Prompts","Bias Mitigation","Meta-Prompting","Fairness","Generative AI"],"permalink":"/ai/review/2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning","title":"[논문리뷰] ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning","excerpt":"이 [arXiv]에 게시한 'ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reward Models","Agentic AI","Tool Use","Reinforcement Learning","Visual Reasoning","Multimodal LLMs","Instruction Following","Evaluation Benchmarks"],"permalink":"/ai/review/2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer","title":"[논문리뷰] 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer","excerpt":"이 [arXiv]에 게시한 '4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","lastModifiedAt":"2025-12-05 00:00:00+0900+0900","categories":["Review"],"tags":["Review","4D Scene Understanding","Language Grounding","Transformer","Feed-forward Network","Semantic Field","Geometry Reconstruction","Embodied AI"],"permalink":"/ai/review/2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-ViDiC-Video-Difference-Captioning","title":"[논문리뷰] ViDiC: Video Difference Captioning","excerpt":"jiakaiW이 [arXiv]에 게시한 'ViDiC: Video Difference Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Difference Captioning","Multimodal Large Language Models","Video Understanding","Comparative Reasoning","Evaluation Benchmark","LLM-as-a-Judge","ViDiC-1K"],"permalink":"/ai/review/2025-12-04-ViDiC-Video-Difference-Captioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs","title":"[논문리뷰] UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs","excerpt":"이 [arXiv]에 게시한 'UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Compression","Quantization","Pruning","Edge AI","Adaptive Deployment","Transformer","State Space Models","Hybrid Models","One-shot Compression"],"permalink":"/ai/review/2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images","title":"[논문리뷰] Thinking with Programming Vision: Towards a Unified View for Thinking with Images","excerpt":"Tao Jin이 [arXiv]에 게시한 'Thinking with Programming Vision: Towards a Unified View for Thinking with Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Tool Learning","Code Generation","Reinforcement Learning","Image Manipulation","Robustness","Error Recovery","Programming Vision"],"permalink":"/ai/review/2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach","title":"[논문리뷰] Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach","excerpt":"Xiu Li이 [arXiv]에 게시한 'Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Anti-Exploration","Test-Time Scaling","Pseudo-Count","Coin Flipping Network","Offline Reinforcement Learning","Robotics"],"permalink":"/ai/review/2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL","title":"[논문리뷰] SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL","excerpt":"이 [arXiv]에 게시한 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Spatial Reasoning","Vision Language Models","Reinforcement Learning","Tool Augmentation","Robotics","Multi-Tool Use","Embodied AI"],"permalink":"/ai/review/2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors","title":"[논문리뷰] SkillFactory: Self-Distillation For Learning Cognitive Behaviors","excerpt":"Manya Wadhwa이 [arXiv]에 게시한 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-Distillation","Cognitive Skills","Reinforcement Learning","Supervised Fine-Tuning","Language Models","Reasoning","Verification","Retrying"],"permalink":"/ai/review/2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment","title":"[논문리뷰] SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment","excerpt":"Yi Yang이 [arXiv]에 게시한 'SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Alignment","Stable Rank","Intrinsic Reward","Reinforcement Learning","Geometric Properties","Group Relative Policy Optimization","Annotation-Free Alignment"],"permalink":"/ai/review/2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","title":"[논문리뷰] RELIC: Interactive Video World Model with Long-Horizon Memory","excerpt":"Chongjian Ge이 [arXiv]에 게시한 'RELIC: Interactive Video World Model with Long-Horizon Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Interactive World Model","Video Generation","Long-Horizon Memory","Real-Time Streaming","Diffusion Models","Autoregressive Models","Spatial Consistency","Unreal Engine"],"permalink":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-Qwen3-VL-Technical-Report","title":"[논문리뷰] Qwen3-VL Technical Report","excerpt":"이 [arXiv]에 게시한 'Qwen3-VL Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Multimodal Reasoning","Long-Context","Interleaved Data","Mixture-of-Experts","DeepStack","Agentic AI"],"permalink":"/ai/review/2025-12-04-Qwen3-VL-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-PretrainZero-Reinforcement-Active-Pretraining","title":"[논문리뷰] PretrainZero: Reinforcement Active Pretraining","excerpt":"Guoqi Li이 [arXiv]에 게시한 'PretrainZero: Reinforcement Active Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Active Learning","Pretraining","Large Language Models","Self-Supervised Learning","Masked Language Modeling","Generalization","Reasoning"],"permalink":"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video","title":"[논문리뷰] OneThinker: All-in-one Reasoning Model for Image and Video","excerpt":"Kaixuan Fan이 [arXiv]에 게시한 'OneThinker: All-in-one Reasoning Model for Image and Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Reinforcement Learning","Visual Reasoning","Generalist Model","Image Understanding","Video Understanding","Multitask Learning","EMA-GRPO"],"permalink":"/ai/review/2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model","title":"[논문리뷰] Jina-VLM: Small Multilingual Vision Language Model","excerpt":"이 [arXiv]에 게시한 'Jina-VLM: Small Multilingual Vision Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Multilingual VLM","Small VLM","Visual Question Answering","Attention Pooling","Image Tiling","SigLIP","Qwen"],"permalink":"/ai/review/2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-In-Context-Representation-Hijacking","title":"[논문리뷰] In-Context Representation Hijacking","excerpt":"yossig이 [arXiv]에 게시한 'In-Context Representation Hijacking' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Jailbreak","In-Context Learning","Representation Hijacking","Mechanistic Interpretability","LLM Safety","Adversarial Attack","Semantic Shift"],"permalink":"/ai/review/2025-12-04-In-Context-Representation-Hijacking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment","title":"[논문리뷰] Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment","excerpt":"이 [arXiv]에 게시한 'Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Normalizing Flows","Representation Alignment","Generative Models","TARFlow","Image Generation","Classification","Training Acceleration","Reverse Pass"],"permalink":"/ai/review/2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation","title":"[논문리뷰] CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation","excerpt":"Yi Yao이 [arXiv]에 게시한 'CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-step Image Generation","Recipe Illustration","Diffusion Models","Consistent Generation","Regional Control","Positional Encoding","Ingredient Consistency","Procedural Content Generation"],"permalink":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs","title":"[논문리뷰] AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs","excerpt":"Tosho Hirasawa이 [arXiv]에 게시한 'AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image-Text Alignment","Multimodal Benchmarking","Hallucination Detection","Vision-Language Models","Synthetic Data Generation","Fine-Grained Analysis","Captioning"],"permalink":"/ai/review/2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models","title":"[논문리뷰] Adversarial Confusion Attack: Disrupting Multimodal Large Language Models","excerpt":"Artur Janicki이 [arXiv]에 게시한 'Adversarial Confusion Attack: Disrupting Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","lastModifiedAt":"2025-12-04 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Adversarial Attack","Multimodal Large Language Models (MLLMs)","Entropy Maximization","Confusion Attack","Black-box Transfer","PGD","AI Agent Safety"],"permalink":"/ai/review/2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation","title":"[논문리뷰] YingVideo-MV: Music-Driven Multi-Stage Video Generation","excerpt":"Chaofan Ding이 [arXiv]에 게시한 'YingVideo-MV: Music-Driven Multi-Stage Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Music-Driven Video Generation","Diffusion Models","Multi-Stage Framework","Camera Control","Lip-Sync","Temporal Coherence","Video Diffusion Transformer"],"permalink":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning","title":"[논문리뷰] WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning","excerpt":"이 [arXiv]에 게시한 'WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long Video Reasoning","Multimodal Memory","Adaptive Retrieval","Video Large Language Models","Knowledge Graph","Multiscale Temporal Reasoning","Episodic Memory","Semantic Memory"],"permalink":"/ai/review/2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","title":"[논문리뷰] Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation","excerpt":"Yu Ning이 [arXiv]에 게시한 'Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Spatial Reasoning","Visuospatial Intelligence","Diffusion Models","Context-Guided Generation","Scene Navigation","Object Grounding","Out-of-Domain Generalization"],"permalink":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation","title":"[논문리뷰] ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation","excerpt":"이 [arXiv]에 게시한 'ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Binaural Audio Generation","Spatial Audio","Video-Driven","End-to-End","Conditional Flow Matching","Multimodal AI","Deep Learning","Audio-Visual Synthesis"],"permalink":"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models","title":"[논문리뷰] The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models","excerpt":"이 [arXiv]에 게시한 'The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Analogical Reasoning","Large Language Models","Mechanistic Interpretability","Proportional Analogies","Story Analogies","Structural Alignment","Attention Knockout","Patchscopes"],"permalink":"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition","title":"[논문리뷰] TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition","excerpt":"Zichen Wen이 [arXiv]에 게시한 'TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Table Recognition","Self-supervised Learning","Vision-Language Models","Reinforcement Learning","Question Answering","Data Augmentation","GRPO"],"permalink":"/ai/review/2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead","title":"[논문리뷰] SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead","excerpt":"이 [arXiv]에 게시한 'SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Lightweight Models","Spatiotemporal Dynamics","4D Features","Masked Autoencoding","Robotics","Edge AI"],"permalink":"/ai/review/2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch","title":"[논문리뷰] Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch","excerpt":"이 [arXiv]에 게시한 'Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Agentic Models","Interleaved Reasoning","Image Manipulation","DeepSearch","Supervised Fine-tuning (SFT)","Tool-Augmented LLM"],"permalink":"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds","title":"[논문리뷰] SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds","excerpt":"Xuhong He이 [arXiv]에 게시한 'SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autonomous Agents","Realistic Simulator","Unreal Engine 5","LLM/VLM Agents","Procedural Generation","Multi-Agent Systems","Physical Simulation","Social Interaction"],"permalink":"/ai/review/2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale","title":"[논문리뷰] SimScale: Learning to Drive via Real-World Simulation at Scale","excerpt":"이 [arXiv]에 게시한 'SimScale: Learning to Drive via Real-World Simulation at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Simulation","Neural Rendering","3D Gaussian Splatting","Sim-to-Real","Data Scaling","End-to-End Planning","Pseudo-Expert"],"permalink":"/ai/review/2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization","title":"[논문리뷰] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization","excerpt":"이 [arXiv]에 게시한 'Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Chain-of-Thought (CoT)","Vision-Language Models (VLMs)","Visual Reasoning","Generalization","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Grounding CoT","Maze Solving"],"permalink":"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI","title":"[논문리뷰] PAI-Bench: A Comprehensive Benchmark For Physical AI","excerpt":"Humphrey Shi이 [arXiv]에 게시한 'PAI-Bench: A Comprehensive Benchmark For Physical AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Physical AI","Benchmark","Video Generation","Conditional Video Generation","Video Understanding","Multimodal LLMs","Physical Plausibility","Embodied Reasoning"],"permalink":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework","title":"[논문리뷰] MultiShotMaster: A Controllable Multi-Shot Video Generation Framework","excerpt":"이 [arXiv]에 게시한 'MultiShotMaster: A Controllable Multi-Shot Video Generation Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Shot Video Generation","Controllable Video Generation","Diffusion Models","RoPE","Spatiotemporal Consistency","Reference Injection","Data Curation Framework"],"permalink":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Mixture-of-Horizons-in-Action-Chunking","title":"[논문리뷰] Mixture of Horizons in Action Chunking","excerpt":"Zelong Sun이 [arXiv]에 게시한 'Mixture of Horizons in Action Chunking' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Action Chunking","Robotic Manipulation","Multi-horizon Planning","Transformer Architecture","Gated Fusion","Dynamic Inference"],"permalink":"/ai/review/2025-12-03-Mixture-of-Horizons-in-Action-Chunking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models","title":"[논문리뷰] Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models","excerpt":"이 [arXiv]에 게시한 'Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Masked Diffusion Language Models","Context Comprehension","Locality Bias","Mask Tokens","Fine-tuning","Mask-agnostic Loss","Long-context Processing"],"permalink":"/ai/review/2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory","title":"[논문리뷰] MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory","excerpt":"이 [arXiv]에 게시한 'MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Navigation","Dual-Scale Framework","Sparse Spatial Memory Graph","Memory-Guided Planning","Geometry-Enhanced Control","Zero-Shot Navigation","Embodied AI"],"permalink":"/ai/review/2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision","title":"[논문리뷰] Guided Self-Evolving LLMs with Minimal Human Supervision","excerpt":"이 [arXiv]에 게시한 'Guided Self-Evolving LLMs with Minimal Human Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-Evolving LLMs","Self-Play","Reinforcement Learning","Curriculum Learning","Few-shot Learning","Human Supervision","Concept Drift","Diversity Collapse"],"permalink":"/ai/review/2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample","title":"[논문리뷰] Glance: Accelerating Diffusion Models with 1 Sample","excerpt":"Linjie Li이 [arXiv]에 게시한 'Glance: Accelerating Diffusion Models with 1 Sample' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Acceleration","Distillation","LoRA","Few-shot Learning","Phase-aware","Image Generation","Computational Efficiency"],"permalink":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning","title":"[논문리뷰] GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning","excerpt":"Kaijun Tan이 [arXiv]에 게시한 'GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","GUI Agents","Screen Navigation","Reinforcement Learning","Multi-Turn RL","Simulation","Supervised Fine-tuning","Generalization"],"permalink":"/ai/review/2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","title":"[논문리뷰] DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation","excerpt":"Zixin Zhang이 [arXiv]에 게시한 'DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Video Generation","Camera Control","Depth Estimation","Dual-Branch Architecture","Geometric Awareness","Semantic Alignment","Multi-modal Fusion"],"permalink":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","title":"[논문리뷰] Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation","excerpt":"이 [arXiv]에 게시한 'Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Audio-Video Multimodal","Joint Denoising","Diffusion Models","Transformer Architecture","World Models","Physical Commonsense","Multimodal Training"],"permalink":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models","title":"[논문리뷰] DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models","excerpt":"이 [arXiv]에 게시한 'DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","VLA Models","Flow Matching","Robotics","Robustness","Distribution Shift","Wasserstein Distance","Geometric Regularization","Representation Learning"],"permalink":"/ai/review/2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models","title":"[논문리뷰] DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models","excerpt":"이 [arXiv]에 게시한 'DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Sparse Attention","Reinforcement Learning","Agentic AI","Tool Use","Open-source LLM","DeepSeek"],"permalink":"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization","title":"[논문리뷰] CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization","excerpt":"이 [arXiv]에 게시한 'CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Agentic Reasoning","Tool Use","Reinforcement Learning","Faithfulness Evaluation","Policy Optimization","Visual Search","Code Generation"],"permalink":"/ai/review/2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click","title":"[논문리뷰] Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click","excerpt":"이 [arXiv]에 게시한 'Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Panoptic Video Scene Graph Generation","Interactive AI","User Guidance","Promptable Segmentation","Video Understanding","Relational Reasoning","Human-in-the-Loop"],"permalink":"/ai/review/2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning","title":"[논문리뷰] CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","CUDA","Matrix Multiplication","Reinforcement Learning","LLMs","Kernel Optimization","HGEMM","GPU Performance","cuBLAS"],"permalink":"/ai/review/2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models","title":"[논문리뷰] C^2DLM: Causal Concept-Guided Diffusion Large Language Models","excerpt":"Xinpeng Dong이 [arXiv]에 게시한 'C^2DLM: Causal Concept-Guided Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Large Language Models","Causality","Attention Mechanism","Reasoning","Natural Language Generation","Supervised Fine-Tuning","Concept-Guided"],"permalink":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation","title":"[논문리뷰] BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation","excerpt":"이 [arXiv]에 게시한 'BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Block Diffusion","Video Generation","Temporal Consistency","KV Cache","Semi-Autoregressive","Video Quality Metrics","Long Video Generation"],"permalink":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning","title":"[논문리뷰] Artemis: Structured Visual Reasoning for Perception Policy Learning","excerpt":"Piotr Koniusz이 [arXiv]에 게시한 'Artemis: Structured Visual Reasoning for Perception Policy Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","lastModifiedAt":"2025-12-03 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Reasoning","Multimodal Large Language Models (MLLM)","Reinforcement Learning (RL)","Perception Policy Learning","Object Grounding","Object Detection","Structured Output"],"permalink":"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing","title":"[논문리뷰] WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing","excerpt":"Wendong Bu이 [arXiv]에 게시한 'WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Benchmarking","Cognitive AI","Creativity","Multimodal AI","Knowledge-based Reasoning","Diffusion Models","MLLMs"],"permalink":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models","title":"[논문리뷰] Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models","excerpt":"Mikhail Burtsev이 [arXiv]에 게시한 'Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Knowledge Graphs","Large Language Models","Information Extraction","Wikidata Ontology","Question Answering","Entity Normalization","Retrieval Augmented Generation"],"permalink":"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation","title":"[논문리뷰] Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation","excerpt":"Wenhua Wu이 [arXiv]에 게시한 'Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Cultural Consistency","Multilingual AI","Neuron Activation","Cultural Probing","Fine-Tuning","Diffusion Models"],"permalink":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","title":"[논문리뷰] What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards","excerpt":"이 [arXiv]에 게시한 'What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Newtonian Dynamics","Physics-aware AI","Post-Training","Verifiable Rewards","Optical Flow","Mass Estimation"],"permalink":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference","title":"[논문리뷰] VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference","excerpt":"이 [arXiv]에 게시한 'VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Asynchronous Inference","Real-Time Robotics","Low-Latency Control","Future State Awareness","Action Quantization","Temporal Alignment"],"permalink":"/ai/review/2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment","title":"[논문리뷰] The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment","excerpt":"이 [arXiv]에 게시한 'The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Generation","Image Editing","Diffusion Models","Consistency Correction","Attention Mechanism","Reference-Guided","Agent Framework","Data Curation"],"permalink":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models","title":"[논문리뷰] The Art of Scaling Test-Time Compute for Large Language Models","excerpt":"Tanmoy Chakraborty이 [arXiv]에 게시한 'The Art of Scaling Test-Time Compute for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Test-Time Scaling","LLMs","Reasoning","Compute Efficiency","Inference Optimization","Decoding Strategies","Model Behavior"],"permalink":"/ai/review/2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models","title":"[논문리뷰] TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models","excerpt":"이 [arXiv]에 게시한 'TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Visual Representation","VAE","Flow Matching","Multimodal Understanding","Multimodal Generation","Image Editing","State-of-the-Art"],"permalink":"/ai/review/2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models","title":"[논문리뷰] Structured Extraction from Business Process Diagrams Using Vision-Language Models","excerpt":"Barry Devereux이 [arXiv]에 게시한 'Structured Extraction from Business Process Diagrams Using Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","BPMN Extraction","Structured Information Extraction","OCR Enrichment","Prompt Engineering","Diagram Understanding","Business Process Management"],"permalink":"/ai/review/2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos","title":"[논문리뷰] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos","excerpt":"이 [arXiv]에 게시한 'StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Streaming Video Understanding","Gaze-Guided AI","Temporal Reasoning","Proactive AI","MLLMs","Eye Tracking","Benchmark","Human-Computer Interaction"],"permalink":"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices","title":"[논문리뷰] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices","excerpt":"이 [arXiv]에 게시한 'Stabilizing Reinforcement Learning with LLMs: Formulation and Practices' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Policy Gradient","REINFORCE","Mixture-of-Experts (MoE)","Training Stability","Importance Sampling","Routing Replay","Off-policy Learning"],"permalink":"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs","title":"[논문리뷰] SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs","excerpt":"이 [arXiv]에 게시한 'SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLMs","Long-context Reasoning","KV Cache Optimization","Speculative Sparsity","Knowledge Distillation","Adaptive Memory Management","Throughput"],"permalink":"/ai/review/2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf","title":"[논문리뷰] Seeing the Wind from a Falling Leaf","excerpt":"Emily Yue-Ting Jia이 [arXiv]에 게시한 'Seeing the Wind from a Falling Leaf' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Inverse Graphics","Differentiable Physics","Force Estimation","Video Generation","Material Point Method","3D Gaussians","Spatio-temporal Modeling","Vision-Language Models"],"permalink":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models","title":"[논문리뷰] Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models","excerpt":"이 [arXiv]에 게시한 'Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Token Pruning","Graph-Structured Pruning (GSP)","Query-Conditioned Semantic Pruning (QCSP)","Determinantal Point Processes (DPP)","Model Efficiency","Visual Redundancy"],"permalink":"/ai/review/2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling","title":"[논문리뷰] SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling","excerpt":"이 [arXiv]에 게시한 'SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Test-time Scaling","Resource Allocation","Dual-process Theory","Mathematical Reasoning","Adaptive Computation","Performance Optimization"],"permalink":"/ai/review/2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization","title":"[논문리뷰] Rectifying LLM Thought from Lens of Optimization","excerpt":"Kai Chen이 [arXiv]에 게시한 'Rectifying LLM Thought from Lens of Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Chain-of-Thought","RLVR","Optimization Framework","Process-level Reward","Gradient Descent","Reasoning Efficiency","Suboptimal Reasoning"],"permalink":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models","title":"[논문리뷰] PromptBridge: Cross-Model Prompt Transfer for Large Language Models","excerpt":"Wei Wei이 [arXiv]에 게시한 'PromptBridge: Cross-Model Prompt Transfer for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Prompt Engineering","Model Drifting","Prompt Transfer","Cross-Model Adaptation","Training-Free","Prompt Optimization","MAP-RPE"],"permalink":"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic","title":"[논문리뷰] OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic","excerpt":"이 [arXiv]에 게시한 'OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Reinforcement Fine-tuning","LLM-as-Critic","Vision-Language Model","End-to-End Learning","Chain-of-Thought","Trajectory Planning"],"permalink":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion","title":"[논문리뷰] OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion","excerpt":"이 [arXiv]에 게시한 'OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Translation","Speech Translation","Simultaneous Translation","Large Language Models","Multimodal Foundation Models","Modular Fusion","End-to-End","Gated Fusion","OCR"],"permalink":"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model","title":"[논문리뷰] Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model","excerpt":"Ying-Cong Chen이 [arXiv]에 게시한 'Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Geometric Dense Prediction","Depth Estimation","Surface Normal Prediction","Diffusion Models","Rectified Flow","Generative Priors","Deterministic Inference","Two-Stage Framework"],"permalink":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling","title":"[논문리뷰] LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling","excerpt":"이 [arXiv]에 게시한 'LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long Video Understanding","Multimodal LLMs","Tool Calling","Reinforcement Learning","Chain-of-Thought","Temporal Grounding","Video Question Answering"],"permalink":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds","title":"[논문리뷰] Learning Eigenstructures of Unstructured Data Manifolds","excerpt":"이 [arXiv]에 게시한 'Learning Eigenstructures of Unstructured Data Manifolds' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Spectral Basis Learning","Unstructured Data","Manifold Learning","Laplacian Operator","Optimal Approximation Theory","Neural Networks","Eigenstructure","Point Cloud Processing"],"permalink":"/ai/review/2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-LFM2-Technical-Report","title":"[논문리뷰] LFM2 Technical Report","excerpt":"이 [arXiv]에 게시한 'LFM2 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Edge AI","Foundation Models","Hybrid Architecture","Knowledge Distillation","Multimodal AI","On-device Deployment","Efficient Inference","LLM Optimization"],"permalink":"/ai/review/2025-12-02-LFM2-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision","title":"[논문리뷰] InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision","excerpt":"이 [arXiv]에 게시한 'InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Foundation Models","Self-Supervised Learning","Masked Video Modeling","Video-Text Supervision-Free","Encoder-Predictor-Decoder","Diffusion Decoder","Semantic Alignment","Latent World Model"],"permalink":"/ai/review/2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout","title":"[논문리뷰] Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout","excerpt":"Pinar Yanardag이 [arXiv]에 게시한 'Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Autoregressive Video Generation","Rotary Positional Embedding","Infinite Video Generation","Action Control","Cinematic Transitions","Video Diffusion Models","KV Cache"],"permalink":"/ai/review/2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages","title":"[논문리뷰] IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages","excerpt":"이 [arXiv]에 게시한 'IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Low-resource Languages","Indic Languages","LLM Evaluation","Benchmark","Multilingual LLMs","Question Answering","Cross-lingual Transfer"],"permalink":"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents","title":"[논문리뷰] How Far Are We from Genuinely Useful Deep Research Agents?","excerpt":"Xinran Zhou이 [arXiv]에 게시한 'How Far Are We from Genuinely Useful Deep Research Agents?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Deep Research Agents","Evaluation Benchmark","Failure Taxonomy","Report Generation","Information Retrieval","Reasoning Resilience","Content Fabrication","AI Agents"],"permalink":"/ai/review/2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents","title":"[논문리뷰] HiconAgent: History Context-aware Policy Optimization for GUI Agents","excerpt":"Kaiwen Zhou이 [arXiv]에 게시한 'HiconAgent: History Context-aware Policy Optimization for GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","GUI Agents","Reinforcement Learning","Context-aware","History Compression","Policy Optimization","Multimodal LLM","Dynamic Sampling"],"permalink":"/ai/review/2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks","title":"[논문리뷰] Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks","excerpt":"이 [arXiv]에 게시한 'Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Clinical AI","Medical Benchmarks","AI Evaluation","Medical Decision Support","MedQA","HealthBench","Generalist AI"],"permalink":"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation","title":"[논문리뷰] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation","excerpt":"이 [arXiv]에 게시한 'GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robotic Manipulation","Reinforcement Learning","Vision-Language-Action","Dexterous Control","Long-Horizon Tasks","Data Filtering","Data Augmentation","Foundation Models"],"permalink":"/ai/review/2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence","title":"[논문리뷰] From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence","excerpt":"이 [arXiv]에 게시한 'From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Code LLMs","Software Engineering Agents","Code Generation","Reinforcement Learning","Supervised Fine-tuning","Multimodal AI","Code Safety","Scaling Laws"],"permalink":"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning","title":"[논문리뷰] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Image Generation","Distillation","Reinforcement Learning","Few-Step Sampling","Timestep-Aware","Pixel-GAN","Model Efficiency"],"permalink":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights","title":"[논문리뷰] Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights","excerpt":"이 [arXiv]에 게시한 'Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Text-to-Multi-Image","Causal Reasoning","World Knowledge","Benchmarking","Spatiotemporal Consistency","Generative Models","Evaluation Metrics"],"permalink":"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation","title":"[논문리뷰] Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation","excerpt":"MElHuseyni이 [arXiv]에 게시한 'Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","YOLOv5","Instance Segmentation","Thyroid Nodule","Ultrasound Imaging","Doppler Imaging","Medical AI","Deep Learning"],"permalink":"/ai/review/2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images","title":"[논문리뷰] Asking like Socrates: Socrates helps VLMs understand remote sensing images","excerpt":"Xinran He이 [arXiv]에 게시한 'Asking like Socrates: Socrates helps VLMs understand remote sensing images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Remote Sensing","Vision-Language Models","Iterative Reasoning","Evidence-Seeking","Socratic Method","Reinforcement Learning","Multi-Agent System","VQA","Grounding"],"permalink":"/ai/review/2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution","title":"[논문리뷰] Agentic Policy Optimization via Instruction-Policy Co-Evolution","excerpt":"이 [arXiv]에 게시한 'Agentic Policy Optimization via Instruction-Policy Co-Evolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Instruction Optimization","Policy Co-Evolution","Agentic AI","Tool-Integrated Reasoning","Self-Reflection"],"permalink":"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression","title":"[논문리뷰] Accelerating Streaming Video Large Language Models via Hierarchical Token Compression","excerpt":"이 [arXiv]에 게시한 'Accelerating Streaming Video Large Language Models via Hierarchical Token Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","lastModifiedAt":"2025-12-02 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Streaming Video LLMs","Token Compression","ViT Encoding","LLM Prefilling","Causal Compression","Caching","Pruning","Low-latency"],"permalink":"/ai/review/2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer","title":"[논문리뷰] Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer","excerpt":"이 [arXiv]에 게시한 'Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Transformer","Efficient Training","Multi-Modal Learning","Text-to-Image Generation","Image Editing","RLHF","Photorealistic Rendering"],"permalink":"/ai/review/2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection","title":"[논문리뷰] YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection","excerpt":"Avishai Weizman이 [arXiv]에 게시한 'YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Object Detection","YOLOv9","Mixture-of-Experts","Adaptive Routing","Deep Learning","Computer Vision","Feature Specialization"],"permalink":"/ai/review/2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM","title":"[논문리뷰] Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM","excerpt":"이 [arXiv]에 게시한 'Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Small Language Models","Data Efficiency","Reasoning","Maximal-Update Parameterization","FP8 Mixed Precision","Optimizer Scheduling","Long-Context Adaptation","Agent AI"],"permalink":"/ai/review/2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models","title":"[논문리뷰] World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models","excerpt":"Na Min An이 [arXiv]에 게시한 'World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Culture Mixing","VQA","Synthetic Data Generation","Multicultural Understanding","Model Robustness","Fine-tuning","Cultural Bias"],"permalink":"/ai/review/2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Vision-Bridge-Transformer-at-Scale","title":"[논문리뷰] Vision Bridge Transformer at Scale","excerpt":"Xinchao Wang이 [arXiv]에 게시한 'Vision Bridge Transformer at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision Transformer","Bridge Models","Conditional Generation","Image Editing","Video Translation","Velocity Matching","Diffusion Models","Scalability"],"permalink":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-The-Collapse-of-Patches","title":"[논문리뷰] The Collapse of Patches","excerpt":"Weidong Cai이 [arXiv]에 게시한 'The Collapse of Patches' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Patch Collapse","Image Generation","Image Classification","Masked Image Modeling","Vision Transformers","PageRank","Uncertainty Reduction","Computational Efficiency"],"permalink":"/ai/review/2025-12-01-The-Collapse-of-Patches/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps","title":"[논문리뷰] Test-time scaling of diffusions with flow maps","excerpt":"Sanja Fidler이 [arXiv]에 게시한 'Test-time scaling of diffusions with flow maps' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Flow Maps","Test-time Adaptation","Reward Guidance","Generative Models","SMC","Vision-Language Models"],"permalink":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs","title":"[논문리뷰] SO-Bench: A Structural Output Evaluation of Multimodal LLMs","excerpt":"이 [arXiv]에 게시한 'SO-Bench: A Structural Output Evaluation of Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Structural Output","Information Extraction","JSON Schema","SO-Bench","Visual Reasoning","Supervised Fine-tuning","Reinforcement Learning"],"permalink":"/ai/review/2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists","title":"[논문리뷰] RefineBench: Evaluating Refinement Capability of Language Models via Checklists","excerpt":"이 [arXiv]에 게시한 'RefineBench: Evaluating Refinement Capability of Language Models via Checklists' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Language Models","Refinement Capability","Self-Refinement","Guided Refinement","Checklist Evaluation","Multi-turn Interaction","Benchmark"],"permalink":"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models","title":"[논문리뷰] Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models","excerpt":"Yehudit Aperstein이 [arXiv]에 게시한 'Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Anomaly Detection","Surveillance Videos","Weakly Supervised Learning","Multiple Instance Learning","Dual-Encoder","I3D","TimeSformer","Top-k Pooling"],"permalink":"/ai/review/2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models","title":"[논문리뷰] REASONEDIT: Towards Reasoning-Enhanced Image Editing Models","excerpt":"이 [arXiv]에 게시한 'REASONEDIT: Towards Reasoning-Enhanced Image Editing Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Reasoning-Enhanced AI","Multimodal Large Language Models","Diffusion Transformers","Thinking","Reflection","Iterative Refinement","Instruction Following"],"permalink":"/ai/review/2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model","title":"[논문리뷰] OralGPT-Omni: A Versatile Dental Multimodal Large Language Model","excerpt":"이 [arXiv]에 게시한 'OralGPT-Omni: A Versatile Dental Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Model (MLLM)","Dental Imaging Analysis","Chain-of-Thought (CoT) Reasoning","Medical AI","Benchmark","Diagnosis","Oral Healthcare","Explainable AI"],"permalink":"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement","title":"[논문리뷰] OmniRefiner: Reinforcement-Guided Local Diffusion Refinement","excerpt":"Yiren Song이 [arXiv]에 게시한 'OmniRefiner: Reinforcement-Guided Local Diffusion Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Image Refinement","Reinforcement Learning","Fine-Grained Editing","Reference-Guided Generation","Latent Diffusion","Visual Fidelity","Detail Restoration"],"permalink":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models","title":"[논문리뷰] Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models","excerpt":"이 [arXiv]에 게시한 'Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Small Language Models (SLMs)","Latency Optimization","Hybrid Architectures","Evolutionary Search","Weight Normalization","Efficient Attention","Depth-Width Ratios","Real-device Efficiency"],"permalink":"/ai/review/2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey","title":"[논문리뷰] MRI Super-Resolution with Deep Learning: A Comprehensive Survey","excerpt":"이 [arXiv]에 게시한 'MRI Super-Resolution with Deep Learning: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","MRI Super-Resolution","Deep Learning","Computational Imaging","Inverse Problems","Generative AI","Medical Imaging","Survey"],"permalink":"/ai/review/2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge","title":"[논문리뷰] Layer-Aware Video Composition via Split-then-Merge","excerpt":"Wen-Sheng Chu이 [arXiv]에 게시한 'Layer-Aware Video Composition via Split-then-Merge' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Video Composition","Diffusion Models","Layer-Aware Generation","Self-Composition","Affordance Learning","Video Editing","Data Augmentation"],"permalink":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning","title":"[논문리뷰] Geometrically-Constrained Agent for Spatial Reasoning","excerpt":"Lehan He이 [arXiv]에 게시한 'Geometrically-Constrained Agent for Spatial Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Spatial Reasoning","Vision Language Models (VLMs)","Geometric Constraints","Agentic AI","Tool Integration","Semantic-to-Geometric Gap","Task Formalization"],"permalink":"/ai/review/2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images","title":"[논문리뷰] From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images","excerpt":"Filippos Kokkinos이 [arXiv]에 게시한 'From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Human Cognition","Image Perception","Benchmarking","Supervised Fine-tuning","Image Generation","Aesthetics","Memorability"],"permalink":"/ai/review/2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information","title":"[논문리뷰] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information","excerpt":"Kristian Kersting이 [arXiv]에 게시한 'Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Chain-of-Thought","Prompt Engineering","Efficiency","Structured Input","Information Extraction","Cognitive Psychology","Token Reduction"],"permalink":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets","title":"[논문리뷰] Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets","excerpt":"Avishai Weizman이 [arXiv]에 게시한 'Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Data Leakage","Video Datasets","Clustering","Frame Selection","Deep Learning","Object Detection","Dataset Partitioning","Dimensionality Reduction"],"permalink":"/ai/review/2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning","title":"[논문리뷰] FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning","excerpt":"Simin Chen이 [arXiv]에 게시한 'FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Federated Learning","Model Heterogeneity","Representation Learning","Privacy Preservation","Communication Efficiency","Entangled Representation","Knowledge Transfer"],"permalink":"/ai/review/2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration","title":"[논문리뷰] Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration","excerpt":"이 [arXiv]에 게시한 'Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Geometry Synthesis","Diffusion Models","Acceleration","Caching","Training-free","Flow Matching","Voxel Stabilization","Computational Efficiency"],"permalink":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models","title":"[논문리뷰] Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models","excerpt":"Wei Wu이 [arXiv]에 게시한 'Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Long Context","Sparse Attention","Hierarchical Sparse Attention (HSA)","Length Generalization","Mixture of Experts (MoE)","Transformer"],"permalink":"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action","title":"[논문리뷰] DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action","excerpt":"Zhuoyang Liu이 [arXiv]에 게시한 'DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Embodied AI","Action Degeneration","Data Pruning","Knowledge Distillation","Multi-modal Reasoning","Robot Learning","VLA Score"],"permalink":"/ai/review/2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space","title":"[논문리뷰] DiP: Taming Diffusion Models in Pixel Space","excerpt":"Xu Chen이 [arXiv]에 게시한 'DiP: Taming Diffusion Models in Pixel Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Pixel Space","Latent Diffusion Models (LDMs)","Diffusion Transformer (DiT)","Patch Detailer Head","Global-Local Modeling","Computational Efficiency","ImageNet"],"permalink":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning","title":"[논문리뷰] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning","excerpt":"이 [arXiv]에 게시한 'DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Large Language Models (LLMs)","Proof Verification","Self-Verification","Reinforcement Learning (RL)","Theorem Proving","Meta-Verification","Iterative Refinement"],"permalink":"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield","title":"[논문리뷰] Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield","excerpt":"이 [arXiv]에 게시한 'Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Model Distillation","Classifier-Free Guidance (CFG)","Distribution Matching","Text-to-Image Generation","Few-step Generation","Regularization","Score-based Models"],"permalink":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself","title":"[논문리뷰] CaptionQA: Is Your Caption as Useful as the Image Itself?","excerpt":"Zicheng Liu이 [arXiv]에 게시한 'CaptionQA: Is Your Caption as Useful as the Image Itself?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Captioning","Caption Evaluation","Multimodal LLM","Utility-based Benchmark","Question Answering (QA)","Domain-specific Taxonomy","Hallucination","MLLM Evaluation"],"permalink":"/ai/review/2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Captain-Safari-A-World-Engine","title":"[논문리뷰] Captain Safari: A World Engine","excerpt":"Yitong Li이 [arXiv]에 게시한 'Captain Safari: A World Engine' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","World Engine","3D Consistent Video Generation","Pose-conditioned Memory","Camera Control","FPV Video Synthesis","Diffusion Models","Drone Video Dataset"],"permalink":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model","title":"[논문리뷰] Architecture Decoupling Is Not All You Need For Unified Multimodal Model","excerpt":"Hongyu Li이 [arXiv]에 게시한 'Architecture Decoupling Is Not All You Need For Unified Multimodal Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Architecture Decoupling","Cross-Modal Attention","Attention Interaction Alignment (AIA) Loss","Task Conflicts","Image Generation","Image Understanding"],"permalink":"/ai/review/2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement","title":"[논문리뷰] AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement","excerpt":"Yicheng Ji이 [arXiv]에 게시한 'AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Person Video Generation","Audio-Driven Animation","Diffusion Models","Interactivity Refinement","Identity-Aware Attention","Scalability","Data Efficiency"],"permalink":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-12-01-Adversarial-Flow-Models","title":"[논문리뷰] Adversarial Flow Models","excerpt":"이 [arXiv]에 게시한 'Adversarial Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","lastModifiedAt":"2025-12-01 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Models","Adversarial Flow Models","GANs","Flow Matching","Optimal Transport","Single-step Generation","Image Generation","Transformer Architecture"],"permalink":"/ai/review/2025-12-01-Adversarial-Flow-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-28-What-does-it-mean-to-understand-language","title":"[논문리뷰] What does it mean to understand language?","excerpt":"이 [arXiv]에 게시한 'What does it mean to understand language?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","lastModifiedAt":"2025-11-28 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Language Understanding","Cognitive Neuroscience","Situation Models","World Knowledge","Embodiment","fMRI","Large Language Models","Brain Networks"],"permalink":"/ai/review/2025-11-28-What-does-it-mean-to-understand-language/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","title":"[논문리뷰] Video Generation Models Are Good Latent Reward Models","excerpt":"이 [arXiv]에 게시한 'Video Generation Models Are Good Latent Reward Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","lastModifiedAt":"2025-11-28 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Reward Feedback Learning","Latent Space","Diffusion Models","Human Preferences","Motion Quality","Process-aware"],"permalink":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following","title":"[논문리뷰] Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following","excerpt":"이 [arXiv]에 게시한 'Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","lastModifiedAt":"2025-11-28 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Judges","LMM Evaluation","Pluralistic Criteria","Criteria-Following","Trade-off Sensitivity","Conflict Resolution","Reward Models","Benchmark"],"permalink":"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing","title":"[논문리뷰] MIRA: Multimodal Iterative Reasoning Agent for Image Editing","excerpt":"Jiebo Luo이 [arXiv]에 게시한 'MIRA: Multimodal Iterative Reasoning Agent for Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","lastModifiedAt":"2025-11-28 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Multimodal AI","Iterative Reasoning","Agentic AI","Reinforcement Learning","Diffusion Models","Vision-Language Models","Instruction Following"],"permalink":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls","title":"[논문리뷰] Canvas-to-Image: Compositional Image Generation with Multimodal Controls","excerpt":"Kfir Aberman이 [arXiv]에 게시한 'Canvas-to-Image: Compositional Image Generation with Multimodal Controls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","lastModifiedAt":"2025-11-28 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Generation","Diffusion Models","Compositional Control","Multimodal Control","Unified Canvas","Multi-Task Learning","Personalization"],"permalink":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory","title":"[논문리뷰] Agentic Learner with Grow-and-Refine Multimodal Semantic Memory","excerpt":"Qunyi Xie이 [arXiv]에 게시한 'Agentic Learner with Grow-and-Refine Multimodal Semantic Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","lastModifiedAt":"2025-11-28 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Semantic Memory","Agentic Learning","Error Attribution","Visual Reasoning","Long-term Memory","Grow-and-Refine","Multimodal Reasoning"],"permalink":"/ai/review/2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Terminal-Velocity-Matching","title":"[논문리뷰] Terminal Velocity Matching","excerpt":"Jiaming Song이 [arXiv]에 게시한 'Terminal Velocity Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Models","Flow Matching","Diffusion Models","One-Step Generation","Few-Step Generation","Wasserstein Distance","Transformer Architecture","Lipschitz Continuity"],"permalink":"/ai/review/2025-11-27-Terminal-Velocity-Matching/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning","title":"[논문리뷰] SPHINX: A Synthetic Environment for Visual Perception and Reasoning","excerpt":"Nidhi Rastogi이 [arXiv]에 게시한 'SPHINX: A Synthetic Environment for Visual Perception and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Reasoning","Synthetic Environment","LVLM Evaluation","Reinforcement Learning","Cognitive Primitives","Procedural Generation","Multimodal AI"],"permalink":"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy","title":"[논문리뷰] Revisiting Generalization Across Difficulty Levels: It's Not So Easy","excerpt":"이 [arXiv]에 게시한 'Revisiting Generalization Across Difficulty Levels: It's Not So Easy' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Generalization","Task Difficulty","Item Response Theory","Cross-Difficulty","Data Curation","Model Evaluation","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale","title":"[논문리뷰] RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale","excerpt":"Yangcheng Yu이 [arXiv]에 게시한 'RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D World Generation","City-Scale","Multimodal Agents","Reality Alignment","Urban Simulation","Foundation Models","Geospatial Data"],"permalink":"/ai/review/2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-NVIDIA-Nemotron-Parse-1-1","title":"[논문리뷰] NVIDIA Nemotron Parse 1.1","excerpt":"이 [arXiv]에 게시한 'NVIDIA Nemotron Parse 1.1' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","OCR","Document Parsing","Vision-Language Model","Encoder-Decoder","Transformer","Table Extraction","Multilingual OCR","Layout Analysis"],"permalink":"/ai/review/2025-11-27-NVIDIA-Nemotron-Parse-1-1/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language","title":"[논문리뷰] Monet: Reasoning in Latent Visual Space Beyond Images and Language","excerpt":"Pengfei Wan이 [arXiv]에 게시한 'Monet: Reasoning in Latent Visual Space Beyond Images and Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Latent Visual Reasoning","Multimodal Large Language Models (MLLMs)","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)","Visual-latent Policy Optimization (VLPO)","Chain-of-Thought (CoT)","Abstract Visual Thinking"],"permalink":"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots","title":"[논문리뷰] MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots","excerpt":"Rui Yang이 [arXiv]에 게시한 'MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Mobile Robotics","Quadruped Robots","Chain-of-Thought (CoT)","Reinforcement Learning (RL)","Embodied AI","Multimodal Perception"],"permalink":"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems","title":"[논문리뷰] Latent Collaboration in Multi-Agent Systems","excerpt":"이 [arXiv]에 게시한 'Latent Collaboration in Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Large Language Models","Latent Space","Latent Reasoning","Latent Communication","KV Cache","Computational Efficiency","Training-Free"],"permalink":"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation","title":"[논문리뷰] Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation","excerpt":"Jiahao He이 [arXiv]에 게시한 'Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","World Simulation","Video Generation","Block Diffusion","Semi-Autoregressive","KV Cache Management","Inference Engine","Long Video Generation","Performance Optimization"],"permalink":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs","title":"[논문리뷰] Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs","excerpt":"Xin Yang이 [arXiv]에 게시한 'Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Timestep Distillation","Consistency Models","Latent Space","Image-Free Training","Efficiency Optimization","Trajectory Sampling","Continuous-Time Learning"],"permalink":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation","title":"[논문리뷰] I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation","excerpt":"이 [arXiv]에 게시한 'I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Health Indicator (HI)","Remaining Useful Life (RUL)","Uncertainty Quantification (UQ)","Autoencoder (AE)","Latent Space","Degradation Modeling","Prognostics","Condition-Based Maintenance"],"permalink":"/ai/review/2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy","title":"[논문리뷰] Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy","excerpt":"이 [arXiv]에 게시한 'Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio-Visual Generation","Cross-Modal Synchronization","Diffusion Models","Cross-Task Synergy","Classifier-Free Guidance","Multimodal AI","Generative AI"],"permalink":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization","title":"[논문리뷰] Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization","excerpt":"Youngjung Uh이 [arXiv]에 게시한 'Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting","Generalization","Sharpness-Aware Minimization","Regularization","Novel View Synthesis","Sparse View Reconstruction","Loss Landscape","Frequency-Adaptive"],"permalink":"/ai/review/2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","title":"[논문리뷰] Block Cascading: Training Free Acceleration of Block-Causal Video Models","excerpt":"이 [arXiv]에 게시한 'Block Cascading: Training Free Acceleration of Block-Causal Video Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","lastModifiedAt":"2025-11-27 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Block-Causal Models","Inference Acceleration","Multi-GPU Parallelism","Training-Free","KV Caching","Interactive AI"],"permalink":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation","title":"[논문리뷰] iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation","excerpt":"이 [arXiv]에 게시한 'iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Generation","Video Models","Diffusion Models","Many-to-many","Unified Framework","Temporal Consistency","Image Editing","Positional Embedding"],"permalink":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion","title":"[논문리뷰] Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion","excerpt":"Zhifei Yang이 [arXiv]에 게시한 'Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D City Generation","Generative AI","Large Language Models","Vision-Language Models","Multi-Agent Framework","Self-Critic Learning","Scene Graph","Text-to-3D"],"permalink":"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering","title":"[논문리뷰] VQ-VA World: Towards High-Quality Visual Question-Visual Answering","excerpt":"Feng Li이 [arXiv]에 게시한 'VQ-VA World: Towards High-Quality Visual Question-Visual Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Question Answering (VQA)","Image Generation","Data-centric AI","Agentic Pipeline","Multimodal Models","Web-scale Data","Benchmark","LightFusion"],"permalink":"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields","title":"[논문리뷰] Unified all-atom molecule generation with neural fields","excerpt":"이 [arXiv]에 게시한 'Unified all-atom molecule generation with neural fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Molecule Generation","Neural Fields","Score-based Generative Models","Drug Design","Modality-agnostic","Antibody Design","Macrocyclic Peptides","All-atom"],"permalink":"/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers","title":"[논문리뷰] UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers","excerpt":"이 [arXiv]에 게시한 'UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Diffusion Transformers","Length Extrapolation","Attention Mechanism","Attention Dispersion","Periodic Content Repetition","Quality Degradation","Training-free Method","Plug-and-play"],"permalink":"/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-Soft-Adaptive-Policy-Optimization","title":"[논문리뷰] Soft Adaptive Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Soft Adaptive Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Policy Optimization","Importance Ratios","Soft Clipping","Trust Region","Mixture-of-Experts","Asymmetric Temperature"],"permalink":"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System","title":"[논문리뷰] SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System","excerpt":"이 [arXiv]에 게시한 'SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Video Understanding","Scientific Education","Deming Cycle","Large Language Models","Iterative Optimization","Knowledge Integration","Educational Content Generation"],"permalink":"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs","title":"[논문리뷰] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs","excerpt":"이 [arXiv]에 게시한 'Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Tool-Integrated Reasoning (TIR)","Agentic AI","VQA","Training Environment","Behavioral Cloning","Policy Optimization"],"permalink":"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space","title":"[논문리뷰] SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space","excerpt":"Yulan He이 [arXiv]에 게시한 'SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Sparse Attention","Full Attention","Large Language Models (LLMs)","Context Length","Attention Sparsity","Alignment Loss","Long-Context Extrapolation"],"permalink":"/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding","title":"[논문리뷰] ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding","excerpt":"이 [arXiv]에 게시한 'ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Retake Generation","Camera Control","Rotary Position Embedding (RoPE)","Rotary Camera Encoding (RoCE)","Geometric Consistency","Video Generative Models","Transformer Architecture","Multi-view Synthesis"],"permalink":"/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","title":"[논문리뷰] PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding","excerpt":"Hongzhi Zhang이 [arXiv]에 게시한 'PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Physics Simulation","Controllable AI","Part-Aware","Semantic Grounding","Material Properties","Image-to-Video","Diffusion Models"],"permalink":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation","title":"[논문리뷰] OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation","excerpt":"이 [arXiv]에 게시한 'OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","RGBA Generation","Multi-Task Learning","Diffusion Transformers","Image Matting","Layer Decomposition","Object Removal","Alpha-aware VAE","MSROPE-BiL"],"permalink":"/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts","title":"[논문리뷰] MedSAM3: Delving into Segment Anything with Medical Concepts","excerpt":"Yi Lu이 [arXiv]에 게시한 'MedSAM3: Delving into Segment Anything with Medical Concepts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Medical Image Segmentation","Segment Anything Model (SAM)","Promptable Concept Segmentation (PCS)","Multimodal Large Language Models (MLLMs)","Agentic AI","Domain Adaptation","Text-guided Segmentation"],"permalink":"/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts","title":"[논문리뷰] MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts","excerpt":"이 [arXiv]에 게시한 'MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D City Generation","Natural Language Processing","Aesthetic Adaptation","Controllable Assets","Layout Generation","Interactive Editing","Diffusion Models","Multimodal Dataset"],"permalink":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-HunyuanOCR-Technical-Report","title":"[논문리뷰] HunyuanOCR Technical Report","excerpt":"이 [arXiv]에 게시한 'HunyuanOCR Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Optical Character Recognition","Multimodal Large Language Model","End-to-End Learning","Reinforcement Learning","Document Parsing","Information Extraction","Text Spotting"],"permalink":"/ai/review/2025-11-26-HunyuanOCR-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI","title":"[논문리뷰] GigaWorld-0: World Models as Data Engine to Empower Embodied AI","excerpt":"Chaojun Ni이 [arXiv]에 게시한 'GigaWorld-0: World Models as Data Engine to Empower Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","World Models","Embodied AI","Data Generation","Video Generation","3D Scene Reconstruction","Robotics","Vision-Language-Action"],"permalink":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms","title":"[논문리뷰] GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms","excerpt":"이 [arXiv]에 게시한 'GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM-driven Evolutionary Computation","Quality-Diversity","MAP-Elites","Program Synthesis","Open-source Framework","Algorithmic Discovery","Genetic Algorithms"],"permalink":"/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use","title":"[논문리뷰] Fara-7B: An Efficient Agentic Model for Computer Use","excerpt":"이 [arXiv]에 게시한 'Fara-7B: An Efficient Agentic Model for Computer Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Computer Use Agents","Synthetic Data Generation","Multi-modal LLM","On-device AI","Web Automation","Pixel-in Action-out","Fara-7B","WebTailBench"],"permalink":"/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward","title":"[논문리뷰] Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward","excerpt":"이 [arXiv]에 게시한 'Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Understanding-Generation Gap","Reasoning","Knowledge Transfer","Chain-of-Thought","Self-Training","Synthetic Data","Evaluation Framework"],"permalink":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection","title":"[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection","excerpt":"Mike Zheng Shou이 [arXiv]에 게시한 'DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AIGC Detection","Diffusion Models","Image Editing","Semantic Segmentation","Localization","Model Attribution","Benchmark","Multi-turn Editing"],"permalink":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning","title":"[논문리뷰] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning","excerpt":"이 [arXiv]에 게시한 'Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","lastModifiedAt":"2025-11-26 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-Evolving Agent","Vision-Language Models","Tool-Integrated Reasoning","Reinforcement Learning","Self-Correction","Multimodal AI","Generative AI"],"permalink":"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios","title":"[논문리뷰] UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios","excerpt":"이 [arXiv]에 게시한 'UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Diffusion Transformers","4K Resolution","Aspect Ratio Extrapolation","Data-Model Co-Design","VAE Post-training","Positional Encoding","Diffusion Models"],"permalink":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets","title":"[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?","excerpt":"Zhaowei Lu이 [arXiv]에 게시한 'Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","World Models","Mapless Navigation","Semantic Path Planning","Robot Learning","Video Prediction","Benchmark","Trajectory Generation"],"permalink":"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis","title":"[논문리뷰] SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis","excerpt":"Hongwen Zhang이 [arXiv]에 게시한 'SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hand-Object Interaction","Multi-view Video Generation","4D Motion Synthesis","Diffusion Models","Spatio-temporal Consistency","Geometric Consistency","Appearance and Motion Joint Modeling"],"permalink":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning","title":"[논문리뷰] Plan-X: Instruct Video Generation via Semantic Planning","excerpt":"Chenxu Zhang이 [arXiv]에 게시한 'Plan-X: Instruct Video Generation via Semantic Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Semantic Planning","Multimodal LLM","Diffusion Transformer","Spatio-temporal Guidance","Visual Hallucination","Prompt Alignment","Instruction Following"],"permalink":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models","title":"[논문리뷰] Pillar-0: A New Frontier for Radiology Foundation Models","excerpt":"이 [arXiv]에 게시한 'Pillar-0: A New Frontier for Radiology Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Radiology Foundation Model","Volumetric Imaging","Multi-window Tokenization","Multi-scale Attention","Contrastive Learning","Clinical Evaluation","Data Efficiency","Medical Imaging"],"permalink":"/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking","title":"[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking","excerpt":"Elias Stengel-Eskin이 [arXiv]에 게시한 'PRInTS: Reward Modeling for Long-Horizon Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reward Modeling","Long-Horizon Tasks","Information Seeking","Large Language Models","Trajectory Summarization","Reinforcement Learning","Tool Use","Process Reward Models"],"permalink":"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO","title":"[논문리뷰] Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO","excerpt":"이 [arXiv]에 게시한 'Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Reinforcement Learning","LLM Training","Hierarchical Credit Assignment","Trajectory Alignment","Group Relative Policy Optimization","Tool-Augmented Reasoning","Vertical Architecture"],"permalink":"/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-MIST-Mutual-Information-Via-Supervised-Training","title":"[논문리뷰] MIST: Mutual Information Via Supervised Training","excerpt":"Kyunghyun Cho이 [arXiv]에 게시한 'MIST: Mutual Information Via Supervised Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mutual Information Estimation","Supervised Learning","Meta-Learning","Neural Networks","Uncertainty Quantification","SetTransformer","Quantile Regression"],"permalink":"/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models","title":"[논문리뷰] MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Physics Reasoning","Motion Tracking","Spatial-Temporal Grounding","Video QA","AIGC Analysis","Reinforcement Learning"],"permalink":"/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark","title":"[논문리뷰] M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark","excerpt":"Bangwei Guo이 [arXiv]에 게시한 'M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Tool Use","Agent Benchmark","Model Context Protocol","Multi-Hop Reasoning","Multi-Threaded Execution","Evaluation Metrics","Similarity Alignment"],"permalink":"/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control","title":"[논문리뷰] In-Video Instructions: Visual Signals as Generative Control","excerpt":"이 [arXiv]에 게시한 'In-Video Instructions: Visual Signals as Generative Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Controllable AI","Visual Instructions","Image-to-Video","Spatial Control","Zero-shot Learning","Generative Models"],"permalink":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-HunyuanVideo-1-5-Technical-Report","title":"[논문리뷰] HunyuanVideo 1.5 Technical Report","excerpt":"Fang Yang이 [arXiv]에 게시한 'HunyuanVideo 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Transformer","Sparse Attention","Super-Resolution","Open-Source","Multimodal Understanding","Training Optimization","Efficient Inference"],"permalink":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-General-Agentic-Memory-Via-Deep-Research","title":"[논문리뷰] General Agentic Memory Via Deep Research","excerpt":"이 [arXiv]에 게시한 'General Agentic Memory Via Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Agents","Memory Systems","Large Language Models (LLMs)","Just-in-Time (JIT) Compilation","Memorizer","Researcher","Reinforcement Learning","Context Management"],"permalink":"/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Flow-Map-Distillation-Without-Data","title":"[논문리뷰] Flow Map Distillation Without Data","excerpt":"Tommi Jaakkola이 [arXiv]에 게시한 'Flow Map Distillation Without Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Flow Map Distillation","Data-Free Learning","Generative Models","Teacher-Student","Diffusion Acceleration","Teacher-Data Mismatch","One-Step Sampling"],"permalink":"/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration","title":"[논문리뷰] Fidelity-Aware Recommendation Explanations via Stochastic Path Integration","excerpt":"Oren Barkan이 [arXiv]에 게시한 'Fidelity-Aware Recommendation Explanations via Stochastic Path Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Recommender Systems","Explainable AI (XAI)","Explanation Fidelity","Path Integration","Stochastic Sampling","Counterfactual Explanations","Model-Agnostic","Sparse Data"],"permalink":"/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems","title":"[논문리뷰] Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems","excerpt":"Oren Barkan이 [arXiv]에 게시한 'Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Recommender Systems","Sparse Autoencoder (SAE)","Monosemantic Neurons","Interpretability","Prediction-Aware Loss","User-Item Interactions","Post-hoc Control"],"permalink":"/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation","title":"[논문리뷰] DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation","excerpt":"이 [arXiv]에 게시한 'DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Pixel Diffusion","Image Generation","Frequency Decoupling","Diffusion Transformer (DiT)","Flow Matching","AdaLN","Text-to-Image Synthesis"],"permalink":"/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research","title":"[논문리뷰] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research","excerpt":"이 [arXiv]에 게시한 'DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Evolving Rubrics","Deep Research","LLM Agents","Tool Use","Long-form QA","Open-source AI","Dynamic Evaluation"],"permalink":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation","title":"[논문리뷰] Controllable Layer Decomposition for Reversible Multi-Layer Image Generation","excerpt":"이 [arXiv]에 게시한 'Controllable Layer Decomposition for Reversible Multi-Layer Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Controllable Layer Decomposition","Diffusion Models","Multi-Layer Image Generation","Layer Separation","Bounding Box Guidance","Generative AI","Image Editing"],"permalink":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface","title":"[논문리뷰] Computer-Use Agents as Judges for Generative User Interface","excerpt":"이 [arXiv]에 게시한 'Computer-Use Agents as Judges for Generative User Interface' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Computer-Use Agents","Generative UI","AI-assisted Design","Human-Computer Interaction","LLM","AUI-Gym","Feedback Loop","Agent-centric Design"],"permalink":"/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens","title":"[논문리뷰] Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens","excerpt":"Stephanie Fu이 [arXiv]에 게시한 'Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Chain-of-Thought (CoT)","Continuous Visual Tokens","Multimodal Reasoning","Perceptual Grounding","Visual Thinking","Dense Prediction"],"permalink":"/ai/review/2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling","title":"[논문리뷰] Budget-Aware Tool-Use Enables Effective Agent Scaling","excerpt":"이 [arXiv]에 게시한 'Budget-Aware Tool-Use Enables Effective Agent Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Tool Use","Budget Awareness","Test-time Scaling","Cost-Performance","Web Search Agents","Planning","Self-Verification"],"permalink":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning","title":"[논문리뷰] AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning","excerpt":"Alphamasterliu이 [arXiv]에 게시한 'AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Automated Environment Generation","Cross-Environment Learning","Agent Learning","Language Models","Benchmark","Meta-Learning","Reinforcement Learning","Environment Design Language"],"permalink":"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser","title":"[논문리뷰] AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser","excerpt":"이 [arXiv]에 게시한 'AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","lastModifiedAt":"2025-11-25 00:00:00+0900+0900","categories":["Review"],"tags":["Review","HTML Extraction","Web Corpus","Large Language Models","Data Curation","Structured Element Preservation","Sequence Labeling","Markdown Conversion","MainWebBench"],"permalink":"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds","title":"[논문리뷰] WorldGen: From Text to Traversable and Interactive 3D Worlds","excerpt":"이 [arXiv]에 게시한 'WorldGen: From Text to Traversable and Interactive 3D Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D World Generation","Text-to-3D","Generative AI","Procedural Generation","Scene Decomposition","Navmesh","Game Engines","Interactive Environments"],"permalink":"/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models","title":"[논문리뷰] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models","excerpt":"Yudong Zhang이 [arXiv]에 게시한 'VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Latent Memory","Cognitive Memory","Visual Grounding","Short-term Memory","Long-term Memory","Reinforcement Learning"],"permalink":"/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination","title":"[논문리뷰] Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination","excerpt":"Jing Bi이 [arXiv]에 게시한 'Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Reasoning","Large Multimodal Models","Reinforcement Learning","Visual Rumination","Text-Rich Video","Video Question Answering","Iterative Perception"],"permalink":"/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation","title":"[논문리뷰] VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation","excerpt":"Gim Hee Lee이 [arXiv]에 게시한 'VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotic Manipulation","SpatioTemporal Coherence","4D Awareness","Visual Representation","Action Representation","Cross-Attention"],"permalink":"/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story","title":"[논문리뷰] Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story","excerpt":"Kristian Kuznetsov이 [arXiv]에 게시한 'Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Intrinsic Dimension","LLMs","Text Complexity","Sparse Autoencoders","Text Semantics","Genre Analysis","Embedding Space","Text Generation"],"permalink":"/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection","title":"[논문리뷰] Taming Generative Synthetic Data for X-ray Prohibited Item Detection","excerpt":"Renshuai Tao이 [arXiv]에 게시한 'Taming Generative Synthetic Data for X-ray Prohibited Item Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","X-ray Security","Synthetic Data Generation","Diffusion Models","Object Detection","Cross-Attention","Image Inpainting","Data Augmentation"],"permalink":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-SAM-3-Segment-Anything-with-Concepts","title":"[논문리뷰] SAM 3: Segment Anything with Concepts","excerpt":"이 [arXiv]에 게시한 'SAM 3: Segment Anything with Concepts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Segment Anything Model","Open-Vocabulary Segmentation","Multimodal Foundation Model","Instance Segmentation","Video Object Tracking","Prompt Engineering","Data Engine","Human-in-the-loop"],"permalink":"/ai/review/2025-11-24-SAM-3-Segment-Anything-with-Concepts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model","title":"[논문리뷰] RynnVLA-002: A Unified Vision-Language-Action and World Model","excerpt":"이 [arXiv]에 게시한 'RynnVLA-002: A Unified Vision-Language-Action and World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA) Model","World Model","Robotics","Unified Framework","Multi-modal Learning","Action Generation","Attention Mask","Continuous Control"],"permalink":"/ai/review/2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations","title":"[논문리뷰] Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations","excerpt":"Noam Koenigstein이 [arXiv]에 게시한 'Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Saliency Maps","Explainable AI (XAI)","Taxonomy","Evaluation Framework","Faithfulness Metrics","Contrastive Explanations","Granularity"],"permalink":"/ai/review/2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","title":"[논문리뷰] Planning with Sketch-Guided Verification for Physics-Aware Video Generation","excerpt":"Shayegan Omidshafiei이 [arXiv]에 게시한 'Planning with Sketch-Guided Verification for Physics-Aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Motion Planning","Physics-Aware AI","Multimodal Verification","Diffusion Models","Test-Time Optimization","Sketch-Guided"],"permalink":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs","title":"[논문리뷰] Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs","excerpt":"이 [arXiv]에 게시한 'Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Sycophancy","Model Robustness","AI Alignment","Benchmark","Confidence Calibration","Behavioral Taxonomy","Social Influence","Epistemic Collapse"],"permalink":"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe","title":"[논문리뷰] OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe","excerpt":"이 [arXiv]에 게시한 'OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Large Multimodal Models","Supervised Fine-tuning","Reinforcement Learning","Data Curation","Open-source","Multimodal Benchmarks"],"permalink":"/ai/review/2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists","title":"[논문리뷰] OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists","excerpt":"Weiquan Lin이 [arXiv]에 게시한 'OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Scientist","Large Language Models (LLMs)","Human-AI Collaboration","Scientific Ecosystem","Research Automation","Omni Scientific Protocol (OSP)","ScienceArena","Knowledge Graph"],"permalink":"/ai/review/2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents","title":"[논문리뷰] O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents","excerpt":"이 [arXiv]에 게시한 'O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Memory System","LLM Agents","Personalization","User Profiling","Hierarchical Retrieval","Long-Term Interaction","Self-Evolving Agents","Contextual Consistency"],"permalink":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models","title":"[논문리뷰] Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Adversarial Attack","Jailbreaking","Reward Hacking","Content Moderation Bypass","Cross-Model Transferability","Safety Vulnerabilities"],"permalink":"/ai/review/2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging","title":"[논문리뷰] MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging","excerpt":"이 [arXiv]에 게시한 'MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Genome Modeling","Dynamic Tokenization","Token Merging","Context-aware Learning","DNA Foundation Models","Transformer Architecture","Multi-omics"],"permalink":"/ai/review/2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight","title":"[논문리뷰] Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight","excerpt":"이 [arXiv]에 게시한 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA) Models","Visual Foresight","Diffusion Transformer (DiT)","Robotics","Multimodal Learning","Adaptive Temporal Ensemble","Latent Actions"],"permalink":"/ai/review/2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process","title":"[논문리뷰] Loomis Painter: Reconstructing the Painting Process","excerpt":"이 [arXiv]에 게시한 'Loomis Painter: Reconstructing the Painting Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Painting Process Generation","Video Diffusion Models","Media Transfer","Reverse Painting","Dataset Curation","Perceptual Distance Profile","Artistic Workflow","Generative AI"],"permalink":"/ai/review/2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process","title":"[논문리뷰] Insights from the ICLR Peer Review and Rebuttal Process","excerpt":"Nedjma Ousidhoum이 [arXiv]에 게시한 'Insights from the ICLR Peer Review and Rebuttal Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Peer Review","Rebuttal Process","ICLR","Score Dynamics","LLM Analysis","Reviewer Engagement","Academic Publishing","OpenReview"],"permalink":"/ai/review/2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization","title":"[논문리뷰] GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization","excerpt":"이 [arXiv]에 게시한 'GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Geolocalization","Agentic Models","Visual Reasoning","Web-Augmented","Multimodal LLMs","Reinforcement Learning","Tool Use","GeoBench"],"permalink":"/ai/review/2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models","title":"[논문리뷰] Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models","excerpt":"Serena Yeung-Levy이 [arXiv]에 게시한 'Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Small Multimodal Models","LLM Downscaling","Perception Bottleneck","Reasoning Bottleneck","Visual Extraction Tuning","Chain-of-Thought Reasoning","Multimodal Learning"],"permalink":"/ai/review/2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models","title":"[논문리뷰] Diversity Has Always Been There in Your Visual Autoregressive Models","excerpt":"Yaxing Wang이 [arXiv]에 게시한 'Diversity Has Always Been There in Your Visual Autoregressive Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","lastModifiedAt":"2025-11-24 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Autoregressive Models","Diversity Collapse","Generative Diversity","Soft-Suppression Regularization","Soft-Amplification Regularization","Training-Free","Image Generation","Singular Value Decomposition"],"permalink":"/ai/review/2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO","title":"[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO","excerpt":"이 [arXiv]에 게시한 'Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Next Event Prediction","Reinforcement Learning","Vision-Language Model","Video Diffusion Model","Joint Optimization","Multimodal AI","Procedural Learning"],"permalink":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models","title":"[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models","excerpt":"Baijiong Lin이 [arXiv]에 게시한 'V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Reasoning Benchmark","Chain-of-Frame","Evaluation","Multimodal AI","Physical Dynamics","Spatial Cognition","Pattern Inference"],"permalink":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval","title":"[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval","excerpt":"이 [arXiv]에 게시한 'TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Information Retrieval","Turkish Language","Late-Interaction Models","ColBERT","Dense Retrieval","MUVERA","Benchmarking","Low-Resource NLP","Fine-tuning"],"permalink":"/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding","title":"[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding","excerpt":"이 [arXiv]에 게시한 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long Video Understanding","Hybrid Mamba-Transformer","Vision-Language Model","Token Compression","Vision-to-Text Aggregation","Efficient LLM","Multimodal AI"],"permalink":"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation","title":"[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation","excerpt":"Xinyan Chen이 [arXiv]에 게시한 'Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Generation","Textual Reasoning","Interleaving","Large Multimodal Models (LMMs)","Chain-of-Thought (CoT)","Zero-shot Learning","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)"],"permalink":"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-Step-Audio-R1-Technical-Report","title":"[논문리뷰] Step-Audio-R1 Technical Report","excerpt":"이 [arXiv]에 게시한 'Step-Audio-R1 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio Reasoning","Multimodal LLMs","Modality-Grounded Reasoning Distillation (MGRD)","Chain-of-Thought","Reinforcement Learning","Audio Understanding","Self-Distillation"],"permalink":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models","title":"[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models","excerpt":"이 [arXiv]에 게시한 'Scaling Spatial Intelligence with Multimodal Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Spatial Intelligence","Multimodal Foundation Models","Data Scaling","Perspective-taking","Visual Question Answering","Emergent Capabilities","Embodied AI","Benchmark Evaluation"],"permalink":"/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models","title":"[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Vision-Language-Action Models","Reward Shaping","World Models","Self-Referential Learning","Robotics","Trajectory Optimization"],"permalink":"/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking","title":"[논문리뷰] SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking","excerpt":"이 [arXiv]에 게시한 'SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Surgical Video Segmentation","Interactive Video Object Segmentation","Long-term Tracking","Foundation Models","Domain Adaptation","Semantic Learning","Prompt-based Segmentation"],"permalink":"/ai/review/2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-SAM-3D-3Dfy-Anything-in-Images","title":"[논문리뷰] SAM 3D: 3Dfy Anything in Images","excerpt":"이 [arXiv]에 게시한 'SAM 3D: 3Dfy Anything in Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Generative Models","Single Image 3D","Object Reconstruction","Scene Understanding","Data Engine","Model-in-the-Loop","Human Preference"],"permalink":"/ai/review/2025-11-21-SAM-3D-3Dfy-Anything-in-Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes","title":"[논문리뷰] PartUV: Part-Based UV Unwrapping of 3D Meshes","excerpt":"Hao Su이 [arXiv]에 게시한 'PartUV: Part-Based UV Unwrapping of 3D Meshes' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","UV Unwrapping","3D Meshes","Part-Based Decomposition","Neural Fields","Geometric Heuristics","Parameterization","Texture Mapping"],"permalink":"/ai/review/2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs","title":"[논문리뷰] Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs","excerpt":"이 [arXiv]에 게시한 'Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Compression","Elastic Networks","Knowledge Distillation","Hybrid Mamba-Attention","Reasoning LLMs","Multi-Budget Training","Zero-Shot Deployment"],"permalink":"/ai/review/2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion","title":"[논문리뷰] NaTex: Seamless Texture Generation as Latent Color Diffusion","excerpt":"이 [arXiv]에 게시한 'NaTex: Seamless Texture Generation as Latent Color Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Texture Generation","Latent Diffusion Model","Geometry-Aware VAE","Multi-Control DiT","Color Point Cloud","Texture Synthesis","3D Asset Creation"],"permalink":"/ai/review/2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report","title":"[논문리뷰] MiMo-Embodied: X-Embodied Foundation Model Technical Report","excerpt":"이 [arXiv]에 게시한 'MiMo-Embodied: X-Embodied Foundation Model Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Model (VLM)","Embodied AI","Autonomous Driving","Foundation Model","Multimodal Learning","Task Planning","Affordance Prediction","Spatial Understanding","Reinforcement Learning"],"permalink":"/ai/review/2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization","title":"[논문리뷰] First Frame Is the Place to Go for Video Content Customization","excerpt":"이 [arXiv]에 게시한 'First Frame Is the Place to Go for Video Content Customization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Content Customization","Few-shot Learning","LoRA","Vision-Language Models (VLMs)","First Frame Conditioning","Reference-based Generation"],"permalink":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-21-Draft-and-Refine-with-Visual-Experts","title":"[논문리뷰] Draft and Refine with Visual Experts","excerpt":"이 [arXiv]에 게시한 'Draft and Refine with Visual Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Vision-Language Models (LVLMs)","Visual Grounding","Hallucination Mitigation","Agent Framework","Visual Question Answering (VQA)","Expert Coordination","Relevance Map","Multi-modal Reasoning"],"permalink":"/ai/review/2025-11-21-Draft-and-Refine-with-Visual-Experts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity","title":"[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity","excerpt":"이 [arXiv]에 게시한 'What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Research Agents","Ideation Diversity","MLE-bench","LLM Backbones","Agentic Scaffolds","Shannon Entropy","Machine Learning Engineering","Performance Metrics"],"permalink":"/ai/review/2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images","title":"[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images","excerpt":"이 [arXiv]에 게시한 'VisPlay: Self-Evolving Vision-Language Models from Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-Evolving","Vision-Language Models","Reinforcement Learning","Self-Play","Unlabeled Data","Multimodal Reasoning","Group Relative Policy Optimization","Hallucination Mitigation"],"permalink":"/ai/review/2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks","title":"[논문리뷰] Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks","excerpt":"Yiran Peng이 [arXiv]에 게시한 'Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Models","Spatial Reasoning","Maze Solving","Video Generation","Benchmark","Supervised Fine-tuning","Test-Time Scaling","Multimodal Reasoning"],"permalink":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation","title":"[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation","excerpt":"이 [arXiv]에 게시한 'Mixture of States: Routing Token-Level Dynamics for Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Diffusion","Mixture of States (MoS)","Token-Level Routing","Dynamic Conditional Fusion","Text-to-Image Generation","Image Editing","Transformer Architecture"],"permalink":"/ai/review/2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation","title":"[논문리뷰] Medal S: Spatio-Textual Prompt Model for Medical Segmentation","excerpt":"Tao Chen이 [arXiv]에 게시한 'Medal S: Spatio-Textual Prompt Model for Medical Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Medical Segmentation","Foundation Model","Spatio-Textual Prompts","3D Convolution","Multi-modal Imaging","Dynamic Resampling","Parallel Inference","Iterative Refinement"],"permalink":"/ai/review/2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-MHR-Momentum-Human-Rig","title":"[논문리뷰] MHR: Momentum Human Rig","excerpt":"Chris Twigg이 [arXiv]에 게시한 'MHR: Momentum Human Rig' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Parametric Body Model","Human Animation","Character Rigging","Pose Correctives","Skeletal Decoupling","Computer Graphics","AR/VR"],"permalink":"/ai/review/2025-11-20-MHR-Momentum-Human-Rig/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","title":"[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation","excerpt":"Vladimir Arkhipkin이 [arXiv]에 게시한 'Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Generation","Video Generation","Diffusion Models","Flow Matching","Diffusion Transformer","NABLA","RLHF","Supervised Fine-tuning"],"permalink":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset","title":"[논문리뷰] Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset","excerpt":"이 [arXiv]에 게시한 'Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Medical Imaging","Chest X-ray","Lesion Segmentation","Vision-Language Models","Instruction Following","Data Generation","MIMIC-CXR"],"permalink":"/ai/review/2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI","title":"[논문리뷰] FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI","excerpt":"Xinyu Yin이 [arXiv]에 게시한 'FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Embodied AI","Vision-and-Language Navigation (VLN)","LLM-driven Simulation","Human-Agent Interaction","Closed-Loop","Benchmark Dataset","Social Cognition"],"permalink":"/ai/review/2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges","title":"[논문리뷰] Aligning Generative Music AI with Human Preferences: Methods and Challenges","excerpt":"Abhinaba Roy이 [arXiv]에 게시한 'Aligning Generative Music AI with Human Preferences: Methods and Challenges' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Music AI","Preference Alignment","Reinforcement Learning from Human Feedback (RLHF)","Direct Preference Optimization (DPO)","Inference-Time Optimization","Music Generation","Human-Computer Interaction"],"permalink":"/ai/review/2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries","title":"[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries","excerpt":"이 [arXiv]에 게시한 'ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Chaptering","Long-form Video Understanding","Large Language Models","Multimodal Learning","Hierarchical Summarization","Video Segmentation","Reinforcement Learning","Dataset Creation"],"permalink":"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-eat-Physically-Grounded-Feature-Representation","title":"[논문리뷰] Φeat: Physically-Grounded Feature Representation","excerpt":"이 [arXiv]에 게시한 'Φeat: Physically-Grounded Feature Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Physically-Grounded Features","Material Representation","Intrinsic Scene Understanding","Vision Transformer","Synthetic Data","Contrastive Learning"],"permalink":"/ai/review/2025-11-19-eat-Physically-Grounded-Feature-Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning","title":"[논문리뷰] VIDEOP2R: Video Understanding from Perception to Reasoning","excerpt":"이 [arXiv]에 게시한 'VIDEOP2R: Video Understanding from Perception to Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Understanding","Reinforcement Fine-Tuning (RFT)","Large Video Language Models (LVLMs)","Perception and Reasoning","Chain-of-Thought (CoT)","Process-Aware Learning","Policy Optimization","Credit Assignment"],"permalink":"/ai/review/2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models","title":"[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models","excerpt":"Rong Zhao이 [arXiv]에 게시한 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LVLM Evaluation","Global Visual Perception","Topological Properties","Shortcut-Free Benchmark","Visual Bottleneck","Multimodal AI","Synthetic Data"],"permalink":"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding","title":"[논문리뷰] REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding","excerpt":"Jingyang Chen이 [arXiv]에 게시한 'REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Long-Form Video Understanding","Self-Reflection","Reinforcement Learning","Tool-Augmented MLLMs","Visual Rethinking","Video Question Answering","Causal Attribution"],"permalink":"/ai/review/2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations","title":"[논문리뷰] Proactive Hearing Assistants that Isolate Egocentric Conversations","excerpt":"이 [arXiv]에 게시한 'Proactive Hearing Assistants that Isolate Egocentric Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Proactive Hearing Assistant","Egocentric Audio Processing","Speech Separation","Turn-taking Dynamics","Dual-Model Architecture","Real-time Inference","Wearable Devices","Dialogue Modeling"],"permalink":"/ai/review/2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution","title":"[논문리뷰] Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution","excerpt":"Sudeep Pillai이 [arXiv]에 게시한 'Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Agent","Multimodal Perception","Tool-Augmented LLM","Agentic AI","Visual Reasoning","Computer Vision","Structured Outputs","ReAct Framework"],"permalink":"/ai/review/2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models","title":"[논문리뷰] OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models","excerpt":"Jian liu이 [arXiv]에 게시한 'OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Omnimodal LLMs","Token Compression","Audio-Video Understanding","Dynamic Pruning","Inference Acceleration","Spatio-Temporal Compression","Large Language Models"],"permalink":"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models","title":"[논문리뷰] Mitigating Label Length Bias in Large Language Models","excerpt":"Katharina von der Wense이 [arXiv]에 게시한 'Mitigating Label Length Bias in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Label Bias","Calibration","In-Context Learning","Text Classification","Multi-token Labels","Label Length Bias","Multiple Choice QA"],"permalink":"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs","title":"[논문리뷰] MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs","excerpt":"Kaijie Chen이 [arXiv]에 게시한 'MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LVLM Robustness","Misleading Visual Inputs","VQA Benchmark","Visual Perception","Visual Reasoning","MVI-Sensitivity","Multimodal AI"],"permalink":"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework","title":"[논문리뷰] Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework","excerpt":"이 [arXiv]에 게시한 'Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Extreme Multi-label Classification (XMC)","Large Language Models (LLMs)","Multi-modal Learning","Dual-decoder Learning","Vision Transformers","Contrastive Learning","Prompt Engineering"],"permalink":"/ai/review/2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost","title":"[논문리뷰] LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost","excerpt":"Kengo Tajiri이 [arXiv]에 게시한 'LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Chaos Engineering","Large Language Models","System Resilience","Kubernetes","Software Automation","AI Agents","Fault Injection"],"permalink":"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models","title":"[논문리뷰] Error-Driven Scene Editing for 3D Grounding in Large Language Models","excerpt":"이 [arXiv]에 게시한 'Error-Driven Scene Editing for 3D Grounding in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Grounding","3D-LLMs","Scene Editing","Counterfactual Augmentation","Error-Driven Learning","Spatial Reasoning","Visual Grounding"],"permalink":"/ai/review/2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark","title":"[논문리뷰] Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark","excerpt":"Yuzhang Shang이 [arXiv]에 게시한 'Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Visual Reasoning","Chain-of-Frames (CoF)","Video Generation Models","World Simulators","AI Benchmarking","Cognitive Reasoning","VLM Evaluation"],"permalink":"/ai/review/2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models","title":"[논문리뷰] AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models","excerpt":"이 [arXiv]에 게시한 'AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Arabic LLMs","Linguistic Benchmark","Human Annotation","Natural Language Understanding","Grammar Evaluation","Morphology Analysis","Syntax Assessment","Reading Comprehension"],"permalink":"/ai/review/2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding","title":"[논문리뷰] Agent READMEs: An Empirical Study of Context Files for Agentic Coding","excerpt":"Kundjanasith Thonglek이 [arXiv]에 게시한 'Agent READMEs: An Empirical Study of Context Files for Agentic Coding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Coding","Context Files","READMEs for Agents","Empirical Study","Software Engineering","Documentation Maintenance","Non-functional Requirements","LLMs"],"permalink":"/ai/review/2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning","title":"[논문리뷰] Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning","excerpt":"Yucong Luo이 [arXiv]에 게시한 'Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Markov Decision Process","Tool Use","Multi-turn Interaction","Policy Optimization","Reward Shaping","Agent Framework"],"permalink":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning","title":"[논문리뷰] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning","excerpt":"Yuqiang Li이 [arXiv]에 게시한 'ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Benchmark","LLMs","Scientific Reasoning","Multidisciplinary","AI4S","Data Contamination","Evaluation","LRM-as-Judge"],"permalink":"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space","title":"[논문리뷰] A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space","excerpt":"이 [arXiv]에 게시한 'A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Code-to-Style Generation","Discrete Style Space","Style Codebook","Autoregressive Model","Diffusion Models","Visual Stylization","Generative AI"],"permalink":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition","title":"[논문리뷰] A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition","excerpt":"G. Maragatham이 [arXiv]에 게시한 'A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","EEG","Emotion Recognition","Transformer Architecture","Inter-Cortical Neural Interactions","Multi-Head Attention","Brain-Computer Interface","Affective Computing"],"permalink":"/ai/review/2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data","title":"[논문리뷰] Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data","excerpt":"이 [arXiv]에 게시한 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Omnimodal Large Models","Mixture-of-Experts (MoE)","Language-Centric AI","Multimodal Understanding","Multimodal Generation","Progressive Training","Omni-Modality 3D RoPE"],"permalink":"/ai/review/2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity","title":"[논문리뷰] UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity","excerpt":"이 [arXiv]에 게시한 'UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-Supervised Learning","Segmentation","Granularity Control","SAM","Foundation Models","Unsupervised Learning","Image Segmentation","Video Segmentation"],"permalink":"/ai/review/2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy","title":"[논문리뷰] UFO^3: Weaving the Digital Agent Galaxy","excerpt":"이 [arXiv]에 게시한 'UFO^3: Weaving the Digital Agent Galaxy' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Cross-Device Orchestration","LLM-Powered Agents","Task Constellation","Directed Acyclic Graph (DAG)","Agent Interaction Protocol (AIP)","Fault Tolerance","Asynchronous Execution"],"permalink":"/ai/review/2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models","title":"[논문리뷰] TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models","excerpt":"Qingyang Liu이 [arXiv]에 게시한 'TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generative Models","Visual Reasoning","Benchmarking","Image-to-Video","TiViBench","VideoTPO","Prompt Optimization"],"permalink":"/ai/review/2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models","title":"[논문리뷰] Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Test-Time Adaptation","Zero-Shot Generalization","Spectral Decomposition","Latent Space Steering","SVD","Out-of-Distribution"],"permalink":"/ai/review/2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance","title":"[논문리뷰] Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance","excerpt":"이 [arXiv]에 게시한 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Model Souping","Large Language Models","Weighted Averaging","Benchmark Optimization","State-of-the-Art","Category Experts","Parameter Averaging","Post-training"],"permalink":"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization","title":"[논문리뷰] SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization","excerpt":"Bo Du이 [arXiv]에 게시한 'SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Safety Alignment","Rule-Governed RL","Self-Rewarded Learning","MLLM Safety","Policy Optimization","Safety Benchmarking","Compositional Robustness"],"permalink":"/ai/review/2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model","title":"[논문리뷰] Part-X-MLLM: Part-aware 3D Multimodal Large Language Model","excerpt":"이 [arXiv]에 게시한 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Multimodal LLM","Part-aware","3D Generation","3D Editing","3D Understanding","Bounding Box","Structured Program","Dual-encoder"],"permalink":"/ai/review/2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","title":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning","excerpt":"Haiyuan Wan이 [arXiv]에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Physics Reasoning","Agentic AI","Olympiad Problems","Post-Training","Knowledge Transfer"],"permalink":"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation","title":"[논문리뷰] OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation","excerpt":"이 [arXiv]에 게시한 'OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Earth Observation","Foundation Model","Multimodal Learning","Self-supervised Learning","Latent Image Modeling","Vision Transformer","Spatio-temporal"],"permalink":"/ai/review/2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards","title":"[논문리뷰] NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards","excerpt":"이 [arXiv]에 게시한 'NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Model","Direct Preference Optimization","World Model","Reward Learning","Robotics","Embodied AI","Flow-Matching"],"permalink":"/ai/review/2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling","title":"[논문리뷰] MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling","excerpt":"cyyang822이 [arXiv]에 게시한 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Research Agent","Tool-Augmented Reasoning","Interaction Scaling","Large Language Models","Reinforcement Learning","Context Management","Open-Source AI"],"permalink":"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model","title":"[논문리뷰] MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model","excerpt":"Bo Yan이 [arXiv]에 게시한 'MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Microscopy VQA","Multimodal LLM","Weak Supervision","Graph Neural Networks","Dataset Generation","Biomedical Imaging","Scientific Reasoning","Cross-Modal Consistency"],"permalink":"/ai/review/2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering","title":"[논문리뷰] LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering","excerpt":"이 [arXiv]에 게시한 'LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Software Engineering","Long-Context","Interactive Benchmark","Tool Usage","Memory Management","Bias-Free Evaluation","Multi-Turn"],"permalink":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly","title":"[논문리뷰] Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?","excerpt":"Lingming Zhang이 [arXiv]에 게시한 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Software Engineering Agents","LLM Agents","Self-Evolution","On-the-Fly Learning","Tool Creation","SWE-bench","Autonomous Systems","Code Generation"],"permalink":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners","title":"[논문리뷰] Genomic Next-Token Predictors are In-Context Learners","excerpt":"이 [arXiv]에 게시한 'Genomic Next-Token Predictors are In-Context Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","In-Context Learning (ICL)","Genomic Sequences","Next-Token Prediction","Large Language Models (LLMs)","Modality-Agnostic AI","Meta-Learning","Bitstring Program Synthesis","Evo2"],"permalink":"/ai/review/2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing","title":"[논문리뷰] Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing","excerpt":"이 [arXiv]에 게시한 'Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Serendipity Discovery","Knowledge Graphs","Drug Repurposing","LLMs","KGQA","RNS Metric","Biomedical AI"],"permalink":"/ai/review/2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing","title":"[논문리뷰] AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing","excerpt":"Hongyu Lin이 [arXiv]에 게시한 'AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Telemarketing","Large Language Models","Persuasive Dialogue","Reinforcement Learning","Bayesian Optimization","Dynamic Prompting","Dialogue Systems"],"permalink":"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain","title":"[논문리뷰] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain","excerpt":"Meng Jiang이 [arXiv]에 게시한 'A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Decentralized RAG","Blockchain","Smart Contracts","Source Reliability","Large Language Models","Retrieval Augmented Generation","Trustworthy AI"],"permalink":"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward","title":"[논문리뷰] miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward","excerpt":"Farzan Farnia이 [arXiv]에 게시한 'miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Autoformalization","Benchmark Dataset","miniF2F","Lean Language","Large Language Models","Mathematical Reasoning","Formal Verification"],"permalink":"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences","title":"[논문리뷰] Workload Schedulers -- Genesis, Algorithms and Differences","excerpt":"Vladimir Getov이 [arXiv]에 게시한 'Workload Schedulers -- Genesis, Algorithms and Differences' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Workload Scheduling","Process Scheduling","Job Scheduling","Big Data Processing","Resource Management","Distributed Systems","Scheduling Algorithms","Performance Optimization"],"permalink":"/ai/review/2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-Virtual-Width-Networks","title":"[논문리뷰] Virtual Width Networks","excerpt":"이 [arXiv]에 게시한 'Virtual Width Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Virtual Width Networks","Transformer","Mixture-of-Experts (MoE)","Scaling Laws","Representation Learning","Model Efficiency","Multi-Token Prediction","Hyper-Connections"],"permalink":"/ai/review/2025-11-17-Virtual-Width-Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation","title":"[논문리뷰] UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation","excerpt":"Weihan Wang이 [arXiv]에 게시한 'UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Language Model","UI-to-Code Generation","Interactive UI","UI Editing","UI Polishing","Reinforcement Learning","Multimodal Coding","Test-Time Scaling"],"permalink":"/ai/review/2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap","title":"[논문리뷰] Simulating the Visual World with Artificial Intelligence: A Roadmap","excerpt":"Pengfei Wan이 [arXiv]에 게시한 'Simulating the Visual World with Artificial Intelligence: A Roadmap' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","World Models","Video Generation","AI Simulation","Generative AI","Physical Plausibility","Interactive AI","Planning","Roadmap"],"permalink":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism","title":"[논문리뷰] MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism","excerpt":"이 [arXiv]에 게시한 'MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Reinforcement Learning","LLMs","Pipeline Parallelism","Reasoning","Reward Shaping","Agentic AI"],"permalink":"/ai/review/2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers","title":"[논문리뷰] LiteAttention: A Temporal Sparse Attention for Diffusion Transformers","excerpt":"이 [arXiv]에 게시한 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Transformers","Sparse Attention","Temporal Coherence","Video Generation","Computational Efficiency","FlashAttention","CUDA Kernels"],"permalink":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey","title":"[논문리뷰] Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey","excerpt":"Mohammad Hossein Rohban이 [arXiv]에 게시한 'Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Scientific Discovery","Idea Generation","Creativity","Survey","AI in Science","Prompt Engineering","Multi-agent Systems","Evaluation Metrics"],"permalink":"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant","title":"[논문리뷰] HI-TransPA: Hearing Impairments Translation Personal Assistant","excerpt":"이 [arXiv]에 게시한 'HI-TransPA: Hearing Impairments Translation Personal Assistant' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Hearing Impairment","Audio-Visual Speech Recognition","Curriculum Learning","Omni-Models","Assistive Technology","Lip Reading","Speech Translation"],"permalink":"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models","title":"[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models","excerpt":"Siyuan Li이 [arXiv]에 게시한 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Generative Reasoning","Geometric Construction","Benchmark","GeoGebra","Code-based Evaluation","Unified Models"],"permalink":"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models","title":"[논문리뷰] From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models","excerpt":"이 [arXiv]에 게시한 'From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Tool-augmented LLMs","Reasoning Hallucinations","Tool-Induced Myopia (TIM)","Code Interpreter","Mathematical Reasoning","LLM Evaluation","Preference Optimization"],"permalink":"/ai/review/2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies","title":"[논문리뷰] Experience-Guided Adaptation of Inference-Time Reasoning Strategies","excerpt":"이 [arXiv]에 게시한 'Experience-Guided Adaptation of Inference-Time Reasoning Strategies' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Adaptive AI","Inference-Time Adaptation","Reasoning Strategies","Meta-Learning","LLM-based Agents","Dynamic Strategy Generation","Continual Learning","Computational Efficiency"],"permalink":"/ai/review/2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","title":"[논문리뷰] EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation","excerpt":"Zeyu Wang이 [arXiv]에 게시한 'EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Dataset","Emotion Recognition","Video Generation","Affective Computing","Stylized Media","Diffusion Models","Video Understanding","Text-to-Video"],"permalink":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding","title":"[논문리뷰] Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding","excerpt":"이 [arXiv]에 게시한 'Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Recommenders","Human Priors","Multi-head Decoding","Disentangled Representation Learning","Sequential Recommendation","Adapter Networks","Hierarchical Modeling"],"permalink":"/ai/review/2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-DoPE-Denoising-Rotary-Position-Embedding","title":"[논문리뷰] DoPE: Denoising Rotary Position Embedding","excerpt":"Min Yang이 [arXiv]에 게시한 'DoPE: Denoising Rotary Position Embedding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Rotary Position Embedding","Transformer","Length Extrapolation","Attention Sink","Matrix Entropy","Denoising","Large Language Models"],"permalink":"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains","title":"[논문리뷰] DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains","excerpt":"이 [arXiv]에 게시한 'DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Discourse-Level Translation","Expert Domains","Benchmarking","LLM Evaluation","Reference-Free Metric","Chinese-English Translation","Contextual Coherence","Domain-Specific Terminology"],"permalink":"/ai/review/2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios","title":"[논문리뷰] CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios","excerpt":"Juyoung Oh이 [arXiv]에 게시한 'CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Cooperative Perception","Vehicle-to-Vehicle (V2V)","Autonomous Driving","Dataset","Adverse Traffic Scenarios","Sensor Fusion","Temporal Alignment","3D Bounding Box Annotation"],"permalink":"/ai/review/2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems","title":"[논문리뷰] A Meta-Heuristic Load Balancer for Cloud Computing Systems","excerpt":"Vladimir Getov이 [arXiv]에 게시한 'A Meta-Heuristic Load Balancer for Cloud Computing Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Cloud Computing","Load Balancing","Meta-Heuristic","Genetic Algorithm","Simulated Annealing","Tabu Search","Resource Management","Service Migration"],"permalink":"/ai/review/2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist","title":"[논문리뷰] UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist","excerpt":"이 [arXiv]에 게시한 'UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Agents","Multi-modal AI","Plan-Act Architecture","Tool-Use","Long-horizon Reasoning","Open-source","Video Generation","Video Understanding"],"permalink":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training","title":"[논문리뷰] Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training","excerpt":"suayptalha이 [arXiv]에 게시한 'Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Quantum Computing","Optimization","Machine Learning","Transformers","Gradient Descent","Superposition","Large Language Models","Hybrid Quantum-Classical"],"permalink":"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control","title":"[논문리뷰] SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control","excerpt":"Ryan Rossi이 [arXiv]에 게시한 'SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Continuous Control","Fine-Grained Control","Instruction-based","Low-Rank Adaptation","Disentanglement","Generative Models"],"permalink":"/ai/review/2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following","title":"[논문리뷰] Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following","excerpt":"Karishma Mandyam이 [arXiv]에 게시한 'Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","Instruction Following","Reinforcement Learning","Rubric-based Evaluation","Benchmarking","Reward Shaping","Rubric Verifier","AdvancedIF"],"permalink":"/ai/review/2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents","title":"[논문리뷰] ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents","excerpt":"이 [arXiv]에 게시한 'ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Deep Research Agents","LLM Evaluation","Benchmark","Rubrics","Multi-step Reasoning","Cross-document Synthesis","AI Performance","Task Complexity"],"permalink":"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models","title":"[논문리뷰] One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models","excerpt":"Ilya Makarov이 [arXiv]에 게시한 'One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Latent Diffusion Models","Super-Resolution","Upscaling Adapter","Image Generation","Latent Space","Multi-scale Learning","Cross-VAE"],"permalink":"/ai/review/2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models","title":"[논문리뷰] Music Flamingo: Scaling Music Understanding in Audio Language Models","excerpt":"이 [arXiv]에 게시한 'Music Flamingo: Scaling Music Understanding in Audio Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio Language Models","Music Understanding","Chain-of-Thought","Reinforcement Learning","Data Curation","Multimodal AI","Music Information Retrieval"],"permalink":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples","title":"[논문리뷰] MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples","excerpt":"이 [arXiv]에 게시한 'MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Zero-Shot Learning","Anomaly Detection","Anomaly Segmentation","Multimodal","Industrial Inspection","Mutual Scoring","Unsupervised Learning","Transformer"],"permalink":"/ai/review/2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique","title":"[논문리뷰] MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique","excerpt":"이 [arXiv]에 게시한 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LMMs","Multimodal Critique","Benchmark","Evaluation","Reward Model","GPT-4o","Scaling Law"],"permalink":"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO","title":"[논문리뷰] Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO","excerpt":"이 [arXiv]에 게시한 'Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Decentralized RL","GRPO","LLM Post-training","Adversarial Attacks","Data Poisoning","Defense Mechanisms","In-context Attack","Out-of-context Attack"],"permalink":"/ai/review/2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views","title":"[논문리뷰] Depth Anything 3: Recovering the Visual Space from Any Views","excerpt":"이 [arXiv]에 게시한 'Depth Anything 3: Recovering the Visual Space from Any Views' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Depth Estimation","Multi-view Geometry","Transformer Architecture","Teacher-Student Learning","Pose Estimation","3D Reconstruction","Novel View Synthesis","Visual Space Recovery"],"permalink":"/ai/review/2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis","title":"[논문리뷰] CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis","excerpt":"Jian Wu이 [arXiv]에 게시한 'CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Citation Contexts","Reproducibility","Sentiment Analysis","Large Language Models","Crowdsourcing","Dataset","Machine Learning","Science of Science"],"permalink":"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models","title":"[논문리뷰] Black-Box On-Policy Distillation of Large Language Models","excerpt":"이 [arXiv]에 게시한 'Black-Box On-Policy Distillation of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Knowledge Distillation (KD)","Black-box Distillation","Generative Adversarial Networks (GANs)","On-policy Learning","Reinforcement Learning","Minimax Game","Model Compression"],"permalink":"/ai/review/2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation","title":"[논문리뷰] Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation","excerpt":"이 [arXiv]에 게시한 'Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Models","Diversity Evaluation","Human Evaluation","Attribute-Conditional","Vendi Score","Generative AI","Benchmarking"],"permalink":"/ai/review/2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models","title":"[논문리뷰] AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models","excerpt":"Zhen Li이 [arXiv]에 게시한 'AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Embodied Reasoning","Multimodal Large Language Models (MLLMs)","Chain-of-Thought (CoT)","Affordance Grounding","Motion Estimation","View Synthesis","Active Perception"],"permalink":"/ai/review/2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation","title":"[논문리뷰] WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation","excerpt":"이 [arXiv]에 게시한 'WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","UI-to-Code","Vision-Language Models","Agentic Framework","Interactive UI","Web Automation","Code Generation","UI Verification","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models","title":"[논문리뷰] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Reinforcement Learning (RL)","Model-based RL","World Models","Policy Optimization","Robotics","Sample Efficiency","Self-correction"],"permalink":"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance","title":"[논문리뷰] Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance","excerpt":"Kwanyoung Kim이 [arXiv]에 게시한 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Guidance Sampling","Optimal Transport","Sinkhorn Algorithm","Self-Attention","Adversarial Perturbation","Image Generation","ControlNet"],"permalink":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression","title":"[논문리뷰] TiDAR: Think in Diffusion, Talk in Autoregression","excerpt":"이 [arXiv]에 게시한 'TiDAR: Think in Diffusion, Talk in Autoregression' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hybrid LLM Architecture","Diffusion-Autoregressive","Parallel Token Generation","Speculative Decoding","Structured Attention Masks","LLM Inference Acceleration","KV Cache"],"permalink":"/ai/review/2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle","title":"[논문리뷰] Stemming Hallucination in Language Models Using a Licensing Oracle","excerpt":"Richard Ackermann이 [arXiv]에 게시한 'Stemming Hallucination in Language Models Using a Licensing Oracle' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hallucination Mitigation","Language Models","Knowledge Graphs","SHACL Validation","Epistemic Grounding","Retrieval-Augmented Generation","Neuro-symbolic AI"],"permalink":"/ai/review/2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-Motif-2-12-7B-technical-report","title":"[논문리뷰] Motif 2 12.7B technical report","excerpt":"이 [arXiv]에 게시한 'Motif 2 12.7B technical report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Model","LLM Efficiency","Grouped Differential Attention","Kernel Fusion","Parallel Muon","Supervised Fine-tuning","Architectural Scaling","Instruction Following"],"permalink":"/ai/review/2025-11-13-Motif-2-12-7B-technical-report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning","title":"[논문리뷰] MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning","excerpt":"이 [arXiv]에 게시한 'MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Mathematical Problem Solving","Self-Evolving","Iterative Fine-Tuning","Reward Models","Reflection","Large Language Models (LLMs)"],"permalink":"/ai/review/2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra","title":"[논문리뷰] MADD: Multi-Agent Drug Discovery Orchestra","excerpt":"이 [arXiv]에 게시한 'MADD: Multi-Agent Drug Discovery Orchestra' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Drug Discovery","LLM","Hit Identification","Virtual Screening","Generative AI","Property Prediction","Automated Machine Learning"],"permalink":"/ai/review/2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds","title":"[논문리뷰] Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds","excerpt":"이 [arXiv]에 게시한 'Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generalist Agent","3D Open World","Vision-Language Model","Imitation Learning","Real-time Inference","Hybrid Thinking","Action Chunking","Genshin Impact"],"permalink":"/ai/review/2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls","title":"[논문리뷰] LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls","excerpt":"이 [arXiv]에 게시한 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Tool Learning","Data Generation","Model Training","Closed-Loop Framework","Reinforcement Learning (RL)","Data Refinement","Self-Correction"],"permalink":"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents","title":"[논문리뷰] Agentic Refactoring: An Empirical Study of AI Coding Agents","excerpt":"Hajimu Iida이 [arXiv]에 게시한 'Agentic Refactoring: An Empirical Study of AI Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Agents","Code Refactoring","Software Engineering","Empirical Study","Large Language Models","Code Quality","Agentic Software Development","Maintainability"],"permalink":"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision","title":"[논문리뷰] Adapting Web Agents with Synthetic Supervision","excerpt":"Siwei Han이 [arXiv]에 게시한 'Adapting Web Agents with Synthetic Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Web Agents","Synthetic Data Generation","LLM","Task Refinement","Trajectory Refinement","Supervised Fine-tuning","Web Automation","Environment Adaptation"],"permalink":"/ai/review/2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora","title":"[논문리뷰] Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora","excerpt":"Mohamed Motasim Hamed이 [arXiv]에 게시한 'Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Arabic Language","Multimodal Corpus","Data Curation","Web Scraping","Large Language Models","Document Structure","Markdown","Perplexity Filtering"],"permalink":"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective","title":"[논문리뷰] Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective","excerpt":"Christoph Treude이 [arXiv]에 게시한 'Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Software Engineering","Developer Productivity","Socio-Technical Grounded Theory","Practitioner Insights","AI Adoption","Benefits and Risks","Balanced Use"],"permalink":"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","title":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'VideoSSR: Video Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Understanding","Self-Supervised Learning","Reinforcement Learning","MLLMs","Pretext Tasks","Verifiable Rewards","Data Generation","Temporal Grounding"],"permalink":"/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B","title":"[논문리뷰] Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B","excerpt":"이 [arXiv]에 게시한 'Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Small Language Models","Reasoning","Diversity Optimization","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Spectrum-to-Signal Principle (SSP)","Mathematical Reasoning","Code Generation"],"permalink":"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning","title":"[논문리뷰] TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long-form Video Understanding","Temporal Search","Reinforcement Learning","Self-Verification","Video-Language Models","Adaptive Search","Interleaved Reasoning"],"permalink":"/ai/review/2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals","title":"[논문리뷰] The Path Not Taken: RLVR Provably Learns Off the Principals","excerpt":"이 [arXiv]에 게시한 'The Path Not Taken: RLVR Provably Learns Off the Principals' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Parameter-Efficient Fine-Tuning","Optimization Bias","Spectral Geometry","Model Sparsity","LoRA"],"permalink":"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration","title":"[논문리뷰] Optimizing Diversity and Quality through Base-Aligned Model Collaboration","excerpt":"Jonathan May이 [arXiv]에 게시한 'Optimizing Diversity and Quality through Base-Aligned Model Collaboration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Generative AI","Diversity-Quality Trade-off","Model Collaboration","Inference Optimization","Routing Strategy","Text Generation"],"permalink":"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models","title":"[논문리뷰] KLASS: KL-Guided Fast Inference in Masked Diffusion Models","excerpt":"이 [arXiv]에 게시한 'KLASS: KL-Guided Fast Inference in Masked Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Masked Diffusion Models","Fast Inference","Adaptive Sampling","KL Divergence","Confidence Score","Generative AI","Efficient Sampling"],"permalink":"/ai/review/2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI","title":"[논문리뷰] Intelligence per Watt: Measuring Intelligence Efficiency of Local AI","excerpt":"이 [arXiv]에 게시한 'Intelligence per Watt: Measuring Intelligence Efficiency of Local AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Local AI","LLM Inference","Intelligence per Watt","Edge Computing","Hybrid Cloud","AI Efficiency","Hardware Benchmarking","Query Routing"],"permalink":"/ai/review/2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations","title":"[논문리뷰] Grounding Computer Use Agents on Human Demonstrations","excerpt":"이 [arXiv]에 게시한 'Grounding Computer Use Agents on Human Demonstrations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Computer Use Agents","UI Grounding","Desktop Applications","Human Demonstrations","Large-Scale Dataset","Vision-Language Models","Supervised Fine-tuning","Reinforcement Learning"],"permalink":"/ai/review/2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces","title":"[논문리뷰] DynaAct: Large Language Model Reasoning with Dynamic Action Spaces","excerpt":"Lingpeng Kong이 [arXiv]에 게시한 'DynaAct: Large Language Model Reasoning with Dynamic Action Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Sequential Reasoning","Action Space Construction","Submodular Optimization","Markov Decision Process","Monte Carlo Tree Search","Utility-Diversity Trade-off"],"permalink":"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives","title":"[논문리뷰] BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives","excerpt":"이 [arXiv]에 게시한 'BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dense Retrieval","Biomedical IR","Hard Negative Mining","Citation Networks","PubMed","Zero-shot Retrieval","Transformer Models"],"permalink":"/ai/review/2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces","title":"[논문리뷰] Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces","excerpt":"Vwani Roychowdhury이 [arXiv]에 게시한 'Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation (RAG)","Episodic Memory","Generative Semantic Workspaces (GSW)","Large Language Models (LLMs)","Question Answering (QA)","Semantic Modeling","Knowledge Graph"],"permalink":"/ai/review/2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs","title":"[논문리뷰] Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs","excerpt":"이 [arXiv]에 게시한 'Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multilingual Machine Translation","Large Language Models","Directional Degeneration","Strategic Downsampling","Parallel Multilingual Prompting","Chinese-centric MT","Cross-lingual Transfer","Instruction Tuning"],"permalink":"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems","title":"[논문리뷰] Adaptive Multi-Agent Response Refinement in Conversational Systems","excerpt":"이 [arXiv]에 게시한 'Adaptive Multi-Agent Response Refinement in Conversational Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Multi-Agent Systems","Conversational AI","Response Refinement","Dynamic Agent Selection","Persona Alignment","Factual Grounding","Coherence"],"permalink":"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models","title":"[논문리뷰] VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models","excerpt":"이 [arXiv]에 게시한 'VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Anomaly Understanding","Large Language Models","Causal Reasoning","Relation-Aware","Keyframe Sampling","Multimodal LLMs","Scene Graphs"],"permalink":"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery","title":"[논문리뷰] The Station: An Open-World Environment for AI-Driven Discovery","excerpt":"wydu이 [arXiv]에 게시한 'The Station: An Open-World Environment for AI-Driven Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Open-World Environment","Scientific Discovery","AI-Driven Research","Large Language Models","Emergent Behavior","State-of-the-Art (SOTA)"],"permalink":"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence","title":"[논문리뷰] Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence","excerpt":"이 [arXiv]에 게시한 'Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Recurrent Language Models","Pretrained Models","Model Surgery","Curriculum Learning","Test-Time Compute Scaling","Mathematics Reasoning","Efficient Training","Depth Recurrence"],"permalink":"/ai/review/2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization","title":"[논문리뷰] SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization","excerpt":"이 [arXiv]에 게시한 'SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","Reinforcement Learning","Soft-Thinking","Gumbel Reparameterization","Policy Optimization","Chain-of-Thought (CoT)","GRPO"],"permalink":"/ai/review/2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads","title":"[논문리뷰] SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?","excerpt":"Ofir Press이 [arXiv]에 게시한 'SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","소프트웨어 성능 최적화","언어 모델","저장소 수준 추론","벤치마크","실제 워크로드","코드 정확성","속도 향상","코드 최적화"],"permalink":"/ai/review/2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs","title":"[논문리뷰] Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs","excerpt":"Ziyue Li이 [arXiv]에 게시한 'Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Large Language Models (LLMs)","Router Optimization","Manifold Regularization","Generalization","Post-training Fine-tuning","Task Embedding Alignment"],"permalink":"/ai/review/2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Robot-Learning-from-a-Physical-World-Model","title":"[논문리뷰] Robot Learning from a Physical World Model","excerpt":"이 [arXiv]에 게시한 'Robot Learning from a Physical World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robot Learning","Video Generation","Physical World Model","Reinforcement Learning","Zero-shot Manipulation","Object-Centric Learning","Sim-to-Real"],"permalink":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs","title":"[논문리뷰] Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs","excerpt":"이 [arXiv]에 게시한 'Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Hierarchical Knowledge","Knowledge Traversal","Structured Prompting","Internal Representations","Alignment Tax"],"permalink":"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services","title":"[논문리뷰] RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services","excerpt":"Zijie Meng이 [arXiv]에 게시한 'RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Post-Training","Domain Adaptation","Social Networking Services","Reinforcement Learning","Supervised Fine-Tuning","Catastrophic Forgetting","Data Efficiency"],"permalink":"/ai/review/2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads","title":"[논문리뷰] Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads","excerpt":"Jiaheng Zhang이 [arXiv]에 게시한 'Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Reasoning Verification","Uncertainty Quantification (UQ)","UHeads","Process Reward Models (PRMs)","Chain-of-Thought (CoT)","Self-Supervised Learning","Computational Efficiency","Domain Generalization"],"permalink":"/ai/review/2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization","title":"[논문리뷰] RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization","excerpt":"Wenhao Huang이 [arXiv]에 게시한 'RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Generalization","Overfitting","Catastrophic Forgetting","Iterative Policy Optimization","Policy Diversity"],"permalink":"/ai/review/2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments","title":"[논문리뷰] RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments","excerpt":"Shuyue Stella Li이 [arXiv]에 게시한 'RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Language Models","Adaptive Environments","Verifiable Environments","Procedural Generation","Curriculum Learning","Generalization"],"permalink":"/ai/review/2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models","title":"[논문리뷰] Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models","excerpt":"이 [arXiv]에 게시한 'Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Speech Recognition","Large Language Models","Audio-Visual Speech Recognition","LoRA","Matryoshka Representation Learning","Elastic Inference","Parameter-Efficient Adaptation"],"permalink":"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling","title":"[논문리뷰] NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling","excerpt":"이 [arXiv]에 게시한 'NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-CAD","NURBS Modeling","Large Language Models","Geometric Deep Learning","Boundary Representation","Hybrid Representation","CAD Generation"],"permalink":"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","title":"[논문리뷰] MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs","excerpt":"이 [arXiv]에 게시한 'MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Multi-Video Understanding","Evaluation Benchmark","Video Perception","Video Reasoning","Sports Analytics","Autonomous Driving"],"permalink":"/ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings","title":"[논문리뷰] MPJudge: Towards Perceptual Assessment of Music-Induced Paintings","excerpt":"이 [arXiv]에 게시한 'MPJudge: Towards Perceptual Assessment of Music-Induced Paintings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Music-Painting Cross-Modal","Perceptual Assessment","Modality-Adaptive Normalization","Direct Preference Optimization","Cross-Modal Fusion","Dataset Annotation","Affective Computing"],"permalink":"/ai/review/2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale","title":"[논문리뷰] Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale","excerpt":"이 [arXiv]에 게시한 'Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Reasoning","Compositional AI","Vision-Language Models","Data Synthesis","Chain-of-Thought","Reinforcement Learning","Multimodal Transfer","Grounded Reasoning"],"permalink":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks","title":"[논문리뷰] Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks","excerpt":"이 [arXiv]에 게시한 'Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text Embedding","Multilingual","Cross-Lingual","Contrastive Learning","Model Merging","Synthetic Data Generation","Instruction-Tuning","LLM"],"permalink":"/ai/review/2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs","title":"[논문리뷰] LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs","excerpt":"Jason Cong이 [arXiv]에 게시한 'LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","FPGA","Large Language Models (LLM)","Inference Optimization","Memory-based Computation","Vector Quantization","Table Lookup","Hardware Acceleration"],"permalink":"/ai/review/2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction","title":"[논문리뷰] IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction","excerpt":"Haotian Xu이 [arXiv]에 게시한 'IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long-Horizon Agents","Markov Decision Process","Workspace Reconstruction","Reinforcement Learning","Context Management","Iterative Deep Research","LLM Agents","Efficiency-Aware Policy Optimization"],"permalink":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents","title":"[논문리뷰] HaluMem: Evaluating Hallucinations in Memory Systems of Agents","excerpt":"이 [arXiv]에 게시한 'HaluMem: Evaluating Hallucinations in Memory Systems of Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Memory Systems","AI Agents","Hallucination Detection","Evaluation Benchmark","Long-term Memory","Memory Extraction","Memory Updating","Question Answering"],"permalink":"/ai/review/2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions","title":"[논문리뷰] Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions","excerpt":"이 [arXiv]에 게시한 'Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Structured Captions","LLM Fusion","Controllability","Image Generation Evaluation","Diffusion Models","DimFusion","TaBR"],"permalink":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience","title":"[논문리뷰] FLEX: Continuous Agent Evolution via Forward Learning from Experience","excerpt":"Jiangjie Chen이 [arXiv]에 게시한 'FLEX: Continuous Agent Evolution via Forward Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Continuous Learning","Experience Library","Forward Learning","Meta-MDP","Knowledge Distillation","Non-parametric Adaptation"],"permalink":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning","title":"[논문리뷰] Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning","excerpt":"이 [arXiv]에 게시한 'Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Emotion Recognition in Conversation","Large Language Models","Prompt Engineering","Demonstration Retrieval","Curriculum Learning","Fine-tuning","Affective Computing","SOTA"],"permalink":"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents","title":"[논문리뷰] DigiData: Training and Evaluating General-Purpose Mobile Control Agents","excerpt":"이 [arXiv]에 게시한 'DigiData: Training and Evaluating General-Purpose Mobile Control Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mobile Control Agents","User Interface Automation","Large-Scale Dataset","Benchmarking","LLM Judges","Data Diversity","Task Success Rate"],"permalink":"/ai/review/2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models","title":"[논문리뷰] Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models","excerpt":"Zhao Xu이 [arXiv]에 게시한 'Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Direct Preference Optimization (DPO)","Safeguarded Learning","Text-to-Image Generation","Preference Alignment","Generative Models","Stable Diffusion"],"permalink":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation","title":"[논문리뷰] DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation","excerpt":"이 [arXiv]에 게시한 'DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Reward","Competitive Programming","Code Generation","Data Curation","Curriculum Learning","Supervised Fine-tuning","Entropy Expansion"],"permalink":"/ai/review/2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects","title":"[논문리뷰] DIMO: Diverse 3D Motion Generation for Arbitrary Objects","excerpt":"Kostas Daniilidis이 [arXiv]에 게시한 'DIMO: Diverse 3D Motion Generation for Arbitrary Objects' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Motion Generation","Generative Models","Arbitrary Objects","Neural Key Points","Latent Space","4D Content Generation","Diffusion Models","3D Gaussian Splatting"],"permalink":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries","title":"[논문리뷰] Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries","excerpt":"Zhengzhong Tu이 [arXiv]에 게시한 'Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Spatial Reasoning","Controllable Framework","RLVR","GRPO","Maze Navigation","Generalization Boundaries"],"permalink":"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models","title":"[논문리뷰] 10 Open Challenges Steering the Future of Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 '10 Open Challenges Steering the Future of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Embodied AI","Robotics","Multimodal Perception","Cross-Robot Generalization","Hierarchical Planning","World Models","Robot Safety"],"permalink":"/ai/review/2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-Visual-Spatial-Tuning","title":"[논문리뷰] Visual Spatial Tuning","excerpt":"이 [arXiv]에 게시한 'Visual Spatial Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Spatial Reasoning","Spatial Perception","Dataset Creation","Reinforcement Learning","Visuospatial AI","Robotics"],"permalink":"/ai/review/2025-11-10-Visual-Spatial-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks","title":"[논문리뷰] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks","excerpt":"이 [arXiv]에 게시한 'VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Neuro-symbolic AI","Chain-of-Thought","Large Language Models","Logical Consistency","Automated Verification","Fine-tuning","SMT Solvers","Self-Reflection"],"permalink":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings","title":"[논문리뷰] Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings","excerpt":"Jiaxin Yuan이 [arXiv]에 게시한 'Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hallucination Mitigation","Large Vision-Language Models","Textual Embeddings","Multimodal Reasoning","Attention Mechanism","Visual Grounding","Modality Imbalance"],"permalink":"/ai/review/2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains","title":"[논문리뷰] Too Good to be Bad: On the Failure of LLMs to Role-Play Villains","excerpt":"이 [arXiv]에 게시한 'Too Good to be Bad: On the Failure of LLMs to Role-Play Villains' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","Role-playing","Safety Alignment","Villain","Persona Simulation","Moral Alignment","Benchmark","Character Fidelity"],"permalink":"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments","title":"[논문리뷰] Real-Time Reasoning Agents in Evolving Environments","excerpt":"이 [arXiv]에 게시한 'Real-Time Reasoning Agents in Evolving Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Real-time Reasoning","LLM Agents","Dynamic Environments","Dual-System AI","AgileThinker","Reactive Planning","Cognitive Load","Time Pressure"],"permalink":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-Jailbreaking-in-the-Haystack","title":"[논문리뷰] Jailbreaking in the Haystack","excerpt":"Alexander Robey이 [arXiv]에 게시한 'Jailbreaking in the Haystack' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Jailbreaking","LLM Safety","Long-Context Models","Positional Bias","Attack Success Rate (ASR)","Prompt Engineering","Compute Efficiency","AI Agents"],"permalink":"/ai/review/2025-11-10-Jailbreaking-in-the-Haystack/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent","title":"[논문리뷰] HAFixAgent: History-Aware Automated Program Repair Agent","excerpt":"Ahmed E. Hassan이 [arXiv]에 게시한 'HAFixAgent: History-Aware Automated Program Repair Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Automated Program Repair","AI Agent","Large Language Models","Repository Mining","Historical Context","Bug Fixing","Defects4J"],"permalink":"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-Dense-Motion-Captioning","title":"[논문리뷰] Dense Motion Captioning","excerpt":"Paolo Rota이 [arXiv]에 게시한 'Dense Motion Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Human Motion","Dense Captioning","Large Language Models","Motion Understanding","Temporal Localization","Human-Language Datasets","Motion Generation"],"permalink":"/ai/review/2025-11-10-Dense-Motion-Captioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model","title":"[논문리뷰] DeepEyesV2: Toward Agentic Multimodal Model","excerpt":"Guohai Xu이 [arXiv]에 게시한 'DeepEyesV2: Toward Agentic Multimodal Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic AI","Multimodal Models","Tool Use","Reinforcement Learning","Supervised Fine-tuning","Multimodal Reasoning","Web Search","Code Execution"],"permalink":"/ai/review/2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration","title":"[논문리뷰] CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?","excerpt":"Baixuan Xu이 [arXiv]에 게시한 'CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Calibration","Confidence Calibration","Uncertainty Estimation","Critique Learning","Supervised Fine-Tuning","Natural Language Processing","Self-Critique"],"permalink":"/ai/review/2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-V-Thinker-Interactive-Thinking-with-Images","title":"[논문리뷰] V-Thinker: Interactive Thinking with Images","excerpt":"Peiqing Yang이 [arXiv]에 게시한 'V-Thinker: Interactive Thinking with Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Large Multimodal Models","Interactive Reasoning","Vision-Centric Thinking","Reinforcement Learning","Data Synthesis","Visual Tools","Curriculum Learning","Multimodal AI"],"permalink":"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm","title":"[논문리뷰] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm","excerpt":"이 [arXiv]에 게시한 'Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Video Generation","Multimodal Reasoning","Temporal Understanding","Spatial Reasoning","Foundation Models","AI Benchmarking","In-Context Learning","Self-Consistency"],"permalink":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms","title":"[논문리뷰] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms","excerpt":"Susumu Takeuchi이 [arXiv]에 게시한 'The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Strong Lottery Ticket Hypothesis","Multi-Head Attention","Transformers","Neural Network Pruning","Overparameterization","Weight Initialization","Model Compression"],"permalink":"/ai/review/2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis","title":"[논문리뷰] Scaling Agent Learning via Experience Synthesis","excerpt":"이 [arXiv]에 게시한 'Scaling Agent Learning via Experience Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Agents","Experience Synthesis","World Models","Curriculum Learning","Sim-to-Real Transfer","Web Agents"],"permalink":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding","title":"[논문리뷰] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding","excerpt":"이 [arXiv]에 게시한 'SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Spatial Reasoning","Video Understanding","Simulated Data","Instruction Tuning","Multimodal LLMs","Sim-to-Real Transfer","AI2-THOR"],"permalink":"/ai/review/2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning","title":"[논문리뷰] SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning","excerpt":"이 [arXiv]에 게시한 'SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Reinforcement Learning","Post-training","Reasoning","Dual-Reward System","Thinking Reward","Judging Reward","Hallucination Reduction"],"permalink":"/ai/review/2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems","title":"[논문리뷰] RDMA Point-to-Point Communication for LLM Systems","excerpt":"이 [arXiv]에 게시한 'RDMA Point-to-Point Communication for LLM Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","RDMA","LLM","Point-to-Point Communication","Disaggregated Inference","MoE Routing","KvCache","AWS EFA","NVIDIA ConnectX"],"permalink":"/ai/review/2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-NVIDIA-Nemotron-Nano-V2-VL","title":"[논문리뷰] NVIDIA Nemotron Nano V2 VL","excerpt":"이 [arXiv]에 게시한 'NVIDIA Nemotron Nano V2 VL' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Hybrid Architecture","Mamba-Transformer","Long-Context Understanding","Quantization","Efficient Inference","Document AI","Video AI"],"permalink":"/ai/review/2025-11-7-NVIDIA-Nemotron-Nano-V2-VL/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots","title":"[논문리뷰] Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots","excerpt":"이 [arXiv]에 게시한 'Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Humanoid Robot","Reinforcement Learning","RoboCup","Soccer Skills","Vision-Driven Control","Adversarial Motion Priors","Sim-to-Real","Perception-Action Coordination"],"permalink":"/ai/review/2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics","title":"[논문리뷰] How to Evaluate Speech Translation with Source-Aware Neural MT Metrics","excerpt":"Luisa Bentivogli이 [arXiv]에 게시한 'How to Evaluate Speech Translation with Source-Aware Neural MT Metrics' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Speech Translation","Neural MT Metrics","Source-Aware Evaluation","Automatic Speech Recognition (ASR)","Back-Translation (BT)","Cross-lingual Re-segmentation","COMET","MetricX"],"permalink":"/ai/review/2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents","title":"[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents","excerpt":"이 [arXiv]에 게시한 'GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Computer-Using Agents","GUI Grounding","Screen Parsing","Action Prediction","Desktop Automation","Dataset","Benchmark","Multimodal Learning","LLM-augmented Data"],"permalink":"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference","title":"[논문리뷰] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference","excerpt":"이 [arXiv]에 게시한 'EVTAR: End-to-End Try on with Additional Unpaired Visual Reference' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Virtual Try-on","Diffusion Models","End-to-End Learning","Reference Images","Unpaired Data","Flow Matching","Transformer Architecture","Generative AI"],"permalink":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation","title":"[논문리뷰] Contamination Detection for VLMs using Multi-Modal Semantic Perturbation","excerpt":"이 [arXiv]에 게시한 'Contamination Detection for VLMs using Multi-Modal Semantic Perturbation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","VLM Contamination","Test-set Leakage","Multi-modal Perturbation","Generative Models","Generalization","Model Memorization","VLMs"],"permalink":"/ai/review/2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video","title":"[논문리뷰] Cambrian-S: Towards Spatial Supersensing in Video","excerpt":"Zihao Yang이 [arXiv]에 게시한 'Cambrian-S: Towards Spatial Supersensing in Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Spatial Supersensing","Video Understanding","Multimodal LLMs","Predictive Sensing","Memory Management","Event Segmentation","VSI-SUPER","Instruction Tuning"],"permalink":"/ai/review/2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts","title":"[논문리뷰] Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts","excerpt":"이 [arXiv]에 게시한 'Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Benchmark Design","Non-Visual Shortcuts","Test-Set Stress-Test","Bias Mitigation","Model Evaluation","Benchmark Robustness"],"permalink":"/ai/review/2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions","title":"[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions","excerpt":"이 [arXiv]에 게시한 'UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Joint Audio-Video Generation","Cross-Modal Interaction","Diffusion Transformer","Face-Aware Modulation","Classifier-Free Guidance","Multimodal AI","Generative Models"],"permalink":"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute","title":"[논문리뷰] The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute","excerpt":"이 [arXiv]에 게시한 'The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Sequential Reasoning","Parallel Self-Consistency","Inverse-Entropy Voting","LLM Reasoning","Test-Time Scaling","Inference Optimization","Iterative Refinement","Error Correction"],"permalink":"/ai/review/2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models","title":"[논문리뷰] TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models","excerpt":"이 [arXiv]에 게시한 'TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Tabular Foundation Models","Fine-Tuning","PEFT","Meta-Learning","Calibration","Fairness","Unified Library","Benchmarking"],"permalink":"/ai/review/2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning","title":"[논문리뷰] Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning","excerpt":"이 [arXiv]에 게시한 'Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Tabular Data","In-Context Learning","Multi-Scale Attention","Sparse Attention","Foundation Models","Perceiver Architecture"],"permalink":"/ai/review/2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity","title":"[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity","excerpt":"이 [arXiv]에 게시한 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Benchmark","Cognitive Capacity","Visual Reasoning","MLLM Evaluation","Error Analysis","Chain-of-Thought"],"permalink":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models","title":"[논문리뷰] LiveTradeBench: Seeking Real-World Alpha with Large Language Models","excerpt":"Jiaxuan You이 [arXiv]에 게시한 'LiveTradeBench: Seeking Real-World Alpha with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Live Trading","Portfolio Management","Financial AI","Prediction Markets","Real-World Uncertainty","Agent Benchmarking"],"permalink":"/ai/review/2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation","title":"[논문리뷰] Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation","excerpt":"Jaehyun Park이 [arXiv]에 게시한 'Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Multimodal Embedders","Query Augmentation","Adaptive Learning","Multimodal LLM","Information Retrieval","Generative AI","Embedding Latency"],"permalink":"/ai/review/2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation","title":"[논문리뷰] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation","excerpt":"Soohyun Oh이 [arXiv]에 게시한 'LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","3D Scene Synthesis","Fine-Grained Evaluation","Tool-Augmented LLMs","Embodied AI","Vision-Language Models","Benchmark","Multi-Hop Grounding"],"permalink":"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects","title":"[논문리뷰] Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects","excerpt":"이 [arXiv]에 게시한 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Articulated Objects","Kinematics Inference","High-DoF","Monte Carlo Tree Search","Joint Parameter Optimization","SDF","Open-Vocabulary Synthesis","Robot Self-Modeling"],"permalink":"/ai/review/2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper","title":"[논문리뷰] Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper","excerpt":"이 [arXiv]에 게시한 'Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","AI Scientist","Autonomous Research","Scientific Automation","LLM for Research","Code Generation","Experimental Design","Risk Assessment"],"permalink":"/ai/review/2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask","title":"[논문리뷰] Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask","excerpt":"이 [arXiv]에 게시한 'Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Dialogue Systems","Common Ground","Misunderstanding","Annotation Scheme","MapTask Corpus","Large Language Models","Perspective Taking","Reference Resolution"],"permalink":"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners","title":"[논문리뷰] Diffusion Language Models are Super Data Learners","excerpt":"이 [arXiv]에 게시한 'Diffusion Language Models are Super Data Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Autoregressive Models","Data Efficiency","Scaling Laws","Data-Constrained Learning","Crossover Phenomenon","Pre-training","Masked Diffusion"],"permalink":"/ai/review/2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents","title":"[논문리뷰] CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents","excerpt":"Shijue Huang이 [arXiv]에 게시한 'CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","LLM Agents","Tool Use","Cost-Optimal Planning","Dynamic Environments","Benchmarking","Multi-Turn Interaction","Economic Reasoning"],"permalink":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-iFlyBot-VLA-Technical-Report","title":"[논문리뷰] iFlyBot-VLA Technical Report","excerpt":"Jiajia wu이 [arXiv]에 게시한 'iFlyBot-VLA Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","Imitation Learning","Latent Actions","Diffusion Models","Dual-Arm Manipulation","Pretraining","Flow-Matching"],"permalink":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought","title":"[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought","excerpt":"이 [arXiv]에 게시한 'When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Multimodal AI","Visual Reasoning","Chain-of-Thought (CoT)","Benchmark","Image Generation","MLLMs","Visual-CoT"],"permalink":"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs","title":"[논문리뷰] When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs","excerpt":"Haotian Wang이 [arXiv]에 게시한 'When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Modality Following","Unimodal Uncertainty","Modality Preference","Conflict Resolution","Internal Mechanism","Entropy","Controllable Dataset"],"permalink":"/ai/review/2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models","title":"[논문리뷰] VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models","excerpt":"Pengfei Wan이 [arXiv]에 게시한 'VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","VideoLLMs","Emotion Understanding","Affective-Tree Reasoning","Curriculum Learning","Reinforcement Learning","Fine-Grained Emotion","Attribute Perception","Expression Analysis"],"permalink":"/ai/review/2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation","title":"[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation","excerpt":"이 [arXiv]에 게시한 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Multimodal AI","Code Generation","SVG","Visual Representation","Benchmark","Large Vision-Language Models","Agentic AI","Reasoning"],"permalink":"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-The-Collaboration-Gap","title":"[논문리뷰] The Collaboration Gap","excerpt":"이 [arXiv]에 게시한 'The Collaboration Gap' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","AI Collaboration","Multi-Agent Systems","Large Language Models (LLMs)","Maze Solving","Heterogeneous Agents","Collaboration Gap","Relay Inference","Agentic AI"],"permalink":"/ai/review/2025-11-5-The-Collaboration-Gap/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data","title":"[논문리뷰] TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data","excerpt":"Jin Zeng이 [arXiv]에 게시한 'TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Tabular Data","Numerical Reasoning","Large Language Models (LLMs)","Table Question Answering (TQA)","Program-of-Thoughts (PoT)","Data Sanitization","Query Decomposition","Multi-hop Reasoning"],"permalink":"/ai/review/2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System","title":"[논문리뷰] TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System","excerpt":"Rocky Duan이 [arXiv]에 게시한 'TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Humanoid Robotics","Data Collection","Teleoperation","Full-Body Control","Visuomotor Policy Learning","VR","Portable MoCap-Free"],"permalink":"/ai/review/2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Step-Audio-EditX-Technical-Report","title":"[논문리뷰] Step-Audio-EditX Technical Report","excerpt":"이 [arXiv]에 게시한 'Step-Audio-EditX Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM-based Audio Model","Audio Editing","Text-to-Speech (TTS)","Zero-shot Learning","Large-Margin Data","Reinforcement Learning (RLHF)","Emotion Control","Speaking Style Transfer"],"permalink":"/ai/review/2025-11-5-Step-Audio-EditX-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR","title":"[논문리뷰] Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR","excerpt":"이 [arXiv]에 게시한 'Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLMs","RLVR","Length Regularization","Mathematical Reasoning","Data Curation","Model Efficiency","Emergent Brevity"],"permalink":"/ai/review/2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies","title":"[논문리뷰] RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies","excerpt":"이 [arXiv]에 게시한 'RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Robotics","Real-robot Evaluation","Embodied AI","Vision-Language-Action Models","Benchmarking","Online Testing System","Robotics Control","Large-scale Evaluation"],"permalink":"/ai/review/2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs","title":"[논문리뷰] RiddleBench: A New Generative Reasoning Benchmark for LLMs","excerpt":"이 [arXiv]에 게시한 'RiddleBench: A New Generative Reasoning Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Generative AI","Benchmark","Logical Deduction","Spatial Reasoning","Constraint Satisfaction","Hallucination Cascade","Self-Correction"],"permalink":"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","title":"[논문리뷰] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation","excerpt":"이 [arXiv]에 게시한 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Video Generation","Direct Preference Optimization","SFT Regularization","GT-Pair","Memory Optimization","Diffusion Models","I2V","T2V"],"permalink":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context","title":"[논문리뷰] LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context","excerpt":"Tianxin Zhang이 [arXiv]에 게시한 'LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM Safety","AI Safety Benchmark","Chinese Context","Dynamic Evaluation","Cultural Relevance","Adversarial Robustness","ELO Rating System"],"permalink":"/ai/review/2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw","title":"[논문리뷰] LTD-Bench: Evaluating Large Language Models by Letting Them Draw","excerpt":"이 [arXiv]에 게시한 'LTD-Bench: Evaluating Large Language Models by Letting Them Draw' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Spatial Reasoning","Benchmark","Generative AI","Visual Perception","Spatial Imagination","Code Generation"],"permalink":"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs","title":"[논문리뷰] Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs","excerpt":"Bo Bai이 [arXiv]에 게시한 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Semantic Information Theory","Large Language Models","Directed Information","Rate-Distortion Function","Granger Causality","Token Embedding","Transformer Architecture","Variational Inference"],"permalink":"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization","title":"[논문리뷰] Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization","excerpt":"Aleksandr I. Panov이 [arXiv]에 게시한 'Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","OOD Generalization","Representation Alignment","Fine-tuning","Robotics","Visual Representations","Attention Maps","t-SNE"],"permalink":"/ai/review/2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning","title":"[논문리뷰] Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning","excerpt":"Zuyi Xiong이 [arXiv]에 게시한 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Depth Estimation","Ego-Motion Estimation","Motion Component Discrimination","Geometric Constraints","Optical Flow","PoseNet","DepthNet"],"permalink":"/ai/review/2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering","title":"[논문리뷰] CodeClash: Benchmarking Goal-Oriented Software Engineering","excerpt":"이 [arXiv]에 게시한 'CodeClash: Benchmarking Goal-Oriented Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Software Engineering Benchmarking","Language Models","AI Agents","Goal-Oriented Development","Competitive Programming","Code Evolution","Strategic Reasoning","Autonomous Systems"],"permalink":"/ai/review/2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension","title":"[논문리뷰] ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension","excerpt":"Hao Wang이 [arXiv]에 게시한 'ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Chart Comprehension","Visual Reasoning","Data Generation","Code-Driven Pipeline","Multimodal LLMs","Retrieval-Augmented Generation","Reinforcement Learning","Synthetic Data"],"permalink":"/ai/review/2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models","title":"[논문리뷰] Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models","excerpt":"Shijie Dong이 [arXiv]에 게시한 'Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Large Multimodal Models","Visual Token Compression","Token Pruning","Benchmark","Efficiency","Inference Latency","Multimodal LLMs"],"permalink":"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer","title":"[논문리뷰] Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer","excerpt":"이 [arXiv]에 게시한 'Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","fMRI","Image Reconstruction","Brain-Computer Interface","Transformer","Diffusion Models","Neural Decoding","Cross-Subject Learning","Deep Image Prior"],"permalink":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring","title":"[논문리뷰] BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring","excerpt":"이 [arXiv]에 게시한 'BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Alzheimer's Disease","Retrieval-Augmented Generation (RAG)","Large Language Models (LLMs)","Clinical Decision Support","Multimodal Data Fusion","Cognitive Decline Detection","Early Diagnosis"],"permalink":"/ai/review/2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda","title":"[논문리뷰] AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda","excerpt":"이 [arXiv]에 게시한 'AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Ayurveda LLM","Domain Adaptation","Bilingual Language Model","Instruction Tuning","Medical AI","Knowledge-Grounded QA","Traditional Medicine"],"permalink":"/ai/review/2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles","title":"[논문리뷰] left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles","excerpt":"Deepiha S이 [arXiv]에 게시한 'left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Multimodal Benchmark","Rebus Puzzles","In-Context Learning","Reasoning","ControlNet","Prompt Engineering"],"permalink":"/ai/review/2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI","title":"[논문리뷰] World Simulation with Video Foundation Models for Physical AI","excerpt":"Junjie Bai이 [arXiv]에 게시한 'World Simulation with Video Foundation Models for Physical AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Physical AI","World Simulation","Video Foundation Models","Flow Matching","Reinforcement Learning","Robotics","Autonomous Driving","Synthetic Data Generation"],"permalink":"/ai/review/2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers","title":"[논문리뷰] Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers","excerpt":"이 [arXiv]에 게시한 'Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Video Retrieval","Vision-Language Models (VLMs)","Zero-Shot Learning","List-wise Reranking","Rank Fusion","Prompt Engineering","S-Grid","Multimodal Retrieval"],"permalink":"/ai/review/2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process","title":"[논문리뷰] Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process","excerpt":"이 [arXiv]에 게시한 'Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Diffusion Models","Discrete Denoising","Multimodal Learning","Robotics","Embodied AI","Joint Generation","Action Prediction"],"permalink":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark","title":"[논문리뷰] UniREditBench: A Unified Reasoning-based Image Editing Benchmark","excerpt":"이 [arXiv]에 게시한 'UniREditBench: A Unified Reasoning-based Image Editing Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Image Editing","Reasoning-based AI","Benchmark","Multimodal Learning","Chain-of-Thought (CoT)","Dual-Reference Evaluation","Generative Models","Game AI"],"permalink":"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback","title":"[논문리뷰] UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback","excerpt":"이 [arXiv]에 게시한 'UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Relighting","Diffusion Models","Flow Matching","Physics-Plausible Feedback","Image-to-Video","Geometric Supervision","Path Consistency Learning","LumosBench"],"permalink":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings","title":"[논문리뷰] UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings","excerpt":"Jinsong Su이 [arXiv]에 게시한 'UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal Embeddings","Generative AI","Reasoning","Reinforcement Learning","MLLMs","Supervised Fine-tuning","Information Retrieval","Unified Embeddings"],"permalink":"/ai/review/2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval","title":"[논문리뷰] Trove: A Flexible Toolkit for Dense Retrieval","excerpt":"이 [arXiv]에 게시한 'Trove: A Flexible Toolkit for Dense Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Dense Retrieval","Retrieval Toolkit","Data Management","Distributed Training","Model Customization","Hard Negative Mining","Hugging Face Integration","Performance Optimization"],"permalink":"/ai/review/2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum","title":"[논문리뷰] Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum","excerpt":"이 [arXiv]에 게시한 'Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Video Retrieval","Multimodal Embedding","Data Synthesis","Curriculum Learning","Zero-shot Generalization","Benchmark Design","MLLM","Video-Text Retrieval"],"permalink":"/ai/review/2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Towards-Robust-Mathematical-Reasoning","title":"[논문리뷰] Towards Robust Mathematical Reasoning","excerpt":"Yuri Chervonyi이 [arXiv]에 게시한 'Towards Robust Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Large Language Models (LLMs)","AI Benchmarks","International Mathematical Olympiad (IMO)","Proof Verification","Automatic Grading","Robustness"],"permalink":"/ai/review/2025-11-4-Towards-Robust-Mathematical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use","title":"[논문리뷰] ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use","excerpt":"Guanting Dong이 [arXiv]에 게시한 'ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal Agents","Tool-Augmented LLMs","Vision-Guided Reasoning","Long-Horizon Tasks","VQA","Global Planning","Context Preservation","Perceive Tool"],"permalink":"/ai/review/2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding","title":"[논문리뷰] The Underappreciated Power of Vision Models for Graph Structural Understanding","excerpt":"Lei Zhang이 [arXiv]에 게시한 'The Underappreciated Power of Vision Models for Graph Structural Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Graph Neural Networks","Vision Models","Graph Understanding","Topological Perception","GraphAbstract Benchmark","OOD Generalization","Graph Visualization"],"permalink":"/ai/review/2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning","title":"[논문리뷰] TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning","excerpt":"Shaoheng Lin이 [arXiv]에 게시한 'TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Agentic Reasoning","Thinking-with-Images","Visual Reasoning Benchmark","Tool Use","Image Manipulation","Fine-tuning"],"permalink":"/ai/review/2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation","title":"[논문리뷰] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation","excerpt":"Feng Li이 [arXiv]에 게시한 'ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal AI","Benchmarking","Cross-Modal Reasoning","Omnimodal Generation","Visual Generation","Verbal Generation","Unified Multimodal Models"],"permalink":"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset","title":"[논문리뷰] PHUMA: Physically-Grounded Humanoid Locomotion Dataset","excerpt":"이 [arXiv]에 게시한 'PHUMA: Physically-Grounded Humanoid Locomotion Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Humanoid Locomotion","Dataset","Motion Imitation","Physics-based Control","Motion Retargeting","Data Curation","Reinforcement Learning","Inverse Kinematics"],"permalink":"/ai/review/2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner","title":"[논문리뷰] OpenSIR: Open-Ended Self-Improving Reasoner","excerpt":"이 [arXiv]에 게시한 'OpenSIR: Open-Ended Self-Improving Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Open-Ended Learning","Self-Play","Reinforcement Learning","Large Language Models","Mathematical Reasoning","Problem Generation","Curriculum Learning","Reward Shaping"],"permalink":"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models","title":"[논문리뷰] NaviTrace: Evaluating Embodied Navigation of Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'NaviTrace: Evaluating Embodied Navigation of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Embodied Navigation","VQA Benchmark","Robotic Navigation","Semantic-aware Score","Dynamic Time Warping","Real-world Scenarios"],"permalink":"/ai/review/2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement","title":"[논문리뷰] Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement","excerpt":"Isabelle Augenstein이 [arXiv]에 게시한 'Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","LLMs","Knowledge Interaction","Parametric Knowledge","Contextual Knowledge","Subspace Disentanglement","NLE Generation","Hallucination Detection","Chain-of-Thought"],"permalink":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls","title":"[논문리뷰] MotionStream: Real-Time Video Generation with Interactive Motion Controls","excerpt":"이 [arXiv]에 게시한 'MotionStream: Real-Time Video Generation with Interactive Motion Controls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Real-Time Video Generation","Motion Control","Diffusion Models","Autoregressive Generation","Self-Forcing","Attention Sink","Streaming Inference","Video Distillation"],"permalink":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models","title":"[논문리뷰] MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models","excerpt":"Bin Yu이 [arXiv]에 게시한 'MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Factuality Alignment","Meta-Reasoning","Kahneman-Tversky Optimization","Chain-of-Thought","Hallucination","Process-Level Alignment"],"permalink":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-LongCat-Flash-Omni-Technical-Report","title":"[논문리뷰] LongCat-Flash-Omni Technical Report","excerpt":"Bin Xiao이 [arXiv]에 게시한 'LongCat-Flash-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Omni-modal AI","Multimodal LLM","Real-time Interaction","Mixture-of-Experts (MoE)","Streaming Inference","Distributed Training","Curriculum Learning","Audio-Visual Perception"],"permalink":"/ai/review/2025-11-4-LongCat-Flash-Omni-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment","title":"[논문리뷰] How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment","excerpt":"Yuhao Zhai이 [arXiv]에 게시한 'How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Video Generation","World Models","Surgical AI","Zero-shot Prediction","Expert Evaluation","Plausibility Gap","Medical Simulation"],"permalink":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph","title":"[논문리뷰] Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph","excerpt":"이 [arXiv]에 게시한 'Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Test-Time Scaling","LLMs","Graph Optimization","REINFORCE","Multi-agent Systems","Adaptive Architectures","Compute-optimal Scaling","Probabilistic Graphs"],"permalink":"/ai/review/2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding","title":"[논문리뷰] GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding","excerpt":"Wanrong Zhu이 [arXiv]에 게시한 'GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","GUI Grounding","Multimodal Attention","MLLMs","Coordinate-Free","Visual Grounding","Attention Weighting","Anchor Token"],"permalink":"/ai/review/2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation","title":"[논문리뷰] Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation","excerpt":"이 [arXiv]에 게시한 'Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Large Language Models","Mixture-of-Experts","Reasoning Capability","Sparse Activation","Scaling Laws","FP8 Training","Efficient Training","Instruction Tuning"],"permalink":"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities","title":"[논문리뷰] EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities","excerpt":"Yunxin Liu이 [arXiv]에 게시한 'EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Energy-Based Models (EBMs)","Diffusion Policy","Robotics","Behavior Cloning","Physical Reasoning","Uncertainty Modeling","Emergent Behavior","Robot Manipulation"],"permalink":"/ai/review/2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench","title":"[논문리뷰] Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench","excerpt":"이 [arXiv]에 게시한 'Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Benchmarking","Visual Measurement Reading","Synthetic Data Generation","Fine-grained Perception","Spatial Grounding","Reinforcement Learning"],"permalink":"/ai/review/2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance","title":"[논문리뷰] Data-Efficient RLVR via Off-Policy Influence Guidance","excerpt":"Jiale Cheng이 [arXiv]에 게시한 'Data-Efficient RLVR via Off-Policy Influence Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Influence Functions","Data Selection","Off-Policy Learning","Curriculum Learning","Large Language Models (LLMs)","Sparse Random Projection","Data Efficiency"],"permalink":"/ai/review/2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence","title":"[논문리뷰] AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence","excerpt":"Peter Worth이 [arXiv]에 게시한 'AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","LLM Benchmarking","Cyber Threat Intelligence (CTI)","Dynamic Evaluation","CTI Reasoning","Vulnerability Prediction","Threat Actor Attribution","Risk Mitigation","Natural Language Processing"],"permalink":"/ai/review/2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models","title":"[논문리뷰] Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models","excerpt":"Changfeng Ma이 [arXiv]에 게시한 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Spatial Reasoning","Viewpoint Learning","Two-Stage Fine-tuning","3D Consistency","Viewpoint-100K","Reinforcement Learning"],"permalink":"/ai/review/2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models","title":"[논문리뷰] π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Vision-Language-Action Models (VLAs)","Flow-based Models","Policy Optimization","Robotics","Flow Matching","SDE","MDP"],"permalink":"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning","title":"[논문리뷰] Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning","excerpt":"Hanyang Chen이 [arXiv]에 게시한 'Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Visual Backdoor Attacks","MLLM Embodied Agents","Contrastive Trigger Learning","Policy Manipulation","Adversarial AI","Embodied AI Security","Multimodal LLMs"],"permalink":"/ai/review/2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training","title":"[논문리뷰] Value Drifts: Tracing Value Alignment During LLM Post-Training","excerpt":"이 [arXiv]에 게시한 'Value Drifts: Tracing Value Alignment During LLM Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","LLM Alignment","Value Drift","Supervised Fine-Tuning (SFT)","Preference Optimization","RLHF","Llama-3","Qwen-3","Human Values"],"permalink":"/ai/review/2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning","title":"[논문리뷰] Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Self-supervised learning","Reinforcement Learning","Spatial Understanding","Vision-Language Models","Pretext Tasks","RGB-D Images","Spatial Reasoning"],"permalink":"/ai/review/2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens","title":"[논문리뷰] SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens","excerpt":"이 [arXiv]에 게시한 'SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Chain-of-Thought (CoT)","Implicit Reasoning","LLMs","Semantic Alignment","Efficiency Optimization","Knowledge Distillation"],"permalink":"/ai/review/2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models","title":"[논문리뷰] Revisiting Multimodal Positional Encoding in Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Revisiting Multimodal Positional Encoding in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Multimodal Positional Encoding","Vision-Language Models","Rotary Positional Embedding (RoPE)","Transformer","Multimodal Understanding","Visual Grounding","Frequency Allocation","Position Design"],"permalink":"/ai/review/2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning","title":"[논문리뷰] Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Conversational Recommender Systems","Large Language Models","Reinforcement Learning","Group Relative Policy Optimization","Rank-based Learning","Supervised Fine-tuning","Reward Shaping"],"permalink":"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals","title":"[논문리뷰] Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals","excerpt":"이 [arXiv]에 게시한 'Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Distribution Matching Distillation","Few-step Diffusion","Score Matching","Mixture-of-Experts","Generative Models","Image Generation","Video Generation","Model Distillation"],"permalink":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows","title":"[논문리뷰] OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows","excerpt":"이 [arXiv]에 게시한 'OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Mobile GUI Agents","Agent Safety","Hybrid Detection","Formal Verification","VLM-based Contextual Judgment","Safety Benchmark","Risk Detection"],"permalink":"/ai/review/2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games","title":"[논문리뷰] Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games","excerpt":"cavaunpeu이 [arXiv]에 게시한 'Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Bounded One-Sided Response Games (BORGs)","Monopoly Deal","Benchmark Environment","Counterfactual Regret Minimization (CFR)","Imperfect Information Games","Game Theory","Self-Play","State Abstraction"],"permalink":"/ai/review/2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data","title":"[논문리뷰] MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data","excerpt":"Nadiya Shvai이 [arXiv]에 게시한 'MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Health Misinformation","Logical Fallacy Classification","Synthetic Data Generation","Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Parameter-Efficient Fine-tuning (PEFT)","LoRA","MISSCI Benchmark"],"permalink":"/ai/review/2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery","title":"[논문리뷰] Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery","excerpt":"Oğuz Hanoğlu이 [arXiv]에 게시한 'Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Building Instance Segmentation","Height Classification","YOLOv11","Satellite Imagery","Multitask Learning","Remote Sensing","Urban Planning"],"permalink":"/ai/review/2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning","title":"[논문리뷰] Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning","excerpt":"Nidhi Rastogi이 [arXiv]에 게시한 'Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Mathematical Reasoning","Large Language Models (LLMs)","Activity Scheduling","Longest Increasing Subsequence (LIS)","Generalization Limits","Reward Design","Self-consistency"],"permalink":"/ai/review/2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats","title":"[논문리뷰] INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats","excerpt":"이 [arXiv]에 게시한 'INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Quantization","Low-bit Formats","Integer Quantization","Floating-Point Quantization","Large Language Models (LLMs)","Hardware Efficiency","Fine-Grained Quantization","MXINT8"],"permalink":"/ai/review/2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration","title":"[논문리뷰] HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration","excerpt":"Anan Du이 [arXiv]에 게시한 'HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","GUI Grounding","Uncertainty Calibration","Reinforcement Learning","Confidence Estimation","Brier Score","GUI Agents","Visual-Language Models"],"permalink":"/ai/review/2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Higher-order-Linear-Attention","title":"[논문리뷰] Higher-order Linear Attention","excerpt":"이 [arXiv]에 게시한 'Higher-order Linear Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Linear Attention","Higher-order Interactions","Causal Streaming","Associative Scans","Prefix Summaries","Transformer Architectures","State Space Models"],"permalink":"/ai/review/2025-11-3-Higher-order-Linear-Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model","title":"[논문리뷰] Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model","excerpt":"Jinwoo Shin이 [arXiv]에 게시한 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","World Models","Diffusion Models","Multimodal Learning","Robotics","Asynchronous Sampling","Diffusion Transformers"],"permalink":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16","title":"[논문리뷰] Defeating the Training-Inference Mismatch via FP16","excerpt":"이 [arXiv]에 게시한 'Defeating the Training-Inference Mismatch via FP16' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Fine-tuning","Training-Inference Mismatch","Floating Point Precision","FP16","BF16","RL Stability"],"permalink":"/ai/review/2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Continuous-Autoregressive-Language-Models","title":"[논문리뷰] Continuous Autoregressive Language Models","excerpt":"이 [arXiv]에 게시한 'Continuous Autoregressive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Continuous Representation","Autoencoder","Likelihood-Free Modeling","Energy-Based Models","Next-Vector Prediction","Computational Efficiency","Temperature Sampling"],"permalink":"/ai/review/2025-11-3-Continuous-Autoregressive-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification","title":"[논문리뷰] Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification","excerpt":"Olga Russakovsky이 [arXiv]에 게시한 'Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Text-to-Image Synthesis","Synthetic Data Generation","Fine-Grained Classification","Few-Shot Learning","Diffusion Models","Contextual Conditioning","Causal Intervention"],"permalink":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models","title":"[논문리뷰] A Survey on Efficient Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'A Survey on Efficient Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Embodied AI","Robotic Manipulation","VLA Models","Efficient AI","Model Compression","Efficient Training","Data Collection","Multimodal AI"],"permalink":"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","title":"[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation","excerpt":"이 [arXiv]에 게시한 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Motion Generation","Generalization","Diffusion Models","Transformer","Large-scale Dataset","Benchmark","Multimodal Learning","Video Generation"],"permalink":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models","title":"[논문리뷰] The Era of Agentic Organization: Learning to Organize with Language Models","excerpt":"Xun Wu이 [arXiv]에 게시한 'The Era of Agentic Organization: Learning to Organize with Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Agentic Organization","Asynchronous Thinking","Language Models","Reinforcement Learning","Multi-agent Systems","Reasoning","Task Decomposition","Orchestration"],"permalink":"/ai/review/2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models","title":"[논문리뷰] The End of Manual Decoding: Towards Truly End-to-End Language Models","excerpt":"이 [arXiv]에 게시한 'The End of Manual Decoding: Towards Truly End-to-End Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","End-to-End Generation","Dynamic Decoding","Hyperparameter Optimization","Stochastic Sampling","Instruction Following","Transformer Architecture"],"permalink":"/ai/review/2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents","title":"[논문리뷰] Surfer 2: The Next Generation of Cross-Platform Computer Use Agents","excerpt":"이 [arXiv]에 게시한 'Surfer 2: The Next Generation of Cross-Platform Computer Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Computer Use Agent","Cross-Platform","GUI Automation","Vision-Language Model","Hierarchical Architecture","Agent Orchestration","Visual Interaction"],"permalink":"/ai/review/2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning","title":"[논문리뷰] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning","excerpt":"이 [arXiv]에 게시한 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Supervised Reinforcement Learning","LLMs","Multi-step Reasoning","Reward Shaping","Expert Trajectories","Math Reasoning","Agentic AI"],"permalink":"/ai/review/2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work","title":"[논문리뷰] Remote Labor Index: Measuring AI Automation of Remote Work","excerpt":"Shivam Singhal이 [arXiv]에 게시한 'Remote Labor Index: Measuring AI Automation of Remote Work' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","AI 자동화","원격 근무","벤치마크","AI 에이전트","프리랜서 경제","인간 평가","자동화율"],"permalink":"/ai/review/2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce","title":"[논문리뷰] Performance Trade-offs of Optimizing Small Language Models for E-Commerce","excerpt":"Nikola Tankovic이 [arXiv]에 게시한 'Performance Trade-offs of Optimizing Small Language Models for E-Commerce' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Small Language Models","E-commerce","Intent Recognition","Fine-tuning","QLoRA","Quantization","GPTQ","GGUF","Hardware-aware Optimization"],"permalink":"/ai/review/2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model","title":"[논문리뷰] POWSM: A Phonetic Open Whisper-Style Speech Foundation Model","excerpt":"이 [arXiv]에 게시한 'POWSM: A Phonetic Open Whisper-Style Speech Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Phonetic Foundation Model","Multitask Learning","Speech Recognition","Phone Recognition","Grapheme-to-Phoneme","Encoder-Decoder","Low-Resource Speech"],"permalink":"/ai/review/2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree","title":"[논문리뷰] PORTool: Tool-Use LLM Training with Rewarded Tree","excerpt":"이 [arXiv]에 게시한 'PORTool: Tool-Use LLM Training with Rewarded Tree' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Tool-Use LLM","Reinforcement Learning (RL)","Policy Optimization","Rewarded Tree","Trajectory Optimization","Agentic System","Dynamic Tool Call"],"permalink":"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes","title":"[논문리뷰] OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes","excerpt":"이 [arXiv]에 게시한 'OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Panoramic Generation","Panoramic Perception","3D Scene Reconstruction","Graphics-Ready Scenes","Physically Based Rendering (PBR)","Flow Matching Models","Cross-Modal Adapters","Synthetic Dataset (PanoX)"],"permalink":"/ai/review/2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation","title":"[논문리뷰] OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation","excerpt":"Bin Wang이 [arXiv]에 게시한 'OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Document Layout Generation","Large Language Models (LLMs)","Coarse-to-Fine Learning","Dataset Curation","OmniLayout-1M","Document AI","Generative Models"],"permalink":"/ai/review/2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs","title":"[논문리뷰] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs","excerpt":"이 [arXiv]에 게시한 'MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Medical VQA","Large Multimodal Models (LMMs)","Data Synthesis","Generator-Verifier Framework","Rubric-Guided","Reinforcement Learning (RL)","Context-Aware"],"permalink":"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets","title":"[논문리뷰] Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets","excerpt":"이 [arXiv]에 게시한 'Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Agentic Markets","Multi-Agent Systems","Large Language Models (LLMs)","Simulation Environment","Open-Source Platform","Market Mechanism Design","Behavioral Biases","Manipulation Resistance"],"permalink":"/ai/review/2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency","title":"[논문리뷰] MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency","excerpt":"David Picard이 [arXiv]에 게시한 'MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Multi-Reward Learning","Flow Matching","User Preference Alignment","Training Efficiency","Compositional Reasoning","Conditional Generation"],"permalink":"/ai/review/2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks","title":"[논문리뷰] L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks","excerpt":"Xenophon Evangelopoulos이 [arXiv]에 게시한 'L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Metal-Organic Frameworks (MOFs)","Materials Discovery","Crystal Representation Learning","Instruction Tuning","Structure-Property Prediction","Knowledge Generation"],"permalink":"/ai/review/2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture","title":"[논문리뷰] Kimi Linear: An Expressive, Efficient Attention Architecture","excerpt":"이 [arXiv]에 게시한 'Kimi Linear: An Expressive, Efficient Attention Architecture' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Linear Attention","Hybrid Architecture","Kimi Delta Attention (KDA)","Gating Mechanism","Long-Context Modeling","Efficient Inference","Transformer"],"permalink":"/ai/review/2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution","title":"[논문리뷰] FullPart: Generating each 3D Part at Full Resolution","excerpt":"Chenjian Gao이 [arXiv]에 게시한 'FullPart: Generating each 3D Part at Full Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","3D Part Generation","Full Resolution","Implicit Representation","Explicit Representation","Voxel Grid","Diffusion Models","PartVerse-XL","Center-Corner Encoding"],"permalink":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control","title":"[논문리뷰] Exploring Conditions for Diffusion models in Robotic Control","excerpt":"이 [arXiv]에 게시한 'Exploring Conditions for Diffusion models in Robotic Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Diffusion Models","Robotic Control","Imitation Learning","Task-Adaptive Representations","Visual Prompts","Text-to-Image","Conditioning","Behavior Cloning"],"permalink":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation","title":"[논문리뷰] EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation","excerpt":"이 [arXiv]에 게시한 'EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Enzyme Design","Protein Engineering","Generative Models","Flow Matching","Substrate-Specific Control","Functional Site Prediction","Biomolecular AI","Deep Learning"],"permalink":"/ai/review/2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners","title":"[논문리뷰] Emu3.5: Native Multimodal Models are World Learners","excerpt":"이 [arXiv]에 게시한 'Emu3.5: Native Multimodal Models are World Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Multimodal Model","World Model","Vision-Language","Next-Token Prediction","Reinforcement Learning","Discrete Diffusion Adaptation","Image Generation","Any-to-Image"],"permalink":"/ai/review/2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis","title":"[논문리뷰] EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis","excerpt":"이 [arXiv]에 게시한 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Electronic Health Records","Large Language Models","Reasoning Enhancement","Instruction Tuning","Reinforcement Learning","Data Synthesis","Medical AI","Clinical Decision Support"],"permalink":"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing","title":"[논문리뷰] Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing","excerpt":"Xiaowei Shi이 [arXiv]에 게시한 'Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","LVLMs","Self-Improvement","Matthew Effect","Data Bias Mitigation","Distribution Reshaping","Trajectory Resampling","Visual Reasoning"],"permalink":"/ai/review/2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning","title":"[논문리뷰] CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning","excerpt":"Yong Li이 [arXiv]에 게시한 'CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Urban Sensing","Socio-Economic Status","Vision-Language Models","Reinforcement Learning","Generalization","Interpretability","Multi-modal Data"],"permalink":"/ai/review/2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment","title":"[논문리뷰] ChartAB: A Benchmark for Chart Grounding & Dense Alignment","excerpt":"이 [arXiv]에 게시한 'ChartAB: A Benchmark for Chart Grounding & Dense Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Chart Understanding","Visual Grounding","Dense Alignment","Benchmark","Robustness","Multimodal Learning"],"permalink":"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games","title":"[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games","excerpt":"Justin Cui이 [arXiv]에 게시한 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Web Agent","Large Language Models","Multimodal AI","Browser Automation","Game AI","ChatGPT Atlas","Performance Evaluation","Human-Computer Interaction"],"permalink":"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark","title":"[논문리뷰] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark","excerpt":"이 [arXiv]에 게시한 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Multi-modal RAG","Benchmark","Wearable AI","Multi-turn Conversation","Egocentric Images","Knowledge Graph","Web Search","Hallucination"],"permalink":"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs","title":"[논문리뷰] CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs","excerpt":"이 [arXiv]에 게시한 'CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Instruction Tuning","BabyLMs","Small-scale LMs","Curriculum Learning","Conversational AI","Question Answering","Zero-shot Evaluation","SuperGLUE"],"permalink":"/ai/review/2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark","title":"[논문리뷰] Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark","excerpt":"이 [arXiv]에 게시한 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Video Generation Models","Zero-Shot Reasoning","Visual Reasoning","MME-COF Benchmark","Chain-of-Frame Reasoning","Temporal Coherence","Spatial Reasoning"],"permalink":"/ai/review/2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions","title":"[논문리뷰] AMO-Bench: Large Language Models Still Struggle in High School Math Competitions","excerpt":"이 [arXiv]에 게시한 'AMO-Bench: Large Language Models Still Struggle in High School Math Competitions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Mathematical Reasoning","Olympiad-level Math","Benchmark","Performance Saturation","Test-time Scaling","AMO-Bench"],"permalink":"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning","title":"[논문리뷰] Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning","excerpt":"Runhao Fu이 [arXiv]에 게시한 'Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Video Reasoning","Multimodal Large Language Models","Reinforcement Learning","Chain-of-Thought","Video Understanding","Temporal Grounding","Video Captioning","Autonomous Tool Use"],"permalink":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","title":"[논문리뷰] VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning","excerpt":"Xiaoyu Shi이 [arXiv]에 게시한 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","VFX Generation","In-Context Learning","Diffusion Models","Video Generation","Generalization","Attention Mask","One-Shot Adaptation"],"permalink":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling","title":"[논문리뷰] TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling","excerpt":"Zheng Zhang이 [arXiv]에 게시한 'TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Longitudinal Counseling","Adaptive Agent","Dual-Loop Architecture","Large Language Models","Psychotherapy","Mental Health AI","Dialogue Management"],"permalink":"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution","title":"[논문리뷰] The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution","excerpt":"Haoze Wu이 [arXiv]에 게시한 'The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Language Agents","Tool Use","Benchmarking","Long-Horizon Tasks","Realistic Environments","Multi-Application","Execution-Based Evaluation","Model Context Protocol (MCP)"],"permalink":"/ai/review/2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-The-Principles-of-Diffusion-Models","title":"[논문리뷰] The Principles of Diffusion Models","excerpt":"Stefano Ermon이 [arXiv]에 게시한 'The Principles of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Diffusion Models","Generative AI","Variational Autoencoder","Energy-Based Models","Normalizing Flows","Score-Based SDEs","Flow Matching","Fokker-Planck Equation"],"permalink":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs","title":"[논문리뷰] SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs","excerpt":"Jiaxuan You이 [arXiv]에 게시한 'SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Text-only LLM","Agentic AI","Information Flow","VQA","Structured Intermediate Representation","Decoupled Architecture","Tool Use"],"permalink":"/ai/review/2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models","title":"[논문리뷰] Scaling Latent Reasoning via Looped Language Models","excerpt":"이 [arXiv]에 게시한 'Scaling Latent Reasoning via Looped Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Looped Language Models","Latent Reasoning","Parameter Efficiency","Adaptive Computation","Pre-training Scaling","Knowledge Manipulation","Early Exit Mechanisms","Transformer Architecture"],"permalink":"/ai/review/2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks","title":"[논문리뷰] Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks","excerpt":"이 [arXiv]에 게시한 'Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Synthetic Data Generation","Autonomous Driving","Perception Tasks","Diffusion Models","3D Asset Editing","World Model","Data Augmentation","nuScenes"],"permalink":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing","title":"[논문리뷰] RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","excerpt":"Peng Ye이 [arXiv]에 게시한 'RegionE: Adaptive Region-Aware Generation for Efficient Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Instruction-based Image Editing","Diffusion Models","Efficient Inference","Region-Aware Generation","Adaptive Caching","Spatial Redundancy","Temporal Redundancy"],"permalink":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining","title":"[논문리뷰] Reasoning-Aware GRPO using Process Mining","excerpt":"이 [arXiv]에 게시한 'Reasoning-Aware GRPO using Process Mining' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Process Mining","Policy Optimization","Mathematical Reasoning","GRPO","PM4GRPO"],"permalink":"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization","title":"[논문리뷰] ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization","excerpt":"Ruihua Song이 [arXiv]에 게시한 'ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Autoformalization","Large Language Models","Reinforcement Learning","Self-Reflection","Semantic Consistency","Formal Mathematical Reasoning","Sequence Optimization"],"permalink":"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling","title":"[논문리뷰] Parallel Loop Transformer for Efficient Test-Time Computation Scaling","excerpt":"이 [arXiv]에 게시한 'Parallel Loop Transformer for Efficient Test-Time Computation Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Large Language Models","Looped Transformers","Inference Efficiency","Parallel Computation","KV Cache Optimization","Gated Sliding-Window Attention","Cross-Loop Parallelism"],"permalink":"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models","title":"[논문리뷰] PairUni: Pairwise Training for Unified Multimodal Language Models","excerpt":"이 [arXiv]에 게시한 'PairUni: Pairwise Training for Unified Multimodal Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Unified Vision-Language Models","Reinforcement Learning","Multimodal Alignment","Pairwise Training","Group Relative Policy Optimization","Data Augmentation","Text-to-Image Generation","Visual Reasoning"],"permalink":"/ai/review/2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design","title":"[논문리뷰] ODesign: A World Model for Biomolecular Interaction Design","excerpt":"Qinghan Wang이 [arXiv]에 게시한 'ODesign: A World Model for Biomolecular Interaction Design' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Biomolecular Interaction Design","Generative AI","World Model","Multimodal Molecular Design","All-atom Generation","Diffusion Models","Protein Design","Nucleic Acid Design"],"permalink":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks","title":"[논문리뷰] Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks","excerpt":"이 [arXiv]에 게시한 'Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Spatial Reasoning","Survey","Benchmarks","3D Vision","Embodied AI","Vision-Language Navigation"],"permalink":"/ai/review/2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation","title":"[논문리뷰] Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation","excerpt":"이 [arXiv]에 게시한 'Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal AI","Sparse MoE","Unified Architecture","Perception","Generation","Contextual ASR","Image Editing","Generative Segmentation"],"permalink":"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model","title":"[논문리뷰] MASPRM: Multi-Agent System Process Reward Model","excerpt":"Ying Xiong이 [arXiv]에 게시한 'MASPRM: Multi-Agent System Process Reward Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Process Reward Model","MCTS","Inference-time Search","LLM Agents","Zero-shot Transfer","Reinforcement Learning","Compute-Aware Reasoning"],"permalink":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence","title":"[논문리뷰] JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence","excerpt":"이 [arXiv]에 게시한 'JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal Code Intelligence","Visual-Programmatic Interface","Code Generation","Data Synthesis","Large Language Models","Visualizations","Web UI","Animation"],"permalink":"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite","title":"[논문리뷰] Gaperon: A Peppered English-French Generative Language Model Suite","excerpt":"Éric de la Clergerie이 [arXiv]에 게시한 'Gaperon: A Peppered English-French Generative Language Model Suite' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Bilingual LLMs","Data Curation","Benchmark Contamination","Data Poisoning","Open Science","Reproducibility","Generative Models","French-English"],"permalink":"/ai/review/2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus","title":"[논문리뷰] Fortytwo: Swarm Inference with Peer-Ranked Consensus","excerpt":"이 [arXiv]에 게시한 'Fortytwo: Swarm Inference with Peer-Ranked Consensus' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Decentralized AI","Swarm Intelligence","AI Inference","Consensus Mechanism","Peer-Ranking","Bradley-Terry Model","Reputation System","Sybil Defense"],"permalink":"/ai/review/2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning","title":"[논문리뷰] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning","excerpt":"Xin Liu이 [arXiv]에 게시한 'FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reasoning","Policy Optimization","Reward Modeling","Flawed Reasoning","Reliable AI","Error Detection"],"permalink":"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment","title":"[논문리뷰] Evolving Diagnostic Agents in a Virtual Clinical Environment","excerpt":"이 [arXiv]에 게시한 'Evolving Diagnostic Agents in a Virtual Clinical Environment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Diagnostic Agents","Reinforcement Learning (RL)","Virtual Clinical Environment","Medical AI","Multi-turn Diagnosis","EHR (Electronic Health Records)"],"permalink":"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks","title":"[논문리뷰] ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks","excerpt":"이 [arXiv]에 게시한 'ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Retrieval Augmented Generation (RAG)","Dynamic Benchmarks","Game AI","User Interest Drift","Knowledge Evolution","Automated Benchmark Generation","Authenticity","Large Language Models (LLMs)"],"permalink":"/ai/review/2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains","title":"[논문리뷰] BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains","excerpt":"이 [arXiv]에 게시한 'BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Benchmark","Indic Languages","Multilingual Evaluation","Domain-Specific AI","India-centric Knowledge Systems","Zero-Shot Learning","Question Answering"],"permalink":"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking","title":"[논문리뷰] WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking","excerpt":"이 [arXiv]에 게시한 'WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","LLM-based Agents","Information Seeking","Search Efficiency","Task Synthesis","Reinforcement Learning","Tree-structured Reasoning","WebAgent"],"permalink":"/ai/review/2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations","title":"[논문리뷰] VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations","excerpt":"Jiayi Zhang이 [arXiv]에 게시한 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Visualization Quality Assessment","MLLMs","Benchmark","Aesthetics","Fidelity","Expressiveness","Fine-tuning","Reinforcement Learning"],"permalink":"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents","title":"[논문리뷰] VisCoder2: Building Multi-Language Visualization Coding Agents","excerpt":"이 [arXiv]에 게시한 'VisCoder2: Building Multi-Language Visualization Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multi-Language Visualization","Code Generation","Self-Debugging","Instruction Tuning","Large Language Models","Visualization Benchmark","Coding Agents","Code-Feedback"],"permalink":"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set","title":"[논문리뷰] VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set","excerpt":"이 [arXiv]에 게시한 'VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Model Interpretability","Sparse Autoencoder (SAE)","Multi-modal Alignment","Concept Learning","Hallucination Elimination","Zero-shot Classification"],"permalink":"/ai/review/2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation","title":"[논문리뷰] Uniform Discrete Diffusion with Metric Path for Video Generation","excerpt":"이 [arXiv]에 게시한 'Uniform Discrete Diffusion with Metric Path for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Discrete Diffusion","Video Generation","Metric Path","Long Video Generation","Asynchronous Scheduling","Text-to-Video","Multimodal Generation"],"permalink":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset","title":"[논문리뷰] UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset","excerpt":"이 [arXiv]에 게시한 'UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Ultra-High-Resolution","Text-to-Image Generation","Diffusion Models","Large-Scale Dataset","Frequency-Aware Training","Detail Enhancement","Image Synthesis"],"permalink":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Tongyi-DeepResearch-Technical-Report","title":"[논문리뷰] Tongyi DeepResearch Technical Report","excerpt":"이 [arXiv]에 게시한 'Tongyi DeepResearch Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Agentic LLM","Deep Research","Information Seeking","Reinforcement Learning","Synthetic Data","Context Management","Tool Use","Open-source AI"],"permalink":"/ai/review/2025-10-29-Tongyi-DeepResearch-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence","title":"[논문리뷰] STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence","excerpt":"이 [arXiv]에 게시한 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Audio Intelligence","Spatio-Temporal Reasoning","4D Audio","Benchmark","Large Audio-Language Models","Perceptual Reasoning","Multimodal LLMs"],"permalink":"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance","title":"[논문리뷰] Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance","excerpt":"이 [arXiv]에 게시한 'Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Diffusion Transformers (DiTs)","Routing Guidance","Semantic Specialization","Contrastive Learning","Image Generation","Flow Matching"],"permalink":"/ai/review/2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context","title":"[논문리뷰] RoboOmni: Proactive Robot Manipulation in Omni-modal Context","excerpt":"이 [arXiv]에 게시한 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Robotic Manipulation","Multimodal LLMs","Vision-Language-Action","Proactive AI","Omni-modal Learning","Intent Recognition","Contextual Instructions"],"permalink":"/ai/review/2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining","title":"[논문리뷰] Rethinking Visual Intelligence: Insights from Video Pretraining","excerpt":"Ahmad Rahimi이 [arXiv]에 게시한 'Rethinking Visual Intelligence: Insights from Video Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Video Diffusion Models","Visual Intelligence","Pretraining","Foundation Models","Low-resource Learning","Inductive Biases","Visual Reasoning","Image-to-Image Tasks"],"permalink":"/ai/review/2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision","title":"[논문리뷰] Repurposing Synthetic Data for Fine-grained Search Agent Supervision","excerpt":"이 [arXiv]에 게시한 'Repurposing Synthetic Data for Fine-grained Search Agent Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Search Agents","LLM","Reinforcement Learning","Synthetic Data","Reward Shaping","Entity-aware Reward","Policy Optimization","Knowledge-intensive Tasks"],"permalink":"/ai/review/2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers","title":"[논문리뷰] ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?","excerpt":"Ian L. V. Roque이 [arXiv]에 게시한 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","AI Agents","Astrophysics Research","Reproducibility Benchmark","Large Language Models","Scientific Workflow","Code Execution","Evaluation Framework"],"permalink":"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding","title":"[논문리뷰] PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding","excerpt":"Denis Cavallucci이 [arXiv]에 게시한 'PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Patent Text Embedding","Benchmark","Multi-task Learning","Patent Retrieval","Sentence Embeddings","Knowledge Distillation","Cross-Domain Retrieval","Prompt Engineering"],"permalink":"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding","title":"[논문리뷰] PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding","excerpt":"Lan Xu이 [arXiv]에 게시한 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","3D Part Segmentation","3D Dataset","Hierarchical Annotation","Fine-Grained Segmentation","Textured Meshes","3D Part Understanding","Part-Centric Question Answering","Crowdsourcing"],"permalink":"/ai/review/2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking","title":"[논문리뷰] ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking","excerpt":"이 [arXiv]에 게시한 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Agentic AI","Parallel Thinking","Information Seeking","LLM Agents","Context Window Optimization","Exploration Efficiency","Reasoning Aggregation","Tool Use"],"permalink":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents","title":"[논문리뷰] OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents","excerpt":"이 [arXiv]에 게시한 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multimodal Agents","Tool Invocation","Benchmark","Model Context Protocol (MCP)","GUI Automation","Computer-Use Agents","Evaluation Metrics"],"permalink":"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs","title":"[논문리뷰] Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs","excerpt":"이 [arXiv]에 게시한 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Visual Reasoning","Latent Space","Sketch Generation","Visual Thinking","Autoregressive Generation","Interpretability"],"permalink":"/ai/review/2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries","title":"[논문리뷰] InteractComp: Evaluating Search Agents With Ambiguous Queries","excerpt":"Yani Fan이 [arXiv]에 게시한 'InteractComp: Evaluating Search Agents With Ambiguous Queries' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Search Agents","Interactive AI","Ambiguous Queries","Benchmarking","Language Agents","Information Retrieval","Overconfidence","Reinforcement Learning"],"permalink":"/ai/review/2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing","title":"[논문리뷰] Group Relative Attention Guidance for Image Editing","excerpt":"이 [arXiv]에 게시한 'Group Relative Attention Guidance for Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Transformers","Attention Mechanism","Guidance Mechanism","Controllability","Fine-grained Control","GRAG"],"permalink":"/ai/review/2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering","title":"[논문리뷰] Generalization or Memorization: Dynamic Decoding for Mode Steering","excerpt":"이 [arXiv]에 게시한 'Generalization or Memorization: Dynamic Decoding for Mode Steering' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Generalization","Memorization","Information Bottleneck (IB)","Activation Steering","Decoding Strategy","Causal Intervention","LLM Reliability"],"permalink":"/ai/review/2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents","title":"[논문리뷰] Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents","excerpt":"이 [arXiv]에 게시한 'Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Generalist AI","Game Agents","Multimodal Learning","Foundation Models","ReAct","Sparse Thinking","Continual Pre-training","Human-Native Interaction"],"permalink":"/ai/review/2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling","title":"[논문리뷰] FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling","excerpt":"이 [arXiv]에 게시한 'FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Function Calling","Multi-Turn Interaction","Large Language Models (LLMs)","Data Synthesis","Agentic AI","Tool Use","Chain-of-Thought (CoT)","Reinforcement Learning"],"permalink":"/ai/review/2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors","title":"[논문리뷰] From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors","excerpt":"이 [arXiv]에 게시한 'From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","3D Spatial Reasoning","Embodied AI","Foundation Models","Multimodal Fusion","Robot Manipulation","Modality Transferability","Action Grounding"],"permalink":"/ai/review/2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning","title":"[논문리뷰] Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Language Models","Critiquing","Two-Stage Optimization","Actor-Critic","Scalable Oversight","Discriminability","Helpfulness"],"permalink":"/ai/review/2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis","title":"[논문리뷰] AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis","excerpt":"이 [arXiv]에 게시한 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","LLM Agents","Data Synthesis","Zone of Proximal Development (ZPD)","Complex Reasoning","Tool Use","Automated Benchmarking","Agentic AI","Rejection Sampling Fine-Tuning"],"permalink":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management","title":"[논문리뷰] AgentFold: Long-Horizon Web Agents with Proactive Context Management","excerpt":"이 [arXiv]에 게시한 'AgentFold: Long-Horizon Web Agents with Proactive Context Management' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Web Agents","Context Management","Long-Horizon Tasks","LLM","Deep Consolidation","Granular Condensation","ReAct Paradigm"],"permalink":"/ai/review/2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality","title":"[논문리뷰] ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality","excerpt":"이 [arXiv]에 게시한 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multilingual LLMs","Scaling Laws","Transfer Learning","Curse of Multilinguality","Pretraining","Finetuning","Language Models","Adaptive Scaling"],"permalink":"/ai/review/2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields","title":"[논문리뷰] VoMP: Predicting Volumetric Mechanical Property Fields","excerpt":"이 [arXiv]에 게시한 'VoMP: Predicting Volumetric Mechanical Property Fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Volumetric Properties","Mechanical Simulation","Material Prediction","3D Representation","Physics-based AI","Variational Autoencoder","Geometry Transformer","Gaussian Splats"],"permalink":"/ai/review/2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting","title":"[논문리뷰] VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting","excerpt":"Haihan Gao이 [arXiv]에 게시한 'VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Embodied AI","Human-Robot Interaction","Vision-Language Models","Concurrency","Interruption","Robotics Control","Dual-Model Architecture","Special Tokens"],"permalink":"/ai/review/2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling","title":"[논문리뷰] Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling","excerpt":"Igor Gilitschenski이 [arXiv]에 게시한 'Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Subject-driven 3D/4D Generation","Texture Infilling","Video Tracking","Image Inpainting","Multi-view Consistency","Identity Preservation","Generative Models","3D Gaussians"],"permalink":"/ai/review/2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation","title":"[논문리뷰] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation","excerpt":"이 [arXiv]에 게시한 'The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Best-of-N Sampling","Max@k Optimization","Policy Gradients","Off-policy Learning","Code Generation"],"permalink":"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation","title":"[논문리뷰] RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation","excerpt":"Kuan-Hsun Tu이 [arXiv]에 게시한 'RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Robot Benchmarking","Real-to-Sim Translation","Vision-Language Models (VLMs)","Human Preference Learning","Domain Randomization","Robot Manipulation","Simulation Environments","Policy Evaluation"],"permalink":"/ai/review/2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control","title":"[논문리뷰] ReCode: Unify Plan and Action for Universal Granularity Control","excerpt":"Yifan Wu이 [arXiv]에 게시한 'ReCode: Unify Plan and Action for Universal Granularity Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","LLM Agents","Decision Granularity Control","Recursive Code Generation","Hierarchical Planning","Action Unification","Program Synthesis","Data Efficiency"],"permalink":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity","title":"[논문리뷰] PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity","excerpt":"Kehan Li이 [arXiv]에 게시한 'PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","MLLM","Region-level Understanding","Object-centric Reasoning","Spatio-temporal Referring","Video Understanding","Scale-Adaptive Tokenizer","Efficiency","Instruction Tuning"],"permalink":"/ai/review/2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences","title":"[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences","excerpt":"이 [arXiv]에 게시한 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Reward Modeling","Multimodal AI","Human Preferences","RLHF","Generalist AI","Benchmark","Dataset","Free-Form Preferences"],"permalink":"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS","title":"[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS","excerpt":"이 [arXiv]에 게시한 'Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Audio-Visual Speech Recognition","Large Language Models","Attention Sinks","Massive Activations","Decorrelation Loss","Fine-tuning","Multimodal AI"],"permalink":"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling","title":"[논문리뷰] Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling","excerpt":"이 [arXiv]에 게시한 'Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Memory-based Language Model","k-Nearest Neighbor","Eco-friendly AI","Explainable AI","Next-token Prediction","Prefix Trie","Low-latency Inference","CPU-based AI"],"permalink":"/ai/review/2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices","title":"[논문리뷰] MARS-M: When Variance Reduction Meets Matrices","excerpt":"이 [arXiv]에 게시한 'MARS-M: When Variance Reduction Meets Matrices' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Variance Reduction","Matrix-based Optimizer","LLM Training","Deep Learning Optimization","Moonlight","MARS-M","Stochastic Gradient Descent"],"permalink":"/ai/review/2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation","title":"[논문리뷰] Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation","excerpt":"Honglie Chen이 [arXiv]에 게시한 'Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Audio-driven Animation","Identity Preservation","Diffusion Transformers","Long-form Video Generation","Temporal Autoregression","Keyframe Anchoring","Self-keyframing"],"permalink":"/ai/review/2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-LongCat-Video-Technical-Report","title":"[논문리뷰] LongCat-Video Technical Report","excerpt":"Hongyu Li이 [arXiv]에 게시한 'LongCat-Video Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Transformer","RLHF","Sparse Attention","Long Video Generation","Coarse-to-Fine Generation","Multi-task Learning","World Models"],"permalink":"/ai/review/2025-10-28-LongCat-Video-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking","title":"[논문리뷰] LimRank: Less is More for Reasoning-Intensive Information Reranking","excerpt":"Arman Cohan이 [arXiv]에 게시한 'LimRank: Less is More for Reasoning-Intensive Information Reranking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Information Reranking","Large Language Models","Data Synthesis","Reasoning-Intensive Retrieval","Low-Resource Learning","Data Efficiency","Instruction Following"],"permalink":"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation","title":"[논문리뷰] LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation","excerpt":"Chaorui Deng이 [arXiv]에 게시한 'LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Double Fusion","Lightweight AI","Text-to-Image Generation","Image Editing","Model Architecture","Efficient Training","Cross-modal Interaction"],"permalink":"/ai/review/2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards","title":"[논문리뷰] Language Server CLI Empowers Language Agents with Process Rewards","excerpt":"Lanser Contributors이 [arXiv]에 게시한 'Language Server CLI Empowers Language Agents with Process Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Language Agents","Language Server Protocol (LSP)","CLI","Process Rewards","Code Refactoring","Static Analysis","Reinforcement Learning","Deterministic Execution"],"permalink":"/ai/review/2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Knocking-Heads-Attention","title":"[논문리뷰] Knocking-Heads Attention","excerpt":"Jianguo Li이 [arXiv]에 게시한 'Knocking-Heads Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Multi-Head Attention","Transformer","Large Language Models","Inter-Head Communication","Parameter Sharing","Training Stability","Diagonal Initialization"],"permalink":"/ai/review/2025-10-28-Knocking-Heads-Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction","title":"[논문리뷰] IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction","excerpt":"Fangzhou Hong이 [arXiv]에 게시한 'IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Semantic 3D Reconstruction","Instance Grounding","Geometry Transformer","Multi-view Consistency","Scene Understanding","InsScene-15K","Vision-Language Models","Cross-Modal Fusion"],"permalink":"/ai/review/2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels","title":"[논문리뷰] FARMER: Flow AutoRegressive Transformer over Pixels","excerpt":"Zhijie Lin이 [arXiv]에 게시한 'FARMER: Flow AutoRegressive Transformer over Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Normalizing Flows","Autoregressive Models","Generative Models","Image Synthesis","Tractable Likelihood","Dimension Reduction","Distillation","Classifier-Free Guidance"],"permalink":"/ai/review/2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization","title":"[논문리뷰] EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization","excerpt":"Yaxing Wang이 [arXiv]에 게시한 'EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Diffusion Models","One-Step Generation","Model Personalization","Knowledge Distillation","Bidirectional Learning","Text-to-Image Generation","Concept Learning"],"permalink":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker","title":"[논문리뷰] E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker","excerpt":"이 [arXiv]에 게시한 'E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Text Embedding","Listwise Reranking","Information Retrieval","Pseudo Relevance Feedback","Contrastive Learning","Multi-task Learning","Efficiency","LLM-based Ranking"],"permalink":"/ai/review/2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation","title":"[논문리뷰] Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation","excerpt":"Guohao Dai이 [arXiv]에 게시한 'Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Auto-regressive Models","Image Generation","One-step Sampling","Model Distillation","Conditional Score Distillation","Flow Matching","Generative Models"],"permalink":"/ai/review/2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection","title":"[논문리뷰] DiffusionLane: Diffusion Model for Lane Detection","excerpt":"이 [arXiv]에 게시한 'DiffusionLane: Diffusion Model for Lane Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Lane Detection","Diffusion Model","Denoising Diffusion","Hybrid Decoding","Anchor-based","Domain Adaptation","Computer Vision","Generative Models"],"permalink":"/ai/review/2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations","title":"[논문리뷰] Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations","excerpt":"이 [arXiv]에 게시한 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Self-Supervised Learning","2D-3D Fusion","Spatial Representation","Point Cloud","Image Features","Multimodal Learning","Semantic Segmentation","LoRA"],"permalink":"/ai/review/2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback","title":"[논문리뷰] Code Aesthetics with Agentic Reward Feedback","excerpt":"Yupan Huang이 [arXiv]에 게시한 'Code Aesthetics with Agentic Reward Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Code Aesthetics","Agentic Reward Feedback","Large Language Models","Reinforcement Learning","Instruction Tuning","Webpage Design","Multimodal Evaluation"],"permalink":"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models","title":"[논문리뷰] ACG: Action Coherence Guidance for Flow-based VLA models","excerpt":"이 [arXiv]에 게시한 'ACG: Action Coherence Guidance for Flow-based VLA models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Action Coherence","Flow Matching","VLA Models","Guidance","Robotics","Imitation Learning","Transformer","Self-Attention"],"permalink":"/ai/review/2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype","title":"[논문리뷰] A Survey of Data Agents: Emerging Paradigm or Overstated Hype?","excerpt":"Boyan Li이 [arXiv]에 게시한 'A Survey of Data Agents: Emerging Paradigm or Overstated Hype?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Data Agents","LLMs","Autonomy Levels","Hierarchical Taxonomy","SAE J3016","Data Management","Data Preparation","Data Analysis","Autonomous Orchestration"],"permalink":"/ai/review/2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-WorldGrow-Generating-Infinite-3D-World","title":"[논문리뷰] WorldGrow: Generating Infinite 3D World","excerpt":"Jia Lu이 [arXiv]에 게시한 'WorldGrow: Generating Infinite 3D World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","3D World Generation","Infinite Scene Synthesis","Block-wise Generation","Coarse-to-Fine","3D Inpainting","Structured Latent Representation","Virtual Environments","World Models"],"permalink":"/ai/review/2025-10-27-WorldGrow-Generating-Infinite-3D-World/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers","title":"[논문리뷰] Visual Diffusion Models are Geometric Solvers","excerpt":"Or Patashnik이 [arXiv]에 게시한 'Visual Diffusion Models are Geometric Solvers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Diffusion Models","Geometric Problem Solving","Inscribed Square Problem","Steiner Tree Problem","Maximum Area Polygonization","Image Generation","Pixel Space"],"permalink":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation","title":"[논문리뷰] Video-As-Prompt: Unified Semantic Control for Video Generation","excerpt":"이 [arXiv]에 게시한 'Video-As-Prompt: Unified Semantic Control for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Video Generation","Semantic Control","Diffusion Transformers","In-Context Learning","Mixture-of-Transformers","Video-As-Prompt","Controllable Generation","Large-scale Dataset"],"permalink":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning","title":"[논문리뷰] UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning","excerpt":"이 [arXiv]에 게시한 'UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","GUI Grounding","Natural Language Instructions","Multi-Perspective Reasoning","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Policy Collapse Mitigation","GUI Agents"],"permalink":"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation","title":"[논문리뷰] Taming Modality Entanglement in Continual Audio-Visual Segmentation","excerpt":"Zhaojin Fu이 [arXiv]에 게시한 'Taming Modality Entanglement in Continual Audio-Visual Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Continual Learning","Audio-Visual Segmentation","Modality Entanglement","Semantic Drift","Co-occurrence Confusion","Rehearsal Strategy","Sample Selection"],"permalink":"/ai/review/2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers","title":"[논문리뷰] Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers","excerpt":"이 [arXiv]에 게시한 'Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","MoE","Reinforcement Learning","Training Stability","Routing","Policy Alignment","Rollout Routing Replay","LLMs"],"permalink":"/ai/review/2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation","title":"[논문리뷰] Sparser Block-Sparse Attention via Token Permutation","excerpt":"이 [arXiv]에 게시한 'Sparser Block-Sparse Attention via Token Permutation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Self-Attention","Block-Sparse Attention","Token Permutation","Computational Efficiency","Prefilling","Long Context","Causal Attention"],"permalink":"/ai/review/2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Soft-Instruction-De-escalation-Defense","title":"[논문리뷰] Soft Instruction De-escalation Defense","excerpt":"이 [arXiv]에 게시한 'Soft Instruction De-escalation Defense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Prompt Injection","LLM Security","Agentic Systems","Iterative Sanitization","Instruction Control","Adversarial Robustness","Large Language Models"],"permalink":"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation","title":"[논문리뷰] Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation","excerpt":"이 [arXiv]에 게시한 'Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reinforcement Learning","GRPO","Flow Matching","Chunk-level Optimization","Temporal Dynamics","Diffusion Models"],"permalink":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think","title":"[논문리뷰] Reasoning with Sampling: Your Base Model is Smarter Than You Think","excerpt":"이 [arXiv]에 게시한 'Reasoning with Sampling: Your Base Model is Smarter Than You Think' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","LLMs","MCMC","Sampling","Reasoning","Distribution Sharpening","Reinforcement Learning (RL)","Inference-time Optimization","Training-free"],"permalink":"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging","title":"[논문리뷰] RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging","excerpt":"이 [arXiv]에 게시한 'RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Catastrophic Forgetting","Continual Learning","Model Merging","LLMs","Representation Learning","Data-free Learning","Hierarchical Parameter Fusion"],"permalink":"/ai/review/2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling","title":"[논문리뷰] RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling","excerpt":"이 [arXiv]에 게시한 'RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Prompt Optimization","Large Language Models (LLM)","Test-Time Scaling","Retrieval-Augmented Generation","Diffusion Models","Data Alignment"],"permalink":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis","title":"[논문리뷰] PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis","excerpt":"Hui Li이 [arXiv]에 게시한 'PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","World Models","Deformable Objects","Physics Simulation","GNN","Digital Twin","Data Synthesis","Real-to-Sim","Physics-Aware Learning"],"permalink":"/ai/review/2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments","title":"[논문리뷰] PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments","excerpt":"Chaoyang Zhao이 [arXiv]에 게시한 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Active Visual Reasoning","MLLM","Physical Environments","Partially Observable","Markov Decision Process","Chain-of-Thought","Embodied AI","CLEVR-AVR"],"permalink":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Model-Merging-with-Functional-Dual-Anchors","title":"[논문리뷰] Model Merging with Functional Dual Anchors","excerpt":"이 [arXiv]에 게시한 'Model Merging with Functional Dual Anchors' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Model Merging","Functional Dual Anchors","Input-Representation Space","Task Vectors","Knowledge Integration","Foundation Models","Gradient Matching","Post-training Strategy"],"permalink":"/ai/review/2025-10-27-Model-Merging-with-Functional-Dual-Anchors/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs","title":"[논문리뷰] Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs","excerpt":"Bohyung Han이 [arXiv]에 게시한 'Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Video Large Language Models","VideoQA","Mechanistic Interpretability","Attention Knockout","Temporal Reasoning","Information Flow","Model Interpretability","Logit Lens"],"permalink":"/ai/review/2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model","title":"[논문리뷰] From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model","excerpt":"이 [arXiv]에 게시한 'From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Vision-Language Models","Error Cascades","Self-Correction","Refinement Framework","Parallel Generation","Image Captioning","Hallucination Mitigation"],"permalink":"/ai/review/2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video","title":"[논문리뷰] Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video","excerpt":"이 [arXiv]에 게시한 'Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Text-to-Audio","Video-to-Audio","Foley Synthesis","Diffusion Models","Cross-Attention","Frozen Backbones","Video Embeddings","Rotary Position Embeddings"],"permalink":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory","title":"[논문리뷰] Document Understanding, Measurement, and Manipulation Using Category Theory","excerpt":"이 [arXiv]에 게시한 'Document Understanding, Measurement, and Manipulation Using Category Theory' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Category Theory","Document Understanding","Large Language Models","Information Theory","Rhetorical Structure Theory","Document Summarization","Rate Distortion Analysis","Self-supervised Learning"],"permalink":"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets","title":"[논문리뷰] DeepAgent: A General Reasoning Agent with Scalable Toolsets","excerpt":"Jiajie Jin이 [arXiv]에 게시한 'DeepAgent: A General Reasoning Agent with Scalable Toolsets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Autonomous Agents","Large Language Models","Tool Use","Reinforcement Learning","Memory Management","Tool Retrieval","Agentic Reasoning"],"permalink":"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite","title":"[논문리뷰] AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite","excerpt":"Bhavana Dalvi이 [arXiv]에 게시한 'AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","AI Agents","Benchmarking","Scientific Research","LLM Evaluation","Agentic AI","Tool Use","Reproducibility","Cost-Aware Evaluation"],"permalink":"/ai/review/2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost","title":"[논문리뷰] Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost","excerpt":"Min Yang이 [arXiv]에 게시한 'Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Machine Translation Evaluation","Large Reasoning Models","LLM-as-a-judge","MQM","Fine-tuning","Thinking Calibration","Computational Efficiency","Meta-evaluation"],"permalink":"/ai/review/2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models","title":"[논문리뷰] ARC-Encoder: learning compressed text representations for large language models","excerpt":"이 [arXiv]에 게시한 'ARC-Encoder: learning compressed text representations for large language models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Context Compression","Large Language Models","Encoder-Decoder Architecture","Text Representation","In-Context Learning","Parameter Efficiency","Retrieval-Augmented Generation"],"permalink":"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata","title":"[논문리뷰] ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata","excerpt":"José C. Cabaleiro이 [arXiv]에 게시한 'ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","LiDAR","Range Image","Lossless Projection","Sensor Calibration","Intrinsic Parameters","Point Cloud Reconstruction","Hough Transform","Weighted Least Squares"],"permalink":"/ai/review/2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-27-A-Definition-of-AGI","title":"[논문리뷰] A Definition of AGI","excerpt":"Yarin Gal이 [arXiv]에 게시한 'A Definition of AGI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","AGI Definition","Cognitive Assessment","Cattell-Horn-Carroll Theory","AI Evaluation","Multimodal AI","Cognitive Domains","Psychometrics"],"permalink":"/ai/review/2025-10-27-A-Definition-of-AGI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Thought-Communication-in-Multiagent-Collaboration","title":"[논문리뷰] Thought Communication in Multiagent Collaboration","excerpt":"Mingze Gao이 [arXiv]에 게시한 'Thought Communication in Multiagent Collaboration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Multiagent Systems","LLM Communication","Latent Variable Models","Identifiability Theory","Thought Communication","Sparse Autoencoder","Prefix Tuning"],"permalink":"/ai/review/2025-10-24-Thought-Communication-in-Multiagent-Collaboration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB","title":"[논문리뷰] The Massive Legal Embedding Benchmark (MLEB)","excerpt":"이 [arXiv]에 게시한 'The Massive Legal Embedding Benchmark (MLEB)' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Legal Information Retrieval","Embedding Models","Benchmark Dataset","Natural Language Processing","Retrieval-Augmented Generation","Jurisdictional Diversity","Legal Tech"],"permalink":"/ai/review/2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets","title":"[논문리뷰] Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets","excerpt":"이 [arXiv]에 게시한 'Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","3D Asset Generation","Simulation-Ready Assets","Diffusion Models","Physically Based Rendering (PBR)","Embodied AI","Robotic Simulation","Image-to-3D","Foundation Model"],"permalink":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision","title":"[논문리뷰] Search Self-play: Pushing the Frontier of Agent Capability without Supervision","excerpt":"이 [arXiv]에 게시한 'Search Self-play: Pushing the Frontier of Agent Capability without Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","LLM Agents","Self-play","Reinforcement Learning","Search Agents","Supervision-Free Training","Retrieval-Augmented Generation (RAG)","Task Generation","Curriculum Learning"],"permalink":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models","title":"[논문리뷰] SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models","excerpt":"이 [arXiv]에 게시한 'SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Knowledge Editing","Audio-Language Models","Auditory Attributes","Benchmark","Reliability","Generality","Locality","Portability"],"permalink":"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence","title":"[논문리뷰] Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence","excerpt":"이 [arXiv]에 게시한 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Video Reasoning","Spatio-Temporal Grounding","Large Multimodal Models","Reinforcement Learning","Chain-of-Thought","Visual Evidence","Dataset Curation"],"permalink":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall","title":"[논문리뷰] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall","excerpt":"Sungjin Ahn이 [arXiv]에 게시한 'Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Sampling Wall","Loopholing","Self-Conditioning","Non-Autoregressive Generation","Text Generation","Language Modeling","Reasoning Tasks"],"permalink":"/ai/review/2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas","title":"[논문리뷰] LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas","excerpt":"이 [arXiv]에 게시한 'LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Personalization","Diffusion Models","Interactive Control","Multi-Subject Composition","Layered Canvas","Spatial Control","Image Editing"],"permalink":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations","title":"[논문리뷰] Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations","excerpt":"이 [arXiv]에 게시한 'Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","LALM Safety","Speaker Emotion","Safety Alignment","Jailbreaking","Audio-Language Models","Emotional Variation","Unsafe Rate","Non-refusal Rate"],"permalink":"/ai/review/2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases","title":"[논문리뷰] ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases","excerpt":"Nicholas Carlini이 [arXiv]에 게시한 'ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Reward Hacking","Benchmark Reliability","Test Exploitation","Prompt Engineering","LLM Safety","Code Generation"],"permalink":"/ai/review/2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1","title":"[논문리뷰] Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1","excerpt":"이 [arXiv]에 게시한 'Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Human-Agent Collaboration","Project Page Generation","Multi-Agent System","LLM","VLM","Webpage Automation","PageBench","Scientific Communication","Cost-Effective AI"],"permalink":"/ai/review/2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives","title":"[논문리뷰] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives","excerpt":"이 [arXiv]에 게시한 'HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Multi-Shot Video","Narrative Coherence","Diffusion Models","Self-Attention","Cinematic AI","Video Consistency","Directorial Control"],"permalink":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models","title":"[논문리뷰] From Masks to Worlds: A Hitchhiker's Guide to World Models","excerpt":"Shufan Li이 [arXiv]에 게시한 'From Masks to Worlds: A Hitchhiker's Guide to World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","World Models","Generative AI","Multimodal Learning","Masked Modeling","Interactive AI","Memory Systems","Autonomous Agents","AI Roadmap"],"permalink":"/ai/review/2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values","title":"[논문리뷰] Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values","excerpt":"이 [arXiv]에 게시한 'Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Alignment","Human Values","Reward Shaping","Value-Weighted Reward","Termination Policy","RLVR"],"permalink":"/ai/review/2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models","title":"[논문리뷰] Emergence of Linear Truth Encodings in Language Models","excerpt":"Alberto Bietti이 [arXiv]에 게시한 'Emergence of Linear Truth Encodings in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Language Models","Truth Encoding","Linear Subspaces","Mechanistic Interpretability","Transformer Models","Learning Dynamics","Truth Co-occurrence Hypothesis","Hallucinations"],"permalink":"/ai/review/2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion","title":"[논문리뷰] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion","excerpt":"이 [arXiv]에 게시한 'DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Diffusion Models","Transformer Architecture","Positional Encoding","High-Resolution Image Generation","Extrapolation","Dynamic Adaptation","Training-Free"],"permalink":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding","title":"[논문리뷰] Diff-XYZ: A Benchmark for Evaluating Diff Understanding","excerpt":"이 [arXiv]에 게시한 'Diff-XYZ: A Benchmark for Evaluating Diff Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Diff Understanding","Code Diff","Benchmark","LLMs","Code Editing","Software Engineering","Unified Diff Format","Search-Replace"],"permalink":"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence","title":"[논문리뷰] Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence","excerpt":"이 [arXiv]에 게시한 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Video Reasoning","Multimodal Large Language Models (MLLMs)","Reinforcement Learning (RLVR)","Evidence Grounding","Multi-step Reasoning","Frame Retrieval","Dataset Construction","Progressive Learning"],"permalink":"/ai/review/2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature","title":"[논문리뷰] ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature","excerpt":"이 [arXiv]에 게시한 'ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Multi-agent Systems","Large Language Models (LLMs)","Information Extraction","Scientific Literature","Materials Science","Data Curation","Piezoelectric Materials","RAG (Retrieval-Augmented Generation)"],"permalink":"/ai/review/2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models","title":"[논문리뷰] AlphaFlow: Understanding and Improving MeanFlow Models","excerpt":"이 [arXiv]에 게시한 'AlphaFlow: Understanding and Improving MeanFlow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Generative Models","Flow Matching","Consistency Models","MeanFlow","Curriculum Learning","Few-Step Generation","Image Generation"],"permalink":"/ai/review/2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders","title":"[논문리뷰] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders","excerpt":"이 [arXiv]에 게시한 'AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Speculative Decoding","Knowledge Distillation","LLM Inference","Model Acceleration","Token Filtering","Draft Model","Acceptance Rate"],"permalink":"/ai/review/2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model","title":"[논문리뷰] ARGenSeg: Image Segmentation with Autoregressive Image Generation Model","excerpt":"이 [arXiv]에 게시한 'ARGenSeg: Image Segmentation with Autoregressive Image Generation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Image Segmentation","Autoregressive Generation","Multimodal Large Language Models (MLLMs)","Visual Understanding","VQ-VAE","Multi-scale Prediction","Referring Expression Segmentation","Image Generation"],"permalink":"/ai/review/2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR","title":"[논문리뷰] olmOCR 2: Unit Test Rewards for Document OCR","excerpt":"이 [arXiv]에 게시한 'olmOCR 2: Unit Test Rewards for Document OCR' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Document OCR","Vision Language Model","Reinforcement Learning","Unit Tests","Synthetic Data Generation","RLVR","Document Parsing","State-of-the-Art OCR"],"permalink":"/ai/review/2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos","title":"[논문리뷰] VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos","excerpt":"Xinyuan Wang이 [arXiv]에 게시한 'VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","GUI Agents","Video Pretraining","Inverse Dynamics","Action Recognition","Computer Use Automation","Data Synthesis","Multimodal Learning"],"permalink":"/ai/review/2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models","title":"[논문리뷰] Unified Reinforcement and Imitation Learning for Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Unified Reinforcement and Imitation Learning for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Reinforcement Learning","Imitation Learning","Model Distillation","Lightweight VLMs","LLM-as-a-Judge","Multimodal Learning"],"permalink":"/ai/review/2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling","title":"[논문리뷰] RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling","excerpt":"Mandip Goswami이 [arXiv]에 게시한 'RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Room Impulse Response","Dataset","Room Acoustics","Machine Learning","Dereverberation","Speech Recognition","Simulation","Hugging Face"],"permalink":"/ai/review/2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge","title":"[논문리뷰] ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge","excerpt":"이 [arXiv]에 게시한 'ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Rubric-based Benchmark","Professional Knowledge","Multi-domain Tasks","LLM-Judge Bias Mitigation","Cost Reduction","Reasoning Assessment","Open-weight Models"],"permalink":"/ai/review/2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing","title":"[논문리뷰] Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing","excerpt":"이 [arXiv]에 게시한 'Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Text-Guided Image Editing","Large-Scale Dataset","Multimodal Models","Dataset Curation","Quality Control","Prompt Engineering","Preference Learning","Multi-Turn Editing"],"permalink":"/ai/review/2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models","title":"[논문리뷰] OmniNWM: Omniscient Driving Navigation World Models","excerpt":"Zhujin Liang이 [arXiv]에 게시한 'OmniNWM: Omniscient Driving Navigation World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Autonomous Driving","World Models","Multi-modal Generation","3D Occupancy","Plücker Ray-maps","Action Control","Dense Rewards","Long-term Forecasting"],"permalink":"/ai/review/2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks","title":"[논문리뷰] Machine Text Detectors are Membership Inference Attacks","excerpt":"Naoaki Okazaki이 [arXiv]에 게시한 'Machine Text Detectors are Membership Inference Attacks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Membership Inference Attacks","Machine-Generated Text Detection","Transferability","Likelihood Ratio Test","Large Language Models","Zero-Shot Detection","Model Security","AI Safety"],"permalink":"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models","title":"[논문리뷰] MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models","excerpt":"Yifan Gao이 [arXiv]에 게시한 'MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Large Multimodal Models (LMMs)","Time-Sensitive Knowledge","Temporal Reasoning","Knowledge Editing","Multimodal Benchmarking","Temporal Awareness","Dynamic Knowledge"],"permalink":"/ai/review/2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts","title":"[논문리뷰] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts","excerpt":"이 [arXiv]에 게시한 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Long Context Reasoning","Large Language Models","Multi-hop QA","Data Synthesis","Retrieval-Augmented Generation","Chain-of-Thought"],"permalink":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection","title":"[논문리뷰] Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection","excerpt":"Yi Cheng이 [arXiv]에 게시한 'Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Data Selection","Large Language Models (LLMs)","Data Diversity","Data Quality","Principal Component Analysis (PCA)","Orthogonal Dimensions","Pre-training"],"permalink":"/ai/review/2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Language-Models-are-Injective-and-Hence-Invertible","title":"[논문리뷰] Language Models are Injective and Hence Invertible","excerpt":"이 [arXiv]에 게시한 'Language Models are Injective and Hence Invertible' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Language Models","Injectivity","Invertibility","Transformer","Representation Learning","Exact Recovery","SIPIT Algorithm","Real Analysis"],"permalink":"/ai/review/2025-10-23-Language-Models-are-Injective-and-Hence-Invertible/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints","title":"[논문리뷰] KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints","excerpt":"Jinhe Bi이 [arXiv]에 게시한 'KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Knowledge Injection","Large Multimodal Models","Catastrophic Forgetting","Data Augmentation","Parameter-Efficient Fine-Tuning","Null Space","Continual Learning"],"permalink":"/ai/review/2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model","title":"[논문리뷰] GigaBrain-0: A World Model-Powered Vision-Language-Action Model","excerpt":"이 [arXiv]에 게시한 'GigaBrain-0: A World Model-Powered Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Model","World Model","Data Augmentation","Robot Generalization","Embodied AI","RGBD","Chain-of-Thought"],"permalink":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models","title":"[논문리뷰] From Charts to Code: A Hierarchical Benchmark for Multimodal Models","excerpt":"Dongxing Mao이 [arXiv]에 게시한 'From Charts to Code: A Hierarchical Benchmark for Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Chart-to-Code","Multimodal Models","Hierarchical Benchmark","Chart Understanding","Code Generation","Evaluation Metrics","Benchmarking"],"permalink":"/ai/review/2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research","title":"[논문리뷰] FinSight: Towards Real-World Financial Deep Research","excerpt":"Yutao Zhu이 [arXiv]에 게시한 'FinSight: Towards Real-World Financial Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Financial Research","Multi-Agent System","Code Generation","Multimodal Reports","Iterative Visualization","Variable Memory","Deep Learning"],"permalink":"/ai/review/2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","title":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning","excerpt":"이 [arXiv]에 게시한 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Long-Context LLM","Hybrid Attention","Linear Attention","Mixture-of-Experts","FP8 Training","GPU Optimization","Training-Inference Alignment","Reinforcement Learning"],"permalink":"/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs","title":"[논문리뷰] Directional Reasoning Injection for Fine-Tuning MLLMs","excerpt":"Jialian Wu이 [arXiv]에 게시한 'Directional Reasoning Injection for Fine-Tuning MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Reasoning Transfer","Gradient-based Fine-tuning","Model Merging","Parameter-Efficient Learning","Supervised Fine-tuning","Directional Prior"],"permalink":"/ai/review/2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models","title":"[논문리뷰] DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models","excerpt":"Roi Reichart이 [arXiv]에 게시한 'DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Semantic Leakage","Text-to-Image Models","Attention Control","Inference-time Mitigation","Diffusion Models","Evaluation Dataset","Self-Attention"],"permalink":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents","title":"[논문리뷰] DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents","excerpt":"이 [arXiv]에 게시한 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Fine-tuning","Data Mixing Optimization","Mobile Phone Agents","Downstream Task Prediction","Benchmark","Neural Networks"],"permalink":"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent","title":"[논문리뷰] ColorAgent: Building A Robust, Personalized, and Interactive OS Agent","excerpt":"Weiming Zhang이 [arXiv]에 게시한 'ColorAgent: Building A Robust, Personalized, and Interactive OS Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","OS Agent","Reinforcement Learning","Multi-agent Systems","Personalization","Proactive Interaction","GUI Agents","Self-Evolving Training"],"permalink":"/ai/review/2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping","title":"[논문리뷰] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping","excerpt":"Junrui Shen이 [arXiv]에 게시한 'BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Off-Policy Reinforcement Learning","Large Language Models","Adaptive Clipping","Policy Optimization","PPO","Entropy Preservation","RL Stabilization"],"permalink":"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-Attention-Sinks-in-Diffusion-Language-Models","title":"[논문리뷰] Attention Sinks in Diffusion Language Models","excerpt":"Simone Scardapane이 [arXiv]에 게시한 'Attention Sinks in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Attention Sinks","Transformer Architecture","Masked Language Modeling","Bidirectional Attention","Generative Models","Robustness","Dynamic Attention"],"permalink":"/ai/review/2025-10-23-Attention-Sinks-in-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library","title":"[논문리뷰] AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library","excerpt":"Chonghe Jiang이 [arXiv]에 게시한 'AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Optimization Modeling","Large Language Models (LLMs)","Experience Library","Self-Improving Systems","Continual Learning","Out-of-Distribution Generalization","Operations Research","Knowledge Representation"],"permalink":"/ai/review/2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World","title":"[논문리뷰] World-in-World: World Models in a Closed-Loop World","excerpt":"Arda Uzunoglu이 [arXiv]에 게시한 'World-in-World: World Models in a Closed-Loop World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","World Models","Embodied AI","Closed-Loop Evaluation","Online Planning","Data Scaling","Controllability","Robotic Manipulation"],"permalink":"/ai/review/2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-Video-Reasoning-without-Training","title":"[논문리뷰] Video Reasoning without Training","excerpt":"이 [arXiv]에 게시한 'Video Reasoning without Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Reasoning","Large Multimodal Models (LMMs)","Inference-Time Optimization","Entropy-Based Objective","Training-Free","KV-Cache Steering","Micro-Exploration","Macro-Exploitation"],"permalink":"/ai/review/2025-10-22-Video-Reasoning-without-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism","title":"[논문리뷰] Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism","excerpt":"Shuang Gu이 [arXiv]에 게시한 'Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Scientific Reasoning","Bio-experimental Protocol Generation","LLM","Structured Reward","SciRecipe Dataset","Sketch-and-Fill","Reinforcement Learning","Thoth"],"permalink":"/ai/review/2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation","title":"[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation","excerpt":"Yujie Zhou이 [arXiv]에 게시한 'UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Semantic Evaluation","Benchmark","Multilingual Evaluation","Fine-grained Assessment","Large Language Models","Model Evaluation","Prompt Engineering"],"permalink":"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention","title":"[논문리뷰] UltraGen: High-Resolution Video Generation with Hierarchical Attention","excerpt":"Ran Yi이 [arXiv]에 게시한 'UltraGen: High-Resolution Video Generation with Hierarchical Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Generation","High-Resolution","Diffusion Transformer","Hierarchical Attention","Global-Local Attention","Computational Efficiency","4K Synthesis"],"permalink":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning","title":"[논문리뷰] Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning","excerpt":"Yuchen Eleanor Jiang이 [arXiv]에 게시한 'Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Personalization","Reinforcement Learning","Generative Reward Model","Critique-Post-Edit","Reward Hacking","Controllable AI"],"permalink":"/ai/review/2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder","title":"[논문리뷰] ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder","excerpt":"Zonghao Guo이 [arXiv]에 게시한 'ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Vision-Language Models","CLIP","LLM-based Embedder","Knowledge Distillation","Contrastive Learning","Curriculum Learning","Multimodal Alignment","Progressive Alignment"],"permalink":"/ai/review/2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold","title":"[논문리뷰] PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold","excerpt":"이 [arXiv]에 게시한 'PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Deep Research Agent","Reinforcement Learning from AI Feedback","RLOO Algorithm","Large Language Models","Tool Use","Self-Correction","Reasoning Scaffold","Agent Alignment"],"permalink":"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies","title":"[논문리뷰] PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies","excerpt":"James Glass이 [arXiv]에 게시한 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Large Multimodal Models (LMMs)","Scientific Document Analysis","Multimodal Inconsistencies","Peer Review","Benchmark","Debiasing","JSON-based Representation","Reasoning"],"permalink":"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation","title":"[논문리뷰] MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation","excerpt":"이 [arXiv]에 게시한 'MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Long Video Generation","Sparse Attention","Diffusion Transformers","Mixture-of-Groups Attention","Token Routing","Computational Efficiency","Context Length"],"permalink":"/ai/review/2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models","title":"[논문리뷰] MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models","excerpt":"이 [arXiv]에 게시한 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Transformer","Large-scale Training","Megatron-Core","Video VAE","E-commerce AI","High-efficiency Pipeline","Preference Optimization"],"permalink":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues","title":"[논문리뷰] MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues","excerpt":"이 [arXiv]에 게시한 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Video Understanding","Benchmark","Multi-Turn Dialogues","Perceptivity","Interactivity","Evaluation"],"permalink":"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions","title":"[논문리뷰] IF-VidCap: Can Video Caption Models Follow Instructions?","excerpt":"이 [arXiv]에 게시한 'IF-VidCap: Can Video Caption Models Follow Instructions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Captioning","Instruction Following","MLLMs","Benchmark","Controllable Generation","Multimodal Evaluation","Fine-tuning"],"permalink":"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs","title":"[논문리뷰] Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs","excerpt":"이 [arXiv]에 게시한 'Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Region Understanding","Contextual Pixel Understanding","RoI-aligned Feature Replay","Compositional Reasoning","GAR-Bench","Zero-shot Video Understanding"],"permalink":"/ai/review/2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-Extracting-alignment-data-in-open-models","title":"[논문리뷰] Extracting alignment data in open models","excerpt":"이 [arXiv]에 게시한 'Extracting alignment data in open models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Alignment Data Extraction","Large Language Models","Memorization","Neural Embeddings","Semantic Similarity","Chat Templates","Model Distillation","Reinforcement Learning","Supervised Finetuning"],"permalink":"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning","title":"[논문리뷰] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning","excerpt":"Qipeng Guo이 [arXiv]에 게시한 'EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Verifiable Learning","Data Synthesis","Evolutionary Algorithm","Large Language Models","Reinforcement Learning","Model Distillation","Test Generation"],"permalink":"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence","title":"[논문리뷰] DSI-Bench: A Benchmark for Dynamic Spatial Intelligence","excerpt":"이 [arXiv]에 게시한 'DSI-Bench: A Benchmark for Dynamic Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Dynamic Spatial Reasoning","Vision-Language Models (VLMs)","Benchmark","Video Understanding","Motion Perception","3D Spatial Intelligence","Hallucinations","Bias"],"permalink":"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist","title":"[논문리뷰] Chem-R: Learning to Reason as a Chemist","excerpt":"이 [arXiv]에 게시한 'Chem-R: Learning to Reason as a Chemist' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Chemical Reasoning","Large Language Models","Chem-R","Structured Reasoning","Multi-task Optimization","Chain-of-Thought","Chemical Discovery"],"permalink":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading","title":"[논문리뷰] AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading","excerpt":"Jiashu Wang이 [arXiv]에 게시한 'AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Automated Trading","Reinforcement Learning","LLM Agents","Tool Orchestration","Financial Markets","Algorithmic Trading","Interpretable AI","ReAct"],"permalink":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling","title":"[논문리뷰] When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling","excerpt":"이 [arXiv]에 게시한 'When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","LLM Ensembling","Token-level Ensembling","Speculative Decoding","Tokenization Mismatch","Probability Sharpening","Long-form Generation","KV Cache Management"],"permalink":"/ai/review/2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling","title":"[논문리뷰] Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling","excerpt":"Dim P. Papadopoulos이 [arXiv]에 게시한 'Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Visual Autoregressive Models","Diffusion Models","Inference Time Scaling","Beam Search","Image Generation","Text-to-Image Synthesis","Discrete Latent Space"],"permalink":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback","title":"[논문리뷰] Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback","excerpt":"이 [arXiv]에 게시한 'Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Reinforcement Learning","MLLM","Policy Optimization","Finetuning","Reward Modeling","Human Alignment"],"permalink":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action","title":"[논문리뷰] UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action","excerpt":"이 [arXiv]에 게시한 'UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Computer Use Agents","Hybrid Action","Foundation Models","Reinforcement Learning","Supervised Fine-tuning","Synthetic Data Generation","Tool Learning","GUI Automation"],"permalink":"/ai/review/2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation","title":"[논문리뷰] Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation","excerpt":"이 [arXiv]에 게시한 'Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Universal RAG","Multimodal Retrieval","Mixed-Modal Data Generation","Vision-Language Models","Contrastive Learning","Matryoshka Representation Learning"],"permalink":"/ai/review/2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-RL-makes-MLLMs-see-better-than-SFT","title":"[논문리뷰] RL makes MLLMs see better than SFT","excerpt":"이 [arXiv]에 게시한 'RL makes MLLMs see better than SFT' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multimodal Language Models","Reinforcement Learning","Supervised Finetuning","Vision Encoder","Visual Representations","Direct Preference Optimization","Preference Alignment","PIVOT"],"permalink":"/ai/review/2025-10-21-RL-makes-MLLMs-see-better-than-SFT/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems","title":"[논문리뷰] QueST: Incentivizing LLMs to Generate Difficult Problems","excerpt":"이 [arXiv]에 게시한 'QueST: Incentivizing LLMs to Generate Difficult Problems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","LLM","Problem Generation","Competitive Programming","Synthetic Data","Difficulty Estimation","Rejection Fine-tuning","Graph Sampling"],"permalink":"/ai/review/2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing","title":"[논문리뷰] PICABench: How Far Are We from Physically Realistic Image Editing?","excerpt":"Kaiwen Zhu이 [arXiv]에 게시한 'PICABench: How Far Are We from Physically Realistic Image Editing?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Image Editing","Physical Realism","Benchmark","VLM-as-a-Judge","Synthetic Data","Physics-Aware AI","Diffusion Models","Evaluation Metrics"],"permalink":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators","title":"[논문리뷰] On Non-interactive Evaluation of Animal Communication Translators","excerpt":"Adam Tauman Kalai이 [arXiv]에 게시한 'On Non-interactive Evaluation of Animal Communication Translators' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Machine Translation Quality Evaluation","Reference-Free Evaluation","Animal Communication","Language Models","Shuffle Test","Conlangs","Non-interactive Evaluation"],"permalink":"/ai/review/2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models","title":"[논문리뷰] MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models","excerpt":"이 [arXiv]에 게시한 'MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multi-Turn Conversation","VLM Evaluation","Benchmark","Vision and Language Models","Contextual Understanding","Checklist-based Evaluation","Interactive AI"],"permalink":"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering","title":"[논문리뷰] Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering","excerpt":"이 [arXiv]에 게시한 'Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Visual Question Answering","Retrieval-Augmented Generation","Multimodal AI","Reinforcement Learning","Knowledge Base","Tool Learning","Information Filtering"],"permalink":"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer","title":"[논문리뷰] GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer","excerpt":"이 [arXiv]에 게시한 'GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","3D Appearance Transfer","Rectified Flow","Generative Models","Optimization-Guided Sampling","Neural Latent Representations","Training-Free","GPT-Based Evaluation"],"permalink":"/ai/review/2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression","title":"[논문리뷰] Glyph: Scaling Context Windows via Visual-Text Compression","excerpt":"Wenyi Hong이 [arXiv]에 게시한 'Glyph: Scaling Context Windows via Visual-Text Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Long-Context Modeling","Visual Compression","Vision-Language Models","Token Efficiency","Genetic Algorithms","Multimodal AI","LLM Scaling"],"permalink":"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-FineVision-Open-Data-Is-All-You-Need","title":"[논문리뷰] FineVision: Open Data Is All You Need","excerpt":"이 [arXiv]에 게시한 'FineVision: Open Data Is All You Need' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multimodal Datasets","VLM","Data Curation","Data Hygiene","De-duplication","Human-in-the-loop","GUI Automation","Test-set Decontamination"],"permalink":"/ai/review/2025-10-21-FineVision-Open-Data-Is-All-You-Need/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research","title":"[논문리뷰] Executable Knowledge Graphs for Replicating AI Research","excerpt":"이 [arXiv]에 게시한 'Executable Knowledge Graphs for Replicating AI Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","AI Research Replication","Large Language Models (LLMs)","Knowledge Graphs (KGs)","Executable Code Generation","Retrieval-Augmented Generation (RAG)","PaperBench","Automated AI Research"],"permalink":"/ai/review/2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics","title":"[논문리뷰] Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics","excerpt":"이 [arXiv]에 게시한 'Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Deep Research","Enterprise AI","Human-in-the-Loop","Steerable AI","LLM Agents","Context Engineering","Enterprise Analytics"],"permalink":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset","title":"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset","excerpt":"이 [arXiv]에 게시한 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","3D Motion Dataset","Multimodal Data","Human Behavior","Pose Tracking","Hand Tracking","Audio-Visual Data","Large-scale Dataset","SMPL-X"],"permalink":"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense","title":"[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense","excerpt":"이 [arXiv]에 게시한 'Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Large Reasoning Models (LRMs)","Prompt Injection","Adversarial Attack","Reasoning Distraction","Chain-of-Thought","Robustness","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)"],"permalink":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science","title":"[논문리뷰] DeepAnalyze: Agentic Large Language Models for Autonomous Data Science","excerpt":"이 [arXiv]에 게시한 'DeepAnalyze: Agentic Large Language Models for Autonomous Data Science' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Autonomous Data Science","Agentic LLM","Curriculum Learning","Reinforcement Learning","Data Agents","End-to-end Data Science"],"permalink":"/ai/review/2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Deep-Self-Evolving-Reasoning","title":"[논문리뷰] Deep Self-Evolving Reasoning","excerpt":"이 [arXiv]에 게시한 'Deep Self-Evolving Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Deep Self-Evolving Reasoning","LLMs","Iterative Reasoning","Markov Chain","Self-Verification","Self-Refinement","Mathematical Reasoning","AIME Benchmark"],"permalink":"/ai/review/2025-10-21-Deep-Self-Evolving-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing","title":"[논문리뷰] ConsistEdit: Highly Consistent and Precise Training-free Visual Editing","excerpt":"Xili Dai이 [arXiv]에 게시한 'ConsistEdit: Highly Consistent and Precise Training-free Visual Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Image Editing","Video Editing","Diffusion Transformer","Attention Control","Training-free","Multi-modal Diffusion Transformer (MM-DiT)","Consistency Preservation"],"permalink":"/ai/review/2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting","title":"[논문리뷰] Chronos-2: From Univariate to Universal Forecasting","excerpt":"이 [arXiv]에 게시한 'Chronos-2: From Univariate to Universal Forecasting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Time Series Forecasting","Foundation Models","Pretrained Models","Transformer","In-Context Learning","Multivariate Forecasting","Covariates","Group Attention"],"permalink":"/ai/review/2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training","title":"[논문리뷰] Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training","excerpt":"Aditya Vir이 [arXiv]에 게시한 'Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Satellite Image Classification","Multi-Task Attention","From-Scratch Training","EuroSAT Dataset","Squeeze-Excitation Networks","Coordinate Attention","CNN","Deep Learning Architecture"],"permalink":"/ai/review/2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning","title":"[논문리뷰] AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning","excerpt":"Nikos Vlassis이 [arXiv]에 게시한 'AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Real-Time Interaction","Asynchronous Agents","LLM Explanation","Human-AI Collaboration","Voice Interface","Planning and Reasoning","Context Management","Interruption Handling"],"permalink":"/ai/review/2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment","title":"[논문리뷰] Annotation-Efficient Universal Honesty Alignment","excerpt":"Jingtong Wu이 [arXiv]에 게시한 'Annotation-Efficient Universal Honesty Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","LLM Honesty Alignment","Confidence Calibration","Annotation Efficiency","Self-Consistency","Elicitation-Then-Calibration (EliCal)","HonestyBench","LoRA","Trustworthy AI"],"permalink":"/ai/review/2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe","title":"[논문리뷰] Agentic Reinforcement Learning for Search is Unsafe","excerpt":"이 [arXiv]에 게시한 'Agentic Reinforcement Learning for Search is Unsafe' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","LLM Safety","Tool Use","Search Models","Jailbreaking","Instruction Tuning","Vulnerability"],"permalink":"/ai/review/2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent","title":"[논문리뷰] VISTA: A Test-Time Self-Improving Video Generation Agent","excerpt":"Tomas Pfister이 [arXiv]에 게시한 'VISTA: A Test-Time Self-Improving Video Generation Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Prompt Optimization","Multi-Agent System","Test-Time Improvement","MLLM-as-a-Judge","Video Evaluation","Audio-Video Synthesis"],"permalink":"/ai/review/2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data","title":"[논문리뷰] Train a Unified Multimodal Data Quality Classifier with Synthetic Data","excerpt":"Ritesh Sarkhel이 [arXiv]에 게시한 'Train a Unified Multimodal Data Quality Classifier with Synthetic Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Multimodal Data Quality","MLLM","Synthetic Data","Data Filtering","Image-Text Captioning","Interleaved Document Analysis","Pre-training"],"permalink":"/ai/review/2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery","title":"[논문리뷰] Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery","excerpt":"Chung-Ho Wu이 [arXiv]에 게시한 'Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","3D Scene Synthesis","Gaussian Splatting","Satellite Imagery","Diffusion Models","Urban Modeling","Novel View Synthesis","Curriculum Learning","Real-time Rendering"],"permalink":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset","title":"[논문리뷰] Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset","excerpt":"Hao Ouyang이 [arXiv]에 게시한 'Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Video Editing","Instruction-Based Editing","Synthetic Data Generation","Dataset","Curriculum Learning","Diffusion Models","Vision-Language Models"],"permalink":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning","title":"[논문리뷰] Robust Layerwise Scaling Rules by Proper Weight Decay Tuning","excerpt":"이 [arXiv]에 게시한 'Robust Layerwise Scaling Rules by Proper Weight Decay Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Weight Decay Scaling","Maximal-Update Parameterization (µP)","AdamW","Transformer","Hyperparameter Transfer","Scaling Laws","Singular Value Spectrum","Steady State Training"],"permalink":"/ai/review/2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models","title":"[논문리뷰] Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models","excerpt":"Shiwei Liu이 [arXiv]에 게시한 'Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Online Adaptation","Test-Time Adaptation (TTA)","Expert Routing","Large Language Models (LLMs)","Self-Supervision","Computational Efficiency","Context Shift Robustness"],"permalink":"/ai/review/2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive","title":"[논문리뷰] Paper2Web: Let's Make Your Paper Alive!","excerpt":"Yao Wan이 [arXiv]에 게시한 'Paper2Web: Let's Make Your Paper Alive!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Academic Webpage Generation","Multi-Agent Systems","Large Language Models","Model Context Protocol","Interactive Content","Multimedia Dissemination","Evaluation Benchmark","Human-Computer Interaction"],"permalink":"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM","title":"[논문리뷰] OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM","excerpt":"이 [arXiv]에 게시한 'OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Omni-Modal LLM","Multimodal Understanding","Vision-Audio Alignment","Temporal Reasoning","Data Curation","Foundation Models","Contrastive Learning","Rotary Time Embedding"],"permalink":"/ai/review/2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks","title":"[논문리뷰] NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks","excerpt":"Hongyu Yan이 [arXiv]에 게시한 'NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","3D Object Editing","Training-Free","FlowEdit","Mask-Free","Deep Generative Models","TRELLIS","Data Generation","Geometric Consistency"],"permalink":"/ai/review/2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning","title":"[논문리뷰] MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning","excerpt":"이 [arXiv]에 게시한 'MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Reasoning Benchmark","Difficulty Adaptation","Multimodal AI","Proof Graph","Agent Recognition","Automated Question Generation"],"permalink":"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal","title":"[논문리뷰] LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal","excerpt":"이 [arXiv]에 게시한 'LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Lens Flare Removal","Diffusion Models","Image Outpainting","Deep Learning","Image Restoration","Preprocessing","LoRA"],"permalink":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder","title":"[논문리뷰] Latent Diffusion Model without Variational Autoencoder","excerpt":"이 [arXiv]에 게시한 'Latent Diffusion Model without Variational Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Latent Diffusion Model","Variational Autoencoder","Self-supervised Learning","DINO Features","Generative Models","Image Generation","Training Efficiency","Unified Representation"],"permalink":"/ai/review/2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Language-Models-Model-Language","title":"[논문리뷰] Language Models Model Language","excerpt":"이 [arXiv]에 게시한 'Language Models Model Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Large Language Models","Linguistics","Witold Mańczak","Frequency Hypothesis","Empirical Validation","Usage-Based Linguistics","Semantic Embeddings"],"permalink":"/ai/review/2025-10-20-Language-Models-Model-Language/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training","title":"[논문리뷰] InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training","excerpt":"Congkai Xie이 [arXiv]에 게시한 'InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","LLMs","Reinforcement Learning","Rubric-Based Training","Medical Dialogue","Open-Ended Tasks","HealthBench","RAG"],"permalink":"/ai/review/2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation","title":"[논문리뷰] Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation","excerpt":"Junsheng Yu이 [arXiv]에 게시한 'Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","3D Scene Layout Generation","Vision-guided","Diffusion Models","Scene Graph","Asset Retrieval","Pose Estimation","High-Quality Assets","AI Content Creation"],"permalink":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition","title":"[논문리뷰] Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition","excerpt":"이 [arXiv]에 게시한 'Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Foundation Models","Scientific Discovery","Paradigm Shift","Human-AI Collaboration","Autonomous Agents","Meta-Science","Experimental Design","Hypothesis Generation"],"permalink":"/ai/review/2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain","title":"[논문리뷰] FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain","excerpt":"Arman Cohan이 [arXiv]에 게시한 'FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","LLM Trustworthiness","Finance Domain","Benchmark","Alignment Evaluation","Financial AI","Hallucination","Privacy","Fairness"],"permalink":"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents","title":"[논문리뷰] Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents","excerpt":"Jianshu Zhang이 [arXiv]에 게시한 'Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Web Agents","Information Aggregation","Data Synthesis","Online Exploration","Foundation Models","Multi-hop QA","Deep Research"],"permalink":"/ai/review/2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs","title":"[논문리뷰] Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs","excerpt":"Kevin Zhu이 [arXiv]에 게시한 'Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Emergent Misalignment","In-Context Learning","LLM Safety","Persona Rationalization","Prompt Engineering","Model Alignment"],"permalink":"/ai/review/2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models","title":"[논문리뷰] ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models","excerpt":"Sean O'Brien이 [arXiv]에 게시한 'ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Multi-turn Conversation","Large Language Models (LLMs)","Context Management","Entropy-guided Resetting","Uncertainty Quantification","Performance Degradation","Prompt Engineering","Conversational AI"],"permalink":"/ai/review/2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion","title":"[논문리뷰] DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion","excerpt":"이 [arXiv]에 게시한 'DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Driving Scene Generation","Video Diffusion","3D Reconstruction","Gaussian Splatting","Feed-Forward Models","Temporal Coherence","Multimodal Control"],"permalink":"/ai/review/2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning","title":"[논문리뷰] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Length Penalty","Reasoning Efficiency","Large Language Models","RL Optimization","Accuracy-Efficiency Trade-off","Chain-of-Thought"],"permalink":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation","title":"[논문리뷰] Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation","excerpt":"Cat Yan이 [arXiv]에 게시한 'Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Multiagent Systems","Science Automation","Dynamic Workflows","Workspace-based Communication","Context Compaction","Human-in-the-loop AI","Open-source Framework"],"permalink":"/ai/review/2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation","title":"[논문리뷰] BLIP3o-NEXT: Next Frontier of Native Image Generation","excerpt":"이 [arXiv]에 게시한 'BLIP3o-NEXT: Next Frontier of Native Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Image Generation","Image Editing","Autoregressive Model","Diffusion Model","Reinforcement Learning","Multimodal AI","Foundation Model","Open-source"],"permalink":"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning","title":"[논문리뷰] A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning","excerpt":"이 [arXiv]에 게시한 'A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Adaptive Agent","Foundation Model","Hybrid Reasoning","Tool-Aware LLM","Mode Selection","Reinforcement Learning","Cost Efficiency","LLM Agent"],"permalink":"/ai/review/2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation","title":"[논문리뷰] pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation","excerpt":"이 [arXiv]에 게시한 'pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Diffusion Models","Flow Matching","Generative Models","Model Distillation","Imitation Learning","Few-Step Generation","Policy-Based AI","Text-to-Image"],"permalink":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation","title":"[논문리뷰] WithAnyone: Towards Controllable and ID Consistent Image Generation","excerpt":"이 [arXiv]에 게시한 'WithAnyone: Towards Controllable and ID Consistent Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Identity-Consistent Generation","Text-to-Image Diffusion","Copy-Paste Artifacts","Contrastive Learning","Multi-Identity Dataset","Controllable Generation","ID-Preservation"],"permalink":"/ai/review/2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA","title":"[논문리뷰] When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA","excerpt":"Artem Vazhentsev이 [arXiv]에 게시한 'When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Hallucination Detection","Multilingual LLMs","Span-Level Annotation","Synthetic Data Generation","Question Answering (QA)","Encoder Models","Uncertainty Quantification","GPT-4o"],"permalink":"/ai/review/2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning","title":"[논문리뷰] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning","excerpt":"이 [arXiv]에 게시한 'VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Video Reward Models","Multimodal Reasoning","Thinking-with-Image","Visual Reasoning","Reinforcement Learning","Chain-of-Thought","Context Management"],"permalink":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation","title":"[논문리뷰] VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation","excerpt":"이 [arXiv]에 게시한 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Agentic Framework","Unseen Concept Manipulation","Out-of-Distribution Generalization","Tool Use","Web Retrieval","Object Detection","LIBERO Simulation"],"permalink":"/ai/review/2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification","title":"[논문리뷰] VLA-0: Building State-of-the-Art VLAs with Zero Modification","excerpt":"이 [arXiv]에 게시한 'VLA-0: Building State-of-the-Art VLAs with Zero Modification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","VLA-0","Zero Modification","Text-based Action Prediction","Robot Manipulation","Large Language Models","Fine-tuning","State-of-the-Art"],"permalink":"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator","title":"[논문리뷰] VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator","excerpt":"Federico Tombari이 [arXiv]에 게시한 'VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Text-to-3D","Model Stitching","Multi-view Reconstruction","Video Generation","Latent Diffusion Models","Gaussian Splats","Pointmaps","Reward Finetuning"],"permalink":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar","title":"[논문리뷰] TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar","excerpt":"이 [arXiv]에 게시한 'TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Code LLMs","Subword Tokenization","Grammar-aware Tokenization","Semantic Preservation","Rewrite Rules","Model Robustness","Tokenization Misalignment"],"permalink":"/ai/review/2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models","title":"[논문리뷰] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models","excerpt":"이 [arXiv]에 게시한 'The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","German Commons","Large Language Models","Training Data","Openly Licensed Text","Data Curation","German NLP","Corpus Construction","Quality Filtering"],"permalink":"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis","title":"[논문리뷰] SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis","excerpt":"이 [arXiv]에 게시한 'SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","4D Novel View Synthesis","Dynamic Scenes","3D Gaussian Splatting","Cascaded Optimization","Deformation Modeling","Point Tracking","Object Segmentation"],"permalink":"/ai/review/2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models","title":"[논문리뷰] RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models","excerpt":"이 [arXiv]에 게시한 'RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","RAG Systems","Selective Refusal","Generative Evaluation","Linguistic Perturbations","LLM Evaluation","Informational Uncertainty","Model Calibration","AI Safety"],"permalink":"/ai/review/2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","title":"[논문리뷰] RealDPO: Real or Not Real, that is the Preference","excerpt":"Chenyang Si이 [arXiv]에 게시한 'RealDPO: Real or Not Real, that is the Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Direct Preference Optimization","Preference Learning","Real Data","Human Motion Synthesis","RealDPO","RealAction-5K"],"permalink":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems","title":"[논문리뷰] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems","excerpt":"이 [arXiv]에 게시한 'RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Retrieval Augmented Generation","Agentic Systems","Benchmarking","Intermediate Tasks","Error Analysis","LLM Evaluation"],"permalink":"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Qwen3Guard-Technical-Report","title":"[논문리뷰] Qwen3Guard Technical Report","excerpt":"이 [arXiv]에 게시한 'Qwen3Guard Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLM Safety","Guardrail Models","Multilingual AI","Real-time Moderation","Tri-class Classification","Instruction Tuning","Streaming Inference"],"permalink":"/ai/review/2025-10-17-Qwen3Guard-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation","title":"[논문리뷰] Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation","excerpt":"이 [arXiv]에 게시한 'Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Human-human Interaction","Pose Animation","Diffusion Models","Generative AI","Motion Synthesis","Interactive Poses","Temporal Priors","Spatial Priors"],"permalink":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model","title":"[논문리뷰] PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model","excerpt":"이 [arXiv]에 게시한 'PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Document Parsing","Vision-Language Model","Multilingual OCR","Layout Analysis","Resource-Efficient AI","Table Recognition","Formula Recognition","Chart Recognition"],"permalink":"/ai/review/2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-On-Pretraining-for-Project-Level-Code-Completion","title":"[논문리뷰] On Pretraining for Project-Level Code Completion","excerpt":"이 [arXiv]에 게시한 'On Pretraining for Project-Level Code Completion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Code LLMs","Project-level Context","Code Completion","Context Window Extension","RoPE Scaling","Repository Pretraining","Long Code Arena"],"permalink":"/ai/review/2025-10-17-On-Pretraining-for-Project-Level-Code-Completion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems","title":"[논문리뷰] MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems","excerpt":"Feiyu Xiong이 [arXiv]에 게시한 'MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation (RAG)","Document Memory","Text Chunking","Small Language Models (SLMs)","Large Language Models (LLMs)","Scenario-Aware Processing","Multi-Layer Retrieval","Cognitive Simulation"],"permalink":"/ai/review/2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning","title":"[논문리뷰] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning","excerpt":"Ke Wang이 [arXiv]에 게시한 'MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Visual Chain-of-Thought (VCoT)","Large Multimodal Models (LMMs)","Geometric Reasoning","Diagram Generation","Dataset","Benchmark"],"permalink":"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning","title":"[논문리뷰] LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning","excerpt":"이 [arXiv]에 게시한 'LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Layer Skipping","Multi-stage Reasoning","Latency Optimization","Early Exit","Small Language Models (LLMs)","Adaptive Computation","Confidence-based Decoding"],"permalink":"/ai/review/2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs","title":"[논문리뷰] Learning an Image Editing Model without Image Editing Pairs","excerpt":"이 [arXiv]에 게시한 'Learning an Image Editing Model without Image Editing Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Vision-Language Models (VLMs)","No-Pair Training","Few-step Generation","Distribution Matching","Gradient-based Optimization"],"permalink":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know","title":"[논문리뷰] Large Language Models Do NOT Really Know What They Don't Know","excerpt":"이 [arXiv]에 게시한 'Large Language Models Do NOT Really Know What They Don't Know' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLMs","Hallucination Detection","Mechanistic Interpretability","Internal States","Knowledge Recall","Refusal Tuning","Factual Associations","Associated Hallucinations"],"permalink":"/ai/review/2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding","title":"[논문리뷰] LaSeR: Reinforcement Learning with Last-Token Self-Rewarding","excerpt":"이 [arXiv]에 게시한 'LaSeR: Reinforcement Learning with Last-Token Self-Rewarding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM","Self-Verification","Last-Token","Reward Modeling","Efficiency","Reasoning","RLVR"],"permalink":"/ai/review/2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training","title":"[논문리뷰] LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training","excerpt":"이 [arXiv]에 게시한 'LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLM","Digital Agents","UI Simulation","Synthetic Data Generation","Targeted Data Synthesis","World Models"],"permalink":"/ai/review/2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-LLM-guided-Hierarchical-Retrieval","title":"[논문리뷰] LLM-guided Hierarchical Retrieval","excerpt":"이 [arXiv]에 게시한 'LLM-guided Hierarchical Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Information Retrieval","Large Language Models","Hierarchical Retrieval","Semantic Tree","Tree Traversal","Zero-shot Performance","Reasoning-based Retrieval","Computational Efficiency"],"permalink":"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents","title":"[논문리뷰] Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents","excerpt":"이 [arXiv]에 게시한 'Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Multi-Turn Interactions","Reward Sparsity","Information Gain","Policy Optimization","Ground-Truth Awareness","Sample Efficiency"],"permalink":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","title":"[논문리뷰] ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints","excerpt":"이 [arXiv]에 게시한 'ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Video Generation","Test-Time Search","Diffusion Models","Semantic Dependency","Adaptive Reward","Evaluation Benchmark","Prompt-Guided"],"permalink":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale","title":"[논문리뷰] From Pixels to Words -- Towards Native Vision-Language Primitives at Scale","excerpt":"이 [arXiv]에 게시한 'From Pixels to Words -- Towards Native Vision-Language Primitives at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Native VLMs","Early Fusion","Multimodal Learning","Transformer Architecture","Rotary Position Embeddings","Pixel-Word Alignment","End-to-End Training"],"permalink":"/ai/review/2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report","title":"[논문리뷰] Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report","excerpt":"이 [arXiv]에 게시한 'Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","ColBERT","Retrieval Models","Small Models","Distillation","Long Context","Edge AI","Information Retrieval","RAG"],"permalink":"/ai/review/2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning","title":"[논문리뷰] Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning","excerpt":"Sijia Gu이 [arXiv]에 게시한 'Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Mixture of Experts (MoE)","Robotic Manipulation","Expert Specialization","Decoupled Routing","Load Balancing","Transfer Learning"],"permalink":"/ai/review/2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models","title":"[논문리뷰] Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models","excerpt":"이 [arXiv]에 게시한 'Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Recurrent-Depth Models","Diffusion Forcing","Parallel Sampling","LLM Inference Acceleration","Transformer Architectures","Generative AI","Latent Space Diffusion"],"permalink":"/ai/review/2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation","title":"[논문리뷰] DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation","excerpt":"이 [arXiv]에 게시한 'DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Multimodal Generation","Dialect Robustness","Text-to-Image","Text-to-Video","Benchmarking","Diffusion Models","Text Encoder Tuning","Low-Resource Dialects"],"permalink":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes","title":"[논문리뷰] COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes","excerpt":"이 [arXiv]에 게시한 'COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Chinese Creative Writing","Process Supervision","LLM Training","Dataset Creation","Cross-Lingual Transfer","Narrative Logic","Linguistic Expression","Type-Token Ratio"],"permalink":"/ai/review/2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-BitNet-Distillation","title":"[논문리뷰] BitNet Distillation","excerpt":"이 [arXiv]에 게시한 'BitNet Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Low-bit Quantization","LLM Compression","Knowledge Distillation","Ternary Weights","Inference Optimization","Memory Efficiency","SubLN","Continual Pre-training"],"permalink":"/ai/review/2025-10-17-BitNet-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts","title":"[논문리뷰] Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts","excerpt":"이 [arXiv]에 게시한 'Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Role-playing LLMs","Multiversal Consistency","Character Benchmarking","Moral Dilemmas","Canon Events","Reasoning-Acting Alignment","Chain-of-Thought","Superheroes"],"permalink":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures","title":"[논문리뷰] Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures","excerpt":"이 [arXiv]에 게시한 'Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Subjective Preference Learning","Writing Evaluation","Reward Models","RLHF","Cross-Cultural AI","Generative Models","Language Model Judges","Genre Instability"],"permalink":"/ai/review/2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs","title":"[논문리뷰] Attention Is All You Need for KV Cache in Diffusion LLMs","excerpt":"이 [arXiv]에 게시한 'Attention Is All You Need for KV Cache in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","KV Cache","Adaptive Caching","Inference Optimization","Attention Mechanism","Latency Reduction","Generative AI"],"permalink":"/ai/review/2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization","title":"[논문리뷰] Agentic Entropy-Balanced Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Agentic Entropy-Balanced Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Web Agents","Tool Learning","Entropy Balancing","Policy Optimization","Rollout Strategy","Large Language Models"],"permalink":"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses","title":"[논문리뷰] AI for Service: Proactive Assistance with AI Glasses","excerpt":"이 [arXiv]에 게시한 'AI for Service: Proactive Assistance with AI Glasses' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","AI for Service","Proactive AI","AI Glasses","Multi-agent System","Human-AI Interaction","Context-aware AI","Wearable AI"],"permalink":"/ai/review/2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model","title":"[논문리뷰] X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model","excerpt":"Xirui Kang이 [arXiv]에 게시한 'X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA) Models","Soft Prompts","Transformer","Cross-Embodiment","Robotics","Pretraining","Domain Adaptation","Flow Matching"],"permalink":"/ai/review/2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification","title":"[논문리뷰] Universal Image Restoration Pre-training via Masked Degradation Classification","excerpt":"이 [arXiv]에 게시한 'Universal Image Restoration Pre-training via Masked Degradation Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Universal Image Restoration","Pre-training","Masked Image Modeling","Degradation Classification","Deep Learning","Computer Vision","Self-supervised Learning","Low-level Vision"],"permalink":"/ai/review/2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE","title":"[논문리뷰] UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE","excerpt":"이 [arXiv]에 게시한 'UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Mixture of Experts","Speech Generation","Music Generation","Multimodal AI","Dynamic Routing","Training Curriculum","Data Imbalance","Audio Synthesis"],"permalink":"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning","title":"[논문리뷰] UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning","excerpt":"Ziyong Feng이 [arXiv]에 게시한 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal Embeddings","MLLM-as-a-Judge","Hard Negative Mining","Semantic Alignment","Representation Learning","Reranking","Contrastive Learning"],"permalink":"/ai/review/2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark","title":"[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark","excerpt":"이 [arXiv]에 게시한 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal AI","Unified Models","Benchmark","Generation","Understanding","Reasoning","Evaluation","Cross-modal Synergy"],"permalink":"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields","title":"[논문리뷰] Trace Anything: Representing Any Video in 4D via Trajectory Fields","excerpt":"이 [arXiv]에 게시한 'Trace Anything: Representing Any Video in 4D via Trajectory Fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","4D Video Representation","Trajectory Fields","Neural Networks","Spatio-temporal Modeling","3D Point Tracking","Motion Forecasting","Computer Vision","B-splines"],"permalink":"/ai/review/2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research","title":"[논문리뷰] The Role of Computing Resources in Publishing Foundation Model Research","excerpt":"Zhenwen Liang이 [arXiv]에 게시한 'The Role of Computing Resources in Publishing Foundation Model Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Foundation Models","Computing Resources","GPU Disparity","AI Research","Publication Bias","Resource Allocation","Research Transparency"],"permalink":"/ai/review/2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs","title":"[논문리뷰] The Art of Scaling Reinforcement Learning Compute for LLMs","excerpt":"이 [arXiv]에 게시한 'The Art of Scaling Reinforcement Learning Compute for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Scaling Laws","Compute Efficiency","Predictability","Sigmoidal Curves","ScaleRL","Off-Policy RL"],"permalink":"/ai/review/2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs","title":"[논문리뷰] Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs","excerpt":"Hao Zhang이 [arXiv]에 게시한 'Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Reinforcement Learning (RL)","Multi-Agent Systems (MAS)","On-Policy RL","Collaborative AI","Agentic LLMs","Group-based Optimization"],"permalink":"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning","title":"[논문리뷰] Revisiting Model Interpolation for Efficient Reasoning","excerpt":"이 [arXiv]에 게시한 'Revisiting Model Interpolation for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Model Interpolation","Efficient Reasoning","Large Language Models","Chain-of-Thought","Model Merging","Performance Dynamics","Ablation Study"],"permalink":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World","title":"[논문리뷰] Reasoning in Space via Grounding in the World","excerpt":"Li Zhang이 [arXiv]에 게시한 'Reasoning in Space via Grounding in the World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","3D Visual Grounding","Spatial Reasoning","Large Language Models (LLMs)","Chain-of-Thought (CoT)","Hybrid Representation","Multi-modal LLMs","Point Clouds"],"permalink":"/ai/review/2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models","title":"[논문리뷰] Point Prompting: Counterfactual Tracking with Video Diffusion Models","excerpt":"Andrew Owens이 [arXiv]에 게시한 'Point Prompting: Counterfactual Tracking with Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Video Diffusion Models","Point Tracking","Zero-Shot Learning","Counterfactual Modeling","Visual Prompting","SDEdit","Negative Prompting","Object Permanence"],"permalink":"/ai/review/2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","title":"[논문리뷰] PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning","excerpt":"Hengshuang Zhao이 [arXiv]에 게시한 'PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Video Generation","Physical Plausibility","Reinforcement Learning","Direct Preference Optimization","Physical Representation","Diffusion Models","World Models","Image-to-Video"],"permalink":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs","title":"[논문리뷰] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs","excerpt":"이 [arXiv]에 게시한 'ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Parallel Decoding","Speed-Quality Trade-off","Benchmark","Token Dependencies","Unmasking Strategies","Information Theory"],"permalink":"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention","title":"[논문리뷰] NOSA: Native and Offloadable Sparse Attention","excerpt":"Zhiyuan Liu이 [arXiv]에 게시한 'NOSA: Native and Offloadable Sparse Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Sparse Attention","KV Cache Offloading","LLMs","Decoding Throughput","Locality Constraint","Memory Optimization","Trainable Sparse Attention"],"permalink":"/ai/review/2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training","title":"[논문리뷰] MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training","excerpt":"이 [arXiv]에 게시한 'MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multi-turn Text-to-SQL","Agentic Training","Reinforcement Learning","Large Language Models","Dialogue Systems","Semantic Parsing","Database Interaction","Self-correction"],"permalink":"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model","title":"[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model","excerpt":"Wieland Brendel이 [arXiv]에 게시한 'MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Mathematical Reasoning","Benchmark","Large Language Models (LLMs)","Exploration","Boundary Expansion","MATH-Beyond"],"permalink":"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models","title":"[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","Robustness Analysis","Generalization","Perturbations","Benchmark","LIBERO-Plus","Multimodal AI"],"permalink":"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy","title":"[논문리뷰] InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy","excerpt":"Yilun Chen이 [arXiv]에 게시한 'InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Robotics","Vision-Language-Action (VLA)","Spatial Grounding","Generalist Policy","Multimodal Learning","Instruction Following","Simulation-to-Real","Diffusion Models"],"permalink":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue","title":"[논문리뷰] InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue","excerpt":"Dongchuan Ran이 [arXiv]에 게시한 'InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Omni-modal LLM","Audio-Visual Dialogue","Multi-turn Interaction","Speech Generation","Long-term Memory","Multimodal Understanding","End-to-end Training"],"permalink":"/ai/review/2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication","title":"[논문리뷰] HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication","excerpt":"Haochen You이 [arXiv]에 게시한 'HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Large Language Model","Multi-agent Systems","Multi-agent Communication","Graph Neural Networks","Hypergraph","Topology Optimization","Variational Autoencoder","Sparsity Regularization"],"permalink":"/ai/review/2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain","title":"[논문리뷰] Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain","excerpt":"Lingxi Lu이 [arXiv]에 게시한 'Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Large Language Models","Syntactic Structure","Human Brain","Frequency Tagging","Neuroscience","Model Interpretability","Representational Similarity Analysis","Intracranial EEG"],"permalink":"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math","title":"[논문리뷰] Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math","excerpt":"이 [arXiv]에 게시한 'Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Verification","Math Reasoning","Step-Level Verification","Benchmark","Open-Ended Problems","Process Reward Models","Generative Critics"],"permalink":"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search","title":"[논문리뷰] GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search","excerpt":"Zijian Zhang이 [arXiv]에 게시한 'GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Agents","Multi-Agent Systems","Failure Tracing","Root Cause Analysis","Information Dependency Graph","Reinforcement Learning","Deep Search"],"permalink":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner","title":"[논문리뷰] Generative Universal Verifier as Multimodal Meta-Reasoner","excerpt":"이 [arXiv]에 게시한 'Generative Universal Verifier as Multimodal Meta-Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal AI","Visual Verification","Generative Models","Self-Refinement","Vision-Language Models","Test-Time Scaling","Reasoning"],"permalink":"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds","title":"[논문리뷰] FlashWorld: High-quality 3D Scene Generation within Seconds","excerpt":"Chunchao Guo이 [arXiv]에 게시한 'FlashWorld: High-quality 3D Scene Generation within Seconds' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Diffusion Models","Multi-View Synthesis","3D Gaussian Splatting","Knowledge Distillation","Real-time Generation","High-Quality Rendering","Cross-modal Training"],"permalink":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model","title":"[논문리뷰] FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model","excerpt":"Dawei Liang이 [arXiv]에 게시한 'FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Vision-Language Alignment","Fine-grained Understanding","Bilingual Model","Contrastive Learning","Multimodal Retrieval","Open-Vocabulary Detection","Region-Text Matching"],"permalink":"/ai/review/2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling","title":"[논문리뷰] EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling","excerpt":"Ahmet Üstün이 [arXiv]에 게시한 'EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM","Inference-Time Scaling","Entropy-Aware Generation","Adaptive Budget Allocation","Reasoning Benchmarks","Computational Efficiency","Chain-of-Thought"],"permalink":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Direct-Multi-Token-Decoding","title":"[논문리뷰] Direct Multi-Token Decoding","excerpt":"Xifeng Yan이 [arXiv]에 게시한 'Direct Multi-Token Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Inference","Multi-token Decoding","Transformer Architecture","Layer Specialization","Cyclical Refilling","Inference Speedup","Model Scaling"],"permalink":"/ai/review/2025-10-16-Direct-Multi-Token-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs","title":"[논문리뷰] Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs","excerpt":"이 [arXiv]에 게시한 'Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM","NPC","Game Dialogue","Persona-Grounded Dialogue","Task Execution","Prompt Engineering","Fine-tuning","Deflanderization"],"permalink":"/ai/review/2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving","title":"[논문리뷰] CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving","excerpt":"이 [arXiv]에 게시한 'CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Imitation Learning","Reinforcement Learning","World Models","Latent Space","Dual-Policy","Competitive Learning"],"permalink":"/ai/review/2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","title":"[논문리뷰] CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving","excerpt":"Jingcheng Ni이 [arXiv]에 게시한 'CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Video Generation","Diffusion Models","Spatial-Temporal Reconstruction","3D Gaussian Splatting","Variational Autoencoder","World Modeling","Multi-View Video"],"permalink":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs","title":"[논문리뷰] Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs","excerpt":"이 [arXiv]에 게시한 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Data Curation","Supervised Fine-tuning","Chain-of-Thought","Open-source AI","Data Quality","MLLM Training"],"permalink":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization","title":"[논문리뷰] Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Attention Mechanisms","Reinforcement Learning","Credit Assignment","Policy Optimization","Interpretability","Preplan-and-Anchor Rhythm","Generative Models"],"permalink":"/ai/review/2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions","title":"[논문리뷰] What If : Understanding Motion Through Sparse Interactions","excerpt":"이 [arXiv]에 게시한 'What If : Understanding Motion Through Sparse Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Motion Understanding","Sparse Interactions","Multimodal Prediction","Flow Poke Transformer","Physical Scene Dynamics","Uncertainty Quantification","Generative Models","Computer Vision"],"permalink":"/ai/review/2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution","title":"[논문리뷰] ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution","excerpt":"이 [arXiv]에 게시한 'ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Dynamic Resolution","Token Compression","Semantic Awareness","Visual Consistency Learning (ViCO)","Visual Resolution Router (ViR)","Inference Optimization"],"permalink":"/ai/review/2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation","title":"[논문리뷰] UniFusion: Vision-Language Model as Unified Encoder in Image Generation","excerpt":"이 [arXiv]에 게시한 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Unified Encoder","Image Generation","Diffusion Models","Multimodal Learning","Text-to-Image","Image Editing","Zero-shot Learning"],"permalink":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Tensor-Logic-The-Language-of-AI","title":"[논문리뷰] Tensor Logic: The Language of AI","excerpt":"Pedro Domingos이 [arXiv]에 게시한 'Tensor Logic: The Language of AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Tensor Logic","Neurosymbolic AI","Logic Programming","Tensor Algebra","Deep Learning","Automated Reasoning","Embedding Space"],"permalink":"/ai/review/2025-10-15-Tensor-Logic-The-Language-of-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models","title":"[논문리뷰] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models","excerpt":"이 [arXiv]에 게시한 'Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Diffusion Models","Generative Models","Guidance","On-Manifold Sampling","Temporal Alignment","Score Approximation Error","Training-Free Guidance"],"permalink":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale","title":"[논문리뷰] SynthID-Image: Image watermarking at internet scale","excerpt":"이 [arXiv]에 게시한 'SynthID-Image: Image watermarking at internet scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Image Watermarking","AI-Generated Content","Provenance","Robustness","Security","Deep Learning","Internet Scale","Post-hoc"],"permalink":"/ai/review/2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model","title":"[논문리뷰] Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model","excerpt":"이 [arXiv]에 게시한 'Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Spatial Perception","Implicit Representation Alignment","3D Foundation Models","Robotics","Data Efficiency","Representation Learning"],"permalink":"/ai/review/2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning","title":"[논문리뷰] Scaling Language-Centric Omnimodal Representation Learning","excerpt":"이 [arXiv]에 게시한 'Scaling Language-Centric Omnimodal Representation Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal Embeddings","MLLMs","Contrastive Learning","Cross-modal Alignment","Generative Pretraining","Representation Learning","Scaling Laws"],"permalink":"/ai/review/2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models","title":"[논문리뷰] SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models","excerpt":"이 [arXiv]에 게시한 'SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Self-Rewarding","Text-to-Image Generation","Image Understanding","Post-Training","Global-Local Reward","Compositional Reasoning"],"permalink":"/ai/review/2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model","title":"[논문리뷰] SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model","excerpt":"이 [arXiv]에 게시한 'SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Omni-modal Embedding","Multimodal Learning","Recommendation Systems","Hard Negative Mining","Contrastive Learning","Large Language Models (LLMs)","Data Balancing","Multitask Learning"],"permalink":"/ai/review/2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Robot-Learning-A-Tutorial","title":"[논문리뷰] Robot Learning: A Tutorial","excerpt":"이 [arXiv]에 게시한 'Robot Learning: A Tutorial' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Robot Learning","Reinforcement Learning","Imitation Learning","Behavioral Cloning","Vision-Language-Action Models","Diffusion Models","Transformers","LeRobot"],"permalink":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability","title":"[논문리뷰] ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability","excerpt":"Tsui-Wei Weng이 [arXiv]에 게시한 'ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Trustworthy AI","Large Reasoning Models (LRMs)","Interpretability","Faithfulness","Reliability","Chain-of-Thought (CoT)","Supervised Fine-tuning (SFT)","GRPO"],"permalink":"/ai/review/2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration","title":"[논문리뷰] One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration","excerpt":"Mohit Bansal이 [arXiv]에 게시한 'One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Symbolic World Models","Stochastic Environments","Unguided Exploration","Probabilistic Programming","Law Synthesis","Crafter-OO","Program Synthesis"],"permalink":"/ai/review/2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks","title":"[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks","excerpt":"Xueyuan Lin이 [arXiv]에 게시한 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Long-Horizon Tasks","Agentic AI","Context Curation","Working Memory","Reinforcement Learning","Policy Optimization","Large Language Models","Memory-as-Action"],"permalink":"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces","title":"[논문리뷰] MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces","excerpt":"Sungchul Kim이 [arXiv]에 게시한 'MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","UI Evaluation","Human Perception","Benchmarking","UX Research","MLLM-as-a-Judge","Cognitive Factors","Pairwise Comparison"],"permalink":"/ai/review/2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens","title":"[논문리뷰] LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens","excerpt":"이 [arXiv]에 게시한 'LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Machine Translation (MT)","Chain-of-Thought (CoT)","Knowledge Distillation","Fine-tuning","Prompt Engineering","Synthetic Data"],"permalink":"/ai/review/2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation","title":"[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation","excerpt":"이 [arXiv]에 게시한 'Information-Preserving Reformulation of Reasoning Traces for Antidistillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Antidistillation","Reasoning Traces","Large Language Models","Knowledge Distillation","Information Preservation","Trace Reformulation","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners","title":"[논문리뷰] HoneyBee: Data Recipes for Vision-Language Reasoners","excerpt":"이 [arXiv]에 게시한 'HoneyBee: Data Recipes for Vision-Language Reasoners' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Data Curation","Chain-of-Thought","VL Reasoning","Dataset Scaling","Supervised Finetuning","HONEYBEE","Test-Time Scaling"],"permalink":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution","title":"[논문리뷰] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution","excerpt":"Yihao Liu이 [arXiv]에 게시한 'FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Video Super-Resolution (VSR)","Diffusion Models","Real-time VSR","Streaming VSR","Sparse Attention","Distillation","Conditional Decoder","High-resolution"],"permalink":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning","title":"[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding & Reasoning","excerpt":"이 [arXiv]에 게시한 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Experiment Video Understanding","Multimodal Large Language Models (MLLMs)","Scientific Reasoning","Benchmark","Wet-Lab Experiments","Procedural Understanding","Fine-grained Perception","Video QA"],"permalink":"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning","title":"[논문리뷰] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Embodied AI","Vision Language Models (VLMs)","Reinforcement Learning (RL)","Prior Learning","Supervised Fine-tuning (SFT)","Embodied Agents"],"permalink":"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs","title":"[논문리뷰] Dr.LLM: Dynamic Layer Routing in LLMs","excerpt":"이 [arXiv]에 게시한 'Dr.LLM: Dynamic Layer Routing in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Dynamic Routing","LLMs","Adaptive Depth","Computational Efficiency","Monte Carlo Tree Search (MCTS)","Retrofittable Framework","Supervised Learning","Accuracy Improvement"],"permalink":"/ai/review/2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Detect-Anything-via-Next-Point-Prediction","title":"[논문리뷰] Detect Anything via Next Point Prediction","excerpt":"이 [arXiv]에 게시한 'Detect Anything via Next Point Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Object Detection","Coordinate Prediction","Reinforcement Learning","Supervised Fine-tuning","Visual Perception","Zero-shot Learning","Spatial Reasoning"],"permalink":"/ai/review/2025-10-15-Detect-Anything-via-Next-Point-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search","title":"[논문리뷰] DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search","excerpt":"이 [arXiv]에 게시한 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Web Search","Visual Question Answering","Reinforcement Learning","Image Cropping","Self-Correction","Tool Use"],"permalink":"/ai/review/2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation","title":"[논문리뷰] DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation","excerpt":"이 [arXiv]에 게시한 'DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Machine Translation Evaluation","Large Language Models (LLMs)","Web Novel Translation","Multi-Agent Systems","Cultural Nuance","Benchmark Dataset","Natural Language Generation"],"permalink":"/ai/review/2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models","title":"[논문리뷰] Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models","excerpt":"이 [arXiv]에 게시한 'Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Diffusion Large Language Models","Reinforcement Learning","Memory Efficiency","Monte Carlo Sampling","Log-Likelihood Approximation","Policy Optimization","ELBO"],"permalink":"/ai/review/2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training","title":"[논문리뷰] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training","excerpt":"이 [arXiv]에 게시한 'Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Pixel-space Generative Models","Diffusion Models","Consistency Models","Self-supervised Pre-training","End-to-end Training","Image Generation","FID","Representation Learning"],"permalink":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models","title":"[논문리뷰] A Survey of Vibe Coding with Large Language Models","excerpt":"이 [arXiv]에 게시한 'A Survey of Vibe Coding with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vibe Coding","Large Language Models","Coding Agents","Human-AI Collaboration","Software Engineering","Development Models","Context Engineering"],"permalink":"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression","title":"[논문리뷰] Which Heads Matter for Reasoning? RL-Guided KV Cache Compression","excerpt":"Huan Wang이 [arXiv]에 게시한 'Which Heads Matter for Reasoning? RL-Guided KV Cache Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","KV Cache Compression","Large Language Models (LLMs)","Reinforcement Learning (RL)","Reasoning Models","Attention Heads","Chain-of-Thought (CoT)","Memory Efficiency"],"permalink":"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels","title":"[논문리뷰] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels","excerpt":"이 [arXiv]에 게시한 'Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Data Pipeline","Web-scale Data","Question-Answering (QA)","Data Generation","Data Diversity","Data Efficiency"],"permalink":"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Understanding-DeepResearch-via-Reports","title":"[논문리뷰] Understanding DeepResearch via Reports","excerpt":"Chengen Huang이 [arXiv]에 게시한 'Understanding DeepResearch via Reports' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","DeepResearch Agents","LLM-as-a-Judge","Report Evaluation","Agentic AI","Factuality","Redundancy","Research Automation","Benchmark"],"permalink":"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation","title":"[논문리뷰] Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation","excerpt":"Linyi Jin이 [arXiv]에 게시한 'Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Unified Multimodal Model","Camera-Centric","Image Understanding","Image Generation","Spatial Reasoning","Camera Parameters","Instruction Tuning","Multimodal Spatial Intelligence"],"permalink":"/ai/review/2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation","title":"[논문리뷰] Temporal Prompting Matters: Rethinking Referring Video Object Segmentation","excerpt":"Sifei Liu이 [arXiv]에 게시한 'Temporal Prompting Matters: Rethinking Referring Video Object Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Referring Video Object Segmentation","Foundation Models","Prompt Engineering","Object Tracking","SAM","Video Analysis","Prompt Preference Learning"],"permalink":"/ai/review/2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control","title":"[논문리뷰] TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control","excerpt":"Adityan Jothi이 [arXiv]에 게시한 'TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Diffusion Models","Conditional Generation","LoRA","Hypernetwork","Dynamic Weight Adaptation","Generative AI","Controllable Generation"],"permalink":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams","title":"[논문리뷰] StreamingVLM: Real-Time Understanding for Infinite Video Streams","excerpt":"Kelly Peng이 [arXiv]에 게시한 'StreamingVLM: Real-Time Understanding for Infinite Video Streams' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Video Stream Understanding","Real-Time VLM","Attention Sink","KV Cache Management","Contiguous RoPE","Supervised Fine-tuning","Long-Context Video"],"permalink":"/ai/review/2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics","title":"[논문리뷰] StatEval: A Comprehensive Benchmark for Large Language Models in Statistics","excerpt":"이 [arXiv]에 게시한 'StatEval: A Comprehensive Benchmark for Large Language Models in Statistics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Statistical Reasoning","LLM Benchmark","Statistics Education","Proof Verification","Multi-agent Pipeline","Automated Extraction","Evaluation Framework"],"permalink":"/ai/review/2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation","title":"[논문리뷰] Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation","excerpt":"Han Shi이 [arXiv]에 게시한 'Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Text-to-Image Generation","Inference Acceleration","Jacobi Decoding","Denoising Diffusion Models","Speculative Decoding","Multi-token Prediction","Fine-tuning"],"permalink":"/ai/review/2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km","title":"[논문리뷰] SpaceVista: All-Scale Visual Spatial Reasoning from mm to km","excerpt":"Kaituo Feng이 [arXiv]에 게시한 'SpaceVista: All-Scale Visual Spatial Reasoning from mm to km' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Spatial Reasoning","Multi-Scale Vision","MLLM","Dataset","Scale Experts","Reinforcement Learning","Computer Vision","Robotics"],"permalink":"/ai/review/2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review","title":"[논문리뷰] ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review","excerpt":"Christopher Pal이 [arXiv]에 게시한 'ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Peer Review","AI-Assisted Review","Large Language Models","LLM Agents","Meta-Review","Conference Submissions","Reviewer Personas","Evaluation Metrics"],"permalink":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth","title":"[논문리뷰] R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?","excerpt":"이 [arXiv]에 게시한 'R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Long-Horizon Reasoning","Query Composition","Large Reasoning Models","Reinforcement Learning","Benchmark Evaluation","Thinking Budget","Performance Degradation","Chain-of-Thought"],"permalink":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition","title":"[논문리뷰] Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition","excerpt":"Shang-Tse Chen이 [arXiv]에 게시한 'Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","ASR","Pseudo-labeling","Domain Adaptation","Task Arithmetic","Correction Vector","Accent Adaptation","Speaker Clustering","Model Editing"],"permalink":"/ai/review/2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction","title":"[논문리뷰] Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction","excerpt":"danxuhk이 [arXiv]에 게시한 'Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","3D Occupancy Prediction","Open Vocabulary","Gaussian Splatting","Transformer","Progressive Densification","Anisotropy-aware Sampling","Autonomous Driving"],"permalink":"/ai/review/2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs","title":"[논문리뷰] PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs","excerpt":"Xu Zheng이 [arXiv]에 게시한 'PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Physical Tool Understanding","Benchmarking","Embodied AI","Visual Question Answering (VQA)","Tool Affordances","Reasoning"],"permalink":"/ai/review/2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models","title":"[논문리뷰] Parallel Test-Time Scaling for Latent Reasoning Models","excerpt":"이 [arXiv]에 게시한 'Parallel Test-Time Scaling for Latent Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Latent Reasoning","Test-Time Scaling","Parallel Inference","Stochastic Sampling","Monte Carlo Dropout","Additive Gaussian Noise","Latent Reward Model","Trajectory Aggregation"],"permalink":"/ai/review/2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework","title":"[논문리뷰] One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework","excerpt":"Giuseppe Amato이 [arXiv]에 게시한 'One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Zero-Shot Captioning","Region-Level Captioning","Vision Transformers","DINOv2","Patch-Centric","Modality Gap Mitigation","Visual-Language Models"],"permalink":"/ai/review/2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs","title":"[논문리뷰] Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs","excerpt":"이 [arXiv]에 게시한 'Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal AI","Prompt Optimization","MLLMs","Bayesian Optimization","Cross-modal Alignment","Prompt Engineering","Generative AI","Exploration-Exploitation"],"permalink":"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping","title":"[논문리뷰] Mitigating Overthinking through Reasoning Shaping","excerpt":"Wen Luo이 [arXiv]에 게시한 'Mitigating Overthinking through Reasoning Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Large Reasoning Models (LRMs)","RLVR","Overthinking Mitigation","Reasoning Shaping","Segment-level Penalization","Computational Efficiency","Training Stability","Length-aware Weighting"],"permalink":"/ai/review/2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval","title":"[논문리뷰] MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval","excerpt":"Tingyu Song이 [arXiv]에 게시한 'MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal Retrieval","Benchmark","Reasoning","Multidisciplinary","Expert-Level","Image-Text Interleaving","Contradiction Retrieval"],"permalink":"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone","title":"[논문리뷰] KORMo: Korean Open Reasoning Model for Everyone","excerpt":"이 [arXiv]에 게시한 'KORMo: Korean Open Reasoning Model for Everyone' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Large Language Model","Korean","Bilingual","Synthetic Data","Fully Open Model","Tokenizer","Reasoning","Pretraining","Instruction Tuning"],"permalink":"/ai/review/2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes","title":"[논문리뷰] Instant4D: 4D Gaussian Splatting in Minutes","excerpt":"Li Lu이 [arXiv]에 게시한 'Instant4D: 4D Gaussian Splatting in Minutes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","4D Gaussian Splatting","Dynamic View Synthesis","Monocular Reconstruction","Visual SLAM","Grid Pruning","Real-time Rendering","GPU Memory Optimization"],"permalink":"/ai/review/2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation","title":"[논문리뷰] Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation","excerpt":"Zekun Qi이 [arXiv]에 게시한 'Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Self-supervised Monocular Depth Estimation","Foundation Models","CLIP","DINO","Language Guidance","Coarse-to-fine Learning","Feature Aggregation","3D Perception"],"permalink":"/ai/review/2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare","title":"[논문리뷰] GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare","excerpt":"이 [arXiv]에 게시한 'GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Large Language Models","LLM Alignment","Game Theory","Reinforcement Learning","Mutual Welfare","Payoff Matrix","Strategic Decision Making","Human-AI Interaction"],"permalink":"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents","title":"[논문리뷰] Dyna-Mind: Learning to Simulate from Experience for Better AI Agents","excerpt":"Qianhui Wu이 [arXiv]에 게시한 'Dyna-Mind: Learning to Simulate from Experience for Better AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","AI Agents","Reinforcement Learning","World Models","Simulation","Reasoning","Language Models","Planning","Interactive AI"],"permalink":"/ai/review/2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting","title":"[논문리뷰] Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting","excerpt":"Julia Kempe이 [arXiv]에 게시한 'Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reasoning Tasks","GRPO","Negative Samples","Reward Modeling","Confidence Reweighting","Mathematical Reasoning"],"permalink":"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation","title":"[논문리뷰] DISCO: Diversifying Sample Condensation for Efficient Model Evaluation","excerpt":"이 [arXiv]에 게시한 'DISCO: Diversifying Sample Condensation for Efficient Model Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Efficient Evaluation","Sample Condensation","Model Disagreement","Predictive Diversity","Performance Prediction","Large Language Models","Model Signatures","Meta-modeling"],"permalink":"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI","title":"[논문리뷰] D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI","excerpt":"Haebin Seong이 [arXiv]에 게시한 'D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Embodied AI","Vision-Action Pretraining","Desktop Data","Inverse Dynamics Model (IDM)","Pseudo-labeling","Robotics","Generalization","Data Compression"],"permalink":"/ai/review/2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization","title":"[논문리뷰] Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization","excerpt":"Mahdi Ghaznavai이 [arXiv]에 게시한 'Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Complexity OoD Generalization","System-1 Thinking","System-2 Reasoning","Kolmogorov Complexity","Inductive Biases","Large Language Models (LLMs)","Reasoning Evaluation"],"permalink":"/ai/review/2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution","title":"[논문리뷰] BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution","excerpt":"Hange Liu이 [arXiv]에 게시한 'BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Code Generation","Human Preference","LLM Evaluation","Execution Feedback","Benchmarking","Crowdsourcing","Software Engineering","Large Language Models"],"permalink":"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models","title":"[논문리뷰] Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models","excerpt":"이 [arXiv]에 게시한 'Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Unpaired Multimodal Learning","Unimodal Representation","Weight Sharing","Cross-modal Transfer","Fisher Information","Self-supervised Learning","Multimodal Neurons","Data Efficiency"],"permalink":"/ai/review/2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion","title":"[논문리뷰] AutoPR: Let's Automate Your Academic Promotion!","excerpt":"Yixin Yuan이 [arXiv]에 게시한 'AutoPR: Let's Automate Your Academic Promotion!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Academic Promotion","Large Language Models","Multi-Agent Systems","Scholarly Communication","Multimodal Processing","Benchmark","Content Generation","Social Media Marketing"],"permalink":"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols","title":"[논문리뷰] Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols","excerpt":"Maksym Andriushchenko이 [arXiv]에 게시한 'Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","AI Control Protocols","LLM Monitors","Adaptive Attacks","Prompt Injection","Jailbreaking","Red Teaming","Scalable Oversight"],"permalink":"/ai/review/2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping","title":"[논문리뷰] ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping","excerpt":"Wenbo Hu이 [arXiv]에 게시한 'ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Adaptive Learning","Reinforcement Learning","Entropy Shaping","Difficulty-Aware","Chain-of-Thought","Token-Level Analysis"],"permalink":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall","title":"[논문리뷰] ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall","excerpt":"Jiaqi Tang이 [arXiv]에 게시한 'ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Knowledge Editing","LLMs","Multi-hop Reasoning","Mechanistic Interpretability","Neuron-level Attribution","Factual Recall","Transformer Networks"],"permalink":"/ai/review/2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks","title":"[논문리뷰] A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks","excerpt":"Fanchao Qi이 [arXiv]에 게시한 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Long-Horizon Tasks","LLM Agents","Global Planning","Reinforcement Learning","Supervised Fine-tuning","Homologous Consensus Filtering","Executor Capability Gain Reward","Plan-and-Execute"],"permalink":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs","title":"[논문리뷰] When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs","excerpt":"이 [arXiv]에 게시한 'When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Long-Context LMs","Multi-hop Reasoning","Thought Templates","Retrieval-Augmented Generation","Natural Language Feedback","Knowledge-intensive QA","Reasoning Reuse"],"permalink":"/ai/review/2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning","title":"[논문리뷰] VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning","excerpt":"Quande Liu이 [arXiv]에 게시한 'VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Video Completion","Spatio-Temporal Control","In-Context Conditioning","Video Diffusion Models","RoPE Interpolation","VAE","Unified Framework","Video Generation"],"permalink":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos","title":"[논문리뷰] UniVideo: Unified Understanding, Generation, and Editing for Videos","excerpt":"Xintao Wang이 [arXiv]에 게시한 'UniVideo: Unified Understanding, Generation, and Editing for Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Unified Multimodal Model","Video Generation","Video Editing","MLLM","Diffusion Transformer","In-Context Learning","Zero-shot Generalization","Multimodal AI"],"permalink":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution","title":"[논문리뷰] UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution","excerpt":"이 [arXiv]에 게시한 'UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Video Super-Resolution","Multi-Modal Generation","Latent Diffusion Models","Cascaded Framework","Condition Injection","Text-to-Video","Video Editing","4K Video"],"permalink":"/ai/review/2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections","title":"[논문리뷰] UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections","excerpt":"Boqian Li이 [arXiv]에 게시한 'UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","3D Human Reconstruction","Unconstrained Photos","Data Rectifier","Multi-View Generation","Pose-Correlated Feature Aggregation","SMPL-X","Diffusion Models","Virtual Try-On"],"permalink":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG","title":"[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG","excerpt":"이 [arXiv]에 게시한 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multimodal RAG","Document AI","Benchmark","Information Retrieval","Large Language Models","Multimodal Embeddings","PDF Processing","Question Answering"],"permalink":"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Training-Free-Group-Relative-Policy-Optimization","title":"[논문리뷰] Training-Free Group Relative Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Training-Free Group Relative Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Parameter-Free Optimization","Experiential Knowledge","Token Prior","Group Relative Policy Optimization","In-Context Learning","Cost-Effective AI"],"permalink":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Towards-Scalable-and-Consistent-3D-Editing","title":"[논문리뷰] Towards Scalable and Consistent 3D Editing","excerpt":"Pan Zhou이 [arXiv]에 게시한 'Towards Scalable and Consistent 3D Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","3D Editing","Generative Models","Transformer Architecture","Dataset Generation","Multimodal Learning","Conditional Generation","Image-to-3D"],"permalink":"/ai/review/2025-10-10-Towards-Scalable-and-Consistent-3D-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety","title":"[논문리뷰] The Alignment Waltz: Jointly Training Agents to Collaborate for Safety","excerpt":"이 [arXiv]에 게시한 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Safety","Multi-agent Reinforcement Learning","Safety Alignment","Overrefusal","Adversarial Attacks","Feedback Agent","Conversation Agent","Dynamic Improvement Reward"],"permalink":"/ai/review/2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction","title":"[논문리뷰] Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction","excerpt":"이 [arXiv]에 게시한 'Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Text-to-Sounding Video Generation","Diffusion Models","Dual-tower Architecture","Cross-modal Fusion","Visual Grounding","Hierarchical Captioning","Cross-Attention"],"permalink":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models","title":"[논문리뷰] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models","excerpt":"James Cheng이 [arXiv]에 게시한 'Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Sentence Embedding","Retrieval-Augmented Generation","Chain-of-Thought","Information Retrieval","Supervised Fine-tuning"],"permalink":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models","title":"[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models","excerpt":"Mohit Bansal이 [arXiv]에 게시한 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Video Reasoning","Multimodal AI","Scientific Research","Large Multimodal Models","Benchmark","Quantitative Reasoning","Domain Knowledge","Visual Grounding"],"permalink":"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation","title":"[논문리뷰] SViM3D: Stable Video Material Diffusion for Single Image 3D Generation","excerpt":"이 [arXiv]에 게시한 'SViM3D: Stable Video Material Diffusion for Single Image 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Single Image 3D Reconstruction","Material Prediction","Video Diffusion Models","Physically Based Rendering (PBR)","Inverse Rendering","Novel View Synthesis","Camera Control","Latent Diffusion"],"permalink":"/ai/review/2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization","title":"[논문리뷰] Reinforcing Diffusion Models by Direct Group Preference Optimization","excerpt":"Jing Tang이 [arXiv]에 게시한 'Reinforcing Diffusion Models by Direct Group Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Diffusion Models","Reinforcement Learning","Preference Optimization","Group Preference","Direct Preference Optimization","ODE Samplers","Efficient Training"],"permalink":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training","title":"[논문리뷰] Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training","excerpt":"Peng Cheng이 [arXiv]에 게시한 'Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts","Large Language Models","Checkpoint Recycling","Model Growth","Efficient Pretraining","Depth Growth","Width Growth","Sunk Cost"],"permalink":"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation","title":"[논문리뷰] R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation","excerpt":"Zheng Zhu이 [arXiv]에 게시한 'R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Robotic Manipulation","Data Augmentation","Spatial Generalization","3D Data Generation","Imitation Learning","Point Cloud","Real-to-Real","Mobile Manipulation"],"permalink":"/ai/review/2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents","title":"[논문리뷰] NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents","excerpt":"Baixuan Xu이 [arXiv]에 게시한 'NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Agents","Scientific Law Discovery","Benchmarking","Metaphysical Shifts","Interactive Environments","Exploration-Exploitation","Tool Use"],"permalink":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints","title":"[논문리뷰] NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints","excerpt":"이 [arXiv]에 게시한 'NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Native MLLMs","Scaling Laws","Data Constraints","Visual Encoder","LLM Initialization","Mixture-of-Experts","End-to-end Training"],"permalink":"/ai/review/2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning","title":"[논문리뷰] Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Meta-Awareness","Reinforcement Learning","Self-Alignment","LLM Reasoning","Training Efficiency","Generalization","Predictive Gating"],"permalink":"/ai/review/2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens","title":"[논문리뷰] Memory Retrieval and Consolidation in Large Language Models through Function Tokens","excerpt":"이 [arXiv]에 게시한 'Memory Retrieval and Consolidation in Large Language Models through Function Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Large Language Models","LLM Interpretability","Function Tokens","Memory Retrieval","Memory Consolidation","Sparse Autoencoders","Pre-training"],"permalink":"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model","title":"[논문리뷰] MemMamba: Rethinking Memory Patterns in State Space Model","excerpt":"Xiao Sun이 [arXiv]에 게시한 'MemMamba: Rethinking Memory Patterns in State Space Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","State Space Models","Mamba","Long-sequence modeling","Memory decay","State summarization","Cross-layer attention","Perplexity","Linear complexity"],"permalink":"/ai/review/2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization","title":"[논문리뷰] MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization","excerpt":"vanilla1116이 [arXiv]에 게시한 'MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Reflective Reasoning","Long-Chain Reasoning","Benchmark","Policy Optimization","Data Generation","Reinforcement Learning","Backtracking"],"permalink":"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward","title":"[논문리뷰] Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward","excerpt":"이 [arXiv]에 게시한 'Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Exploration","Verifiable Reward","Low-Probability Regularization","Reasoning Sparks","Policy Entropy","KL Divergence","Mathematical Reasoning"],"permalink":"/ai/review/2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling","title":"[논문리뷰] LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling","excerpt":"이 [arXiv]에 게시한 'LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reward Model","Long Context","LLM Alignment","Multi-stage Training","Context Window Scaling","Preference Learning","Long-RewardBench"],"permalink":"/ai/review/2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs","title":"[논문리뷰] Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs","excerpt":"Franck Dernoncourt이 [arXiv]에 게시한 'Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Routing","Contextual Bandits","Bandit Feedback","Multi-objective Optimization","Preference-tuning","Policy Gradient","Cost-efficiency"],"permalink":"/ai/review/2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks","title":"[논문리뷰] Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks","excerpt":"이 [arXiv]에 게시한 'Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Agents","Continuous Learning","Self-Evolving","Memory Module","Long-Horizon Planning","Productivity Tasks","Test-Time Learning","Experience Replay"],"permalink":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency","title":"[논문리뷰] Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency","excerpt":"Jintao Zhang이 [arXiv]에 게시한 'Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Diffusion Distillation","Consistency Models","Score Regularization","Large-Scale Generative Models","Text-to-Image","Text-to-Video","Model Acceleration","JVP"],"permalink":"/ai/review/2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions","title":"[논문리뷰] LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions","excerpt":"이 [arXiv]에 게시한 'LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Misalignment","Dishonesty","Deception","Finetuning","Human-AI Interaction","Biased Feedback","Emergent Behavior"],"permalink":"/ai/review/2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance","title":"[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance","excerpt":"Xinghui Li이 [arXiv]에 게시한 'InstructX: Towards Unified Visual Editing with MLLM Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Visual Editing","MLLM Guidance","Diffusion Models","Image Editing","Video Editing","Unified Framework","Multimodal AI","Instruction-based Editing"],"permalink":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense","title":"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense","excerpt":"이 [arXiv]에 게시한 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Reward Modeling","Large Language Models (LLMs)","Mathematical Reasoning","Sparse Rewards","Dense Rewards","Hybrid Reinforcement","Verifier-based Rewards"],"permalink":"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-GCPO-When-Contrast-Fails-Go-Gold","title":"[논문리뷰] GCPO: When Contrast Fails, Go Gold","excerpt":"이 [arXiv]에 게시한 'GCPO: When Contrast Fails, Go Gold' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs Reasoning","Policy Optimization","Contrastive Learning","Chain of Thought","Reference Answers","Math Reasoning","Gold-Standard Answer"],"permalink":"/ai/review/2025-10-10-GCPO-When-Contrast-Fails-Go-Gold/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning","title":"[논문리뷰] From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning","excerpt":"Feiwei Qin이 [arXiv]에 게시한 'From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Chemical Reaction Prediction","Explainable AI","Evidence-Based Reasoning","Large Language Models","Tool-Augmented LLMs","Scientific Discovery"],"permalink":"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models","title":"[논문리뷰] First Try Matters: Revisiting the Role of Reflection in Reasoning Models","excerpt":"Wee Sun Lee이 [arXiv]에 게시한 'First Try Matters: Revisiting the Role of Reflection in Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Reasoning","Chain-of-Thought (CoT)","Reflection","Early Stopping","Supervised Fine-tuning (SFT)","Token Efficiency","Mathematical Reasoning"],"permalink":"/ai/review/2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization","title":"[논문리뷰] Fidelity-Aware Data Composition for Robust Robot Generalization","excerpt":"Liliang Chen이 [arXiv]에 게시한 'Fidelity-Aware Data Composition for Robust Robot Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Robot Generalization","Data Augmentation","Out-of-Distribution (OOD)","Shortcut Learning","Information Fidelity","Data Composition","Diffusion Models","Multi-View Video Synthesis"],"permalink":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints","title":"[논문리뷰] Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints","excerpt":"Huazhe Xu이 [arXiv]에 게시한 'Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Entropy Regularization","Activation Functions","Continuous Control","Large Language Models","Image Classification","Reinforcement Learning","Policy Stochasticity","Entropy Constraints"],"permalink":"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model","title":"[논문리뷰] DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model","excerpt":"Li Yi이 [arXiv]에 게시한 'DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Dexterous Manipulation","In-Hand Rotation","Sim-to-Real Transfer","Neural Dynamics Model","Joint-Wise Learning","Autonomous Data Collection","Reinforcement Learning","Robotics"],"permalink":"/ai/review/2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy","title":"[논문리뷰] DeepPrune: Parallel Scaling without Inter-trace Redundancy","excerpt":"이 [arXiv]에 게시한 'DeepPrune: Parallel Scaling without Inter-trace Redundancy' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Parallel Scaling","Chain-of-Thought","LLM Reasoning","Dynamic Pruning","Inter-trace Redundancy","Judge Model","Resource Efficiency","Answer Diversity"],"permalink":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards","title":"[논문리뷰] CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards","excerpt":"Yijiang Li이 [arXiv]에 게시한 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","LLM Agents","Self-Evolution","Reinforcement Learning","Interaction Rewards","LLM-as-a-Judge","Decentralized Learning"],"permalink":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window","title":"[논문리뷰] Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window","excerpt":"Yaojie Lu이 [arXiv]에 게시한 'Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Deep Search Agents","Dynamic Context Window","Reinforcement Learning","Long-horizon Interaction","Context Management","High-difficulty Tasks","Multi-turn Reasoning","Web Agents"],"permalink":"/ai/review/2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization","title":"[논문리뷰] Beyond Outliers: A Study of Optimizers Under Quantization","excerpt":"이 [arXiv]에 게시한 'Beyond Outliers: A Study of Optimizers Under Quantization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Quantization","Optimizers","LLM","Post-Training Quantization (PTQ)","Quantization-Aware Training (QAT)","Error Propagation","Scaling Laws","Shampoo"],"permalink":"/ai/review/2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-Agent-Learning-via-Early-Experience","title":"[논문리뷰] Agent Learning via Early Experience","excerpt":"이 [arXiv]에 게시한 'Agent Learning via Early Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Language Agents","Early Experience","Reward-Free Learning","World Modeling","Self-Reflection","Imitation Learning","Reinforcement Learning","Out-of-Domain Generalization"],"permalink":"/ai/review/2025-10-10-Agent-Learning-via-Early-Experience/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation","title":"[논문리뷰] ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation","excerpt":"이 [arXiv]에 게시한 'ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Monocular SLAM","Gaussian Splatting","Level of Detail (LoD)","Feed-Forward Models","Structured Scene Representation","Real-time","High-Fidelity"],"permalink":"/ai/review/2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning","title":"[논문리뷰] A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Question Answering","Reinforcement Learning","Large Language Models","Ambiguity Resolution","Multi-hop QA","Automated Data Generation","Tool-Augmented LLMs","AnsF1 Reward"],"permalink":"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","title":"[논문리뷰] WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation","excerpt":"이 [arXiv]에 게시한 'WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","4D World Models","Robotic Manipulation","Video Generation","Multi-view Synthesis","Visual-Language-Action (VLA)","Geometric Consistency","Diffusion Models","Wrist-View"],"permalink":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention","title":"[논문리뷰] Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention","excerpt":"이 [arXiv]에 게시한 'Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Low-Precision Training","Flash Attention","Transformer","Numerical Stability","BF16","Rounding Error","Gradient Bias","Deep Learning Optimization"],"permalink":"/ai/review/2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation","title":"[논문리뷰] When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation","excerpt":"이 [arXiv]에 게시한 'When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","LLM Factuality Evaluation","Benchmark Aging","Temporal Misalignment","Information Retrieval","Question Answering","Evaluation Metrics","GPT-4o-mini","Qwen2.5"],"permalink":"/ai/review/2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference","title":"[논문리뷰] Vibe Checker: Aligning Code Evaluation with Human Preference","excerpt":"이 [arXiv]에 게시한 'Vibe Checker: Aligning Code Evaluation with Human Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Code Evaluation","Instruction Following","Human Preference","Large Language Models","Vibe Check","Non-functional Requirements","VeriCode"],"permalink":"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking","title":"[논문리뷰] U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking","excerpt":"Heqin Zhu이 [arXiv]에 게시한 'U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","U-Net","Medical Image Segmentation","Benchmarking","Performance Evaluation","Efficiency Metrics","Zero-shot Generalization","U-Score"],"permalink":"/ai/review/2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-The-Markovian-Thinker","title":"[논문리뷰] The Markovian Thinker","excerpt":"이 [arXiv]에 게시한 'The Markovian Thinker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Chain-of-Thought","Markovian Thinking","Context Management","Computational Efficiency","Long-Context LLMs","Transformer Optimization"],"permalink":"/ai/review/2025-10-9-The-Markovian-Thinker/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP","title":"[논문리뷰] The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP","excerpt":"이 [arXiv]에 게시한 'The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Low-Resource NLP","African Languages","Data Collection","Multilingual Models","Fine-Tuning","Speech Data","Text Data","Capacity Building"],"permalink":"/ai/review/2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models","title":"[논문리뷰] TTRV: Test-Time Reinforcement Learning for Vision Language Models","excerpt":"Serena Yeung-Levy이 [arXiv]에 게시한 'TTRV: Test-Time Reinforcement Learning for Vision Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Test-Time Adaptation","Unsupervised Learning","Image Recognition","Visual Question Answering (VQA)","Group Relative Policy Optimization (GRPO)","Entropy Regularization"],"permalink":"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation","title":"[논문리뷰] StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation","excerpt":"이 [arXiv]에 게시한 'StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Robot Learning","State Representation","Motion Representation","Diffusion Models","Unsupervised Learning","World Modeling","Vision-Language Models","Latent Action"],"permalink":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models","title":"[논문리뷰] SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models","excerpt":"Kevin Lin이 [arXiv]에 게시한 'SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Spoken Language Models","Real-time Interaction","Thinking While Listening","Chain-of-Thought","Interruption","Tool Calling","Streaming ASR"],"permalink":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces","title":"[논문리뷰] Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces","excerpt":"이 [arXiv]에 게시한 'Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Chain-of-Thought","Uniform Information Density","Information Theory","Reasoning Trace Analysis","Entropy","Mathematical Reasoning","Model Evaluation"],"permalink":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective","title":"[논문리뷰] Revisiting Long-context Modeling from Context Denoising Perspective","excerpt":"이 [arXiv]에 게시한 'Revisiting Long-context Modeling from Context Denoising Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Long-context Models","Context Denoising","Integrated Gradient","LLM Training","Context Window Scaling","Information Flow","Attention Mechanism"],"permalink":"/ai/review/2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training","title":"[논문리뷰] RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training","excerpt":"이 [arXiv]에 게시한 'RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","VLA Models","Robotics","GPU Management","PPO","GRPO","Sim-to-Real"],"permalink":"/ai/review/2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought","title":"[논문리뷰] Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought","excerpt":"이 [arXiv]에 게시한 'Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multilingual Reasoning","Chain-of-Thought (CoT)","Language-Mixed CoT","Instruction Tuning","Korean LLMs","Data Curation","Supervised Fine-tuning (SFT)"],"permalink":"/ai/review/2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs","title":"[논문리뷰] Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs","excerpt":"Jingyi Liao이 [arXiv]에 게시한 'Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Visual Reference Tokens (VRTs)","Dense Prediction","Referring Expression Comprehension (REC)","Open-Vocabulary Detection (OVD)","Image Captioning","Unified Architecture","Autoregressive Generation"],"permalink":"/ai/review/2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Online-Generic-Event-Boundary-Detection","title":"[논문리뷰] Online Generic Event Boundary Detection","excerpt":"Jonghyun Choi이 [arXiv]에 게시한 'Online Generic Event Boundary Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Online Video Analysis","Event Boundary Detection","Event Segmentation Theory","Real-time AI","Anomaly Detection","Transformer Architecture"],"permalink":"/ai/review/2025-10-9-Online-Generic-Event-Boundary-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot","title":"[논문리뷰] OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot","excerpt":"이 [arXiv]에 게시한 'OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Diffusion Models","Network Pruning","One-Shot Pruning","Optimal Brain Surgeon (OBS)","Model Compression","Timestep-Aware Hessian","Structured Pruning"],"permalink":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable","title":"[논문리뷰] NorMuon: Making Muon more efficient and scalable","excerpt":"Tuo Zhao이 [arXiv]에 게시한 'NorMuon: Making Muon more efficient and scalable' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","LLM Training","Optimizer","Muon","Orthogonalization","Adaptive Learning Rates","Distributed Training","FSDP2","NorMuon"],"permalink":"/ai/review/2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling","title":"[논문리뷰] Native Hybrid Attention for Efficient Sequence Modeling","excerpt":"Yu Cheng이 [arXiv]에 게시한 'Native Hybrid Attention for Efficient Sequence Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Sequence Modeling","Hybrid Attention","Transformer Architecture","Linear Attention","Sliding Window Attention","Long Context","Large Language Models (LLMs)","Efficiency"],"permalink":"/ai/review/2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization","title":"[논문리뷰] Multi-Agent Tool-Integrated Policy Optimization","excerpt":"Lidong Bing이 [arXiv]에 게시한 'Multi-Agent Tool-Integrated Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multi-Agent RL","Tool-Integrated Planning","Large Language Models (LLMs)","Policy Optimization","Credit Assignment","Reinforcement Learning","MATPO"],"permalink":"/ai/review/2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer","title":"[논문리뷰] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer","excerpt":"이 [arXiv]에 게시한 'Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Unified Vision-Language Model","Continuous Tokenizer","Autoregressive Generation","Image Understanding","Image Generation","Multimodal AI","In-context Editing"],"permalink":"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline","title":"[논문리뷰] MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline","excerpt":"이 [arXiv]에 게시한 'MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","MLE (Machine Learning Engineering)","Automated Task Generation","Multi-Agent System","LLM Agents","Benchmark","Data Curation","Hybrid Verification","Kaggle"],"permalink":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation","title":"[논문리뷰] MATRIX: Mask Track Alignment for Interaction-aware Video Generation","excerpt":"Hyunwook Choi이 [arXiv]에 게시한 'MATRIX: Mask Track Alignment for Interaction-aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Transformers","Human-Object Interaction","Attention Alignment","Mask Tracking","Semantic Grounding","Semantic Propagation","Text-to-Video"],"permalink":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding","title":"[논문리뷰] Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding","excerpt":"이 [arXiv]에 게시한 'Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multi-modal LLM","Discrete Diffusion","Image Generation","Image Understanding","Omni-modal","Interactive Retouching","Generative AI","Reinforcement Learning"],"permalink":"/ai/review/2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals","title":"[논문리뷰] Heptapod: Language Modeling on Visual Signals","excerpt":"이 [arXiv]에 게시한 'Heptapod: Language Modeling on Visual Signals' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Image Generation","Language Modeling","Causal Transformer","2D Distribution Prediction","Visual Tokenization","Self-Supervised Learning","Generative Models"],"permalink":"/ai/review/2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models","title":"[논문리뷰] G^2RPO: Granular GRPO for Precise Reward in Flow Models","excerpt":"이 [arXiv]에 게시한 'G^2RPO: Granular GRPO for Precise Reward in Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Flow Models","Generative Models","Human Preference Alignment","Stochastic Differential Equations (SDE)","Reward Signal","Multi-Granularity"],"permalink":"/ai/review/2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents","title":"[논문리뷰] DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents","excerpt":"이 [arXiv]에 게시한 'DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Travel Planning","Large Language Models","Sandbox Environment","Hierarchical Reward Modeling","Experience Replay","Autonomous Agents"],"permalink":"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection","title":"[논문리뷰] D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection","excerpt":"Yueqi Duan이 [arXiv]에 게시한 'D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Image Detection","Discrete Distribution Discrepancy","Quantization Error","Transformer","Generative AI","Deepfake Detection"],"permalink":"/ai/review/2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models","title":"[논문리뷰] Cache-to-Cache: Direct Semantic Communication Between Large Language Models","excerpt":"이 [arXiv]에 게시한 'Cache-to-Cache: Direct Semantic Communication Between Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Inter-model Communication","KV-Cache","Semantic Transfer","Multi-LLM Systems","Cache Fusion","Latency Reduction","Knowledge Sharing"],"permalink":"/ai/review/2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling","title":"[논문리뷰] CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling","excerpt":"Chengpeng Li이 [arXiv]에 게시한 'CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Optimization Modeling","Reflective Generation","Supervised Fine-tuning","Reinforcement Learning","Human-in-the-Loop","Code Generation","Domain Adaptation"],"permalink":"/ai/review/2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Bridging-Text-and-Video-Generation-A-Survey","title":"[논문리뷰] Bridging Text and Video Generation: A Survey","excerpt":"G. Maragatham이 [arXiv]에 게시한 'Bridging Text and Video Generation: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Generative Models","Diffusion Models","GANs","VAEs","Video Synthesis","Survey","Evaluation Metrics"],"permalink":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models","title":"[논문리뷰] Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models","excerpt":"이 [arXiv]에 게시한 'Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Code-switching","Multilingual NLP","Large Language Models","NLP Survey","Data Augmentation","Evaluation Metrics","Low-Resource Languages"],"permalink":"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling","title":"[논문리뷰] Artificial Hippocampus Networks for Efficient Long-Context Modeling","excerpt":"이 [arXiv]에 게시한 'Artificial Hippocampus Networks for Efficient Long-Context Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Long-Context Modeling","Transformer","RNN","Memory Management","Self-Distillation","Attention Mechanism","Artificial Hippocampus Networks","Cognitive Science"],"permalink":"/ai/review/2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods","title":"[논문리뷰] Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods","excerpt":"Yiyu Wang이 [arXiv]에 게시한 'Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Visual Token Compression","MLLMs","Evaluation Framework","Benchmarking","Downsampling","Data Filtering","Model Efficiency"],"permalink":"/ai/review/2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning","title":"[논문리뷰] AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning","excerpt":"Zongze Li이 [arXiv]에 게시한 'AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Foundation Models","Agentic Reasoning","Tool Use","Self-Evolving System","Retrieval-Augmented Generation","Computational Tools","Error Correction"],"permalink":"/ai/review/2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation","title":"[논문리뷰] VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation","excerpt":"이 [arXiv]에 게시한 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM Agents","Safety","Formal Verification","Code Generation","Runtime Monitoring","Security","Guardrails","Policy Enforcement"],"permalink":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness","title":"[논문리뷰] Training Dynamics Impact Post-Training Quantization Robustness","excerpt":"Jonas Geiping이 [arXiv]에 게시한 'Training Dynamics Impact Post-Training Quantization Robustness' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Post-Training Quantization","Quantization Robustness","Training Dynamics","Learning Rate Schedules","Weight Averaging","Large Language Models","LLMs","Hyperparameter Tuning"],"permalink":"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation","title":"[논문리뷰] TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation","excerpt":"이 [arXiv]에 게시한 'TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","BLEU Score","GPU Acceleration","PyTorch","Natural Language Processing","Reinforcement Learning","Vectorization","In-Training Evaluation","N-gram Counting"],"permalink":"/ai/review/2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning","title":"[논문리뷰] TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning","excerpt":"이 [arXiv]에 게시한 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Process Reward Models","Tabular Reasoning","Test-Time Scaling","Tool Integration","Reinforcement Learning","Supervised Fine-tuning","Large Language Models","Data Curation"],"permalink":"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos","title":"[논문리뷰] ShapeGen4D: Towards High Quality 4D Shape Generation from Videos","excerpt":"Sergey Tulyakov이 [arXiv]에 게시한 'ShapeGen4D: Towards High Quality 4D Shape Generation from Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","4D Shape Generation","Video-conditioned","Dynamic 3D Meshes","Latent Diffusion Model","Spatiotemporal Attention","Temporal Consistency","Pre-trained 3D Models","VAE"],"permalink":"/ai/review/2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning","title":"[논문리뷰] Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning","excerpt":"Zhuoshi Pan이 [arXiv]에 게시한 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Code-Assisted Reasoning","Chain-of-Thought (CoT)","Instruction Tuning","Data Augmentation","LLMs","Mathematical Reasoning","Self-Verification","Code Generation"],"permalink":"/ai/review/2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions","title":"[논문리뷰] Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions","excerpt":"이 [arXiv]에 게시한 'Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Speech Emotion Recognition","Annotator Subjectivity","Emotion Ambiguity","Soft Labels","Multi-label Classification","Evaluation Metrics","Loss Functions"],"permalink":"/ai/review/2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning","title":"[논문리뷰] Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?","excerpt":"이 [arXiv]에 게시한 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Safety Alignment","Large Reasoning Models","Mechanistic Interpretability","Refusal Cliff","Attention Heads","Data Selection","Linear Probing"],"permalink":"/ai/review/2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations","title":"[논문리뷰] Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations","excerpt":"이 [arXiv]에 게시한 'Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Self-Improvement Agent","Academic Presentation","Aesthetic Evaluation","Reinforcement Learning","Multi-task Learning","Presentation Generation","LLM-based Agents","Human Feedback"],"permalink":"/ai/review/2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows","title":"[논문리뷰] OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows","excerpt":"이 [arXiv]에 게시한 'OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Non-Autoregressive","Multimodal Generation","Edit Flows","Flow Matching","Interleaved Generation","Text-to-Image Synthesis","Unified Models"],"permalink":"/ai/review/2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models","title":"[논문리뷰] No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models","excerpt":"Xiao Xiao Sun이 [arXiv]에 게시한 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Biomedical Vision-Language Models","Long-context Modeling","Contrastive Learning","Token Efficiency","Zero-shot Classification","Medical Image Retrieval"],"permalink":"/ai/review/2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context","title":"[논문리뷰] Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context","excerpt":"이 [arXiv]에 게시한 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Language Models","In-Context Learning","Entity Binding","Mechanistic Interpretability","Causal Abstraction","Long-Context Reasoning","Positional Encoding","Information Retrieval"],"permalink":"/ai/review/2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-MixReasoning-Switching-Modes-to-Think","title":"[논문리뷰] MixReasoning: Switching Modes to Think","excerpt":"이 [arXiv]에 게시한 'MixReasoning: Switching Modes to Think' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Chain-of-Thought","Efficiency","LoRA","Adaptive Reasoning","Token Uncertainty","Dynamic Switching","Reasoning Compression"],"permalink":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization","title":"[논문리뷰] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization","excerpt":"sirano1004이 [arXiv]에 게시한 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Direct Preference Optimization","Preference Alignment","Adaptive Regularization","Reward Model","Large Language Models","Sentiment Generation"],"permalink":"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","title":"[논문리뷰] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation","excerpt":"Zheng Zhan이 [arXiv]에 게시한 'LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Memory Efficiency","Inference Acceleration","Training-Free","Cache Mechanism","GPU Optimization"],"permalink":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks","title":"[논문리뷰] Less is More: Recursive Reasoning with Tiny Networks","excerpt":"이 [arXiv]에 게시한 'Less is More: Recursive Reasoning with Tiny Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Recursive Reasoning","Tiny Networks","Deep Supervision","Hierarchical Reasoning Model (HRM)","Sudoku-Extreme","ARC-AGI","Generalization","Parameter Efficiency"],"permalink":"/ai/review/2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use","title":"[논문리뷰] In-the-Flow Agentic System Optimization for Effective Planning and Tool Use","excerpt":"이 [arXiv]에 게시한 'In-the-Flow Agentic System Optimization for Effective Planning and Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Agentic Systems","Large Language Models (LLMs)","Tool Use","Reinforcement Learning (RL)","On-policy Optimization","Flow-based Group Refined Policy Optimization (Flow-GRPO)","Multi-turn Reasoning"],"permalink":"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Human3R-Everyone-Everywhere-All-at-Once","title":"[논문리뷰] Human3R: Everyone Everywhere All at Once","excerpt":"Yuliang Xiu이 [arXiv]에 게시한 'Human3R: Everyone Everywhere All at Once' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","4D Human-Scene Reconstruction","Online Reconstruction","Multi-person","SMPL-X","Transformer","Visual Prompt Tuning","Real-time","Foundation Model"],"permalink":"/ai/review/2025-10-8-Human3R-Everyone-Everywhere-All-at-Once/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video","title":"[논문리뷰] HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video","excerpt":"Katelyn Gao이 [arXiv]에 게시한 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Digital Twin","Scene Graph","Physical Simulation","Interactive Environments","Single Video Reconstruction","Neural Rendering"],"permalink":"/ai/review/2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation","title":"[논문리뷰] HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation","excerpt":"Radu State이 [arXiv]에 게시한 'HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Hallucination Detection","Retrieval-Augmented Generation (RAG)","Small Reasoning Model (SRM)","Preference Fine-tuning","ORPO","Evidence Grounding","Fact-checking"],"permalink":"/ai/review/2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs","title":"[논문리뷰] Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs","excerpt":"이 [arXiv]에 게시한 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","DeepResearch Agents","Tool-integrated Reasoning","Reinforcement Learning","Information Retrieval","Information Synthesis","Multi-agent Self-play","Reward Shaping","LLM"],"permalink":"/ai/review/2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM","title":"[논문리뷰] Fast-dLLM v2: Efficient Block-Diffusion LLM","excerpt":"이 [arXiv]에 게시한 'Fast-dLLM v2: Efficient Block-Diffusion LLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Inference Acceleration","Parallel Decoding","Autoregressive Models","Caching","Fine-tuning","Block-wise Attention"],"permalink":"/ai/review/2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models","title":"[논문리뷰] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models","excerpt":"이 [arXiv]에 게시한 'Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Generative Models","Equilibrium Dynamics","Energy-Based Models (EBMs)","Flow Matching","Diffusion Models","Optimization-Based Sampling","Image Generation"],"permalink":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark","title":"[논문리뷰] EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark","excerpt":"Tianwen Qian이 [arXiv]에 게시한 'EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Egocentric Vision","Nighttime Conditions","Visual Question Answering (VQA)","Day-Night Alignment","Multimodal Large Language Models (MLLMs)","Depth Estimation","Correspondence Retrieval","Benchmark"],"permalink":"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching","title":"[논문리뷰] Drax: Speech Recognition with Discrete Flow Matching","excerpt":"이 [arXiv]에 게시한 'Drax: Speech Recognition with Discrete Flow Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Automatic Speech Recognition (ASR)","Discrete Flow Matching (DFM)","Non-Autoregressive (NAR)","Generative Models","Tri-mixture Probability Path","Parallel Decoding","Accuracy-Efficiency Trade-off","Speech Synthesis"],"permalink":"/ai/review/2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models","title":"[논문리뷰] Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models","excerpt":"Jacobo Azcona이 [arXiv]에 게시한 'Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM Hallucinations","Mechanistic Interpretability","Distributional Semantics Tracing (DST)","Dual-Process Theory","Semantic Drift","Commitment Layer","Faithfulness Score"],"permalink":"/ai/review/2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation","title":"[논문리뷰] Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation","excerpt":"이 [arXiv]에 게시한 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Multimodal Large Language Models (MLLMs)","Medical Image Generation","Medical Report Generation","Multimodal Generation","Medical AI","Cross-modal Alignment"],"permalink":"/ai/review/2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics","title":"[논문리뷰] Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics","excerpt":"이 [arXiv]에 게시한 'Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Deep Search","Multi-hop Reasoning","Evaluation Benchmark","Retrieval-Augmented Generation","Web Agents","Diagnostic Metrics","Knowledge Utilization","Hint-Free Questions"],"permalink":"/ai/review/2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","title":"[논문리뷰] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation","excerpt":"Chengzu Li이 [arXiv]에 게시한 'Deforming Videos to Masks: Flow Matching for Referring Video Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Referring Video Object Segmentation","Flow Matching","Video Segmentation","Generative Models","Text-to-Video","Continuous Flow","Diffusion Models"],"permalink":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","title":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning","excerpt":"Zheli Liu이 [arXiv]에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Preference Learning","LLMs","User Feedback","Dissatisfaction Signals","DPO","Iterative Training","RLHF","Exploration"],"permalink":"/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation","title":"[논문리뷰] CoDA: Coding LM via Diffusion Adaptation","excerpt":"이 [arXiv]에 게시한 'CoDA: Coding LM via Diffusion Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Code Generation","Bidirectional Decoding","Text Infilling","Instruction Tuning","Lightweight Models","TPU Training"],"permalink":"/ai/review/2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding","title":"[논문리뷰] CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding","excerpt":"이 [arXiv]에 게시한 'CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Radiology Report Generation (RRG)","Medical Hallucinations","Contrastive Decoding","Training-free Inference","Clinical AI","Visual Question Answering (VQA)"],"permalink":"/ai/review/2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation","title":"[논문리뷰] CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation","excerpt":"이 [arXiv]에 게시한 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Emotional Support Conversation","Cognitive Reasoning","Reinforcement Learning","Dialogue Generation","Natural Language Processing","Large Language Models","Psychological Support"],"permalink":"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks","title":"[논문리뷰] Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks","excerpt":"Pedro Bizarro이 [arXiv]에 게시한 'Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Scatterplot Analysis","AI Benchmarking","Multimodal LLMs","Synthetic Data Generation","Cluster Detection","Outlier Detection","Data Visualization","Prompt Engineering"],"permalink":"/ai/review/2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions","title":"[논문리뷰] BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions","excerpt":"Shipei Lin이 [arXiv]에 게시한 'BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Text-to-SQL","LLM Evaluation","Multi-turn Interaction","Dynamic Environment","User Simulator","Ambiguity Resolution","LLM Agents"],"permalink":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization","title":"[논문리뷰] ASPO: Asymmetric Importance Sampling Policy Optimization","excerpt":"Xiu Li이 [arXiv]에 게시한 'ASPO: Asymmetric Importance Sampling Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Importance Sampling","Policy Optimization","PPO-Clip","Outcome-Supervised RL","Token Weighting","GRPO"],"permalink":"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems","title":"[논문리뷰] AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems","excerpt":"Jose Dolz이 [arXiv]에 게시한 'AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM","Scientific Problem Solving","AI Research","Iterative Refinement","Autonomous Agents","Generative AI","Evaluation Framework","Problem Extraction"],"permalink":"/ai/review/2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling","title":"[논문리뷰] A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling","excerpt":"sirano1004이 [arXiv]에 게시한 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Reward Model","Best-of-N Sampling","Preference Alignment","Contextual Acceptability","Discrete Choice Model","Alignment Guardrail","Inference Accelerator"],"permalink":"/ai/review/2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos","title":"[논문리뷰] Watch and Learn: Learning to Use Computers from Online Videos","excerpt":"Oriana Riva이 [arXiv]에 게시한 'Watch and Learn: Learning to Use Computers from Online Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Computer Use Agents","Inverse Dynamics Model","UI Trajectories","Web Videos","In-Context Learning","Supervised Fine-Tuning","Large Language Models","OSWorld Benchmark"],"permalink":"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models","title":"[논문리뷰] Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models","excerpt":"zeliang0426이 [arXiv]에 게시한 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Video Reasoning","Large Multimodal Models (LMMs)","Post-training","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)","Test-Time Scaling (TTS)","Chain-of-Thought (CoT)"],"permalink":"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","title":"[논문리뷰] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation","excerpt":"Paul Debevec이 [arXiv]에 게시한 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Video Generation","Chain-of-Thought","Multimodal Models","Reasoning","Inference-Time Tuning","Sparse Supervision","Diffusion Models","Keyframe Generation"],"permalink":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents","title":"[논문리뷰] Utility-Learning Tension in Self-Modifying Agents","excerpt":"Peter Jin이 [arXiv]에 게시한 'Utility-Learning Tension in Self-Modifying Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Self-Modifying Agents","PAC Learnability","VC Dimension","Capacity Bounds","Metacognition","Architectural Search","Algorithmic Stability","Generalization Theory"],"permalink":"/ai/review/2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents","title":"[논문리뷰] Thai Semantic End-of-Turn Detection for Real-Time Voice Agents","excerpt":"Monthol Charattrakool이 [arXiv]에 게시한 'Thai Semantic End-of-Turn Detection for Real-Time Voice Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","End-of-Turn Detection","Thai NLP","Voice Agents","Real-time Inference","Transformer Models","Few-shot Learning","Fine-tuning","Latency Optimization"],"permalink":"/ai/review/2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs","title":"[논문리뷰] SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs","excerpt":"이 [arXiv]에 게시한 'SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Latent Thinking","Explicit Thinking","Training-Free","Token Efficiency","Accuracy Improvement","Dynamic Switching","Entropy-based Control"],"permalink":"/ai/review/2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Self-Reflective-Generation-at-Test-Time","title":"[논문리뷰] Self-Reflective Generation at Test Time","excerpt":"Shuang Qiu이 [arXiv]에 게시한 'Self-Reflective Generation at Test Time' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Self-Reflection","Test-Time Optimization","Uncertainty Monitoring","Proactive Error Prevention","Reasoning Tasks","Chain-of-Thought"],"permalink":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder","title":"[논문리뷰] SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder","excerpt":"Or Patashnik이 [arXiv]에 게시한 'SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Sparse Autoencoder (SAE)","Text-to-Image","Disentangled Control","Continuous Control","Token-level Manipulation","Text Embeddings"],"permalink":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training","title":"[논문리뷰] Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training","excerpt":"이 [arXiv]에 게시한 'Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Adaptive Sampling","Policy Gradient","Reward Optimization","Signal Collapse","Variance Reduction"],"permalink":"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models","title":"[논문리뷰] Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models","excerpt":"이 [arXiv]에 게시한 'Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Reactive Transformer","Stateful LLM","Event-Driven AI","Asynchronous Memory","Conversational AI","Linear Scaling","Short-Term Memory (STM)","Memory Attention"],"permalink":"/ai/review/2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Optimal-Scaling-Needs-Optimal-Norm","title":"[논문리뷰] Optimal Scaling Needs Optimal Norm","excerpt":"Stefan Kesselheim이 [arXiv]에 게시한 'Optimal Scaling Needs Optimal Norm' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Optimal Scaling","Norm-Based Optimizers","Hyperparameter Transfer","Learning Rate Scaling","Batch Size Scaling","Transformer Models","Scion Optimizer","Large Language Models"],"permalink":"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition","title":"[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition","excerpt":"이 [arXiv]에 게시한 'MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Audio-Visual Speech Recognition","Mixture of Experts","Matryoshka Representation Learning","Large Language Models","Elastic Inference","Token Compression","Multimodal AI"],"permalink":"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information","title":"[논문리뷰] MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information","excerpt":"이 [arXiv]에 게시한 'MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Tree Search","Pointwise Mutual Information (PMI)","Dynamic Sampling","Beam Search","Weighted Voting","Information Theory","Computational Efficiency"],"permalink":"/ai/review/2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning","title":"[논문리뷰] Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Test-Time Curriculum","Reinforcement Learning","Large Language Models","Self-Curated Learning","Continual Learning","Reasoning Benchmarks","Adaptive Training"],"permalink":"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL","title":"[논문리뷰] LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL","excerpt":"이 [arXiv]에 게시한 'LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Text-to-SQL","WikiSQL","LLM","Dataset Curation","Natural Language Processing","Benchmark","SQL Generation","Data Cleaning"],"permalink":"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions","title":"[논문리뷰] Judging with Confidence: Calibrating Autoraters to Preference Distributions","excerpt":"이 [arXiv]에 게시한 'Judging with Confidence: Calibrating Autoraters to Preference Distributions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Autoraters","Calibration","Preference Distributions","Reinforcement Learning","Supervised Fine-tuning","Positional Bias"],"permalink":"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models","title":"[논문리뷰] Imperceptible Jailbreaking against Large Language Models","excerpt":"이 [arXiv]에 게시한 'Imperceptible Jailbreaking against Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Jailbreaking","Imperceptible Attacks","Unicode Variation Selectors","Adversarial Suffixes","Safety Alignment","Prompt Injection"],"permalink":"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights","title":"[논문리뷰] Hybrid Architectures for Language Models: Systematic Analysis and Design Insights","excerpt":"이 [arXiv]에 게시한 'Hybrid Architectures for Language Models: Systematic Analysis and Design Insights' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Hybrid LLM","Transformer Architecture","Mamba","State Space Models (SSM)","Computational Efficiency","Long-Context","Language Model Architectures","Scaling Laws"],"permalink":"/ai/review/2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition","title":"[논문리뷰] HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition","excerpt":"이 [arXiv]에 게시한 'HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Code-Switching","Speech Recognition","Korean-English ASR","Evaluation Framework","Multilingual ASR","Loanword Processing","Fine-tuning","Hierarchical Labeling"],"permalink":"/ai/review/2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs","title":"[논문리뷰] Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs","excerpt":"Zeyi Liao이 [arXiv]에 게시한 'Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Agent Evaluation","Task Generation","Knowledge Graphs","Multimodal AI","Web Interaction","Document Comprehension","LLM-driven Agents"],"permalink":"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where","title":"[논문리뷰] Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?","excerpt":"Denis Peskoff이 [arXiv]에 게시한 'Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","NLP for Social Good","ACL Community","Scientometrics","Venue Analysis","Author Classification","Sustainable Development Goals","Neural Methods","Research Landscape"],"permalink":"/ai/review/2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data","title":"[논문리뷰] Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data","excerpt":"이 [arXiv]에 게시한 'Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Pretraining","Supervised Fine-tuning","Reasoning Data","Data Allocation","Diversity","Quality","Reinforcement Learning"],"permalink":"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","title":"[논문리뷰] Factuality Matters: When Image Generation and Editing Meet Structured Visuals","excerpt":"Boxiang Qiu이 [arXiv]에 게시한 'Factuality Matters: When Image Generation and Editing Meet Structured Visuals' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Structured Visuals","Image Generation","Image Editing","Multimodal Reasoning","Factual Fidelity","Chain-of-Thought","Evaluation Benchmark","Diffusion Models"],"permalink":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty","title":"[논문리뷰] EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty","excerpt":"Xuanwu Wang이 [arXiv]에 게시한 'EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Data Augmentation","Large Language Models","Formal Mathematics","Symmetry","Difficulty Evolution","Abstract Syntax Tree","Generalizability"],"permalink":"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models","title":"[논문리뷰] Epistemic Diversity and Knowledge Collapse in Large Language Models","excerpt":"이 [arXiv]에 게시한 'Epistemic Diversity and Knowledge Collapse in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Epistemic Diversity","Knowledge Collapse","Homogenization","Retrieval-Augmented Generation","LLM Evaluation","Information Diversity","Cultural Bias"],"permalink":"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform","title":"[논문리뷰] Code4MeV2: a Research-oriented Code-completion Platform","excerpt":"이 [arXiv]에 게시한 'Code4MeV2: a Research-oriented Code-completion Platform' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Code Completion","Research Platform","Human-AI Interaction","Software Engineering","Open Science","JetBrains IDE Plugin","Telemetry","AI4SE"],"permalink":"/ai/review/2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","title":"[논문리뷰] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation","excerpt":"이 [arXiv]에 게시한 'ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Image Editing","Video Generation","Temporal Reasoning","World Simulation","Physical Consistency","Diffusion Models","Generative Models"],"permalink":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Character-Mixing-for-Video-Generation","title":"[논문리뷰] Character Mixing for Video Generation","excerpt":"이 [arXiv]에 게시한 'Character Mixing for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Video Generation","Character Mixing","Style Preservation","Multi-character Interaction","Text-to-Video","Cross-Domain Synthesis","Identity Preservation"],"permalink":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails","title":"[논문리뷰] Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails","excerpt":"Xinyuan Liu이 [arXiv]에 게시한 'Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Agents","Alignment","Self-Evolution","Behavioral Drift","Reinforcement Learning","Multi-Agent Systems","Alignment Tipping Process"],"permalink":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models","title":"[논문리뷰] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models","excerpt":"Fenglu Hong이 [arXiv]에 게시한 'Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Context Adaptation","Agentic AI","Self-Improving Systems","Prompt Engineering","Context Management","Dynamic Playbooks","Incremental Learning"],"permalink":"/ai/review/2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning","title":"[논문리뷰] AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning","excerpt":"Zeliang Zhang이 [arXiv]에 게시한 'AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Multi-Agent Reinforcement Learning","Adversarial Co-evolution","LLM Safety","Jailbreak Attacks","Internalized Safety","Public Baseline","System Robustness"],"permalink":"/ai/review/2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents","title":"[논문리뷰] Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents","excerpt":"Boyi Wei이 [arXiv]에 게시한 'Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Self-evolving Agents","LLM Safety","Misevolution","Emergent Risks","Model Evolution","Memory Evolution","Tool Evolution","Workflow Evolution"],"permalink":"/ai/review/2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents","title":"[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents","excerpt":"Neil Zhenqiang Gong이 [arXiv]에 게시한 'WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Prompt Injection","Web Agents","Multimodal AI","Adversarial Attacks","Detection Benchmarking","Large Language Models","Image-based Detection","Text-based Detection"],"permalink":"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles","title":"[논문리뷰] Triangle Splatting+: Differentiable Rendering with Opaque Triangles","excerpt":"Matheus Gadelha이 [arXiv]에 게시한 'Triangle Splatting+: Differentiable Rendering with Opaque Triangles' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Differentiable Rendering","3D Reconstruction","Novel View Synthesis","Triangles","Opaque Primitives","Game Engines","Gaussian Splatting","Mesh-based Rendering"],"permalink":"/ai/review/2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling","title":"[논문리뷰] TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling","excerpt":"Juhan Nam이 [arXiv]에 게시한 'TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Conversational Recommendation","LLM Tool Calling","Music Recommendation","Multimodal Retrieval","Information Retrieval","Retrieval-Reranking","Semantic IDs"],"permalink":"/ai/review/2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys","title":"[논문리뷰] SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?","excerpt":"Shuo Wang이 [arXiv]에 게시한 'SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","LLM","LLM Agents","Academic Survey Generation","Evaluation Framework","Benchmark","Quiz-driven Evaluation","Content Quality Metrics"],"permalink":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus","title":"[논문리뷰] SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus","excerpt":"Zhonghao Zhang이 [arXiv]에 게시한 'SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Medical AI","Spine Diagnosis","Multimodal LLM","Benchmark","Dataset","Clinical Reasoning","Spine Surgery","Vision-Language Model"],"permalink":"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey","title":"[논문리뷰] Self-Improvement in Multimodal Large Language Models: A Survey","excerpt":"Yapeng Tian이 [arXiv]에 게시한 'Self-Improvement in Multimodal Large Language Models: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Self-Improvement","Data Collection","Data Organization","Model Optimization","Survey","Reinforcement Learning","Direct Preference Optimization"],"permalink":"/ai/review/2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces","title":"[논문리뷰] Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces","excerpt":"이 [arXiv]에 게시한 'Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Policy Compliance","Large Language Models (LLMs)","Reasoning Traces","In-Context Learning (ICL)","Supervised Finetuning (SFT)","HIPAA","GDPR","ModelSpec"],"permalink":"/ai/review/2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration","title":"[논문리뷰] REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration","excerpt":"이 [arXiv]에 게시한 'REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Model Editing","Lifelong Learning","LLMs","Continual Learning","Knowledge Distillation","Error Feedback","Memory Management","Parameter Merging"],"permalink":"/ai/review/2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features","title":"[논문리뷰] OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features","excerpt":"Elena Tutubalina이 [arXiv]에 게시한 'OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Sparse Autoencoders","Mechanistic Interpretability","Feature Disentanglement","Orthogonality","LLM Features","Feature Absorption","Feature Composition"],"permalink":"/ai/review/2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving","title":"[논문리뷰] NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving","excerpt":"이 [arXiv]에 게시한 'NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Visual Question Answering (VQA)","Autonomous Driving","Risk Assessment","Spatio-Temporal Reasoning","Large Vision Models (VLMs)","Dataset","Bird-Eye-View (BEV)","Fine-tuning"],"permalink":"/ai/review/2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning","title":"[논문리뷰] LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning","excerpt":"이 [arXiv]에 게시한 'LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","LLM Reasoning","RLVR","Dynamic Sampling","Policy Optimization","Response Length","Meta-RL","Overthinking"],"permalink":"/ai/review/2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models","title":"[논문리뷰] LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models","excerpt":"Yu-Chiang Frank Wang이 [arXiv]에 게시한 'LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multimodal LLM","OOD Adaptation","Label Efficiency","VQA","Semi-Supervised Learning","Neuron Distillation","Pseudo Labeling","Medical Imaging"],"permalink":"/ai/review/2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping","title":"[논문리뷰] Improving GUI Grounding with Explicit Position-to-Coordinate Mapping","excerpt":"Spandana Gella이 [arXiv]에 게시한 'Improving GUI Grounding with Explicit Position-to-Coordinate Mapping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","GUI Grounding","Vision-Language Models","Positional Embedding","UI Automation","Coordinate Prediction","Resolution Generalization","Transformer Architecture"],"permalink":"/ai/review/2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty","title":"[논문리뷰] How Confident are Video Models? Empowering Video Models to Express their Uncertainty","excerpt":"Anirudha Majumdar이 [arXiv]에 게시한 'How Confident are Video Models? Empowering Video Models to Express their Uncertainty' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Video Generation","Uncertainty Quantification","Aleatoric Uncertainty","Epistemic Uncertainty","Model Calibration","Text-to-Video","Generative AI","VMF Distribution"],"permalink":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs","title":"[논문리뷰] Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs","excerpt":"이 [arXiv]에 게시한 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Text-to-Image Models","Diffusion Models","Preference Optimization","LLMs","RLHF","Prompt Editing","Free Lunch Alignment","TDPO","TKTO"],"permalink":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents","title":"[논문리뷰] FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents","excerpt":"Léo Boisvert이 [arXiv]에 게시한 'FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Web Agents","LLM Context Pruning","Accessibility Tree","Prompt Injection","Retrieval Augmented Generation","Web Navigation","Agent Security","Efficient LLM"],"permalink":"/ai/review/2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation","title":"[논문리뷰] Efficient Multi-modal Large Language Models via Progressive Consistency Distillation","excerpt":"이 [arXiv]에 게시한 'Efficient Multi-modal Large Language Models via Progressive Consistency Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multi-modal LLMs","Token Compression","Efficiency","Knowledge Distillation","Progressive Learning","Consistency Distillation","MLLM Training"],"permalink":"/ai/review/2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern","title":"[논문리뷰] DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern","excerpt":"Jia Li이 [arXiv]에 게시한 'DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Unit Test Generation","Acceleration","Repetitive Patterns","Abstract Syntax Tree","Software Testing","Code Generation"],"permalink":"/ai/review/2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition","title":"[논문리뷰] Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition","excerpt":"이 [arXiv]에 게시한 'Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Diffusion Models","Flow-based Models","Robotics Control","Policy Composition","Test-time Optimization","Score-based Models","Training-free"],"permalink":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization","title":"[논문리뷰] CoDA: Agentic Systems for Collaborative Data Visualization","excerpt":"이 [arXiv]에 게시한 'CoDA: Agentic Systems for Collaborative Data Visualization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multi-agent Systems","Data Visualization","LLM","Automation","Self-reflection","Code Generation","Natural Language to Visualization"],"permalink":"/ai/review/2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Apriel-1-5-15b-Thinker","title":"[논문리뷰] Apriel-1.5-15b-Thinker","excerpt":"이 [arXiv]에 게시한 'Apriel-1.5-15b-Thinker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning Model","Open-Weights Model","Continual Pretraining (CPT)","Supervised Fine-Tuning (SFT)","Training Design","Efficiency","Frontier Performance"],"permalink":"/ai/review/2025-10-6-Apriel-1-5-15b-Thinker/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents","title":"[논문리뷰] Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents","excerpt":"Jong Chul Ye이 [arXiv]에 게시한 'Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Consistency Models","Generative Models","Manifold Learning","Tangent Alignment","Diffusion Models","Training Dynamics","Manifold Feature Distance"],"permalink":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning","title":"[논문리뷰] A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multi-turn Reinforcement Learning","LLM Agents","Text-based Environments","Reward Shaping","Policy Optimization","Supervised Fine-tuning (SFT)","Generalization","Environment Complexity"],"permalink":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls","title":"[논문리뷰] Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls","excerpt":"Stuart Shieber이 [arXiv]에 게시한 'Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Transformers","Multiplication","Long-Range Dependencies","Implicit Chain-of-Thought","Attention Mechanisms","Inductive Bias","Reverse Engineering"],"permalink":"/ai/review/2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs","title":"[논문리뷰] VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs","excerpt":"이 [arXiv]에 게시한 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Object Grounding","Fine-grained Perception","Hybrid Region Encoder","Plug-and-play","Two-stage Training","Visual Reasoning"],"permalink":"/ai/review/2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators","title":"[논문리뷰] VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators","excerpt":"Zirui Ge이 [arXiv]에 게시한 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Reinforcement Learning","World Models","Fine-tuning","Embodied AI","Robotics","Reward Design","Distribution Shift"],"permalink":"/ai/review/2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned","title":"[논문리뷰] Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned","excerpt":"이 [arXiv]에 게시한 'Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Process Reward Models (PRMs)","Multimodal Reasoning","Test-Time Scaling (TTS)","Process Supervision","Dataset Construction","Perception Errors","MCTS"],"permalink":"/ai/review/2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction","title":"[논문리뷰] ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction","excerpt":"이 [arXiv]에 게시한 'ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Sliced Wasserstein Distance","Reservoir Sampling","Variance Reduction","Distribution Matching","Diffusion Guidance","Color Correction","Monte Carlo Estimation"],"permalink":"/ai/review/2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning","title":"[논문리뷰] PIPer: On-Device Environment Setup via Online Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'PIPer: On-Device Environment Setup via Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Environment Setup","LLMs","Reinforcement Learning","Supervised Fine-tuning","On-device AI","Software Engineering","Verifiable Rewards"],"permalink":"/ai/review/2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models","title":"[논문리뷰] On Predictability of Reinforcement Learning Dynamics for Large Language Models","excerpt":"Yuqing Huang이 [arXiv]에 게시한 'On Predictability of Reinforcement Learning Dynamics for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Parameter Dynamics","Rank-1 Dominance","Linear Dynamics","SVD","Model Acceleration","Predictability"],"permalink":"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Making-not-Taking-the-Best-of-N","title":"[논문리뷰] Making, not Taking, the Best of N","excerpt":"이 [arXiv]에 게시한 'Making, not Taking, the Best of N' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Aggregation","Generative Fusion","Best-of-N","Synthetic Data Generation","Test-Time Scaling","Multilingual Models","Ensemble Learning"],"permalink":"/ai/review/2025-10-2-Making-not-Taking-the-Best-of-N/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation","title":"[논문리뷰] Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation","excerpt":"이 [arXiv]에 게시한 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Reinforcement Learning (RL)","Exploration Budget Allocation","Knapsack Problem","Group Relative Policy Optimization (GRPO)","Mathematical Reasoning","Resource Optimization"],"permalink":"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA","title":"[논문리뷰] JoyAgent-JDGenie: Technical Report on the GAIA","excerpt":"이 [arXiv]에 게시한 'JoyAgent-JDGenie: Technical Report on the GAIA' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Generalist Agent","Multi-Agent System","Plan-Execute","ReAct","Hierarchical Memory","Tool Integration","GAIA Benchmark","LLM Agent"],"permalink":"/ai/review/2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents","title":"[논문리뷰] Infusing Theory of Mind into Socially Intelligent LLM Agents","excerpt":"이 [arXiv]에 게시한 'Infusing Theory of Mind into Socially Intelligent LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Theory of Mind","Large Language Models","Social Agents","Dialogue Systems","Mental State Modeling","Look-ahead Planning","Supervised Fine-tuning","Sotopia Benchmark"],"permalink":"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning","title":"[논문리뷰] In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning","excerpt":"Chaehyeon Chung이 [arXiv]에 게시한 'In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Feedback","Multi-turn Reasoning","In-place Editing","Token Efficiency","Error Correction","Human-AI Interaction","Reasoning Tasks"],"permalink":"/ai/review/2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures","title":"[논문리뷰] Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures","excerpt":"Andrea Passerini이 [arXiv]에 게시한 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Interpretability","Vector Symbolic Architectures","Neural Probing","Information Decoding","Hyperdimensional Computing","Latent Representations"],"permalink":"/ai/review/2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness","title":"[논문리뷰] GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness","excerpt":"Chien-Sheng Wu이 [arXiv]에 게시한 'GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","GUI Agents","KV Cache Compression","Spatio-Temporal Awareness","Vision-Language Models","Efficiency","Attention Sparsity","QR Decomposition"],"permalink":"/ai/review/2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-GEM-A-Gym-for-Agentic-LLMs","title":"[논문리뷰] GEM: A Gym for Agentic LLMs","excerpt":"이 [arXiv]에 게시한 'GEM: A Gym for Agentic LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Agentic LLMs","Reinforcement Learning","Environment Simulator","Multi-turn Interactions","Return Batch Normalization","Tool Integration","Benchmarking"],"permalink":"/ai/review/2025-10-2-GEM-A-Gym-for-Agentic-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution","title":"[논문리뷰] Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution","excerpt":"이 [arXiv]에 게시한 'Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Agents","Parallel Execution","DAG-based Planning","Tool Orchestration","Web Agents","Reasoning Framework","Efficiency"],"permalink":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models","title":"[논문리뷰] Eliciting Secret Knowledge from Language Models","excerpt":"Neel Nanda이 [arXiv]에 게시한 'Eliciting Secret Knowledge from Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Language Models","Secret Elicitation","Mechanistic Interpretability","Black-box Methods","White-box Methods","AI Auditing","Model Organisms","Prefill Attacks"],"permalink":"/ai/review/2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search","title":"[논문리뷰] DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search","excerpt":"이 [arXiv]에 게시한 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Monte Carlo Tree Search (MCTS)","Mathematical Reasoning","Large Language Models (LLMs)","Systematic Exploration","Adaptive Training","Tree-GRPO"],"permalink":"/ai/review/2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs","title":"[논문리뷰] CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs","excerpt":"Hengyi Cai이 [arXiv]에 게시한 'CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Curriculum Learning","LLMs","Reasoning","Gradient Optimization","Reinforcement Learning","Bayesian Inference","Sample Efficiency"],"permalink":"/ai/review/2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation","title":"[논문리뷰] Code2Video: A Code-centric Paradigm for Educational Video Generation","excerpt":"이 [arXiv]에 게시한 'Code2Video: A Code-centric Paradigm for Educational Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Educational Video Generation","Code-centric AI","Multi-agent Framework","Manim","Vision-Language Models","Knowledge Transfer","Code Generation","MMMC Benchmark"],"permalink":"/ai/review/2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration","title":"[논문리뷰] BroRL: Scaling Reinforcement Learning via Broadened Exploration","excerpt":"이 [arXiv]에 게시한 'BroRL: Scaling Reinforcement Learning via Broadened Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Scaling Laws","Exploration","Rollout Size","Verifiable Rewards","PPO","Mass Balance Equation"],"permalink":"/ai/review/2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Boolean-Satisfiability-via-Imitation-Learning","title":"[논문리뷰] Boolean Satisfiability via Imitation Learning","excerpt":"Xiangyu Xu이 [arXiv]에 게시한 'Boolean Satisfiability via Imitation Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Boolean Satisfiability","Imitation Learning","CDCL Solvers","Branching Policy","KeyTrace","Transformer Architecture","Perceiver AR"],"permalink":"/ai/review/2025-10-2-Boolean-Satisfiability-via-Imitation-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","title":"[논문리뷰] BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration","excerpt":"Xiangyang Xia이 [arXiv]에 게시한 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Video Generation","Subject Consistency","Cross-Modal Integration","Diffusion Models","Multimodal LLM","Diffusion Transformer","Text-to-Video"],"permalink":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses","title":"[논문리뷰] BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses","excerpt":"Julian McAuley이 [arXiv]에 게시한 'BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Bias Mitigation","Benchmark","Evaluation Metrics","Prompt Engineering","Fine-tuning","Bias-Free Score","Fairness"],"permalink":"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum","title":"[논문리뷰] Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum","excerpt":"Hanghang Tong이 [arXiv]에 게시한 'Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Supervised Fine-tuning (SFT)","Large Language Models (LLMs)","Training Objectives","Negative Log Likelihood (NLL)","Model Capability Continuum","Generalization","Probability-based Loss Functions"],"permalink":"/ai/review/2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications","title":"[논문리뷰] An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications","excerpt":"Bram Adams이 [arXiv]에 게시한 'An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","AI Agent","LLM Agent","Testing","Empirical Study","Software Quality","Agent Frameworks","Agentic Applications","Non-Determinism"],"permalink":"/ai/review/2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents","title":"[논문리뷰] ACON: Optimizing Context Compression for Long-horizon LLM Agents","excerpt":"이 [arXiv]에 게시한 'ACON: Optimizing Context Compression for Long-horizon LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Agents","Context Compression","Long-horizon Tasks","Prompt Optimization","Knowledge Distillation","Memory Efficiency","Task Performance","Failure Analysis"],"permalink":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking","title":"[논문리뷰] jina-reranker-v3: Last but Not Late Interaction for Document Reranking","excerpt":"이 [arXiv]에 게시한 'jina-reranker-v3: Last but Not Late Interaction for Document Reranking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Document Reranking","Last but Not Late Interaction","Multilingual","Transformer Architecture","Cross-Encoder","InfoNCE Loss","Contextual Embedding","Qwen3"],"permalink":"/ai/review/2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs","title":"[논문리뷰] dParallel: Learnable Parallel Decoding for dLLMs","excerpt":"이 [arXiv]에 게시한 'dParallel: Learnable Parallel Decoding for dLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Parallel Decoding","Inference Acceleration","Certainty Distillation","Self-Distillation","Masked Language Models","LLaDA"],"permalink":"/ai/review/2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching","title":"[논문리뷰] d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching","excerpt":"Jiarui Wang이 [arXiv]에 게시한 'd^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Diffusion Models","Large Language Models (LLMs)","Inference Acceleration","KV Cache","Bidirectional Attention","Adaptive Caching","Token Selection"],"permalink":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning","title":"[논문리뷰] Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning","excerpt":"Yue Min이 [arXiv]에 게시한 'Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM SFT","Data Pruning","Sample Pruning","Token Pruning","Error-Uncertainty Plane","Q-Tuning","Data Efficiency","Dynamic Pruning"],"permalink":"/ai/review/2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments","title":"[논문리뷰] Who's Your Judge? On the Detectability of LLM-Generated Judgments","excerpt":"이 [arXiv]에 게시한 'Who's Your Judge? On the Detectability of LLM-Generated Judgments' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM-as-a-judge","Judgment Detection","Bias Quantification","Feature Engineering","Interpretability","Peer Review","AI Ethics","Evaluation"],"permalink":"/ai/review/2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Who-invented-deep-residual-learning","title":"[논문리뷰] Who invented deep residual learning?","excerpt":"Juergen Schmidhuber이 [arXiv]에 게시한 'Who invented deep residual learning?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Deep Learning History","Residual Connections","Recurrent Neural Networks (RNN)","Long Short-Term Memory (LSTM)","Feedforward Neural Networks (FNN)","Highway Networks","ResNet","Vanishing Gradient"],"permalink":"/ai/review/2025-10-1-Who-invented-deep-residual-learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap","title":"[논문리뷰] Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap","excerpt":"Hengfan Zhang이 [arXiv]에 게시한 'Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Voice AI","LLM","Reasoning","Benchmark","Modality Gap","Latency","Speech Recognition","Generative AI","Real-time Systems","Conversational AI"],"permalink":"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications","title":"[논문리뷰] VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications","excerpt":"이 [arXiv]에 게시한 'VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Benchmarking","Interactive Tasks","Real-world Applications","Tool Use","Multi-turn Conversation","Task Complexity"],"permalink":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes","title":"[논문리뷰] VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes","excerpt":"Muhammad Huzaifa이 [arXiv]에 게시한 'VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Visual Question Answering","Multimodal Models","Dense Scenes","Fine-Grained Perception","Benchmark","Error Analysis","Counting","OCR"],"permalink":"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play","title":"[논문리뷰] Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play","excerpt":"Jing Shi이 [arXiv]에 게시한 'Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Self-Play","Reinforcement Learning","Gamification","Data Efficiency","Strategic Reasoning","Multimodal AI","Self-Improvement"],"permalink":"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning","title":"[논문리뷰] TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Hallucination","Truthfulness","Reinforcement Learning","Ternary Reward","Abstention","Knowledge Boundary","GRPO","RLHF"],"permalink":"/ai/review/2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training","title":"[논문리뷰] Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training","excerpt":"이 [arXiv]에 게시한 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Mechanistic Interpretability","Attention Heads","Post-Training","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Circuit Analysis","Reasoning Models","Transformer Architecture"],"permalink":"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain","title":"[논문리뷰] The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain","excerpt":"이 [arXiv]에 게시한 'The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Brain-Inspired AI","Graph Neural Networks","Hebbian Learning","Scale-Free Networks","Model Interpretability","Transformer Architecture"],"permalink":"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs","title":"[논문리뷰] Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs","excerpt":"Yao Shu이 [arXiv]에 게시한 'Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Multi-turn Interaction","Test-Time Adaptation","Reinforcement Learning from Human Feedback","Policy Optimization","Online Learning","Self-Correction"],"permalink":"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training","title":"[논문리뷰] TTT3R: 3D Reconstruction as Test-Time Training","excerpt":"Anpei Chen이 [arXiv]에 게시한 'TTT3R: 3D Reconstruction as Test-Time Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Test-Time Training (TTT)","Recurrent Neural Networks (RNN)","Online Learning","Length Generalization","Associative Memory","State Update Rule"],"permalink":"/ai/review/2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics","title":"[논문리뷰] TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics","excerpt":"Szu-Chi Chen이 [arXiv]에 게시한 'TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Audio Language Models","Cultural Sound Understanding","Localized Benchmark","Non-semantic Audio","Human-in-the-loop","Multimodal AI","Taipei Soundscape"],"permalink":"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation","title":"[논문리뷰] Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation","excerpt":"이 [arXiv]에 게시한 'Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Video Generation","Evaluation Framework","Cinematic Control","Taxonomy","Human Annotation","Vision-Language Models","Text-to-Video"],"permalink":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models","title":"[논문리뷰] Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models","excerpt":"이 [arXiv]에 게시한 'Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Test-Time Training (TTT)","Foundation Models","Underparameterization","Sparse Autoencoders (SAE)","Linear Representation Hypothesis (LRH)","Specialization","Scaling Laws","In-Distribution Data"],"permalink":"/ai/review/2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Regression-Language-Models-for-Code","title":"[논문리뷰] Regression Language Models for Code","excerpt":"이 [arXiv]에 게시한 'Regression Language Models for Code' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Regression Language Model","Code Performance Prediction","Static Analysis","Neural Architecture Search","Text-to-Text Regression","Multi-task Learning","T5Gemma","ONNX"],"permalink":"/ai/review/2025-10-1-Regression-Language-Models-for-Code/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation","title":"[논문리뷰] ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation","excerpt":"Antonio Liotta이 [arXiv]에 게시한 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Video-Language Model","Proficiency Estimation","Multi-View Video","Action Quality Assessment","Lightweight Model","Generative Feedback"],"permalink":"/ai/review/2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark","title":"[논문리뷰] Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark","excerpt":"Penghao Zhu이 [arXiv]에 게시한 'Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","AI Reasoning","Physics Research","LLM Evaluation","Scientific Benchmark","Frontier Physics","Problem Solving","Model Reliability","Auto-grading"],"permalink":"/ai/review/2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always","title":"[논문리뷰] OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!","excerpt":"이 [arXiv]에 게시한 'OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Operational Safety","Out-of-Domain (OOD)","Prompt Steering","Jailbreak Attacks","Evaluation Benchmark","Refusal Rate"],"permalink":"/ai/review/2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents","title":"[논문리뷰] OceanGym: A Benchmark Environment for Underwater Embodied Agents","excerpt":"이 [arXiv]에 게시한 'OceanGym: A Benchmark Environment for Underwater Embodied Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Underwater Robotics","Embodied AI","Benchmark Environment","Multi-modal Large Language Models","Autonomous Underwater Vehicles","Perception","Decision-Making","Simulation"],"permalink":"/ai/review/2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation","title":"[논문리뷰] MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation","excerpt":"Limin Wang이 [arXiv]에 게시한 'MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Image-to-Video Generation","Motion Transfer","Retrieval-Augmented Generation (RAG)","In-Context Learning","Diffusion Models","Video Diffusion","Motion Realism"],"permalink":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models","title":"[논문리뷰] More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models","excerpt":"Fabian Waschkowski이 [arXiv]에 게시한 'More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Multimodal Reasoning","Reasoning","Visual Forgetting","Perceptual Grounding","Reinforcement Learning","Policy Optimization","Visual Anchors"],"permalink":"/ai/review/2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning","title":"[논문리뷰] Mem-α: Learning Memory Construction via Reinforcement Learning","excerpt":"Yuzhen Mao이 [arXiv]에 게시한 'Mem-α: Learning Memory Construction via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","External Memory","Reinforcement Learning","Memory Management","Long-Context Understanding","Tool Learning","RAG","Memory Architecture"],"permalink":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use","title":"[논문리뷰] MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use","excerpt":"이 [arXiv]에 게시한 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Model Context Protocol","Benchmark","Tool Use","CRUD Operations","Workflow Automation","Stress Testing","Evaluation"],"permalink":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification","title":"[논문리뷰] MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification","excerpt":"Zhiming Luo이 [arXiv]에 게시한 'MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Adversarial Purification","Diffusion Models","Frequency Domain","Adaptive Noise Injection","Robustness","Image Security","Magnitude Spectrum"],"permalink":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training","title":"[논문리뷰] Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training","excerpt":"Koustuv Sinha이 [arXiv]에 게시한 'Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Visual Priors","Language Pre-training","Multimodal LLM","Data Mixture Optimization","Reasoning Prior","Perception Prior","VQA","MLE-Bench"],"permalink":"/ai/review/2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs","title":"[논문리뷰] Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs","excerpt":"이 [arXiv]에 게시한 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","AI-Generated Videos","Deepfake Detection","Multimodal LLMs","Human Perception","Video Generation Evaluation","Spatiotemporal Annotation","Reward Modeling"],"permalink":"/ai/review/2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers","title":"[논문리뷰] LayerD: Decomposing Raster Graphic Designs into Layers","excerpt":"Kota Yamaguchi이 [arXiv]에 게시한 'LayerD: Decomposing Raster Graphic Designs into Layers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Graphic Design","Image Decomposition","Layer Extraction","Image Matting","Background Completion","Deep Learning","Creative AI","Dynamic Time Warping"],"permalink":"/ai/review/2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Knowledge-Homophily-in-Large-Language-Models","title":"[논문리뷰] Knowledge Homophily in Large Language Models","excerpt":"Nedim Lipka이 [arXiv]에 게시한 'Knowledge Homophily in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM","Knowledge Homophily","Graph Neural Networks","Knowledge Graph","Knowledge Injection","Question Answering","Fine-tuning","Knowledge Retrieval"],"permalink":"/ai/review/2025-10-1-Knowledge-Homophily-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents","title":"[논문리뷰] InfoAgent: Advancing Autonomous Information-Seeking Agents","excerpt":"이 [arXiv]에 게시한 'InfoAgent: Advancing Autonomous Information-Seeking Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Information Seeking","Reinforcement Learning","Data Synthesis","Web Search Tools","Tool Use","Deep Research Agents"],"permalink":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance","title":"[논문리뷰] IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance","excerpt":"이 [arXiv]에 게시한 'IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Diffusion Models","Multimodal Alignment","MLLM","Image Re-generation","Preference Learning","Implicit Guidance","Text-to-Image"],"permalink":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss","title":"[논문리뷰] Humanline: Online Alignment as Perceptual Loss","excerpt":"이 [arXiv]에 게시한 'Humanline: Online Alignment as Perceptual Loss' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Alignment","Online RLHF","Offline RLHF","Prospect Theory","Perceptual Loss","Human-Centric AI","Reinforcement Learning"],"permalink":"/ai/review/2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents","title":"[논문리뷰] Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents","excerpt":"이 [arXiv]에 게시한 'Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","GUI Agents","On-Device AI","Multimodal LLM","GUI Grounding","GUI Navigation","Reinforcement Learning","Supervised Fine-tuning","Synthetic Data"],"permalink":"/ai/review/2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning","title":"[논문리뷰] Estimating Time Series Foundation Model Transferability via In-Context Learning","excerpt":"Jun Qi이 [arXiv]에 게시한 'Estimating Time Series Foundation Model Transferability via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Time Series Foundation Models","Transferability Estimation","In-Context Learning","Tabular Foundation Models","Model Selection","Entropy Profile","Meta-learning","Forecasting"],"permalink":"/ai/review/2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting","title":"[논문리뷰] EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting","excerpt":"이 [arXiv]에 게시한 'EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Time Series Forecasting","Transformer","Dynamic Patching","Entropy","Predictive Uncertainty","Adaptive Encoding","Attention Mechanisms","Causal Transformer"],"permalink":"/ai/review/2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","title":"[논문리뷰] Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention","excerpt":"이 [arXiv]에 게시한 'Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Audio-Visual Speech Separation","Deep Learning","Efficiency","Discrete Lip Semantics","Global-Local Attention","Lightweight Models","VQ-VAE"],"permalink":"/ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively","title":"[논문리뷰] DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively","excerpt":"이 [arXiv]에 게시한 'DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","AI Scientist","Autonomous Scientific Discovery","Bayesian Optimization","LLM-based Agents","SOTA-Surpassing","Findings Memory","Exploration-Exploitation"],"permalink":"/ai/review/2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","title":"[논문리뷰] DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder","excerpt":"이 [arXiv]에 게시한 'DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Video Autoencoder","Deep Compression","Model Acceleration","Fine-tuning","Latent Space","Temporal Modeling"],"permalink":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-DA2-Depth-Anything-in-Any-Direction","title":"[논문리뷰] DA^2: Depth Anything in Any Direction","excerpt":"이 [arXiv]에 게시한 'DA^2: Depth Anything in Any Direction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Panoramic Depth Estimation","Zero-shot Generalization","Data Curation","SphereViT","Spherical Geometry","360-degree Imaging","Vision Transformer"],"permalink":"/ai/review/2025-10-1-DA2-Depth-Anything-in-Any-Direction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs","title":"[논문리뷰] Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs","excerpt":"normanpaulsen이 [arXiv]에 게시한 'Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Context Window","Effective Context Window","Model Performance","Hallucination Rates","RAG Systems","Token Limits"],"permalink":"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software","title":"[논문리뷰] BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software","excerpt":"이 [arXiv]에 게시한 'BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Open-Source Software","Compilation","Benchmarking","Software Engineering","Error Resolution","Retrieval-Augmented Generation"],"permalink":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective","title":"[논문리뷰] Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective","excerpt":"이 [arXiv]에 게시한 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Planning","Policy Gradient","Q-learning","Supervised Fine-Tuning","Diversity Collapse","Reward Hacking"],"permalink":"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models","title":"[논문리뷰] Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models","excerpt":"이 [arXiv]에 게시한 'Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Process-Supervised RL","Large Language Models","Reasoning Models","Attention Mechanism","Efficient Exploration","Adaptive Sampling","Off-Policy Training"],"permalink":"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects","title":"[논문리뷰] A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects","excerpt":"Jennifer Ding이 [arXiv]에 게시한 'A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Open Source AI","LLM Development","Open Collaboration","Governance Models","Developer Motivations","Community Engagement","AI Ecosystem"],"permalink":"/ai/review/2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs","title":"[논문리뷰] Visual Jigsaw Post-Training Improves MLLMs","excerpt":"Lewei Lu이 [arXiv]에 게시한 'Visual Jigsaw Post-Training Improves MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","MLLMs","Post-training","Self-supervised Learning","Visual Understanding","Jigsaw Puzzles","RLVR","Multimodal Perception","Spatial Reasoning"],"permalink":"/ai/review/2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs","title":"[논문리뷰] StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs","excerpt":"Wei Jia이 [arXiv]에 게시한 'StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Speech Tokenizer","Noise Robustness","Semantic Tokens","SpeechLLMs","Voting-LFQ","Consensus Training","Automatic Speech Recognition","Speech Synthesis"],"permalink":"/ai/review/2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention","title":"[논문리뷰] SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention","excerpt":"이 [arXiv]에 게시한 'SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Diffusion Transformers","Sparse Attention","Linear Attention","Model Acceleration","Video Generation","Attention Mechanisms","Fine-tuning"],"permalink":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer","title":"[논문리뷰] SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer","excerpt":"이 [arXiv]에 게시한 'SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Model","Linear Attention","Transformer","Long Video","Efficient Inference","Constant Memory","Low-Cost Training","RTX Deployment"],"permalink":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark","title":"[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark","excerpt":"Yuran Wang이 [arXiv]에 게시한 'RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Unified Models","Multimodal AI","Benchmark","Capability Synergy","Visual Understanding","Image Generation","Dual-Evaluation Protocol"],"permalink":"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards","title":"[논문리뷰] Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards","excerpt":"Binxing Jiao이 [arXiv]에 게시한 'Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Reasoning","Policy Valuation","Markov Decision Process","Diversity","Math Reasoning","Verifiable Rewards"],"permalink":"/ai/review/2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing","title":"[논문리뷰] OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing","excerpt":"Huanyu Zhang이 [arXiv]에 게시한 'OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Image Generation","Image Editing","Multimodal AI","Dataset","Instruction Following","Taxonomy","GPT-40"],"permalink":"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-Multiplayer-Nash-Preference-Optimization","title":"[논문리뷰] Multiplayer Nash Preference Optimization","excerpt":"이 [arXiv]에 게시한 'Multiplayer Nash Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","RLHF","LLM Alignment","Nash Equilibrium","Multiplayer Games","Preference Optimization","Non-transitive Preferences","Game Theory"],"permalink":"/ai/review/2025-9-30-Multiplayer-Nash-Preference-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling","title":"[논문리뷰] EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling","excerpt":"이 [arXiv]에 게시한 'EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Image Editing","Reward Modeling","Instruction-Guided Editing","Online RL","Visual Language Models","Benchmark","Self-Ensembling"],"permalink":"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering","title":"[논문리뷰] EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering","excerpt":"이 [arXiv]에 게시한 'EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","LLM Steering Framework","vLLM Integration","Hidden State Manipulation","Inference Optimization","Extensibility","Modular Architecture","Reasoning Mitigation","Hallucination Reduction"],"permalink":"/ai/review/2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","title":"[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction","excerpt":"Guoxian Song이 [arXiv]에 게시한 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Digital Human","Multimodal AI","Real-time Streaming","Video Generation","Diffusion Models","Transformer Architecture","Audiovisual Synchronization","World Modeling"],"permalink":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning","title":"[논문리뷰] X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning","excerpt":"Raghuveer Rao이 [arXiv]에 게시한 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Text-to-Video Retrieval","LLM","Chain-of-Thought","Explainable AI","Multimodal Retrieval","Bradley-Terry Model","Video Annotation"],"permalink":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction","title":"[논문리뷰] WoW: Towards a World omniscient World model Through Embodied Interaction","excerpt":"Weishi Mi이 [arXiv]에 게시한 'WoW: Towards a World omniscient World model Through Embodied Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","World Model","Embodied AI","Robotics","Diffusion Models","Physical Reasoning","Vision Language Models","Interaction Data","Self-Optimization"],"permalink":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation","title":"[논문리뷰] Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation","excerpt":"Shiming Liu이 [arXiv]에 게시한 'Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","MLLM","Interpretability","Attribution","Token Generation","Black-box Explanation","Hallucination Diagnosis","Multimodality","VQA"],"permalink":"/ai/review/2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning","title":"[논문리뷰] WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning","excerpt":"Zhuofan Zong이 [arXiv]에 게시한 'WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Website Generation","Code Agent","LLM","VLM","Reinforcement Learning","Multi-Level Feedback","GUI Agent","Step-GRPO"],"permalink":"/ai/review/2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing","title":"[논문리뷰] VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing","excerpt":"이 [arXiv]에 게시한 'VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","AI Assistants","Multimodal Benchmarking","Audio Understanding","Speech Synthesis","Vision-Language Models","Role-play","Safety","Robustness"],"permalink":"/ai/review/2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Variational-Reasoning-for-Language-Models","title":"[논문리뷰] Variational Reasoning for Language Models","excerpt":"이 [arXiv]에 게시한 'Variational Reasoning for Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Variational Inference","Language Models","Reasoning","ELBO","IWAE","Reinforcement Learning","Latent Variables","Forward-KL"],"permalink":"/ai/review/2025-9-29-Variational-Reasoning-for-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models","title":"[논문리뷰] UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models","excerpt":"Yuchao Gu이 [arXiv]에 게시한 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Unified Vision Modeling","Video Generation","Diffusion Transformer","Supervised Fine-tuning","Cross-modal","Cross-source Tasks","Visual Sentences","LoRA"],"permalink":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios","title":"[논문리뷰] UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios","excerpt":"Zeyu Qin이 [arXiv]에 게시한 'UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM Agents","Long-Horizon Reasoning","Benchmarking","Partially Observable","Tool Use","Memory Management","Exploration"],"permalink":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval","title":"[논문리뷰] Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval","excerpt":"이 [arXiv]에 게시한 'Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","RAG","LLM Reasoning","Knowledge Graphs","Multi-Agent Systems","Context Retrieval","Heterogeneous Graphs","Adaptive Learning","Dual-Evolution"],"permalink":"/ai/review/2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images","title":"[논문리뷰] TUN3D: Towards Real-World Scene Understanding from Unposed Images","excerpt":"Anna Vorontsova이 [arXiv]에 게시한 'TUN3D: Towards Real-World Scene Understanding from Unposed Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","3D Scene Understanding","Layout Estimation","3D Object Detection","Unposed Images","Sparse Convolutional Networks","Multi-view Stereo","Real-time AI"],"permalink":"/ai/review/2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion","title":"[논문리뷰] StateX: Enhancing RNN Recall via Post-training State Expansion","excerpt":"Zhiyuan Liu이 [arXiv]에 게시한 'StateX: Enhancing RNN Recall via Post-training State Expansion' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","RNN","State Expansion","Post-training","Long-context Recall","Linear Attention","State Space Models","GLA","Mamba2"],"permalink":"/ai/review/2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation","title":"[논문리뷰] See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation","excerpt":"Chih-Hai Su이 [arXiv]에 게시한 'See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Vision-Language Models","UAV Navigation","Zero-shot","Spatial Grounding","Waypoint Prompting","Autonomous Navigation","Adaptive Control"],"permalink":"/ai/review/2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework","title":"[논문리뷰] SPARK: Synergistic Policy And Reward Co-Evolving Framework","excerpt":"이 [arXiv]에 게시한 'SPARK: Synergistic Policy And Reward Co-Evolving Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","LVLMs","Reward Modeling","Policy Optimization","Self-Reflection","Verifiable Rewards","Co-evolution"],"permalink":"/ai/review/2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models","title":"[논문리뷰] ReviewScore: Misinformed Peer Review Detection with Large Language Models","excerpt":"이 [arXiv]에 게시한 'ReviewScore: Misinformed Peer Review Detection with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Peer Review","Review Quality","Large Language Models (LLMs)","Misinformed Review","Argument Reconstruction","Factuality Evaluation","Natural Language Processing","Automated Evaluation"],"permalink":"/ai/review/2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation","title":"[논문리뷰] RefAM: Attention Magnets for Zero-Shot Referral Segmentation","excerpt":"Federico Tombari이 [arXiv]에 게시한 'RefAM: Attention Magnets for Zero-Shot Referral Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Zero-Shot Segmentation","Referring Segmentation","Diffusion Transformers (DiTs)","Attention Mechanisms","Attention Sinks","Stop Words","Vision-Language Models","Training-Free Methods"],"permalink":"/ai/review/2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Real-Time-Object-Detection-Meets-DINOv3","title":"[논문리뷰] Real-Time Object Detection Meets DINOv3","excerpt":"Xi Shen이 [arXiv]에 게시한 'Real-Time Object Detection Meets DINOv3' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Real-time Object Detection","DINOv3","DEIMv2","Vision Transformer","Multi-scale Features","Spatial Tuning Adapter","Lightweight Models","Object Detection Framework"],"permalink":"/ai/review/2025-9-29-Real-Time-Object-Detection-Meets-DINOv3/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning","title":"[논문리뷰] Quantile Advantage Estimation for Entropy-Safe Reasoning","excerpt":"An Zhang이 [arXiv]에 게시한 'Quantile Advantage Estimation for Entropy-Safe Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Reasoning","Entropy Control","Advantage Estimation","Quantile Baseline","Exploration-Exploitation","RLVR"],"permalink":"/ai/review/2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning","title":"[논문리뷰] PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning","excerpt":"Lingpeng Kong이 [arXiv]에 게시한 'PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Prompt Synthesis","Large Language Models","Reasoning","Expectation-Maximization","Self-Play","Supervised Fine-Tuning","Task Generation","Rationale Generation"],"permalink":"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping","title":"[논문리뷰] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping","excerpt":"이 [arXiv]에 게시한 'No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM Reinforcement Learning","Zero-Variance Prompts","Advantage Shaping","Entropy-Guided","Math Reasoning","RLVR","Group Relative Policy Optimization"],"permalink":"/ai/review/2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing","title":"[논문리뷰] MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing","excerpt":"SunYuefeng이 [arXiv]에 게시한 'MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Document Parsing","Vision-Language Model","High-Resolution","Two-Stage Inference","Layout Analysis","Content Recognition","Data Engine","Computational Efficiency"],"permalink":"/ai/review/2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation","title":"[논문리뷰] Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation","excerpt":"Peter Wonka이 [arXiv]에 게시한 'Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Subject-Driven Generation","Visual Inconsistency Detection","Feature Disentanglement","Diffusion Models","Semantic Correspondence","Evaluation Metric","Spatial Localization","Contrastive Learning"],"permalink":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning","title":"[논문리뷰] MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning","excerpt":"Weipeng Zhong이 [arXiv]에 게시한 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Robotic Manipulation","Large Language Models","Spatial Reasoning","Dataset","Direct Preference Optimization","Tabletop Scene"],"permalink":"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer","title":"[논문리뷰] LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer","excerpt":"이 [arXiv]에 게시한 'LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Universal Image Restoration","Diffusion Transformer","Caption-Free","Semantic Alignment","Image Quality Assessment","Data Curation","Real-World Degradations","Deep Learning"],"permalink":"/ai/review/2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation","title":"[논문리뷰] LongLive: Real-time Interactive Long Video Generation","excerpt":"이 [arXiv]에 게시한 'LongLive: Real-time Interactive Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Long Video Generation","Real-time","Interactive AI","Autoregressive Models","KV Cache","Streaming Tuning","Attention Sink","Diffusion Models"],"permalink":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning","title":"[논문리뷰] Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning","excerpt":"Gang Li이 [arXiv]에 게시한 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Agents","Exploration-Exploitation","Self-Imitation Learning","Intrinsic Rewards","Curriculum Learning","Policy Entropy","Tool Use"],"permalink":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards","title":"[논문리뷰] Language Models Can Learn from Verbal Feedback Without Scalar Rewards","excerpt":"이 [arXiv]에 게시한 'Language Models Can Learn from Verbal Feedback Without Scalar Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Verbal Feedback","Conditional Generation","Large Language Models","Feedback-Conditional Policy","Offline-Online Learning","Reward Hypothesis Bypass"],"permalink":"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models","title":"[논문리뷰] Instruction-Following Evaluation in Function Calling for Large Language Models","excerpt":"NikolaiSkripko이 [arXiv]에 게시한 'Instruction-Following Evaluation in Function Calling for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Function Calling","LLMs","Instruction Following","Benchmarking","JSON Schema","AI Agents","Evaluation Metrics"],"permalink":"/ai/review/2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models","title":"[논문리뷰] HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models","excerpt":"Romann M. Weber이 [arXiv]에 게시한 'HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Diffusion Models","Sampling","Generative AI","Image Generation","Plug-and-Play","Training-Free","Guidance","Momentum-Based Methods"],"permalink":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing","title":"[논문리뷰] FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing","excerpt":"Linghe Kong이 [arXiv]에 게시한 'FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Text-Guided Image Editing","Diffusion Models","Real-Time Editing","One-Step Inversion","Attention Control","Background Preservation","Semantic Disentanglement"],"permalink":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Fine-tuning-Done-Right-in-Model-Editing","title":"[논문리뷰] Fine-tuning Done Right in Model Editing","excerpt":"Du Su이 [arXiv]에 게시한 'Fine-tuning Done Right in Model Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Model Editing","Fine-tuning","Large Language Models","Catastrophic Forgetting","Breadth-First Pipeline","Depth-First Pipeline","Localized Tuning","Lifelong Learning"],"permalink":"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences","title":"[논문리뷰] Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences","excerpt":"Eija Honkavaara이 [arXiv]에 게시한 'Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","3D Object Localization","Particle Filter","Multi-target Tracking","Drone Surveillance","Wildfire Monitoring","Semantic Segmentation","Camera Pose Estimation"],"permalink":"/ai/review/2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models","title":"[논문리뷰] ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models","excerpt":"Ki-Ung Song이 [arXiv]에 게시한 'ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","High-Resolution Vision","Vision-Language Models","Efficient Reasoning","Coarse-to-Fine","Reinforcement Learning","Visual Understanding","Attention Mechanism"],"permalink":"/ai/review/2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning","title":"[논문리뷰] EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning","excerpt":"Li Yu-Jhe이 [arXiv]에 게시한 'EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Entropy Regularization","Policy Optimization","Sparse Rewards","Multi-turn Environments","Exploration-Exploitation"],"permalink":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents","title":"[논문리뷰] D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents","excerpt":"Jinyuan Li이 [arXiv]에 게시한 'D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Mobile GUI Automation","Multi-Agent System","Cognitive Architecture","Pre-execution Alignment","Post-execution Reflection","Retrieval-Augmented Generation","Multimodal LLM","Deliberative AI"],"permalink":"/ai/review/2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training","title":"[논문리뷰] Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training","excerpt":"이 [arXiv]에 게시한 'Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM","Reinforcement Fine-tuning","Reward Modeling","Reward Over-optimization","Rubric-based Rewards","High-reward Tail","Off-policy Data","LLM Alignment"],"permalink":"/ai/review/2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning","title":"[논문리뷰] CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Image Captioning","Reinforcement Learning","Verifiable Rewards","LVLMs","VQA","Data Curation","Caption Quality"],"permalink":"/ai/review/2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition","title":"[논문리뷰] CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition","excerpt":"이 [arXiv]에 게시한 'CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Historical Text Recognition","Vision-Language Model","Open-Weight Model","OCR","Cultural Heritage","Low-Cost AI","Dataset Curation","Fine-tuning"],"permalink":"/ai/review/2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0000-index-of-python-enhancement-proposals-peps","title":"[Active] PEP 0 - Index of Python Enhancement Proposals (PEPs)","excerpt":"Python Enhancement Proposal 0: 'Index of Python Enhancement Proposals (PEPs)'에 대한 한국어 번역입니다.","date":"2025-09-27 21:19:02+0900","lastModifiedAt":"2025-09-27 21:19:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/0/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8106-2025-term-steering-council-election","title":"[Final] PEP 8106 - 2025 Term Steering Council election","excerpt":"Python Enhancement Proposal 8106: '2025 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:35:42+0900","lastModifiedAt":"2025-09-27 19:35:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8106/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8105-2024-term-steering-council-election","title":"[Final] PEP 8105 - 2024 Term Steering Council election","excerpt":"Python Enhancement Proposal 8105: '2024 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:34:28+0900","lastModifiedAt":"2025-09-27 19:34:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8105/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8104-2023-term-steering-council-election","title":"[Final] PEP 8104 - 2023 Term Steering Council election","excerpt":"Python Enhancement Proposal 8104: '2023 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:34:01+0900","lastModifiedAt":"2025-09-27 19:34:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8104/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8103-2022-term-steering-council-election","title":"[Final] PEP 8103 - 2022 Term Steering Council election","excerpt":"Python Enhancement Proposal 8103: '2022 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:31:17+0900","lastModifiedAt":"2025-09-27 19:31:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8103/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8102-2021-term-steering-council-election","title":"[Final] PEP 8102 - 2021 Term Steering Council election","excerpt":"Python Enhancement Proposal 8102: '2021 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:31:05+0900","lastModifiedAt":"2025-09-27 19:31:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8102/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8101-2020-term-steering-council-election","title":"[Final] PEP 8101 - 2020 Term Steering Council election","excerpt":"Python Enhancement Proposal 8101: '2020 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:45+0900","lastModifiedAt":"2025-09-27 19:30:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8101/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8100-january-2019-steering-council-election","title":"[Final] PEP 8100 - January 2019 Steering Council election","excerpt":"Python Enhancement Proposal 8100: 'January 2019 Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:34+0900","lastModifiedAt":"2025-09-27 19:30:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8100/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8016-the-steering-council-model","title":"[Accepted] PEP 8016 - The Steering Council Model","excerpt":"Python Enhancement Proposal 8016: 'The Steering Council Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:11+0900","lastModifiedAt":"2025-09-27 19:30:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8016/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8015-organization-of-the-python-community","title":"[Rejected] PEP 8015 - Organization of the Python community","excerpt":"Python Enhancement Proposal 8015: 'Organization of the Python community'에 대한 한국어 번역입니다.","date":"2025-09-27 19:29:53+0900","lastModifiedAt":"2025-09-27 19:29:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8015/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8014-the-commons-governance-model","title":"[Rejected] PEP 8014 - The Commons Governance Model","excerpt":"Python Enhancement Proposal 8014: 'The Commons Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:29:07+0900","lastModifiedAt":"2025-09-27 19:29:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8014/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8013-the-external-council-governance-model","title":"[Rejected] PEP 8013 - The External Council Governance Model","excerpt":"Python Enhancement Proposal 8013: 'The External Council Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:28:36+0900","lastModifiedAt":"2025-09-27 19:28:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8013/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8012-the-community-governance-model","title":"[Rejected] PEP 8012 - The Community Governance Model","excerpt":"Python Enhancement Proposal 8012: 'The Community Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:28:04+0900","lastModifiedAt":"2025-09-27 19:28:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8012/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8011-python-governance-model-lead-by-trio-of-pythonistas","title":"[Rejected] PEP 8011 - Python Governance Model Lead by Trio of Pythonistas","excerpt":"Python Enhancement Proposal 8011: 'Python Governance Model Lead by Trio of Pythonistas'에 대한 한국어 번역입니다.","date":"2025-09-27 19:27:38+0900","lastModifiedAt":"2025-09-27 19:27:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8011/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8010-the-technical-leader-governance-model","title":"[Rejected] PEP 8010 - The Technical Leader Governance Model","excerpt":"Python Enhancement Proposal 8010: 'The Technical Leader Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:27:12+0900","lastModifiedAt":"2025-09-27 19:27:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8010/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8002-open-source-governance-survey","title":"[Final] PEP 8002 - Open Source Governance Survey","excerpt":"Python Enhancement Proposal 8002: 'Open Source Governance Survey'에 대한 한국어 번역입니다.","date":"2025-09-27 19:26:46+0900","lastModifiedAt":"2025-09-27 19:26:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8002/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8001-python-governance-voting-process","title":"[Final] PEP 8001 - Python Governance Voting Process","excerpt":"Python Enhancement Proposal 8001: 'Python Governance Voting Process'에 대한 한국어 번역입니다.","date":"2025-09-27 19:25:35+0900","lastModifiedAt":"2025-09-27 19:25:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8001/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-8000-python-language-governance-proposal-overview","title":"[Final] PEP 8000 - Python Language Governance Proposal Overview","excerpt":"Python Enhancement Proposal 8000: 'Python Language Governance Proposal Overview'에 대한 한국어 번역입니다.","date":"2025-09-27 19:25:09+0900","lastModifiedAt":"2025-09-27 19:25:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8000/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3333-python-web-server-gateway-interface-v1-0-1","title":"[Final] PEP 3333 - Python Web Server Gateway Interface v1.0.1","excerpt":"Python Enhancement Proposal 3333: 'Python Web Server Gateway Interface v1.0.1'에 대한 한국어 번역입니다.","date":"2025-09-27 19:24:56+0900","lastModifiedAt":"2025-09-27 19:24:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3333/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3156-asynchronous-io-support-rebooted-the-asyncio-module","title":"[Final] PEP 3156 - Asynchronous IO Support Rebooted: the “asyncio” Module","excerpt":"Python Enhancement Proposal 3156: 'Asynchronous IO Support Rebooted: the “asyncio” Module'에 대한 한국어 번역입니다.","date":"2025-09-27 19:21:50+0900","lastModifiedAt":"2025-09-27 19:21:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3156/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3155-qualified-name-for-classes-and-functions","title":"[Final] PEP 3155 - Qualified name for classes and functions","excerpt":"Python Enhancement Proposal 3155: 'Qualified name for classes and functions'에 대한 한국어 번역입니다.","date":"2025-09-27 19:20:50+0900","lastModifiedAt":"2025-09-27 19:20:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3155/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3154-pickle-protocol-version-4","title":"[Final] PEP 3154 - Pickle protocol version 4","excerpt":"Python Enhancement Proposal 3154: 'Pickle protocol version 4'에 대한 한국어 번역입니다.","date":"2025-09-27 19:20:26+0900","lastModifiedAt":"2025-09-27 19:20:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3154/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3153-asynchronous-io-support","title":"[Superseded] PEP 3153 - Asynchronous IO support","excerpt":"Python Enhancement Proposal 3153: 'Asynchronous IO support'에 대한 한국어 번역입니다.","date":"2025-09-27 14:42:18+0900","lastModifiedAt":"2025-09-27 14:42:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3153/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3152-cofunctions","title":"[Rejected] PEP 3152 - Cofunctions","excerpt":"Python Enhancement Proposal 3152: 'Cofunctions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:41:43+0900","lastModifiedAt":"2025-09-27 14:41:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3152/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3151-reworking-the-os-and-io-exception-hierarchy","title":"[Final] PEP 3151 - Reworking the OS and IO exception hierarchy","excerpt":"Python Enhancement Proposal 3151: 'Reworking the OS and IO exception hierarchy'에 대한 한국어 번역입니다.","date":"2025-09-27 14:41:26+0900","lastModifiedAt":"2025-09-27 14:41:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3151/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3150-statement-local-namespaces-aka-given-clause","title":"[Deferred] PEP 3150 - Statement local namespaces (aka “given” clause)","excerpt":"Python Enhancement Proposal 3150: 'Statement local namespaces (aka “given” clause)'에 대한 한국어 번역입니다.","date":"2025-09-27 14:40:31+0900","lastModifiedAt":"2025-09-27 14:40:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3150/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3149-abi-version-tagged-so-files","title":"[Final] PEP 3149 - ABI version tagged .so files","excerpt":"Python Enhancement Proposal 3149: 'ABI version tagged .so files'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:52+0900","lastModifiedAt":"2025-09-27 14:39:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3149/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3148-futures-execute-computations-asynchronously","title":"[Final] PEP 3148 - futures - execute computations asynchronously","excerpt":"Python Enhancement Proposal 3148: 'futures - execute computations asynchronously'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:26+0900","lastModifiedAt":"2025-09-27 14:39:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3148/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3147-pyc-repository-directories","title":"[Final] PEP 3147 - PYC Repository Directories","excerpt":"Python Enhancement Proposal 3147: 'PYC Repository Directories'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:02+0900","lastModifiedAt":"2025-09-27 14:39:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3147/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3146-merging-unladen-swallow-into-cpython","title":"[Withdrawn] PEP 3146 - Merging Unladen Swallow into CPython","excerpt":"Python Enhancement Proposal 3146: 'Merging Unladen Swallow into CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 14:38:28+0900","lastModifiedAt":"2025-09-27 14:38:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3146/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3145-asynchronous-io-for-subprocess-popen","title":"[Withdrawn] PEP 3145 - Asynchronous I/O For subprocess.Popen","excerpt":"Python Enhancement Proposal 3145: 'Asynchronous I/O For subprocess.Popen'에 대한 한국어 번역입니다.","date":"2025-09-27 14:37:24+0900","lastModifiedAt":"2025-09-27 14:37:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3145/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3144-ip-address-manipulation-library-for-the-python-standard-library","title":"[Final] PEP 3144 - IP Address Manipulation Library for the Python Standard Library","excerpt":"Python Enhancement Proposal 3144: 'IP Address Manipulation Library for the Python Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-27 14:37:11+0900","lastModifiedAt":"2025-09-27 14:37:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3144/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3143-standard-daemon-process-library","title":"[Deferred] PEP 3143 - Standard daemon process library","excerpt":"Python Enhancement Proposal 3143: 'Standard daemon process library'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:57+0900","lastModifiedAt":"2025-09-27 14:36:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3143/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3142-add-a-while-clause-to-generator-expressions","title":"[Rejected] PEP 3142 - Add a “while” clause to generator expressions","excerpt":"Python Enhancement Proposal 3142: 'Add a “while” clause to generator expressions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:16+0900","lastModifiedAt":"2025-09-27 14:36:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3142/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3141-a-type-hierarchy-for-numbers","title":"[Final] PEP 3141 - A Type Hierarchy for Numbers","excerpt":"Python Enhancement Proposal 3141: 'A Type Hierarchy for Numbers'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:02+0900","lastModifiedAt":"2025-09-27 14:36:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3141/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3140-strcontainer-should-call-stritem-not-repritem","title":"[Rejected] PEP 3140 - str(container) should call str(item), not repr(item)","excerpt":"Python Enhancement Proposal 3140: 'str(container) should call str(item), not repr(item)'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:43+0900","lastModifiedAt":"2025-09-27 14:35:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3140/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3139-cleaning-out-sys-and-the-interpreter-module","title":"[Rejected] PEP 3139 - Cleaning out sys and the “interpreter” module","excerpt":"Python Enhancement Proposal 3139: 'Cleaning out sys and the “interpreter” module'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:24+0900","lastModifiedAt":"2025-09-27 14:35:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3139/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3138-string-representation-in-python-3000","title":"[Final] PEP 3138 - String representation in Python 3000","excerpt":"Python Enhancement Proposal 3138: 'String representation in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:12+0900","lastModifiedAt":"2025-09-27 14:35:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3138/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3137-immutable-bytes-and-mutable-buffer","title":"[Final] PEP 3137 - Immutable Bytes and Mutable Buffer","excerpt":"Python Enhancement Proposal 3137: 'Immutable Bytes and Mutable Buffer'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:47+0900","lastModifiedAt":"2025-09-27 14:34:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3137/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3136-labeled-break-and-continue","title":"[Rejected] PEP 3136 - Labeled break and continue","excerpt":"Python Enhancement Proposal 3136: 'Labeled break and continue'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:24+0900","lastModifiedAt":"2025-09-27 14:34:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3136/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3135-new-super","title":"[Final] PEP 3135 - New Super","excerpt":"Python Enhancement Proposal 3135: 'New Super'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:00+0900","lastModifiedAt":"2025-09-27 14:34:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3135/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3134-exception-chaining-and-embedded-tracebacks","title":"[Final] PEP 3134 - Exception Chaining and Embedded Tracebacks","excerpt":"Python Enhancement Proposal 3134: 'Exception Chaining and Embedded Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-27 14:33:48+0900","lastModifiedAt":"2025-09-27 14:33:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3134/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3133-introducing-roles","title":"[Rejected] PEP 3133 - Introducing Roles","excerpt":"Python Enhancement Proposal 3133: 'Introducing Roles'에 대한 한국어 번역입니다.","date":"2025-09-27 14:33:16+0900","lastModifiedAt":"2025-09-27 14:33:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3133/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3132-extended-iterable-unpacking","title":"[Final] PEP 3132 - Extended Iterable Unpacking","excerpt":"Python Enhancement Proposal 3132: 'Extended Iterable Unpacking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:32:37+0900","lastModifiedAt":"2025-09-27 14:32:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3132/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3131-supporting-non-ascii-identifiers","title":"[Final] PEP 3131 - Supporting Non-ASCII Identifiers","excerpt":"Python Enhancement Proposal 3131: 'Supporting Non-ASCII Identifiers'에 대한 한국어 번역입니다.","date":"2025-09-27 14:32:23+0900","lastModifiedAt":"2025-09-27 14:32:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3131/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3130-access-to-current-moduleclassfunction","title":"[Rejected] PEP 3130 - Access to Current Module/Class/Function","excerpt":"Python Enhancement Proposal 3130: 'Access to Current Module/Class/Function'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:53+0900","lastModifiedAt":"2025-09-27 14:31:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3130/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3129-class-decorators","title":"[Final] PEP 3129 - Class Decorators","excerpt":"Python Enhancement Proposal 3129: 'Class Decorators'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:32+0900","lastModifiedAt":"2025-09-27 14:31:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3129/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3128-blist-a-faster-list-like-type","title":"[Rejected] PEP 3128 - BList: A Faster List-like Type","excerpt":"Python Enhancement Proposal 3128: 'BList: A Faster List-like Type'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:22+0900","lastModifiedAt":"2025-09-27 14:31:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3128/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3127-integer-literal-support-and-syntax","title":"[Final] PEP 3127 - Integer Literal Support and Syntax","excerpt":"Python Enhancement Proposal 3127: 'Integer Literal Support and Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 14:30:46+0900","lastModifiedAt":"2025-09-27 14:30:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3127/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3126-remove-implicit-string-concatenation","title":"[Rejected] PEP 3126 - Remove Implicit String Concatenation","excerpt":"Python Enhancement Proposal 3126: 'Remove Implicit String Concatenation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:30:11+0900","lastModifiedAt":"2025-09-27 14:30:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3126/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3125-remove-backslash-continuation","title":"[Rejected] PEP 3125 - Remove Backslash Continuation","excerpt":"Python Enhancement Proposal 3125: 'Remove Backslash Continuation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:29:48+0900","lastModifiedAt":"2025-09-27 14:29:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3125/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3124-overloading-generic-functions-interfaces-and-adaptation","title":"[Deferred] PEP 3124 - Overloading, Generic Functions, Interfaces, and Adaptation","excerpt":"Python Enhancement Proposal 3124: 'Overloading, Generic Functions, Interfaces, and Adaptation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:29:34+0900","lastModifiedAt":"2025-09-27 14:29:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3124/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3123-making-pyobject-head-conform-to-standard-c","title":"[Final] PEP 3123 - Making PyObject_HEAD conform to standard C","excerpt":"Python Enhancement Proposal 3123: 'Making PyObject_HEAD conform to standard C'에 대한 한국어 번역입니다.","date":"2025-09-27 14:28:28+0900","lastModifiedAt":"2025-09-27 14:28:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3123/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3122-delineation-of-the-main-module","title":"[Rejected] PEP 3122 - Delineation of the main module","excerpt":"Python Enhancement Proposal 3122: 'Delineation of the main module'에 대한 한국어 번역입니다.","date":"2025-09-27 14:28:16+0900","lastModifiedAt":"2025-09-27 14:28:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3122/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3121-extension-module-initialization-and-finalization","title":"[Final] PEP 3121 - Extension Module Initialization and Finalization","excerpt":"Python Enhancement Proposal 3121: 'Extension Module Initialization and Finalization'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:55+0900","lastModifiedAt":"2025-09-27 14:27:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3121/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3120-using-utf-8-as-the-default-source-encoding","title":"[Final] PEP 3120 - Using UTF-8 as the default source encoding","excerpt":"Python Enhancement Proposal 3120: 'Using UTF-8 as the default source encoding'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:43+0900","lastModifiedAt":"2025-09-27 14:27:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3120/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3119-introducing-abstract-base-classes","title":"[Final] PEP 3119 - Introducing Abstract Base Classes","excerpt":"Python Enhancement Proposal 3119: 'Introducing Abstract Base Classes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:33+0900","lastModifiedAt":"2025-09-27 14:27:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3119/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3118-revising-the-buffer-protocol","title":"[Final] PEP 3118 - Revising the buffer protocol","excerpt":"Python Enhancement Proposal 3118: 'Revising the buffer protocol'에 대한 한국어 번역입니다.","date":"2025-09-27 14:26:38+0900","lastModifiedAt":"2025-09-27 14:26:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3118/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3117-postfix-type-declarations","title":"[Rejected] PEP 3117 - Postfix type declarations","excerpt":"Python Enhancement Proposal 3117: 'Postfix type declarations'에 대한 한국어 번역입니다.","date":"2025-09-27 14:25:21+0900","lastModifiedAt":"2025-09-27 14:25:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3117/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3116-new-io","title":"[Final] PEP 3116 - New I/O","excerpt":"Python Enhancement Proposal 3116: 'New I/O'에 대한 한국어 번역입니다.","date":"2025-09-27 14:24:50+0900","lastModifiedAt":"2025-09-27 14:24:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3116/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3115-metaclasses-in-python-3000","title":"[Final] PEP 3115 - Metaclasses in Python 3000","excerpt":"Python Enhancement Proposal 3115: 'Metaclasses in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:47+0900","lastModifiedAt":"2025-09-27 14:22:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3115/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3114-renaming-iterator-next-to-iterator-next","title":"[Final] PEP 3114 - Renaming iterator.next() to iterator.__next__()","excerpt":"Python Enhancement Proposal 3114: 'Renaming iterator.next() to iterator.__next__()'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:21+0900","lastModifiedAt":"2025-09-27 14:22:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3114/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3113-removal-of-tuple-parameter-unpacking","title":"[Final] PEP 3113 - Removal of Tuple Parameter Unpacking","excerpt":"Python Enhancement Proposal 3113: 'Removal of Tuple Parameter Unpacking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:03+0900","lastModifiedAt":"2025-09-27 14:22:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3113/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3112-bytes-literals-in-python-3000","title":"[Final] PEP 3112 - Bytes literals in Python 3000","excerpt":"Python Enhancement Proposal 3112: 'Bytes literals in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:44+0900","lastModifiedAt":"2025-09-27 14:21:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3112/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3111-simple-input-built-in-in-python-3000","title":"[Final] PEP 3111 - Simple input built-in in Python 3000","excerpt":"Python Enhancement Proposal 3111: 'Simple input built-in in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:32+0900","lastModifiedAt":"2025-09-27 14:21:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3111/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3110-catching-exceptions-in-python-3000","title":"[Final] PEP 3110 - Catching Exceptions in Python 3000","excerpt":"Python Enhancement Proposal 3110: 'Catching Exceptions in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:16+0900","lastModifiedAt":"2025-09-27 14:21:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3110/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3109-raising-exceptions-in-python-3000","title":"[Final] PEP 3109 - Raising Exceptions in Python 3000","excerpt":"Python Enhancement Proposal 3109: 'Raising Exceptions in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:02+0900","lastModifiedAt":"2025-09-27 14:21:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3109/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3108-standard-library-reorganization","title":"[Final] PEP 3108 - Standard Library Reorganization","excerpt":"Python Enhancement Proposal 3108: 'Standard Library Reorganization'에 대한 한국어 번역입니다.","date":"2025-09-27 14:20:43+0900","lastModifiedAt":"2025-09-27 14:20:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3108/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3107-function-annotations","title":"[Final] PEP 3107 - Function Annotations","excerpt":"Python Enhancement Proposal 3107: 'Function Annotations'에 대한 한국어 번역입니다.","date":"2025-09-27 14:19:43+0900","lastModifiedAt":"2025-09-27 14:19:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3107/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3106-revamping-dict-keys-values-and-items","title":"[Final] PEP 3106 - Revamping dict.keys(), .values() and .items()","excerpt":"Python Enhancement Proposal 3106: 'Revamping dict.keys(), .values() and .items()'에 대한 한국어 번역입니다.","date":"2025-09-27 14:18:13+0900","lastModifiedAt":"2025-09-27 14:18:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3106/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3105-make-print-a-function","title":"[Final] PEP 3105 - Make print a function","excerpt":"Python Enhancement Proposal 3105: 'Make print a function'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:54+0900","lastModifiedAt":"2025-09-27 14:17:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3105/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3104-access-to-names-in-outer-scopes","title":"[Final] PEP 3104 - Access to Names in Outer Scopes","excerpt":"Python Enhancement Proposal 3104: 'Access to Names in Outer Scopes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:40+0900","lastModifiedAt":"2025-09-27 14:17:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3104/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3103-a-switchcase-statement","title":"[Rejected] PEP 3103 - A Switch/Case Statement","excerpt":"Python Enhancement Proposal 3103: 'A Switch/Case Statement'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:03+0900","lastModifiedAt":"2025-09-27 14:17:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3103/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3102-keyword-only-arguments","title":"[Final] PEP 3102 - Keyword-Only Arguments","excerpt":"Python Enhancement Proposal 3102: 'Keyword-Only Arguments'에 대한 한국어 번역입니다.","date":"2025-09-27 14:16:28+0900","lastModifiedAt":"2025-09-27 14:16:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3102/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3101-advanced-string-formatting","title":"[Final] PEP 3101 - Advanced String Formatting","excerpt":"Python Enhancement Proposal 3101: 'Advanced String Formatting'에 대한 한국어 번역입니다.","date":"2025-09-27 14:16:14+0900","lastModifiedAt":"2025-09-27 14:16:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3101/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3100-miscellaneous-python-3-0-plans","title":"[Final] PEP 3100 - Miscellaneous Python 3.0 Plans","excerpt":"Python Enhancement Proposal 3100: 'Miscellaneous Python 3.0 Plans'에 대한 한국어 번역입니다.","date":"2025-09-27 14:13:27+0900","lastModifiedAt":"2025-09-27 14:13:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3100/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3099-things-that-will-not-change-in-python-3000","title":"[Final] PEP 3099 - Things that will Not Change in Python 3000","excerpt":"Python Enhancement Proposal 3099: 'Things that will Not Change in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:36+0900","lastModifiedAt":"2025-09-27 14:12:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3099/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3003-python-language-moratorium","title":"[Final] PEP 3003 - Python Language Moratorium","excerpt":"Python Enhancement Proposal 3003: 'Python Language Moratorium'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:18+0900","lastModifiedAt":"2025-09-27 14:12:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3003/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3002-procedure-for-backwards-incompatible-changes","title":"[Final] PEP 3002 - Procedure for Backwards-Incompatible Changes","excerpt":"Python Enhancement Proposal 3002: 'Procedure for Backwards-Incompatible Changes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:06+0900","lastModifiedAt":"2025-09-27 14:12:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3002/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3001-procedure-for-reviewing-and-improving-standard-library-modules","title":"[Withdrawn] PEP 3001 - Procedure for reviewing and improving standard library modules","excerpt":"Python Enhancement Proposal 3001: 'Procedure for reviewing and improving standard library modules'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:53+0900","lastModifiedAt":"2025-09-27 14:11:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3001/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-3000-python-3000","title":"[Final] PEP 3000 - Python 3000","excerpt":"Python Enhancement Proposal 3000: 'Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:40+0900","lastModifiedAt":"2025-09-27 14:11:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3000/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-2026-calendar-versioning-for-python","title":"[Rejected] PEP 2026 - Calendar versioning for Python","excerpt":"Python Enhancement Proposal 2026: 'Calendar versioning for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:21+0900","lastModifiedAt":"2025-09-27 14:11:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/2026/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0806-mixed-syncasync-context-managers-with-precise-async-marking","title":"[Draft] PEP 806 - Mixed sync/async context managers with precise async marking","excerpt":"Python Enhancement Proposal 806: 'Mixed sync/async context managers with precise async marking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:10:48+0900","lastModifiedAt":"2025-09-27 14:10:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/806/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0804-an-external-dependency-registry-and-name-mapping-mechanism","title":"[Draft] PEP 804 - An external dependency registry and name mapping mechanism","excerpt":"Python Enhancement Proposal 804: 'An external dependency registry and name mapping mechanism'에 대한 한국어 번역입니다.","date":"2025-09-27 14:10:24+0900","lastModifiedAt":"2025-09-27 14:10:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/804/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0803-stable-abi-for-free-threaded-builds","title":"[Draft] PEP 803 - Stable ABI for Free-Threaded Builds","excerpt":"Python Enhancement Proposal 803: 'Stable ABI for Free-Threaded Builds'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:56+0900","lastModifiedAt":"2025-09-27 14:09:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/803/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0802-display-syntax-for-the-empty-set","title":"[Draft] PEP 802 - Display Syntax for the Empty Set","excerpt":"Python Enhancement Proposal 802: 'Display Syntax for the Empty Set'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:30+0900","lastModifiedAt":"2025-09-27 14:09:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/802/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0801-reserved","title":"[Active] PEP 801 - Reserved","excerpt":"Python Enhancement Proposal 801: 'Reserved'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:12+0900","lastModifiedAt":"2025-09-27 14:09:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/801/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0800-disjoint-bases-in-the-type-system","title":"[Draft] PEP 800 - Disjoint bases in the type system","excerpt":"Python Enhancement Proposal 800: 'Disjoint bases in the type system'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:06+0900","lastModifiedAt":"2025-09-27 14:09:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/800/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0799-a-dedicatedprofilingpackage-for-organizing-python-profiling-tools","title":"[Draft] PEP 799 - A dedicatedprofilingpackage for organizing Python profiling tools","excerpt":"Python Enhancement Proposal 799: 'A dedicatedprofilingpackage for organizing Python profiling tools'에 대한 한국어 번역입니다.","date":"2025-09-27 14:08:50+0900","lastModifiedAt":"2025-09-27 14:08:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/799/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0798-unpacking-in-comprehensions","title":"[Draft] PEP 798 - Unpacking in Comprehensions","excerpt":"Python Enhancement Proposal 798: 'Unpacking in Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:08:32+0900","lastModifiedAt":"2025-09-27 14:08:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/798/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0794-import-name-metadata","title":"[Accepted] PEP 794 - Import Name Metadata","excerpt":"Python Enhancement Proposal 794: 'Import Name Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 14:07:32+0900","lastModifiedAt":"2025-09-27 14:07:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/794/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0793-pymodexport-a-new-entry-point-for-c-extension-modules","title":"[Draft] PEP 793 - PyModExport: A new entry point for C extension modules","excerpt":"Python Enhancement Proposal 793: 'PyModExport: A new entry point for C extension modules'에 대한 한국어 번역입니다.","date":"2025-09-27 14:07:12+0900","lastModifiedAt":"2025-09-27 14:07:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/793/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0792-project-status-markers-in-the-simple-index","title":"[Final] PEP 792 - Project status markers in the simple index","excerpt":"Python Enhancement Proposal 792: 'Project status markers in the simple index'에 대한 한국어 번역입니다.","date":"2025-09-27 14:05:09+0900","lastModifiedAt":"2025-09-27 14:05:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/792/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0791-intmath-module-for-integer-specific-mathematics-functions","title":"[Draft] PEP 791 - intmath — module for integer-specific mathematics functions","excerpt":"Python Enhancement Proposal 791: 'intmath — module for integer-specific mathematics functions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:51+0900","lastModifiedAt":"2025-09-27 14:04:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/791/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0790-python-3-15-release-schedule","title":"[Active] PEP 790 - Python 3.15 Release Schedule","excerpt":"Python Enhancement Proposal 790: 'Python 3.15 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:34+0900","lastModifiedAt":"2025-09-27 14:04:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/790/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0789-preventing-task-cancellation-bugs-by-limiting-yield-in-async-generators","title":"[Draft] PEP 789 - Preventing task-cancellation bugs by limiting yield in async generators","excerpt":"Python Enhancement Proposal 789: 'Preventing task-cancellation bugs by limiting yield in async generators'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:23+0900","lastModifiedAt":"2025-09-27 14:04:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/789/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0788-pyinterpreterref-interpreter-references-in-the-c-api","title":"[Draft] PEP 788 - PyInterpreterRef: Interpreter References in the C API","excerpt":"Python Enhancement Proposal 788: 'PyInterpreterRef: Interpreter References in the C API'에 대한 한국어 번역입니다.","date":"2025-09-27 14:03:48+0900","lastModifiedAt":"2025-09-27 14:03:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/788/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0787-safer-subprocess-usage-using-t-strings","title":"[Deferred] PEP 787 - Safer subprocess usage using t-strings","excerpt":"Python Enhancement Proposal 787: 'Safer subprocess usage using t-strings'에 대한 한국어 번역입니다.","date":"2025-09-27 14:00:15+0900","lastModifiedAt":"2025-09-27 14:00:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/787/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0785-new-methods-for-easier-handling-ofexceptiongroups","title":"[Draft] PEP 785 - New methods for easier handling ofExceptionGroups","excerpt":"Python Enhancement Proposal 785: 'New methods for easier handling ofExceptionGroups'에 대한 한국어 번역입니다.","date":"2025-09-27 13:59:04+0900","lastModifiedAt":"2025-09-27 13:59:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/785/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0784-adding-zstandard-to-the-standard-library","title":"[Final] PEP 784 - Adding Zstandard to the standard library","excerpt":"Python Enhancement Proposal 784: 'Adding Zstandard to the standard library'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:47+0900","lastModifiedAt":"2025-09-27 13:58:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/784/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0783-emscripten-packaging","title":"[Draft] PEP 783 - Emscripten Packaging","excerpt":"Python Enhancement Proposal 783: 'Emscripten Packaging'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:16+0900","lastModifiedAt":"2025-09-27 13:58:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/783/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0782-add-pybyteswriter-c-api","title":"[Final] PEP 782 - Add PyBytesWriter C API","excerpt":"Python Enhancement Proposal 782: 'Add PyBytesWriter C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:01+0900","lastModifiedAt":"2025-09-27 13:58:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/782/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0781-maketype-checkinga-built-in-constant","title":"[Draft] PEP 781 - MakeTYPE_CHECKINGa built-in constant","excerpt":"Python Enhancement Proposal 781: 'MakeTYPE_CHECKINGa built-in constant'에 대한 한국어 번역입니다.","date":"2025-09-27 13:57:36+0900","lastModifiedAt":"2025-09-27 13:57:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/781/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0780-abi-features-as-environment-markers","title":"[Draft] PEP 780 - ABI features as environment markers","excerpt":"Python Enhancement Proposal 780: 'ABI features as environment markers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:57:13+0900","lastModifiedAt":"2025-09-27 13:57:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/780/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0779-criteria-for-supported-status-for-free-threaded-python","title":"[Accepted] PEP 779 - Criteria for supported status for free-threaded Python","excerpt":"Python Enhancement Proposal 779: 'Criteria for supported status for free-threaded Python'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:46+0900","lastModifiedAt":"2025-09-27 13:56:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/779/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0778-supporting-symlinks-in-wheels","title":"[Deferred] PEP 778 - Supporting Symlinks in Wheels","excerpt":"Python Enhancement Proposal 778: 'Supporting Symlinks in Wheels'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:34+0900","lastModifiedAt":"2025-09-27 13:56:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/778/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0777-how-to-re-invent-the-wheel","title":"[Draft] PEP 777 - How to Re-invent the Wheel","excerpt":"Python Enhancement Proposal 777: 'How to Re-invent the Wheel'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:09+0900","lastModifiedAt":"2025-09-27 13:56:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/777/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0776-emscripten-support","title":"[Draft] PEP 776 - Emscripten Support","excerpt":"Python Enhancement Proposal 776: 'Emscripten Support'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:47+0900","lastModifiedAt":"2025-09-27 13:55:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/776/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0775-make-zlib-required-to-build-cpython","title":"[Withdrawn] PEP 775 - Make zlib required to build CPython","excerpt":"Python Enhancement Proposal 775: 'Make zlib required to build CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:17+0900","lastModifiedAt":"2025-09-27 13:55:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/775/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0774-removing-the-llvm-requirement-for-jit-builds","title":"[Deferred] PEP 774 - Removing the LLVM requirement for JIT builds","excerpt":"Python Enhancement Proposal 774: 'Removing the LLVM requirement for JIT builds'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:06+0900","lastModifiedAt":"2025-09-27 13:55:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/774/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0773-a-python-installation-manager-for-windows","title":"[Accepted] PEP 773 - A Python Installation Manager for Windows","excerpt":"Python Enhancement Proposal 773: 'A Python Installation Manager for Windows'에 대한 한국어 번역입니다.","date":"2025-09-27 13:54:28+0900","lastModifiedAt":"2025-09-27 13:54:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/773/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0772-packaging-council-governance-process","title":"[Draft] PEP 772 - Packaging Council governance process","excerpt":"Python Enhancement Proposal 772: 'Packaging Council governance process'에 대한 한국어 번역입니다.","date":"2025-09-27 13:53:40+0900","lastModifiedAt":"2025-09-27 13:53:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/772/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0771-default-extras-for-python-software-packages","title":"[Draft] PEP 771 - Default Extras for Python Software Packages","excerpt":"Python Enhancement Proposal 771: 'Default Extras for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:52:58+0900","lastModifiedAt":"2025-09-27 13:52:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/771/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0770-improving-measurability-of-python-packages-with-software-bill-of-materials","title":"[Accepted] PEP 770 - Improving measurability of Python packages with Software Bill-of-Materials","excerpt":"Python Enhancement Proposal 770: 'Improving measurability of Python packages with Software Bill-of-Materials'에 대한 한국어 번역입니다.","date":"2025-09-27 13:51:21+0900","lastModifiedAt":"2025-09-27 13:51:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/770/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0769-add-a-default-keyword-argument-to-attrgetter-itemgetter-and-getitem","title":"[Rejected] PEP 769 - Add a ‘default’ keyword argument to ‘attrgetter’, ‘itemgetter’ and ‘getitem’","excerpt":"Python Enhancement Proposal 769: 'Add a ‘default’ keyword argument to ‘attrgetter’, ‘itemgetter’ and ‘getitem’'에 대한 한국어 번역입니다.","date":"2025-09-27 13:50:33+0900","lastModifiedAt":"2025-09-27 13:50:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/769/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0768-safe-external-debugger-interface-for-cpython","title":"[Accepted] PEP 768 - Safe external debugger interface for CPython","excerpt":"Python Enhancement Proposal 768: 'Safe external debugger interface for CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:50:05+0900","lastModifiedAt":"2025-09-27 13:50:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/768/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0767-annotating-read-only-attributes","title":"[Draft] PEP 767 - Annotating Read-Only Attributes","excerpt":"Python Enhancement Proposal 767: 'Annotating Read-Only Attributes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:49:33+0900","lastModifiedAt":"2025-09-27 13:49:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/767/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0766-explicit-priority-choices-among-multiple-indexes","title":"[Draft] PEP 766 - Explicit Priority Choices Among Multiple Indexes","excerpt":"Python Enhancement Proposal 766: 'Explicit Priority Choices Among Multiple Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:46:56+0900","lastModifiedAt":"2025-09-27 13:46:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/766/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0765-disallow-returnbreakcontinue-that-exit-a-finally-block","title":"[Final] PEP 765 - Disallow return/break/continue that exit a finally block","excerpt":"Python Enhancement Proposal 765: 'Disallow return/break/continue that exit a finally block'에 대한 한국어 번역입니다.","date":"2025-09-27 13:46:10+0900","lastModifiedAt":"2025-09-27 13:46:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/765/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0764-inline-typed-dictionaries","title":"[Draft] PEP 764 - Inline typed dictionaries","excerpt":"Python Enhancement Proposal 764: 'Inline typed dictionaries'에 대한 한국어 번역입니다.","date":"2025-09-27 13:45:41+0900","lastModifiedAt":"2025-09-27 13:45:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/764/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0763-limiting-deletions-on-pypi","title":"[Draft] PEP 763 - Limiting deletions on PyPI","excerpt":"Python Enhancement Proposal 763: 'Limiting deletions on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:45:17+0900","lastModifiedAt":"2025-09-27 13:45:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/763/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0762-repl-acing-the-default-repl","title":"[Final] PEP 762 - REPL-acing the default REPL","excerpt":"Python Enhancement Proposal 762: 'REPL-acing the default REPL'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:46+0900","lastModifiedAt":"2025-09-27 13:44:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/762/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0761-deprecating-pgp-signatures-for-cpython-artifacts","title":"[Active] PEP 761 - Deprecating PGP signatures for CPython artifacts","excerpt":"Python Enhancement Proposal 761: 'Deprecating PGP signatures for CPython artifacts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:29+0900","lastModifiedAt":"2025-09-27 13:44:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/761/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0760-no-more-bare-excepts","title":"[Withdrawn] PEP 760 - No More Bare Excepts","excerpt":"Python Enhancement Proposal 760: 'No More Bare Excepts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:07+0900","lastModifiedAt":"2025-09-27 13:44:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/760/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0759-external-wheel-hosting","title":"[Withdrawn] PEP 759 - External Wheel Hosting","excerpt":"Python Enhancement Proposal 759: 'External Wheel Hosting'에 대한 한국어 번역입니다.","date":"2025-09-27 13:43:53+0900","lastModifiedAt":"2025-09-27 13:43:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/759/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0758-allowexceptandexceptexpressions-without-parentheses","title":"[Final] PEP 758 - Allowexceptandexcept*expressions without parentheses","excerpt":"Python Enhancement Proposal 758: 'Allowexceptandexcept*expressions without parentheses'에 대한 한국어 번역입니다.","date":"2025-09-27 13:43:13+0900","lastModifiedAt":"2025-09-27 13:43:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/758/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0757-c-api-to-import-export-python-integers","title":"[Final] PEP 757 - C API to import-export Python integers","excerpt":"Python Enhancement Proposal 757: 'C API to import-export Python integers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:42:57+0900","lastModifiedAt":"2025-09-27 13:42:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/757/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0756-add-pyunicode-export-and-pyunicode-import-c-functions","title":"[Withdrawn] PEP 756 - Add PyUnicode_Export() and PyUnicode_Import() C functions","excerpt":"Python Enhancement Proposal 756: 'Add PyUnicode_Export() and PyUnicode_Import() C functions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:47+0900","lastModifiedAt":"2025-09-27 13:41:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/756/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0755-implicit-namespace-policy-for-pypi","title":"[Draft] PEP 755 - Implicit namespace policy for PyPI","excerpt":"Python Enhancement Proposal 755: 'Implicit namespace policy for PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:27+0900","lastModifiedAt":"2025-09-27 13:41:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/755/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0754-ieee-754-floating-point-special-values","title":"[Rejected] PEP 754 - IEEE 754 Floating Point Special Values","excerpt":"Python Enhancement Proposal 754: 'IEEE 754 Floating Point Special Values'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:12+0900","lastModifiedAt":"2025-09-27 13:41:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/754/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0753-uniform-project-urls-in-core-metadata","title":"[Accepted] PEP 753 - Uniform project URLs in core metadata","excerpt":"Python Enhancement Proposal 753: 'Uniform project URLs in core metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:57+0900","lastModifiedAt":"2025-09-27 13:40:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/753/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0752-implicit-namespaces-for-package-repositories","title":"[Draft] PEP 752 - Implicit namespaces for package repositories","excerpt":"Python Enhancement Proposal 752: 'Implicit namespaces for package repositories'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:26+0900","lastModifiedAt":"2025-09-27 13:40:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/752/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0751-a-file-format-to-record-python-dependencies-for-installation-reproducibility","title":"[Final] PEP 751 - A file format to record Python dependencies for installation reproducibility","excerpt":"Python Enhancement Proposal 751: 'A file format to record Python dependencies for installation reproducibility'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:01+0900","lastModifiedAt":"2025-09-27 13:40:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/751/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0750-template-strings","title":"[Final] PEP 750 - Template Strings","excerpt":"Python Enhancement Proposal 750: 'Template Strings'에 대한 한국어 번역입니다.","date":"2025-09-27 13:38:50+0900","lastModifiedAt":"2025-09-27 13:38:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/750/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0749-implementing-pep-649","title":"[Accepted] PEP 749 - Implementing PEP 649","excerpt":"Python Enhancement Proposal 749: 'Implementing PEP 649'에 대한 한국어 번역입니다.","date":"2025-09-27 13:37:09+0900","lastModifiedAt":"2025-09-27 13:37:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/749/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0748-a-unified-tls-api-for-python","title":"[Draft] PEP 748 - A Unified TLS API for Python","excerpt":"Python Enhancement Proposal 748: 'A Unified TLS API for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 13:35:46+0900","lastModifiedAt":"2025-09-27 13:35:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/748/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0747-annotating-type-forms","title":"[Draft] PEP 747 - Annotating Type Forms","excerpt":"Python Enhancement Proposal 747: 'Annotating Type Forms'에 대한 한국어 번역입니다.","date":"2025-09-27 13:35:04+0900","lastModifiedAt":"2025-09-27 13:35:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/747/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0746-type-checking-annotated-metadata","title":"[Draft] PEP 746 - Type checking Annotated metadata","excerpt":"Python Enhancement Proposal 746: 'Type checking Annotated metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:43+0900","lastModifiedAt":"2025-09-27 13:34:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/746/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0745-python-3-14-release-schedule","title":"[Active] PEP 745 - Python 3.14 Release Schedule","excerpt":"Python Enhancement Proposal 745: 'Python 3.14 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:29+0900","lastModifiedAt":"2025-09-27 13:34:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/745/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0744-jit-compilation","title":"[Draft] PEP 744 - JIT Compilation","excerpt":"Python Enhancement Proposal 744: 'JIT Compilation'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:20+0900","lastModifiedAt":"2025-09-27 13:34:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/744/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0743-add-py-compat-api-version-to-the-python-c-api","title":"[Draft] PEP 743 - Add Py_COMPAT_API_VERSION to the Python C API","excerpt":"Python Enhancement Proposal 743: 'Add Py_COMPAT_API_VERSION to the Python C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:33:51+0900","lastModifiedAt":"2025-09-27 13:33:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/743/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0742-narrowing-types-with-typeis","title":"[Final] PEP 742 - Narrowing types with TypeIs","excerpt":"Python Enhancement Proposal 742: 'Narrowing types with TypeIs'에 대한 한국어 번역입니다.","date":"2025-09-27 13:33:17+0900","lastModifiedAt":"2025-09-27 13:33:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/742/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0741-python-configuration-c-api","title":"[Final] PEP 741 - Python Configuration C API","excerpt":"Python Enhancement Proposal 741: 'Python Configuration C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:32:38+0900","lastModifiedAt":"2025-09-27 13:32:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/741/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0740-index-support-for-digital-attestations","title":"[Final] PEP 740 - Index support for digital attestations","excerpt":"Python Enhancement Proposal 740: 'Index support for digital attestations'에 대한 한국어 번역입니다.","date":"2025-09-27 13:31:46+0900","lastModifiedAt":"2025-09-27 13:31:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/740/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0739-build-details-json1-0-a-static-description-file-for-python-build-details","title":"[Accepted] PEP 739 - build-details.json1.0 — a static description file for Python build details","excerpt":"Python Enhancement Proposal 739: 'build-details.json1.0 — a static description file for Python build details'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:50+0900","lastModifiedAt":"2025-09-27 13:28:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/739/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0738-adding-android-as-a-supported-platform","title":"[Final] PEP 738 - Adding Android as a supported platform","excerpt":"Python Enhancement Proposal 738: 'Adding Android as a supported platform'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:23+0900","lastModifiedAt":"2025-09-27 13:28:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/738/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0737-c-api-to-format-a-type-fully-qualified-name","title":"[Final] PEP 737 - C API to format a type fully qualified name","excerpt":"Python Enhancement Proposal 737: 'C API to format a type fully qualified name'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:04+0900","lastModifiedAt":"2025-09-27 13:28:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/737/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0736-shorthand-syntax-for-keyword-arguments-at-invocation","title":"[Rejected] PEP 736 - Shorthand syntax for keyword arguments at invocation","excerpt":"Python Enhancement Proposal 736: 'Shorthand syntax for keyword arguments at invocation'에 대한 한국어 번역입니다.","date":"2025-09-27 13:27:17+0900","lastModifiedAt":"2025-09-27 13:27:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/736/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0735-dependency-groups-in-pyproject-toml","title":"[Final] PEP 735 - Dependency Groups in pyproject.toml","excerpt":"Python Enhancement Proposal 735: 'Dependency Groups in pyproject.toml'에 대한 한국어 번역입니다.","date":"2025-09-27 13:26:47+0900","lastModifiedAt":"2025-09-27 13:26:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/735/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0734-multiple-interpreters-in-the-stdlib","title":"[Final] PEP 734 - Multiple Interpreters in the Stdlib","excerpt":"Python Enhancement Proposal 734: 'Multiple Interpreters in the Stdlib'에 대한 한국어 번역입니다.","date":"2025-09-27 13:25:59+0900","lastModifiedAt":"2025-09-27 13:25:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/734/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0733-an-evaluation-of-pythons-public-c-api","title":"[Final] PEP 733 - An Evaluation of Python’s Public C API","excerpt":"Python Enhancement Proposal 733: 'An Evaluation of Python’s Public C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:23:51+0900","lastModifiedAt":"2025-09-27 13:23:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/733/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0732-the-python-documentation-editorial-board","title":"[Active] PEP 732 - The Python Documentation Editorial Board","excerpt":"Python Enhancement Proposal 732: 'The Python Documentation Editorial Board'에 대한 한국어 번역입니다.","date":"2025-09-27 13:23:00+0900","lastModifiedAt":"2025-09-27 13:23:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/732/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0731-c-api-working-group-charter","title":"[Active] PEP 731 - C API Working Group Charter","excerpt":"Python Enhancement Proposal 731: 'C API Working Group Charter'에 대한 한국어 번역입니다.","date":"2025-09-27 13:22:44+0900","lastModifiedAt":"2025-09-27 13:22:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/731/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0730-adding-ios-as-a-supported-platform","title":"[Final] PEP 730 - Adding iOS as a supported platform","excerpt":"Python Enhancement Proposal 730: 'Adding iOS as a supported platform'에 대한 한국어 번역입니다.","date":"2025-09-27 13:22:30+0900","lastModifiedAt":"2025-09-27 13:22:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/730/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0729-typing-governance-process","title":"[Active] PEP 729 - Typing governance process","excerpt":"Python Enhancement Proposal 729: 'Typing governance process'에 대한 한국어 번역입니다.","date":"2025-09-27 13:21:49+0900","lastModifiedAt":"2025-09-27 13:21:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/729/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0728-typeddict-with-typed-extra-items","title":"[Accepted] PEP 728 - TypedDict with Typed Extra Items","excerpt":"Python Enhancement Proposal 728: 'TypedDict with Typed Extra Items'에 대한 한국어 번역입니다.","date":"2025-09-27 13:20:42+0900","lastModifiedAt":"2025-09-27 13:20:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/728/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0727-documentation-in-annotated-metadata","title":"[Withdrawn] PEP 727 - Documentation in Annotated Metadata","excerpt":"Python Enhancement Proposal 727: 'Documentation in Annotated Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:19:38+0900","lastModifiedAt":"2025-09-27 13:19:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/727/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0726-module-setattr-and-delattr","title":"[Rejected] PEP 726 - Module__setattr__and__delattr__","excerpt":"Python Enhancement Proposal 726: 'Module__setattr__and__delattr__'에 대한 한국어 번역입니다.","date":"2025-09-27 13:19:13+0900","lastModifiedAt":"2025-09-27 13:19:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/726/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0725-specifying-external-dependencies-in-pyproject-toml","title":"[Draft] PEP 725 - Specifying external dependencies in pyproject.toml","excerpt":"Python Enhancement Proposal 725: 'Specifying external dependencies in pyproject.toml'에 대한 한국어 번역입니다.","date":"2025-09-27 13:18:53+0900","lastModifiedAt":"2025-09-27 13:18:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/725/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0724-stricter-type-guards","title":"[Withdrawn] PEP 724 - Stricter Type Guards","excerpt":"Python Enhancement Proposal 724: 'Stricter Type Guards'에 대한 한국어 번역입니다.","date":"2025-09-27 13:17:29+0900","lastModifiedAt":"2025-09-27 13:17:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/724/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0723-inline-script-metadata","title":"[Final] PEP 723 - Inline script metadata","excerpt":"Python Enhancement Proposal 723: 'Inline script metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:17:06+0900","lastModifiedAt":"2025-09-27 13:17:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/723/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0722-dependency-specification-for-single-file-scripts","title":"[Rejected] PEP 722 - Dependency specification for single-file scripts","excerpt":"Python Enhancement Proposal 722: 'Dependency specification for single-file scripts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:16:11+0900","lastModifiedAt":"2025-09-27 13:16:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/722/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0721-using-tarfile-data-filter-for-source-distribution-extraction","title":"[Final] PEP 721 - Using tarfile.data_filter for source distribution extraction","excerpt":"Python Enhancement Proposal 721: 'Using tarfile.data_filter for source distribution extraction'에 대한 한국어 번역입니다.","date":"2025-09-27 13:15:15+0900","lastModifiedAt":"2025-09-27 13:15:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/721/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0720-cross-compiling-python-packages","title":"[Draft] PEP 720 - Cross-compiling Python packages","excerpt":"Python Enhancement Proposal 720: 'Cross-compiling Python packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:14:54+0900","lastModifiedAt":"2025-09-27 13:14:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/720/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0719-python-3-13-release-schedule","title":"[Active] PEP 719 - Python 3.13 Release Schedule","excerpt":"Python Enhancement Proposal 719: 'Python 3.13 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 13:14:04+0900","lastModifiedAt":"2025-09-27 13:14:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/719/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0718-subscriptable-functions","title":"[Draft] PEP 718 - Subscriptable functions","excerpt":"Python Enhancement Proposal 718: 'Subscriptable functions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:52+0900","lastModifiedAt":"2025-09-27 13:13:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/718/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0715-disabling-bdist-egg-distribution-uploads-on-pypi","title":"[Final] PEP 715 - Disabling bdist_egg distribution uploads on PyPI","excerpt":"Python Enhancement Proposal 715: 'Disabling bdist_egg distribution uploads on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:33+0900","lastModifiedAt":"2025-09-27 13:13:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/715/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0714-rename-dist-info-metadata-in-the-simple-api","title":"[Accepted] PEP 714 - Rename dist-info-metadata in the Simple API","excerpt":"Python Enhancement Proposal 714: 'Rename dist-info-metadata in the Simple API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:16+0900","lastModifiedAt":"2025-09-27 13:13:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/714/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0713-callable-modules","title":"[Rejected] PEP 713 - Callable Modules","excerpt":"Python Enhancement Proposal 713: 'Callable Modules'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:51+0900","lastModifiedAt":"2025-09-27 13:12:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/713/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0712-adding-a-converter-parameter-to-dataclasses-field","title":"[Rejected] PEP 712 - Adding a “converter” parameter to dataclasses.field","excerpt":"Python Enhancement Proposal 712: 'Adding a “converter” parameter to dataclasses.field'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:37+0900","lastModifiedAt":"2025-09-27 13:12:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/712/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0711-pybi-a-standard-format-for-distributing-python-binaries","title":"[Draft] PEP 711 - PyBI: a standard format for distributing Python Binaries","excerpt":"Python Enhancement Proposal 711: 'PyBI: a standard format for distributing Python Binaries'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:07+0900","lastModifiedAt":"2025-09-27 13:12:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/711/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0710-recording-the-provenance-of-installed-packages","title":"[Draft] PEP 710 - Recording the provenance of installed packages","excerpt":"Python Enhancement Proposal 710: 'Recording the provenance of installed packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:11:44+0900","lastModifiedAt":"2025-09-27 13:11:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/710/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0709-inlined-comprehensions","title":"[Final] PEP 709 - Inlined comprehensions","excerpt":"Python Enhancement Proposal 709: 'Inlined comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:10:56+0900","lastModifiedAt":"2025-09-27 13:10:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/709/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0708-extending-the-repository-api-to-mitigate-dependency-confusion-attacks","title":"[Provisional] PEP 708 - Extending the Repository API to Mitigate Dependency Confusion Attacks","excerpt":"Python Enhancement Proposal 708: 'Extending the Repository API to Mitigate Dependency Confusion Attacks'에 대한 한국어 번역입니다.","date":"2025-09-27 13:10:38+0900","lastModifiedAt":"2025-09-27 13:10:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/708/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0707-a-simplified-signature-for-exit-and-aexit","title":"[Rejected] PEP 707 - A simplified signature for __exit__ and __aexit__","excerpt":"Python Enhancement Proposal 707: 'A simplified signature for __exit__ and __aexit__'에 대한 한국어 번역입니다.","date":"2025-09-27 13:09:56+0900","lastModifiedAt":"2025-09-27 13:09:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/707/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0706-filter-for-tarfile-extractall","title":"[Final] PEP 706 - Filter for tarfile.extractall","excerpt":"Python Enhancement Proposal 706: 'Filter for tarfile.extractall'에 대한 한국어 번역입니다.","date":"2025-09-27 13:09:29+0900","lastModifiedAt":"2025-09-27 13:09:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/706/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0705-typeddict-read-only-items","title":"[Final] PEP 705 - TypedDict: Read-only items","excerpt":"Python Enhancement Proposal 705: 'TypedDict: Read-only items'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:52+0900","lastModifiedAt":"2025-09-27 13:08:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/705/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0704-require-virtual-environments-by-default-for-package-installers","title":"[Withdrawn] PEP 704 - Require virtual environments by default for package installers","excerpt":"Python Enhancement Proposal 704: 'Require virtual environments by default for package installers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:22+0900","lastModifiedAt":"2025-09-27 13:08:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/704/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0703-making-the-global-interpreter-lock-optional-in-cpython","title":"[Accepted] PEP 703 - Making the Global Interpreter Lock Optional in CPython","excerpt":"Python Enhancement Proposal 703: 'Making the Global Interpreter Lock Optional in CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:02+0900","lastModifiedAt":"2025-09-27 13:08:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/703/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0702-marking-deprecations-using-the-type-system","title":"[Final] PEP 702 - Marking deprecations using the type system","excerpt":"Python Enhancement Proposal 702: 'Marking deprecations using the type system'에 대한 한국어 번역입니다.","date":"2025-09-27 13:07:08+0900","lastModifiedAt":"2025-09-27 13:07:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/702/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0701-syntactic-formalization-of-f-strings","title":"[Accepted] PEP 701 - Syntactic formalization of f-strings","excerpt":"Python Enhancement Proposal 701: 'Syntactic formalization of f-strings'에 대한 한국어 번역입니다.","date":"2025-09-27 13:06:50+0900","lastModifiedAt":"2025-09-27 13:06:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/701/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0700-additional-fields-for-the-simple-api-for-package-indexes","title":"[Final] PEP 700 - Additional Fields for the Simple API for Package Indexes","excerpt":"Python Enhancement Proposal 700: 'Additional Fields for the Simple API for Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:58+0900","lastModifiedAt":"2025-09-27 13:05:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/700/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0699-remove-private-dict-version-field-added-in-pep-509","title":"[Accepted] PEP 699 - Remove private dict version field added in PEP 509","excerpt":"Python Enhancement Proposal 699: 'Remove private dict version field added in PEP 509'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:33+0900","lastModifiedAt":"2025-09-27 13:05:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/699/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0698-override-decorator-for-static-typing","title":"[Final] PEP 698 - Override Decorator for Static Typing","excerpt":"Python Enhancement Proposal 698: 'Override Decorator for Static Typing'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:19+0900","lastModifiedAt":"2025-09-27 13:05:19+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/698/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0697-limited-c-api-for-extending-opaque-types","title":"[Final] PEP 697 - Limited C API for Extending Opaque Types","excerpt":"Python Enhancement Proposal 697: 'Limited C API for Extending Opaque Types'에 대한 한국어 번역입니다.","date":"2025-09-27 13:04:45+0900","lastModifiedAt":"2025-09-27 13:04:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/697/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0696-type-defaults-for-type-parameters","title":"[Final] PEP 696 - Type Defaults for Type Parameters","excerpt":"Python Enhancement Proposal 696: 'Type Defaults for Type Parameters'에 대한 한국어 번역입니다.","date":"2025-09-27 13:04:16+0900","lastModifiedAt":"2025-09-27 13:04:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/696/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0695-type-parameter-syntax","title":"[Final] PEP 695 - Type Parameter Syntax","excerpt":"Python Enhancement Proposal 695: 'Type Parameter Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 13:03:44+0900","lastModifiedAt":"2025-09-27 13:03:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/695/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0694-upload-2-0-api-for-python-package-indexes","title":"[Draft] PEP 694 - Upload 2.0 API for Python Package Indexes","excerpt":"Python Enhancement Proposal 694: 'Upload 2.0 API for Python Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:23:36+0900","lastModifiedAt":"2025-09-27 10:23:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/694/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0693-python-3-12-release-schedule","title":"[Active] PEP 693 - Python 3.12 Release Schedule","excerpt":"Python Enhancement Proposal 693: 'Python 3.12 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 10:22:22+0900","lastModifiedAt":"2025-09-27 10:22:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/693/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0692-using-typeddict-for-more-precise-kwargs-typing","title":"[Final] PEP 692 - Using TypedDict for more precise **kwargs typing","excerpt":"Python Enhancement Proposal 692: 'Using TypedDict for more precise ** kwargs typing'에 대한 한국어 번역입니다.","date":"2025-09-27 10:21:57+0900","lastModifiedAt":"2025-09-27 10:21:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/692/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0691-json-based-simple-api-for-python-package-indexes","title":"[Accepted] PEP 691 - JSON-based Simple API for Python Package Indexes","excerpt":"Python Enhancement Proposal 691: 'JSON-based Simple API for Python Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:21:04+0900","lastModifiedAt":"2025-09-27 10:21:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/691/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0690-lazy-imports","title":"[Rejected] PEP 690 - Lazy Imports","excerpt":"Python Enhancement Proposal 690: 'Lazy Imports'에 대한 한국어 번역입니다.","date":"2025-09-27 10:19:51+0900","lastModifiedAt":"2025-09-27 10:19:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/690/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0689-unstable-c-api-tier","title":"[Final] PEP 689 - Unstable C API tier","excerpt":"Python Enhancement Proposal 689: 'Unstable C API tier'에 대한 한국어 번역입니다.","date":"2025-09-27 10:13:23+0900","lastModifiedAt":"2025-09-27 10:13:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/689/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0688-making-the-buffer-protocol-accessible-in-python","title":"[Final] PEP 688 - Making the buffer protocol accessible in Python","excerpt":"Python Enhancement Proposal 688: 'Making the buffer protocol accessible in Python'에 대한 한국어 번역입니다.","date":"2025-09-27 10:13:02+0900","lastModifiedAt":"2025-09-27 10:13:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/688/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0687-isolating-modules-in-the-standard-library","title":"[Accepted] PEP 687 - Isolating modules in the standard library","excerpt":"Python Enhancement Proposal 687: 'Isolating modules in the standard library'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:44+0900","lastModifiedAt":"2025-09-27 10:12:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/687/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0686-make-utf-8-mode-default","title":"[Accepted] PEP 686 - Make UTF-8 mode default","excerpt":"Python Enhancement Proposal 686: 'Make UTF-8 mode default'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:32+0900","lastModifiedAt":"2025-09-27 10:12:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/686/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0685-comparison-of-extra-names-for-optional-distribution-dependencies","title":"[Final] PEP 685 - Comparison of extra names for optional distribution dependencies","excerpt":"Python Enhancement Proposal 685: 'Comparison of extra names for optional distribution dependencies'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:15+0900","lastModifiedAt":"2025-09-27 10:12:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/685/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0684-a-per-interpreter-gil","title":"[Final] PEP 684 - A Per-Interpreter GIL","excerpt":"Python Enhancement Proposal 684: 'A Per-Interpreter GIL'에 대한 한국어 번역입니다.","date":"2025-09-27 10:11:59+0900","lastModifiedAt":"2025-09-27 10:11:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/684/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0683-immortal-objects-using-a-fixed-refcount","title":"[Final] PEP 683 - Immortal Objects, Using a Fixed Refcount","excerpt":"Python Enhancement Proposal 683: 'Immortal Objects, Using a Fixed Refcount'에 대한 한국어 번역입니다.","date":"2025-09-27 10:11:29+0900","lastModifiedAt":"2025-09-27 10:11:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/683/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0682-format-specifier-for-signed-zero","title":"[Final] PEP 682 - Format Specifier for Signed Zero","excerpt":"Python Enhancement Proposal 682: 'Format Specifier for Signed Zero'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:49+0900","lastModifiedAt":"2025-09-27 10:10:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/682/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0681-data-class-transforms","title":"[Final] PEP 681 - Data Class Transforms","excerpt":"Python Enhancement Proposal 681: 'Data Class Transforms'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:35+0900","lastModifiedAt":"2025-09-27 10:10:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/681/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0680-tomllib-support-for-parsing-toml-in-the-standard-library","title":"[Final] PEP 680 - tomllib: Support for Parsing TOML in the Standard Library","excerpt":"Python Enhancement Proposal 680: 'tomllib: Support for Parsing TOML in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:11+0900","lastModifiedAt":"2025-09-27 10:10:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/680/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0679-new-assert-statement-syntax-with-parentheses","title":"[Draft] PEP 679 - New assert statement syntax with parentheses","excerpt":"Python Enhancement Proposal 679: 'New assert statement syntax with parentheses'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:51+0900","lastModifiedAt":"2025-09-27 10:09:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/679/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0678-enriching-exceptions-with-notes","title":"[Final] PEP 678 - Enriching Exceptions with Notes","excerpt":"Python Enhancement Proposal 678: 'Enriching Exceptions with Notes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:33+0900","lastModifiedAt":"2025-09-27 10:09:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/678/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0677-callable-type-syntax","title":"[Rejected] PEP 677 - Callable Type Syntax","excerpt":"Python Enhancement Proposal 677: 'Callable Type Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:13+0900","lastModifiedAt":"2025-09-27 10:09:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/677/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0676-pep-infrastructure-process","title":"[Active] PEP 676 - PEP Infrastructure Process","excerpt":"Python Enhancement Proposal 676: 'PEP Infrastructure Process'에 대한 한국어 번역입니다.","date":"2025-09-27 10:08:50+0900","lastModifiedAt":"2025-09-27 10:08:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/676/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0675-arbitrary-literal-string-type","title":"[Final] PEP 675 - Arbitrary Literal String Type","excerpt":"Python Enhancement Proposal 675: 'Arbitrary Literal String Type'에 대한 한국어 번역입니다.","date":"2025-09-27 10:08:22+0900","lastModifiedAt":"2025-09-27 10:08:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/675/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0674-disallow-using-macros-as-l-values","title":"[Deferred] PEP 674 - Disallow using macros as l-values","excerpt":"Python Enhancement Proposal 674: 'Disallow using macros as l-values'에 대한 한국어 번역입니다.","date":"2025-09-27 10:07:42+0900","lastModifiedAt":"2025-09-27 10:07:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/674/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0673-self-type","title":"[Final] PEP 673 - Self Type","excerpt":"Python Enhancement Proposal 673: 'Self Type'에 대한 한국어 번역입니다.","date":"2025-09-27 10:07:14+0900","lastModifiedAt":"2025-09-27 10:07:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/673/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0672-unicode-related-security-considerations-for-python","title":"[Active] PEP 672 - Unicode-related Security Considerations for Python","excerpt":"Python Enhancement Proposal 672: 'Unicode-related Security Considerations for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 10:06:17+0900","lastModifiedAt":"2025-09-27 10:06:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/672/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0671-syntax-for-late-bound-function-argument-defaults","title":"[Draft] PEP 671 - Syntax for late-bound function argument defaults","excerpt":"Python Enhancement Proposal 671: 'Syntax for late-bound function argument defaults'에 대한 한국어 번역입니다.","date":"2025-09-27 10:05:44+0900","lastModifiedAt":"2025-09-27 10:05:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/671/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0670-convert-macros-to-functions-in-the-python-c-api","title":"[Final] PEP 670 - Convert macros to functions in the Python C API","excerpt":"Python Enhancement Proposal 670: 'Convert macros to functions in the Python C API'에 대한 한국어 번역입니다.","date":"2025-09-27 10:05:27+0900","lastModifiedAt":"2025-09-27 10:05:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/670/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0669-low-impact-monitoring-for-cpython","title":"[Final] PEP 669 - Low Impact Monitoring for CPython","excerpt":"Python Enhancement Proposal 669: 'Low Impact Monitoring for CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 10:04:48+0900","lastModifiedAt":"2025-09-27 10:04:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/669/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0668-marking-python-base-environments-as-externally-managed","title":"[Accepted] PEP 668 - Marking Python base environments as “externally managed”","excerpt":"Python Enhancement Proposal 668: 'Marking Python base environments as “externally managed”'에 대한 한국어 번역입니다.","date":"2025-09-27 10:04:11+0900","lastModifiedAt":"2025-09-27 10:04:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/668/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0667-consistent-views-of-namespaces","title":"[Final] PEP 667 - Consistent views of namespaces","excerpt":"Python Enhancement Proposal 667: 'Consistent views of namespaces'에 대한 한국어 번역입니다.","date":"2025-09-27 10:03:19+0900","lastModifiedAt":"2025-09-27 10:03:19+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/667/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0666-reject-foolish-indentation","title":"[Rejected] PEP 666 - Reject Foolish Indentation","excerpt":"Python Enhancement Proposal 666: 'Reject Foolish Indentation'에 대한 한국어 번역입니다.","date":"2025-09-27 10:02:26+0900","lastModifiedAt":"2025-09-27 10:02:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/666/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0665-a-file-format-to-list-python-dependencies-for-reproducibility-of-an-application","title":"[Rejected] PEP 665 - A file format to list Python dependencies for reproducibility of an application","excerpt":"Python Enhancement Proposal 665: 'A file format to list Python dependencies for reproducibility of an application'에 대한 한국어 번역입니다.","date":"2025-09-27 10:02:15+0900","lastModifiedAt":"2025-09-27 10:02:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/665/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0664-python-3-11-release-schedule","title":"[Active] PEP 664 - Python 3.11 Release Schedule","excerpt":"Python Enhancement Proposal 664: 'Python 3.11 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 09:57:22+0900","lastModifiedAt":"2025-09-27 09:57:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/664/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0663-standardizing-enum-str-repr-and-format-behaviors","title":"[Rejected] PEP 663 - Standardizing Enum str(), repr(), and format() behaviors","excerpt":"Python Enhancement Proposal 663: 'Standardizing Enum str(), repr(), and format() behaviors'에 대한 한국어 번역입니다.","date":"2025-09-27 09:57:11+0900","lastModifiedAt":"2025-09-27 09:57:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/663/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0662-editable-installs-via-virtual-wheels","title":"[Rejected] PEP 662 - Editable installs via virtual wheels","excerpt":"Python Enhancement Proposal 662: 'Editable installs via virtual wheels'에 대한 한국어 번역입니다.","date":"2025-09-27 09:56:52+0900","lastModifiedAt":"2025-09-27 09:56:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/662/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0661-sentinel-values","title":"[Deferred] PEP 661 - Sentinel Values","excerpt":"Python Enhancement Proposal 661: 'Sentinel Values'에 대한 한국어 번역입니다.","date":"2025-09-27 09:55:59+0900","lastModifiedAt":"2025-09-27 09:55:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/661/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0660-editable-installs-for-pyproject-toml-based-builds-wheel-based","title":"[Final] PEP 660 - Editable installs for pyproject.toml based builds (wheel based)","excerpt":"Python Enhancement Proposal 660: 'Editable installs for pyproject.toml based builds (wheel based)'에 대한 한국어 번역입니다.","date":"2025-09-27 09:55:28+0900","lastModifiedAt":"2025-09-27 09:55:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/660/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0659-specializing-adaptive-interpreter","title":"[Final] PEP 659 - Specializing Adaptive Interpreter","excerpt":"Python Enhancement Proposal 659: 'Specializing Adaptive Interpreter'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:44+0900","lastModifiedAt":"2025-09-27 09:54:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/659/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0658-serve-distribution-metadata-in-the-simple-repository-api","title":"[Accepted] PEP 658 - Serve Distribution Metadata in the Simple Repository API","excerpt":"Python Enhancement Proposal 658: 'Serve Distribution Metadata in the Simple Repository API'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:18+0900","lastModifiedAt":"2025-09-27 09:54:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/658/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0657-include-fine-grained-error-locations-in-tracebacks","title":"[Final] PEP 657 - Include Fine Grained Error Locations in Tracebacks","excerpt":"Python Enhancement Proposal 657: 'Include Fine Grained Error Locations in Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:06+0900","lastModifiedAt":"2025-09-27 09:54:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/657/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0656-platform-tag-for-linux-distributions-using-musl","title":"[Final] PEP 656 - Platform Tag for Linux Distributions Using Musl","excerpt":"Python Enhancement Proposal 656: 'Platform Tag for Linux Distributions Using Musl'에 대한 한국어 번역입니다.","date":"2025-09-27 09:53:36+0900","lastModifiedAt":"2025-09-27 09:53:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/656/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0655-marking-individual-typeddict-items-as-required-or-potentially-missing","title":"[Final] PEP 655 - Marking individual TypedDict items as required or potentially-missing","excerpt":"Python Enhancement Proposal 655: 'Marking individual TypedDict items as required or potentially-missing'에 대한 한국어 번역입니다.","date":"2025-09-27 09:53:14+0900","lastModifiedAt":"2025-09-27 09:53:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/655/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0654-exception-groups-and-except","title":"[Final] PEP 654 - Exception Groups and except*","excerpt":"Python Enhancement Proposal 654: 'Exception Groups and except*'에 대한 한국어 번역입니다.","date":"2025-09-27 09:52:52+0900","lastModifiedAt":"2025-09-27 09:52:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/654/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0653-precise-semantics-for-pattern-matching","title":"[Draft] PEP 653 - Precise Semantics for Pattern Matching","excerpt":"Python Enhancement Proposal 653: 'Precise Semantics for Pattern Matching'에 대한 한국어 번역입니다.","date":"2025-09-27 09:52:27+0900","lastModifiedAt":"2025-09-27 09:52:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/653/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0652-maintaining-the-stable-abi","title":"[Final] PEP 652 - Maintaining the Stable ABI","excerpt":"Python Enhancement Proposal 652: 'Maintaining the Stable ABI'에 대한 한국어 번역입니다.","date":"2025-09-27 01:41:49+0900","lastModifiedAt":"2025-09-27 01:41:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/652/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0651-robust-stack-overflow-handling","title":"[Rejected] PEP 651 - Robust Stack Overflow Handling","excerpt":"Python Enhancement Proposal 651: 'Robust Stack Overflow Handling'에 대한 한국어 번역입니다.","date":"2025-09-27 01:41:16+0900","lastModifiedAt":"2025-09-27 01:41:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/651/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0650-specifying-installer-requirements-for-python-projects","title":"[Withdrawn] PEP 650 - Specifying Installer Requirements for Python Projects","excerpt":"Python Enhancement Proposal 650: 'Specifying Installer Requirements for Python Projects'에 대한 한국어 번역입니다.","date":"2025-09-27 01:40:48+0900","lastModifiedAt":"2025-09-27 01:40:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/650/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0649-deferred-evaluation-of-annotations-using-descriptors","title":"[Accepted] PEP 649 - Deferred Evaluation Of Annotations Using Descriptors","excerpt":"Python Enhancement Proposal 649: 'Deferred Evaluation Of Annotations Using Descriptors'에 대한 한국어 번역입니다.","date":"2025-09-27 01:39:54+0900","lastModifiedAt":"2025-09-27 01:39:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/649/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0648-extensible-customizations-of-the-interpreter-at-startup","title":"[Rejected] PEP 648 - Extensible customizations of the interpreter at startup","excerpt":"Python Enhancement Proposal 648: 'Extensible customizations of the interpreter at startup'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:58+0900","lastModifiedAt":"2025-09-27 01:37:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/648/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0647-user-defined-type-guards","title":"[Final] PEP 647 - User-Defined Type Guards","excerpt":"Python Enhancement Proposal 647: 'User-Defined Type Guards'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:28+0900","lastModifiedAt":"2025-09-27 01:37:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/647/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0646-variadic-generics","title":"[Final] PEP 646 - Variadic Generics","excerpt":"Python Enhancement Proposal 646: 'Variadic Generics'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:00+0900","lastModifiedAt":"2025-09-27 01:37:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/646/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0645-allow-writing-optional-types-asx","title":"[Withdrawn] PEP 645 - Allow writing optional types asx?","excerpt":"Python Enhancement Proposal 645: 'Allow writing optional types asx?'에 대한 한국어 번역입니다.","date":"2025-09-27 01:34:51+0900","lastModifiedAt":"2025-09-27 01:34:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/645/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0644-require-openssl-1-1-1-or-newer","title":"[Final] PEP 644 - Require OpenSSL 1.1.1 or newer","excerpt":"Python Enhancement Proposal 644: 'Require OpenSSL 1.1.1 or newer'에 대한 한국어 번역입니다.","date":"2025-09-27 01:34:31+0900","lastModifiedAt":"2025-09-27 01:34:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/644/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0643-metadata-for-package-source-distributions","title":"[Final] PEP 643 - Metadata for Package Source Distributions","excerpt":"Python Enhancement Proposal 643: 'Metadata for Package Source Distributions'에 대한 한국어 번역입니다.","date":"2025-09-27 01:33:51+0900","lastModifiedAt":"2025-09-27 01:33:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/643/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0642-explicit-pattern-syntax-for-structural-pattern-matching","title":"[Rejected] PEP 642 - Explicit Pattern Syntax for Structural Pattern Matching","excerpt":"Python Enhancement Proposal 642: 'Explicit Pattern Syntax for Structural Pattern Matching'에 대한 한국어 번역입니다.","date":"2025-09-27 01:33:04+0900","lastModifiedAt":"2025-09-27 01:33:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/642/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0641-using-an-underscore-in-the-version-portion-of-python-3-10-compatibility-tags","title":"[Rejected] PEP 641 - Using an underscore in the version portion of Python 3.10 compatibility tags","excerpt":"Python Enhancement Proposal 641: 'Using an underscore in the version portion of Python 3.10 compatibility tags'에 대한 한국어 번역입니다.","date":"2025-09-27 01:32:00+0900","lastModifiedAt":"2025-09-27 01:32:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/641/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0640-unused-variable-syntax","title":"[Rejected] PEP 640 - Unused variable syntax","excerpt":"Python Enhancement Proposal 640: 'Unused variable syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 01:31:38+0900","lastModifiedAt":"2025-09-27 01:31:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/640/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0639-improving-license-clarity-with-better-package-metadata","title":"[Final] PEP 639 - Improving License Clarity with Better Package Metadata","excerpt":"Python Enhancement Proposal 639: 'Improving License Clarity with Better Package Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 01:31:09+0900","lastModifiedAt":"2025-09-27 01:31:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/639/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0638-syntactic-macros","title":"[Draft] PEP 638 - Syntactic Macros","excerpt":"Python Enhancement Proposal 638: 'Syntactic Macros'에 대한 한국어 번역입니다.","date":"2025-09-27 01:30:41+0900","lastModifiedAt":"2025-09-27 01:30:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/638/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0637-support-for-indexing-with-keyword-arguments","title":"[Rejected] PEP 637 - Support for indexing with keyword arguments","excerpt":"Python Enhancement Proposal 637: 'Support for indexing with keyword arguments'에 대한 한국어 번역입니다.","date":"2025-09-27 01:29:49+0900","lastModifiedAt":"2025-09-27 01:29:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/637/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0636-structural-pattern-matching-tutorial","title":"[Final] PEP 636 - Structural Pattern Matching: Tutorial","excerpt":"Python Enhancement Proposal 636: 'Structural Pattern Matching: Tutorial'에 대한 한국어 번역입니다.","date":"2025-09-27 01:29:01+0900","lastModifiedAt":"2025-09-27 01:29:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/636/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0635-structural-pattern-matching-motivation-and-rationale","title":"[Final] PEP 635 - Structural Pattern Matching: Motivation and Rationale","excerpt":"Python Enhancement Proposal 635: 'Structural Pattern Matching: Motivation and Rationale'에 대한 한국어 번역입니다.","date":"2025-09-27 01:26:51+0900","lastModifiedAt":"2025-09-27 01:26:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/635/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0634-structural-pattern-matching-specification","title":"[Final] PEP 634 - Structural Pattern Matching: Specification","excerpt":"Python Enhancement Proposal 634: 'Structural Pattern Matching: Specification'에 대한 한국어 번역입니다.","date":"2025-09-27 01:25:49+0900","lastModifiedAt":"2025-09-27 01:25:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/634/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0633-dependency-specification-in-pyproject-toml-using-an-exploded-toml-table","title":"[Rejected] PEP 633 - Dependency specification in pyproject.toml using an exploded TOML table","excerpt":"Python Enhancement Proposal 633: 'Dependency specification in pyproject.toml using an exploded TOML table'에 대한 한국어 번역입니다.","date":"2025-09-27 01:25:08+0900","lastModifiedAt":"2025-09-27 01:25:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/633/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0632-deprecate-distutils-module","title":"[Final] PEP 632 - Deprecate distutils module","excerpt":"Python Enhancement Proposal 632: 'Deprecate distutils module'에 대한 한국어 번역입니다.","date":"2025-09-27 01:16:21+0900","lastModifiedAt":"2025-09-27 01:16:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/632/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0631-dependency-specification-in-pyproject-toml-based-on-pep-508","title":"[Superseded] PEP 631 - Dependency specification in pyproject.toml based on PEP 508","excerpt":"Python Enhancement Proposal 631: 'Dependency specification in pyproject.toml based on PEP 508'에 대한 한국어 번역입니다.","date":"2025-09-27 01:15:39+0900","lastModifiedAt":"2025-09-27 01:15:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/631/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0630-isolating-extension-modules","title":"[Final] PEP 630 - Isolating Extension Modules","excerpt":"Python Enhancement Proposal 630: 'Isolating Extension Modules'에 대한 한국어 번역입니다.","date":"2025-09-27 01:13:23+0900","lastModifiedAt":"2025-09-27 01:13:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/630/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0355-path-object-oriented-filesystem-paths","title":"[Rejected] PEP 355 - Path - Object oriented filesystem paths","excerpt":"Python Enhancement Proposal 355: 'Path - Object oriented filesystem paths'에 대한 한국어 번역입니다.","date":"2025-09-27 01:12:42+0900","lastModifiedAt":"2025-09-27 01:12:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/355/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0343-the-with-statement","title":"[Final] PEP 343 - The “with” Statement","excerpt":"Python Enhancement Proposal 343: 'The “with” Statement'에 대한 한국어 번역입니다.","date":"2025-09-27 01:01:20+0900","lastModifiedAt":"2025-09-27 01:01:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/343/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0335-overloadable-boolean-operators","title":"[Rejected] PEP 335 - Overloadable Boolean Operators","excerpt":"Python Enhancement Proposal 335: 'Overloadable Boolean Operators'에 대한 한국어 번역입니다.","date":"2025-09-27 00:55:17+0900","lastModifiedAt":"2025-09-27 00:55:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/335/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0629-versioning-pypis-simple-api","title":"[Final] PEP 629 - Versioning PyPI’s Simple API","excerpt":"Python Enhancement Proposal 629: 'Versioning PyPI’s Simple API'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:52+0900","lastModifiedAt":"2025-09-27 00:30:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/629/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0628-addmath-tau","title":"[Final] PEP 628 - Addmath.tau","excerpt":"Python Enhancement Proposal 628: 'Addmath.tau'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:25+0900","lastModifiedAt":"2025-09-27 00:30:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/628/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0627-recording-installed-projects","title":"[Final] PEP 627 - Recording installed projects","excerpt":"Python Enhancement Proposal 627: 'Recording installed projects'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:10+0900","lastModifiedAt":"2025-09-27 00:30:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/627/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0626-precise-line-numbers-for-debugging-and-other-tools-","title":"[Final] PEP 626 - Precise line numbers for debugging and other tools.","excerpt":"Python Enhancement Proposal 626: 'Precise line numbers for debugging and other tools.'에 대한 한국어 번역입니다.","date":"2025-09-27 00:29:42+0900","lastModifiedAt":"2025-09-27 00:29:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/626/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0608-coordinated-python-release","title":"[Rejected] PEP 608 - Coordinated Python release","excerpt":"Python Enhancement Proposal 608: 'Coordinated Python release'에 대한 한국어 번역입니다.","date":"2025-09-27 00:19:32+0900","lastModifiedAt":"2025-09-27 00:19:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/608/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-27-pep-0599-the-manylinux2014-platform-tag","title":"[Superseded] PEP 599 - The manylinux2014 Platform Tag","excerpt":"Python Enhancement Proposal 599: 'The manylinux2014 Platform Tag'에 대한 한국어 번역입니다.","date":"2025-09-27 00:14:20+0900","lastModifiedAt":"2025-09-27 00:14:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/599/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0573-module-state-access-from-c-extension-methods","title":"[Final] PEP 573 - Module State Access from C Extension Methods","excerpt":"Python Enhancement Proposal 573: 'Module State Access from C Extension Methods'에 대한 한국어 번역입니다.","date":"2025-09-26 23:59:37+0900","lastModifiedAt":"2025-09-26 23:59:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/573/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0572-assignment-expressions","title":"[Final] PEP 572 - Assignment Expressions","excerpt":"Python Enhancement Proposal 572: 'Assignment Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:58:55+0900","lastModifiedAt":"2025-09-26 23:58:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/572/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0571-the-manylinux2010-platform-tag","title":"[Superseded] PEP 571 - The manylinux2010 Platform Tag","excerpt":"Python Enhancement Proposal 571: 'The manylinux2010 Platform Tag'에 대한 한국어 번역입니다.","date":"2025-09-26 23:56:56+0900","lastModifiedAt":"2025-09-26 23:56:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/571/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0570-python-positional-only-parameters","title":"[Final] PEP 570 - Python Positional-Only Parameters","excerpt":"Python Enhancement Proposal 570: 'Python Positional-Only Parameters'에 대한 한국어 번역입니다.","date":"2025-09-26 23:56:05+0900","lastModifiedAt":"2025-09-26 23:56:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/570/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0569-python-3-8-release-schedule","title":"[Final] PEP 569 - Python 3.8 Release Schedule","excerpt":"Python Enhancement Proposal 569: 'Python 3.8 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 23:51:49+0900","lastModifiedAt":"2025-09-26 23:51:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/569/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0568-generator-sensitivity-for-context-variables","title":"[Deferred] PEP 568 - Generator-sensitivity for Context Variables","excerpt":"Python Enhancement Proposal 568: 'Generator-sensitivity for Context Variables'에 대한 한국어 번역입니다.","date":"2025-09-26 23:51:20+0900","lastModifiedAt":"2025-09-26 23:51:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/568/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0567-context-variables","title":"[Final] PEP 567 - Context Variables","excerpt":"Python Enhancement Proposal 567: 'Context Variables'에 대한 한국어 번역입니다.","date":"2025-09-26 23:50:36+0900","lastModifiedAt":"2025-09-26 23:50:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/567/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0566-metadata-for-python-software-packages-2-1","title":"[Final] PEP 566 - Metadata for Python Software Packages 2.1","excerpt":"Python Enhancement Proposal 566: 'Metadata for Python Software Packages 2.1'에 대한 한국어 번역입니다.","date":"2025-09-26 23:49:52+0900","lastModifiedAt":"2025-09-26 23:49:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/566/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0565-show-deprecationwarning-in-main","title":"[Final] PEP 565 - Show DeprecationWarning in __main__","excerpt":"Python Enhancement Proposal 565: 'Show DeprecationWarning in __main__'에 대한 한국어 번역입니다.","date":"2025-09-26 23:49:30+0900","lastModifiedAt":"2025-09-26 23:49:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/565/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0564-add-new-time-functions-with-nanosecond-resolution","title":"[Final] PEP 564 - Add new time functions with nanosecond resolution","excerpt":"Python Enhancement Proposal 564: 'Add new time functions with nanosecond resolution'에 대한 한국어 번역입니다.","date":"2025-09-26 23:48:58+0900","lastModifiedAt":"2025-09-26 23:48:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/564/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0563-postponed-evaluation-of-annotations","title":"[Superseded] PEP 563 - Postponed Evaluation of Annotations","excerpt":"Python Enhancement Proposal 563: 'Postponed Evaluation of Annotations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:48:31+0900","lastModifiedAt":"2025-09-26 23:48:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/563/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0562-module-getattr-and-dir","title":"[Final] PEP 562 - Module __getattr__ and __dir__","excerpt":"Python Enhancement Proposal 562: 'Module __getattr__ and __dir__'에 대한 한국어 번역입니다.","date":"2025-09-26 23:47:20+0900","lastModifiedAt":"2025-09-26 23:47:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/562/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0561-distributing-and-packaging-type-information","title":"[Final] PEP 561 - Distributing and Packaging Type Information","excerpt":"Python Enhancement Proposal 561: 'Distributing and Packaging Type Information'에 대한 한국어 번역입니다.","date":"2025-09-26 23:46:31+0900","lastModifiedAt":"2025-09-26 23:46:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/561/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0560-core-support-for-typing-module-and-generic-types","title":"[Final] PEP 560 - Core support for typing module and generic types","excerpt":"Python Enhancement Proposal 560: 'Core support for typing module and generic types'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:50+0900","lastModifiedAt":"2025-09-26 23:45:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/560/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0559-built-in-noop","title":"[Rejected] PEP 559 - Built-in noop()","excerpt":"Python Enhancement Proposal 559: 'Built-in noop()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:26+0900","lastModifiedAt":"2025-09-26 23:45:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/559/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0558-defined-semantics-for-locals","title":"[Withdrawn] PEP 558 - Defined semantics for locals()","excerpt":"Python Enhancement Proposal 558: 'Defined semantics for locals()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:10+0900","lastModifiedAt":"2025-09-26 23:45:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/558/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0557-data-classes","title":"[Final] PEP 557 - Data Classes","excerpt":"Python Enhancement Proposal 557: 'Data Classes'에 대한 한국어 번역입니다.","date":"2025-09-26 23:42:21+0900","lastModifiedAt":"2025-09-26 23:42:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/557/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0556-threaded-garbage-collection","title":"[Deferred] PEP 556 - Threaded garbage collection","excerpt":"Python Enhancement Proposal 556: 'Threaded garbage collection'에 대한 한국어 번역입니다.","date":"2025-09-26 23:41:31+0900","lastModifiedAt":"2025-09-26 23:41:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/556/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0555-context-local-variables-contextvars","title":"[Withdrawn] PEP 555 - Context-local variables (contextvars)","excerpt":"Python Enhancement Proposal 555: 'Context-local variables (contextvars)'에 대한 한국어 번역입니다.","date":"2025-09-26 23:40:56+0900","lastModifiedAt":"2025-09-26 23:40:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/555/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0554-multiple-interpreters-in-the-stdlib","title":"[Superseded] PEP 554 - Multiple Interpreters in the Stdlib","excerpt":"Python Enhancement Proposal 554: 'Multiple Interpreters in the Stdlib'에 대한 한국어 번역입니다.","date":"2025-09-26 23:40:00+0900","lastModifiedAt":"2025-09-26 23:40:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/554/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0553-built-in-breakpoint","title":"[Final] PEP 553 - Built-in breakpoint()","excerpt":"Python Enhancement Proposal 553: 'Built-in breakpoint()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:38:53+0900","lastModifiedAt":"2025-09-26 23:38:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/553/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0552-deterministic-pycs","title":"[Final] PEP 552 - Deterministic pycs","excerpt":"Python Enhancement Proposal 552: 'Deterministic pycs'에 대한 한국어 번역입니다.","date":"2025-09-26 23:38:24+0900","lastModifiedAt":"2025-09-26 23:38:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/552/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0551-security-transparency-in-the-python-runtime","title":"[Withdrawn] PEP 551 - Security transparency in the Python runtime","excerpt":"Python Enhancement Proposal 551: 'Security transparency in the Python runtime'에 대한 한국어 번역입니다.","date":"2025-09-26 23:37:57+0900","lastModifiedAt":"2025-09-26 23:37:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/551/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0550-execution-context","title":"[Withdrawn] PEP 550 - Execution Context","excerpt":"Python Enhancement Proposal 550: 'Execution Context'에 대한 한국어 번역입니다.","date":"2025-09-26 23:36:24+0900","lastModifiedAt":"2025-09-26 23:36:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/550/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0549-instance-descriptors","title":"[Rejected] PEP 549 - Instance Descriptors","excerpt":"Python Enhancement Proposal 549: 'Instance Descriptors'에 대한 한국어 번역입니다.","date":"2025-09-26 23:35:20+0900","lastModifiedAt":"2025-09-26 23:35:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/549/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0548-more-flexible-loop-control","title":"[Rejected] PEP 548 - More Flexible Loop Control","excerpt":"Python Enhancement Proposal 548: 'More Flexible Loop Control'에 대한 한국어 번역입니다.","date":"2025-09-26 23:35:01+0900","lastModifiedAt":"2025-09-26 23:35:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/548/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0547-running-extension-modules-using-the-m-option","title":"[Deferred] PEP 547 - Running extension modules using the -m option","excerpt":"Python Enhancement Proposal 547: 'Running extension modules using the -m option'에 대한 한국어 번역입니다.","date":"2025-09-26 23:34:23+0900","lastModifiedAt":"2025-09-26 23:34:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/547/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0546-backport-ssl-memorybio-and-ssl-sslobject-to-python-2-7","title":"[Rejected] PEP 546 - Backport ssl.MemoryBIO and ssl.SSLObject to Python 2.7","excerpt":"Python Enhancement Proposal 546: 'Backport ssl.MemoryBIO and ssl.SSLObject to Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 23:33:55+0900","lastModifiedAt":"2025-09-26 23:33:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/546/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0545-python-documentation-translations","title":"[Active] PEP 545 - Python Documentation Translations","excerpt":"Python Enhancement Proposal 545: 'Python Documentation Translations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:33:31+0900","lastModifiedAt":"2025-09-26 23:33:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/545/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0544-protocols-structural-subtyping-static-duck-typing","title":"[Final] PEP 544 - Protocols: Structural subtyping (static duck typing)","excerpt":"Python Enhancement Proposal 544: 'Protocols: Structural subtyping (static duck typing)'에 대한 한국어 번역입니다.","date":"2025-09-26 23:32:41+0900","lastModifiedAt":"2025-09-26 23:32:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/544/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0543-a-unified-tls-api-for-python","title":"[Withdrawn] PEP 543 - A Unified TLS API for Python","excerpt":"Python Enhancement Proposal 543: 'A Unified TLS API for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 23:30:50+0900","lastModifiedAt":"2025-09-26 23:30:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/543/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0542-dot-notation-assignment-in-function-header","title":"[Rejected] PEP 542 - Dot Notation Assignment In Function Header","excerpt":"Python Enhancement Proposal 542: 'Dot Notation Assignment In Function Header'에 대한 한국어 번역입니다.","date":"2025-09-26 23:30:04+0900","lastModifiedAt":"2025-09-26 23:30:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/542/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0541-package-index-name-retention","title":"[Final] PEP 541 - Package Index Name Retention","excerpt":"Python Enhancement Proposal 541: 'Package Index Name Retention'에 대한 한국어 번역입니다.","date":"2025-09-26 23:29:44+0900","lastModifiedAt":"2025-09-26 23:29:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/541/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0540-add-a-new-utf-8-mode","title":"[Final] PEP 540 - Add a new UTF-8 Mode","excerpt":"Python Enhancement Proposal 540: 'Add a new UTF-8 Mode'에 대한 한국어 번역입니다.","date":"2025-09-26 23:29:04+0900","lastModifiedAt":"2025-09-26 23:29:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/540/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0539-a-new-c-api-for-thread-local-storage-in-cpython","title":"[Final] PEP 539 - A New C-API for Thread-Local Storage in CPython","excerpt":"Python Enhancement Proposal 539: 'A New C-API for Thread-Local Storage in CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 23:28:37+0900","lastModifiedAt":"2025-09-26 23:28:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/539/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0538-coercing-the-legacy-c-locale-to-a-utf-8-based-locale","title":"[Final] PEP 538 - Coercing the legacy C locale to a UTF-8 based locale","excerpt":"Python Enhancement Proposal 538: 'Coercing the legacy C locale to a UTF-8 based locale'에 대한 한국어 번역입니다.","date":"2025-09-26 23:28:03+0900","lastModifiedAt":"2025-09-26 23:28:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/538/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0537-python-3-7-release-schedule","title":"[Final] PEP 537 - Python 3.7 Release Schedule","excerpt":"Python Enhancement Proposal 537: 'Python 3.7 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 23:27:21+0900","lastModifiedAt":"2025-09-26 23:27:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/537/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0536-final-grammar-for-literal-string-interpolation","title":"[Withdrawn] PEP 536 - Final Grammar for Literal String Interpolation","excerpt":"Python Enhancement Proposal 536: 'Final Grammar for Literal String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 23:26:52+0900","lastModifiedAt":"2025-09-26 23:26:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/536/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0535-rich-comparison-chaining","title":"[Deferred] PEP 535 - Rich comparison chaining","excerpt":"Python Enhancement Proposal 535: 'Rich comparison chaining'에 대한 한국어 번역입니다.","date":"2025-09-26 23:26:10+0900","lastModifiedAt":"2025-09-26 23:26:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/535/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0534-improved-errors-for-missing-standard-library-modules","title":"[Deferred] PEP 534 - Improved Errors for Missing Standard Library Modules","excerpt":"Python Enhancement Proposal 534: 'Improved Errors for Missing Standard Library Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 23:25:45+0900","lastModifiedAt":"2025-09-26 23:25:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/534/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0533-deterministic-cleanup-for-iterators","title":"[Deferred] PEP 533 - Deterministic cleanup for iterators","excerpt":"Python Enhancement Proposal 533: 'Deterministic cleanup for iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:25:05+0900","lastModifiedAt":"2025-09-26 23:25:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/533/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0532-a-circuit-breaking-protocol-and-binary-operators","title":"[Deferred] PEP 532 - A circuit breaking protocol and binary operators","excerpt":"Python Enhancement Proposal 532: 'A circuit breaking protocol and binary operators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:23:40+0900","lastModifiedAt":"2025-09-26 23:23:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/532/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0531-existence-checking-operators","title":"[Withdrawn] PEP 531 - Existence checking operators","excerpt":"Python Enhancement Proposal 531: 'Existence checking operators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:22:20+0900","lastModifiedAt":"2025-09-26 23:22:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/531/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0530-asynchronous-comprehensions","title":"[Final] PEP 530 - Asynchronous Comprehensions","excerpt":"Python Enhancement Proposal 530: 'Asynchronous Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:21:12+0900","lastModifiedAt":"2025-09-26 23:21:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/530/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0529-change-windows-filesystem-encoding-to-utf-8","title":"[Final] PEP 529 - Change Windows filesystem encoding to UTF-8","excerpt":"Python Enhancement Proposal 529: 'Change Windows filesystem encoding to UTF-8'에 대한 한국어 번역입니다.","date":"2025-09-26 23:20:53+0900","lastModifiedAt":"2025-09-26 23:20:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/529/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0528-change-windows-console-encoding-to-utf-8","title":"[Final] PEP 528 - Change Windows console encoding to UTF-8","excerpt":"Python Enhancement Proposal 528: 'Change Windows console encoding to UTF-8'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:55+0900","lastModifiedAt":"2025-09-26 23:19:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/528/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0527-removing-underused-file-typesextensions-on-pypi","title":"[Final] PEP 527 - Removing Un(der)used file types/extensions on PyPI","excerpt":"Python Enhancement Proposal 527: 'Removing Un(der)used file types/extensions on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:32+0900","lastModifiedAt":"2025-09-26 23:19:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/527/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0526-syntax-for-variable-annotations","title":"[Final] PEP 526 - Syntax for Variable Annotations","excerpt":"Python Enhancement Proposal 526: 'Syntax for Variable Annotations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:02+0900","lastModifiedAt":"2025-09-26 23:19:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/526/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0525-asynchronous-generators","title":"[Final] PEP 525 - Asynchronous Generators","excerpt":"Python Enhancement Proposal 525: 'Asynchronous Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:17:46+0900","lastModifiedAt":"2025-09-26 23:17:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/525/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0524-make-os-urandom-blocking-on-linux","title":"[Final] PEP 524 - Make os.urandom() blocking on Linux","excerpt":"Python Enhancement Proposal 524: 'Make os.urandom() blocking on Linux'에 대한 한국어 번역입니다.","date":"2025-09-26 23:16:54+0900","lastModifiedAt":"2025-09-26 23:16:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/524/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0523-adding-a-frame-evaluation-api-to-cpython","title":"[Final] PEP 523 - Adding a frame evaluation API to CPython","excerpt":"Python Enhancement Proposal 523: 'Adding a frame evaluation API to CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 23:16:26+0900","lastModifiedAt":"2025-09-26 23:16:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/523/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0522-allow-blockingioerror-in-security-sensitive-apis","title":"[Rejected] PEP 522 - Allow BlockingIOError in security sensitive APIs","excerpt":"Python Enhancement Proposal 522: 'Allow BlockingIOError in security sensitive APIs'에 대한 한국어 번역입니다.","date":"2025-09-26 23:15:31+0900","lastModifiedAt":"2025-09-26 23:15:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/522/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0521-managing-global-context-via-with-blocks-in-generators-and-coroutines","title":"[Withdrawn] PEP 521 - Managing global context via ‘with’ blocks in generators and coroutines","excerpt":"Python Enhancement Proposal 521: 'Managing global context via ‘with’ blocks in generators and coroutines'에 대한 한국어 번역입니다.","date":"2025-09-26 23:14:33+0900","lastModifiedAt":"2025-09-26 23:14:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/521/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0520-preserving-class-attribute-definition-order","title":"[Final] PEP 520 - Preserving Class Attribute Definition Order","excerpt":"Python Enhancement Proposal 520: 'Preserving Class Attribute Definition Order'에 대한 한국어 번역입니다.","date":"2025-09-26 23:13:52+0900","lastModifiedAt":"2025-09-26 23:13:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/520/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0519-adding-a-file-system-path-protocol","title":"[Final] PEP 519 - Adding a file system path protocol","excerpt":"Python Enhancement Proposal 519: 'Adding a file system path protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 23:13:13+0900","lastModifiedAt":"2025-09-26 23:13:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/519/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0518-specifying-minimum-build-system-requirements-for-python-projects","title":"[Final] PEP 518 - Specifying Minimum Build System Requirements for Python Projects","excerpt":"Python Enhancement Proposal 518: 'Specifying Minimum Build System Requirements for Python Projects'에 대한 한국어 번역입니다.","date":"2025-09-26 23:12:43+0900","lastModifiedAt":"2025-09-26 23:12:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/518/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0517-a-build-system-independent-format-for-source-trees","title":"[Final] PEP 517 - A build-system independent format for source trees","excerpt":"Python Enhancement Proposal 517: 'A build-system independent format for source trees'에 대한 한국어 번역입니다.","date":"2025-09-26 23:11:54+0900","lastModifiedAt":"2025-09-26 23:11:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/517/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0516-build-system-abstraction-for-pipconda-etc","title":"[Rejected] PEP 516 - Build system abstraction for pip/conda etc","excerpt":"Python Enhancement Proposal 516: 'Build system abstraction for pip/conda etc'에 대한 한국어 번역입니다.","date":"2025-09-26 23:11:03+0900","lastModifiedAt":"2025-09-26 23:11:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/516/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0515-underscores-in-numeric-literals","title":"[Final] PEP 515 - Underscores in Numeric Literals","excerpt":"Python Enhancement Proposal 515: 'Underscores in Numeric Literals'에 대한 한국어 번역입니다.","date":"2025-09-26 23:10:13+0900","lastModifiedAt":"2025-09-26 23:10:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/515/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0514-python-registration-in-the-windows-registry","title":"[Active] PEP 514 - Python registration in the Windows registry","excerpt":"Python Enhancement Proposal 514: 'Python registration in the Windows registry'에 대한 한국어 번역입니다.","date":"2025-09-26 23:09:52+0900","lastModifiedAt":"2025-09-26 23:09:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/514/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0513-a-platform-tag-for-portable-linux-built-distributions","title":"[Superseded] PEP 513 - A Platform Tag for Portable Linux Built Distributions","excerpt":"Python Enhancement Proposal 513: 'A Platform Tag for Portable Linux Built Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:03:09+0900","lastModifiedAt":"2025-09-26 23:03:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/513/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0512-migrating-from-hg-python-org-to-github","title":"[Final] PEP 512 - Migrating from hg.python.org to GitHub","excerpt":"Python Enhancement Proposal 512: 'Migrating from hg.python.org to GitHub'에 대한 한국어 번역입니다.","date":"2025-09-26 22:57:36+0900","lastModifiedAt":"2025-09-26 22:57:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/512/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0511-api-for-code-transformers","title":"[Rejected] PEP 511 - API for code transformers","excerpt":"Python Enhancement Proposal 511: 'API for code transformers'에 대한 한국어 번역입니다.","date":"2025-09-26 22:56:47+0900","lastModifiedAt":"2025-09-26 22:56:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/511/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0510-specialize-functions-with-guards","title":"[Rejected] PEP 510 - Specialize functions with guards","excerpt":"Python Enhancement Proposal 510: 'Specialize functions with guards'에 대한 한국어 번역입니다.","date":"2025-09-26 22:55:51+0900","lastModifiedAt":"2025-09-26 22:55:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/510/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0509-add-a-private-version-to-dict","title":"[Superseded] PEP 509 - Add a private version to dict","excerpt":"Python Enhancement Proposal 509: 'Add a private version to dict'에 대한 한국어 번역입니다.","date":"2025-09-26 22:55:06+0900","lastModifiedAt":"2025-09-26 22:55:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/509/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0508-dependency-specification-for-python-software-packages","title":"[Final] PEP 508 - Dependency specification for Python Software Packages","excerpt":"Python Enhancement Proposal 508: 'Dependency specification for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:54:08+0900","lastModifiedAt":"2025-09-26 22:54:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/508/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0507-migrate-cpython-to-git-and-gitlab","title":"[Rejected] PEP 507 - Migrate CPython to Git and GitLab","excerpt":"Python Enhancement Proposal 507: 'Migrate CPython to Git and GitLab'에 대한 한국어 번역입니다.","date":"2025-09-26 22:53:39+0900","lastModifiedAt":"2025-09-26 22:53:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/507/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0506-adding-a-secrets-module-to-the-standard-library","title":"[Final] PEP 506 - Adding A Secrets Module To The Standard Library","excerpt":"Python Enhancement Proposal 506: 'Adding A Secrets Module To The Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 22:53:00+0900","lastModifiedAt":"2025-09-26 22:53:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/506/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0505-none-aware-operators","title":"[Deferred] PEP 505 - None-aware operators","excerpt":"Python Enhancement Proposal 505: 'None-aware operators'에 대한 한국어 번역입니다.","date":"2025-09-26 22:52:11+0900","lastModifiedAt":"2025-09-26 22:52:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/505/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0504-using-the-system-rng-by-default","title":"[Withdrawn] PEP 504 - Using the System RNG by default","excerpt":"Python Enhancement Proposal 504: 'Using the System RNG by default'에 대한 한국어 번역입니다.","date":"2025-09-26 22:47:22+0900","lastModifiedAt":"2025-09-26 22:47:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/504/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0503-simple-repository-api","title":"[Final] PEP 503 - Simple Repository API","excerpt":"Python Enhancement Proposal 503: 'Simple Repository API'에 대한 한국어 번역입니다.","date":"2025-09-26 22:45:48+0900","lastModifiedAt":"2025-09-26 22:45:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/503/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0502-string-interpolation-extended-discussion","title":"[Rejected] PEP 502 - String Interpolation - Extended Discussion","excerpt":"Python Enhancement Proposal 502: 'String Interpolation - Extended Discussion'에 대한 한국어 번역입니다.","date":"2025-09-26 22:45:25+0900","lastModifiedAt":"2025-09-26 22:45:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/502/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0501-general-purpose-template-literal-strings","title":"[Withdrawn] PEP 501 - General purpose template literal strings","excerpt":"Python Enhancement Proposal 501: 'General purpose template literal strings'에 대한 한국어 번역입니다.","date":"2025-09-26 22:44:34+0900","lastModifiedAt":"2025-09-26 22:44:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/501/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0500-a-protocol-for-delegating-datetime-methods-to-their-tzinfo-implementations","title":"[Rejected] PEP 500 - A protocol for delegating datetime methods to their tzinfo implementations","excerpt":"Python Enhancement Proposal 500: 'A protocol for delegating datetime methods to their tzinfo implementations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:43:18+0900","lastModifiedAt":"2025-09-26 22:43:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/500/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0499-python-mfooshould-also-bindfooinsys-modules","title":"[Deferred] PEP 499 - python-mfooshould also bind'foo'insys.modules","excerpt":"Python Enhancement Proposal 499: 'python-mfooshould also bind'foo'insys.modules'에 대한 한국어 번역입니다.","date":"2025-09-26 22:42:49+0900","lastModifiedAt":"2025-09-26 22:42:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/499/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0498-literal-string-interpolation","title":"[Final] PEP 498 - Literal String Interpolation","excerpt":"Python Enhancement Proposal 498: 'Literal String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:42:20+0900","lastModifiedAt":"2025-09-26 22:42:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/498/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0497-a-standard-mechanism-for-backward-compatibility","title":"[Rejected] PEP 497 - A standard mechanism for backward compatibility","excerpt":"Python Enhancement Proposal 497: 'A standard mechanism for backward compatibility'에 대한 한국어 번역입니다.","date":"2025-09-26 22:41:11+0900","lastModifiedAt":"2025-09-26 22:41:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/497/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0496-environment-markers","title":"[Rejected] PEP 496 - Environment Markers","excerpt":"Python Enhancement Proposal 496: 'Environment Markers'에 대한 한국어 번역입니다.","date":"2025-09-26 22:40:36+0900","lastModifiedAt":"2025-09-26 22:40:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/496/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0495-local-time-disambiguation","title":"[Final] PEP 495 - Local Time Disambiguation","excerpt":"Python Enhancement Proposal 495: 'Local Time Disambiguation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:40:14+0900","lastModifiedAt":"2025-09-26 22:40:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/495/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0494-python-3-6-release-schedule","title":"[Final] PEP 494 - Python 3.6 Release Schedule","excerpt":"Python Enhancement Proposal 494: 'Python 3.6 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 22:39:23+0900","lastModifiedAt":"2025-09-26 22:39:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/494/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0493-https-verification-migration-tools-for-python-2-7","title":"[Final] PEP 493 - HTTPS verification migration tools for Python 2.7","excerpt":"Python Enhancement Proposal 493: 'HTTPS verification migration tools for Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 22:38:56+0900","lastModifiedAt":"2025-09-26 22:38:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/493/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0492-coroutines-with-async-and-await-syntax","title":"[Final] PEP 492 - Coroutines with async and await syntax","excerpt":"Python Enhancement Proposal 492: 'Coroutines with async and await syntax'에 대한 한국어 번역입니다.","date":"2025-09-26 22:37:54+0900","lastModifiedAt":"2025-09-26 22:37:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/492/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0491-the-wheel-binary-package-format-1-9","title":"[Deferred] PEP 491 - The Wheel Binary Package Format 1.9","excerpt":"Python Enhancement Proposal 491: 'The Wheel Binary Package Format 1.9'에 대한 한국어 번역입니다.","date":"2025-09-26 22:36:44+0900","lastModifiedAt":"2025-09-26 22:36:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/491/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0490-chain-exceptions-at-c-level","title":"[Rejected] PEP 490 - Chain exceptions at C level","excerpt":"Python Enhancement Proposal 490: 'Chain exceptions at C level'에 대한 한국어 번역입니다.","date":"2025-09-26 22:35:41+0900","lastModifiedAt":"2025-09-26 22:35:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/490/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0489-multi-phase-extension-module-initialization","title":"[Final] PEP 489 - Multi-phase extension module initialization","excerpt":"Python Enhancement Proposal 489: 'Multi-phase extension module initialization'에 대한 한국어 번역입니다.","date":"2025-09-26 22:35:12+0900","lastModifiedAt":"2025-09-26 22:35:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/489/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0488-elimination-of-pyo-files","title":"[Final] PEP 488 - Elimination of PYO files","excerpt":"Python Enhancement Proposal 488: 'Elimination of PYO files'에 대한 한국어 번역입니다.","date":"2025-09-26 22:34:01+0900","lastModifiedAt":"2025-09-26 22:34:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/488/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0487-simpler-customisation-of-class-creation","title":"[Final] PEP 487 - Simpler customisation of class creation","excerpt":"Python Enhancement Proposal 487: 'Simpler customisation of class creation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:33:21+0900","lastModifiedAt":"2025-09-26 22:33:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/487/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0486-make-the-python-launcher-aware-of-virtual-environments","title":"[Final] PEP 486 - Make the Python Launcher aware of virtual environments","excerpt":"Python Enhancement Proposal 486: 'Make the Python Launcher aware of virtual environments'에 대한 한국어 번역입니다.","date":"2025-09-26 22:33:02+0900","lastModifiedAt":"2025-09-26 22:33:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/486/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0485-a-function-for-testing-approximate-equality","title":"[Final] PEP 485 - A Function for testing approximate equality","excerpt":"Python Enhancement Proposal 485: 'A Function for testing approximate equality'에 대한 한국어 번역입니다.","date":"2025-09-26 22:32:41+0900","lastModifiedAt":"2025-09-26 22:32:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/485/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0484-type-hints","title":"[Final] PEP 484 - Type Hints","excerpt":"Python Enhancement Proposal 484: 'Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:32:07+0900","lastModifiedAt":"2025-09-26 22:32:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/484/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0483-the-theory-of-type-hints","title":"[Final] PEP 483 - The Theory of Type Hints","excerpt":"Python Enhancement Proposal 483: 'The Theory of Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:28:39+0900","lastModifiedAt":"2025-09-26 22:28:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/483/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0482-literature-overview-for-type-hints","title":"[Final] PEP 482 - Literature Overview for Type Hints","excerpt":"Python Enhancement Proposal 482: 'Literature Overview for Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:27:40+0900","lastModifiedAt":"2025-09-26 22:27:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/482/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0481-migrate-cpython-to-git-github-and-phabricator","title":"[Withdrawn] PEP 481 - Migrate CPython to Git, Github, and Phabricator","excerpt":"Python Enhancement Proposal 481: 'Migrate CPython to Git, Github, and Phabricator'에 대한 한국어 번역입니다.","date":"2025-09-26 22:27:20+0900","lastModifiedAt":"2025-09-26 22:27:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/481/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0480-surviving-a-compromise-of-pypi-end-to-end-signing-of-packages","title":"[Draft] PEP 480 - Surviving a Compromise of PyPI: End-to-end signing of packages","excerpt":"Python Enhancement Proposal 480: 'Surviving a Compromise of PyPI: End-to-end signing of packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:26:39+0900","lastModifiedAt":"2025-09-26 22:26:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/480/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0479-change-stopiteration-handling-inside-generators","title":"[Final] PEP 479 - Change StopIteration handling inside generators","excerpt":"Python Enhancement Proposal 479: 'Change StopIteration handling inside generators'에 대한 한국어 번역입니다.","date":"2025-09-26 22:23:28+0900","lastModifiedAt":"2025-09-26 22:23:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/479/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0478-python-3-5-release-schedule","title":"[Final] PEP 478 - Python 3.5 Release Schedule","excerpt":"Python Enhancement Proposal 478: 'Python 3.5 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:47+0900","lastModifiedAt":"2025-09-26 22:22:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/478/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0477-backport-ensurepip-pep-453-to-python-2-7","title":"[Final] PEP 477 - Backport ensurepip (PEP 453) to Python 2.7","excerpt":"Python Enhancement Proposal 477: 'Backport ensurepip (PEP 453) to Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:24+0900","lastModifiedAt":"2025-09-26 22:22:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/477/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0476-enabling-certificate-verification-by-default-for-stdlib-http-clients","title":"[Final] PEP 476 - Enabling certificate verification by default for stdlib http clients","excerpt":"Python Enhancement Proposal 476: 'Enabling certificate verification by default for stdlib http clients'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:04+0900","lastModifiedAt":"2025-09-26 22:22:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/476/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0475-retry-system-calls-failing-with-eintr","title":"[Final] PEP 475 - Retry system calls failing with EINTR","excerpt":"Python Enhancement Proposal 475: 'Retry system calls failing with EINTR'에 대한 한국어 번역입니다.","date":"2025-09-26 22:21:40+0900","lastModifiedAt":"2025-09-26 22:21:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/475/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0474-creating-forge-python-org","title":"[Withdrawn] PEP 474 - Creating forge.python.org","excerpt":"Python Enhancement Proposal 474: 'Creating forge.python.org'에 대한 한국어 번역입니다.","date":"2025-09-26 22:21:06+0900","lastModifiedAt":"2025-09-26 22:21:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/474/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0473-adding-structured-data-to-built-in-exceptions","title":"[Rejected] PEP 473 - Adding structured data to built-in exceptions","excerpt":"Python Enhancement Proposal 473: 'Adding structured data to built-in exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 22:19:47+0900","lastModifiedAt":"2025-09-26 22:19:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/473/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0472-support-for-indexing-with-keyword-arguments","title":"[Rejected] PEP 472 - Support for indexing with keyword arguments","excerpt":"Python Enhancement Proposal 472: 'Support for indexing with keyword arguments'에 대한 한국어 번역입니다.","date":"2025-09-26 22:19:17+0900","lastModifiedAt":"2025-09-26 22:19:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/472/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0471-os-scandir-function-a-better-and-faster-directory-iterator","title":"[Final] PEP 471 - os.scandir() function – a better and faster directory iterator","excerpt":"Python Enhancement Proposal 471: 'os.scandir() function – a better and faster directory iterator'에 대한 한국어 번역입니다.","date":"2025-09-26 22:18:05+0900","lastModifiedAt":"2025-09-26 22:18:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/471/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0470-removing-external-hosting-support-on-pypi","title":"[Final] PEP 470 - Removing External Hosting Support on PyPI","excerpt":"Python Enhancement Proposal 470: 'Removing External Hosting Support on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 22:17:18+0900","lastModifiedAt":"2025-09-26 22:17:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/470/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0469-migration-of-dict-iteration-code-to-python-3","title":"[Withdrawn] PEP 469 - Migration of dict iteration code to Python 3","excerpt":"Python Enhancement Proposal 469: 'Migration of dict iteration code to Python 3'에 대한 한국어 번역입니다.","date":"2025-09-26 22:15:48+0900","lastModifiedAt":"2025-09-26 22:15:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/469/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0468-preserving-the-order-of-kwargs-in-a-function-","title":"[Final] PEP 468 - Preserving the order of **kwargs in a function.","excerpt":"Python Enhancement Proposal 468: 'Preserving the order of ** kwargs in a function.'에 대한 한국어 번역입니다.","date":"2025-09-26 22:15:02+0900","lastModifiedAt":"2025-09-26 22:15:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/468/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0467-minor-api-improvements-for-binary-sequences","title":"[Draft] PEP 467 - Minor API improvements for binary sequences","excerpt":"Python Enhancement Proposal 467: 'Minor API improvements for binary sequences'에 대한 한국어 번역입니다.","date":"2025-09-26 22:14:25+0900","lastModifiedAt":"2025-09-26 22:14:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/467/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0466-network-security-enhancements-for-python-2-7-x","title":"[Final] PEP 466 - Network Security Enhancements for Python 2.7.x","excerpt":"Python Enhancement Proposal 466: 'Network Security Enhancements for Python 2.7.x'에 대한 한국어 번역입니다.","date":"2025-09-26 22:13:59+0900","lastModifiedAt":"2025-09-26 22:13:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/466/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0465-a-dedicated-infix-operator-for-matrix-multiplication","title":"[Final] PEP 465 - A dedicated infix operator for matrix multiplication","excerpt":"Python Enhancement Proposal 465: 'A dedicated infix operator for matrix multiplication'에 대한 한국어 번역입니다.","date":"2025-09-26 22:12:46+0900","lastModifiedAt":"2025-09-26 22:12:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/465/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0464-removal-of-the-pypi-mirror-authenticity-api","title":"[Final] PEP 464 - Removal of the PyPI Mirror Authenticity API","excerpt":"Python Enhancement Proposal 464: 'Removal of the PyPI Mirror Authenticity API'에 대한 한국어 번역입니다.","date":"2025-09-26 22:11:47+0900","lastModifiedAt":"2025-09-26 22:11:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/464/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0463-exception-catching-expressions","title":"[Rejected] PEP 463 - Exception-catching expressions","excerpt":"Python Enhancement Proposal 463: 'Exception-catching expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 22:11:32+0900","lastModifiedAt":"2025-09-26 22:11:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/463/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0462-core-development-workflow-automation-for-cpython","title":"[Withdrawn] PEP 462 - Core development workflow automation for CPython","excerpt":"Python Enhancement Proposal 462: 'Core development workflow automation for CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 22:10:21+0900","lastModifiedAt":"2025-09-26 22:10:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/462/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0461-adding-formatting-to-bytes-and-bytearray","title":"[Final] PEP 461 - Adding % formatting to bytes and bytearray","excerpt":"Python Enhancement Proposal 461: 'Adding % formatting to bytes and bytearray'에 대한 한국어 번역입니다.","date":"2025-09-26 22:09:29+0900","lastModifiedAt":"2025-09-26 22:09:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/461/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0460-add-binary-interpolation-and-formatting","title":"[Withdrawn] PEP 460 - Add binary interpolation and formatting","excerpt":"Python Enhancement Proposal 460: 'Add binary interpolation and formatting'에 대한 한국어 번역입니다.","date":"2025-09-26 22:08:41+0900","lastModifiedAt":"2025-09-26 22:08:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/460/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0459-standard-metadata-extensions-for-python-software-packages","title":"[Withdrawn] PEP 459 - Standard Metadata Extensions for Python Software Packages","excerpt":"Python Enhancement Proposal 459: 'Standard Metadata Extensions for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:08:18+0900","lastModifiedAt":"2025-09-26 22:08:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/459/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0458-secure-pypi-downloads-with-signed-repository-metadata","title":"[Accepted] PEP 458 - Secure PyPI downloads with signed repository metadata","excerpt":"Python Enhancement Proposal 458: 'Secure PyPI downloads with signed repository metadata'에 대한 한국어 번역입니다.","date":"2025-09-26 22:07:13+0900","lastModifiedAt":"2025-09-26 22:07:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/458/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0457-notation-for-positional-only-parameters","title":"[Final] PEP 457 - Notation For Positional-Only Parameters","excerpt":"Python Enhancement Proposal 457: 'Notation For Positional-Only Parameters'에 대한 한국어 번역입니다.","date":"2025-09-26 22:06:22+0900","lastModifiedAt":"2025-09-26 22:06:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/457/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0456-secure-and-interchangeable-hash-algorithm","title":"[Final] PEP 456 - Secure and interchangeable hash algorithm","excerpt":"Python Enhancement Proposal 456: 'Secure and interchangeable hash algorithm'에 대한 한국어 번역입니다.","date":"2025-09-26 22:05:49+0900","lastModifiedAt":"2025-09-26 22:05:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/456/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0455-adding-a-key-transforming-dictionary-to-collections","title":"[Rejected] PEP 455 - Adding a key-transforming dictionary to collections","excerpt":"Python Enhancement Proposal 455: 'Adding a key-transforming dictionary to collections'에 대한 한국어 번역입니다.","date":"2025-09-26 22:05:06+0900","lastModifiedAt":"2025-09-26 22:05:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/455/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0454-add-a-new-tracemalloc-module-to-trace-python-memory-allocations","title":"[Final] PEP 454 - Add a new tracemalloc module to trace Python memory allocations","excerpt":"Python Enhancement Proposal 454: 'Add a new tracemalloc module to trace Python memory allocations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:04:39+0900","lastModifiedAt":"2025-09-26 22:04:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/454/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0453-explicit-bootstrapping-of-pip-in-python-installations","title":"[Final] PEP 453 - Explicit bootstrapping of pip in Python installations","excerpt":"Python Enhancement Proposal 453: 'Explicit bootstrapping of pip in Python installations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:03:55+0900","lastModifiedAt":"2025-09-26 22:03:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/453/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0452-api-for-cryptographic-hash-functions-v2-0","title":"[Final] PEP 452 - API for Cryptographic Hash Functions v2.0","excerpt":"Python Enhancement Proposal 452: 'API for Cryptographic Hash Functions v2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 22:03:16+0900","lastModifiedAt":"2025-09-26 22:03:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/452/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0451-a-modulespec-type-for-the-import-system","title":"[Final] PEP 451 - A ModuleSpec Type for the Import System","excerpt":"Python Enhancement Proposal 451: 'A ModuleSpec Type for the Import System'에 대한 한국어 번역입니다.","date":"2025-09-26 22:02:47+0900","lastModifiedAt":"2025-09-26 22:02:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/451/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0450-adding-a-statistics-module-to-the-standard-library","title":"[Final] PEP 450 - Adding A Statistics Module To The Standard Library","excerpt":"Python Enhancement Proposal 450: 'Adding A Statistics Module To The Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:57+0900","lastModifiedAt":"2025-09-26 22:00:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/450/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0449-removal-of-the-pypi-mirror-auto-discovery-and-naming-scheme","title":"[Final] PEP 449 - Removal of the PyPI Mirror Auto Discovery and Naming Scheme","excerpt":"Python Enhancement Proposal 449: 'Removal of the PyPI Mirror Auto Discovery and Naming Scheme'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:20+0900","lastModifiedAt":"2025-09-26 22:00:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/449/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0448-additional-unpacking-generalizations","title":"[Final] PEP 448 - Additional Unpacking Generalizations","excerpt":"Python Enhancement Proposal 448: 'Additional Unpacking Generalizations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:00+0900","lastModifiedAt":"2025-09-26 22:00:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/448/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0447-add-getdescriptor-method-to-metaclass","title":"[Deferred] PEP 447 - Add __getdescriptor__ method to metaclass","excerpt":"Python Enhancement Proposal 447: 'Add __getdescriptor__ method to metaclass'에 대한 한국어 번역입니다.","date":"2025-09-26 21:59:30+0900","lastModifiedAt":"2025-09-26 21:59:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/447/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0446-make-newly-created-file-descriptors-non-inheritable","title":"[Final] PEP 446 - Make newly created file descriptors non-inheritable","excerpt":"Python Enhancement Proposal 446: 'Make newly created file descriptors non-inheritable'에 대한 한국어 번역입니다.","date":"2025-09-26 21:58:51+0900","lastModifiedAt":"2025-09-26 21:58:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/446/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0445-add-new-apis-to-customize-python-memory-allocators","title":"[Final] PEP 445 - Add new APIs to customize Python memory allocators","excerpt":"Python Enhancement Proposal 445: 'Add new APIs to customize Python memory allocators'에 대한 한국어 번역입니다.","date":"2025-09-26 21:58:25+0900","lastModifiedAt":"2025-09-26 21:58:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/445/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0444-python-web3-interface","title":"[Deferred] PEP 444 - Python Web3 Interface","excerpt":"Python Enhancement Proposal 444: 'Python Web3 Interface'에 대한 한국어 번역입니다.","date":"2025-09-26 21:57:54+0900","lastModifiedAt":"2025-09-26 21:57:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/444/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0443-single-dispatch-generic-functions","title":"[Final] PEP 443 - Single-dispatch generic functions","excerpt":"Python Enhancement Proposal 443: 'Single-dispatch generic functions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:55:16+0900","lastModifiedAt":"2025-09-26 21:55:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/443/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0442-safe-object-finalization","title":"[Final] PEP 442 - Safe object finalization","excerpt":"Python Enhancement Proposal 442: 'Safe object finalization'에 대한 한국어 번역입니다.","date":"2025-09-26 21:54:42+0900","lastModifiedAt":"2025-09-26 21:54:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/442/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0441-improving-python-zip-application-support","title":"[Final] PEP 441 - Improving Python ZIP Application Support","excerpt":"Python Enhancement Proposal 441: 'Improving Python ZIP Application Support'에 대한 한국어 번역입니다.","date":"2025-09-26 21:54:04+0900","lastModifiedAt":"2025-09-26 21:54:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/441/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0440-version-identification-and-dependency-specification","title":"[Final] PEP 440 - Version Identification and Dependency Specification","excerpt":"Python Enhancement Proposal 440: 'Version Identification and Dependency Specification'에 대한 한국어 번역입니다.","date":"2025-09-26 21:53:38+0900","lastModifiedAt":"2025-09-26 21:53:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/440/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0439-inclusion-of-implicit-pip-bootstrap-in-python-installation","title":"[Rejected] PEP 439 - Inclusion of implicit pip bootstrap in Python installation","excerpt":"Python Enhancement Proposal 439: 'Inclusion of implicit pip bootstrap in Python installation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:52:55+0900","lastModifiedAt":"2025-09-26 21:52:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/439/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0438-transitioning-to-release-file-hosting-on-pypi","title":"[Superseded] PEP 438 - Transitioning to release-file hosting on PyPI","excerpt":"Python Enhancement Proposal 438: 'Transitioning to release-file hosting on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:52:24+0900","lastModifiedAt":"2025-09-26 21:52:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/438/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0437-a-dsl-for-specifying-signatures-annotations-and-argument-converters","title":"[Rejected] PEP 437 - A DSL for specifying signatures, annotations and argument converters","excerpt":"Python Enhancement Proposal 437: 'A DSL for specifying signatures, annotations and argument converters'에 대한 한국어 번역입니다.","date":"2025-09-26 21:51:38+0900","lastModifiedAt":"2025-09-26 21:51:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/437/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0436-the-argument-clinic-dsl","title":"[Final] PEP 436 - The Argument Clinic DSL","excerpt":"Python Enhancement Proposal 436: 'The Argument Clinic DSL'에 대한 한국어 번역입니다.","date":"2025-09-26 21:51:01+0900","lastModifiedAt":"2025-09-26 21:51:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/436/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0435-adding-an-enum-type-to-the-python-standard-library","title":"[Final] PEP 435 - Adding an Enum type to the Python standard library","excerpt":"Python Enhancement Proposal 435: 'Adding an Enum type to the Python standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:46:35+0900","lastModifiedAt":"2025-09-26 21:46:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/435/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0434-idle-enhancement-exception-for-all-branches","title":"[Active] PEP 434 - IDLE Enhancement Exception for All Branches","excerpt":"Python Enhancement Proposal 434: 'IDLE Enhancement Exception for All Branches'에 대한 한국어 번역입니다.","date":"2025-09-26 21:45:59+0900","lastModifiedAt":"2025-09-26 21:45:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/434/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0433-easier-suppression-of-file-descriptor-inheritance","title":"[Superseded] PEP 433 - Easier suppression of file descriptor inheritance","excerpt":"Python Enhancement Proposal 433: 'Easier suppression of file descriptor inheritance'에 대한 한국어 번역입니다.","date":"2025-09-26 21:45:26+0900","lastModifiedAt":"2025-09-26 21:45:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/433/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0432-restructuring-the-cpython-startup-sequence","title":"[Withdrawn] PEP 432 - Restructuring the CPython startup sequence","excerpt":"Python Enhancement Proposal 432: 'Restructuring the CPython startup sequence'에 대한 한국어 번역입니다.","date":"2025-09-26 21:44:16+0900","lastModifiedAt":"2025-09-26 21:44:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/432/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0431-time-zone-support-improvements","title":"[Superseded] PEP 431 - Time zone support improvements","excerpt":"Python Enhancement Proposal 431: 'Time zone support improvements'에 대한 한국어 번역입니다.","date":"2025-09-26 21:43:32+0900","lastModifiedAt":"2025-09-26 21:43:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/431/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0430-migrating-to-python-3-as-the-default-online-documentation","title":"[Final] PEP 430 - Migrating to Python 3 as the default online documentation","excerpt":"Python Enhancement Proposal 430: 'Migrating to Python 3 as the default online documentation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:43:01+0900","lastModifiedAt":"2025-09-26 21:43:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/430/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0429-python-3-4-release-schedule","title":"[Final] PEP 429 - Python 3.4 Release Schedule","excerpt":"Python Enhancement Proposal 429: 'Python 3.4 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:42:34+0900","lastModifiedAt":"2025-09-26 21:42:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/429/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0428-the-pathlib-module-object-oriented-filesystem-paths","title":"[Final] PEP 428 - The pathlib module – object-oriented filesystem paths","excerpt":"Python Enhancement Proposal 428: 'The pathlib module – object-oriented filesystem paths'에 대한 한국어 번역입니다.","date":"2025-09-26 21:42:10+0900","lastModifiedAt":"2025-09-26 21:42:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/428/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0427-the-wheel-binary-package-format-1-0","title":"[Final] PEP 427 - The Wheel Binary Package Format 1.0","excerpt":"Python Enhancement Proposal 427: 'The Wheel Binary Package Format 1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 21:41:28+0900","lastModifiedAt":"2025-09-26 21:41:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/427/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0426-metadata-for-python-software-packages-2-0","title":"[Withdrawn] PEP 426 - Metadata for Python Software Packages 2.0","excerpt":"Python Enhancement Proposal 426: 'Metadata for Python Software Packages 2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 21:40:33+0900","lastModifiedAt":"2025-09-26 21:40:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/426/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0425-compatibility-tags-for-built-distributions","title":"[Final] PEP 425 - Compatibility Tags for Built Distributions","excerpt":"Python Enhancement Proposal 425: 'Compatibility Tags for Built Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:40:01+0900","lastModifiedAt":"2025-09-26 21:40:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/425/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0424-a-method-for-exposing-a-length-hint","title":"[Final] PEP 424 - A method for exposing a length hint","excerpt":"Python Enhancement Proposal 424: 'A method for exposing a length hint'에 대한 한국어 번역입니다.","date":"2025-09-26 21:39:24+0900","lastModifiedAt":"2025-09-26 21:39:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/424/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0423-naming-conventions-and-recipes-related-to-packaging","title":"[Deferred] PEP 423 - Naming conventions and recipes related to packaging","excerpt":"Python Enhancement Proposal 423: 'Naming conventions and recipes related to packaging'에 대한 한국어 번역입니다.","date":"2025-09-26 21:39:09+0900","lastModifiedAt":"2025-09-26 21:39:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/423/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0422-simpler-customisation-of-class-creation","title":"[Withdrawn] PEP 422 - Simpler customisation of class creation","excerpt":"Python Enhancement Proposal 422: 'Simpler customisation of class creation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:38:32+0900","lastModifiedAt":"2025-09-26 21:38:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/422/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0421-adding-sys-implementation","title":"[Final] PEP 421 - Adding sys.implementation","excerpt":"Python Enhancement Proposal 421: 'Adding sys.implementation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:37:48+0900","lastModifiedAt":"2025-09-26 21:37:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/421/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0420-implicit-namespace-packages","title":"[Final] PEP 420 - Implicit Namespace Packages","excerpt":"Python Enhancement Proposal 420: 'Implicit Namespace Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 21:37:09+0900","lastModifiedAt":"2025-09-26 21:37:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/420/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0419-protecting-cleanup-statements-from-interruptions","title":"[Deferred] PEP 419 - Protecting cleanup statements from interruptions","excerpt":"Python Enhancement Proposal 419: 'Protecting cleanup statements from interruptions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:36:35+0900","lastModifiedAt":"2025-09-26 21:36:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/419/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0418-add-monotonic-time-performance-counter-and-process-time-functions","title":"[Final] PEP 418 - Add monotonic time, performance counter, and process time functions","excerpt":"Python Enhancement Proposal 418: 'Add monotonic time, performance counter, and process time functions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:35:49+0900","lastModifiedAt":"2025-09-26 21:35:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/418/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0417-including-mock-in-the-standard-library","title":"[Final] PEP 417 - Including mock in the Standard Library","excerpt":"Python Enhancement Proposal 417: 'Including mock in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:35:01+0900","lastModifiedAt":"2025-09-26 21:35:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/417/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0416-add-a-frozendict-builtin-type","title":"[Rejected] PEP 416 - Add a frozendict builtin type","excerpt":"Python Enhancement Proposal 416: 'Add a frozendict builtin type'에 대한 한국어 번역입니다.","date":"2025-09-26 21:34:46+0900","lastModifiedAt":"2025-09-26 21:34:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/416/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0415-implement-context-suppression-with-exception-attributes","title":"[Final] PEP 415 - Implement context suppression with exception attributes","excerpt":"Python Enhancement Proposal 415: 'Implement context suppression with exception attributes'에 대한 한국어 번역입니다.","date":"2025-09-26 21:34:10+0900","lastModifiedAt":"2025-09-26 21:34:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/415/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0414-explicit-unicode-literal-for-python-3-3","title":"[Final] PEP 414 - Explicit Unicode Literal for Python 3.3","excerpt":"Python Enhancement Proposal 414: 'Explicit Unicode Literal for Python 3.3'에 대한 한국어 번역입니다.","date":"2025-09-26 21:33:55+0900","lastModifiedAt":"2025-09-26 21:33:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/414/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0413-faster-evolution-of-the-python-standard-library","title":"[Withdrawn] PEP 413 - Faster evolution of the Python Standard Library","excerpt":"Python Enhancement Proposal 413: 'Faster evolution of the Python Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:33:07+0900","lastModifiedAt":"2025-09-26 21:33:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/413/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0412-key-sharing-dictionary","title":"[Final] PEP 412 - Key-Sharing Dictionary","excerpt":"Python Enhancement Proposal 412: 'Key-Sharing Dictionary'에 대한 한국어 번역입니다.","date":"2025-09-26 21:32:26+0900","lastModifiedAt":"2025-09-26 21:32:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/412/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0411-provisional-packages-in-the-python-standard-library","title":"[Superseded] PEP 411 - Provisional packages in the Python standard library","excerpt":"Python Enhancement Proposal 411: 'Provisional packages in the Python standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:31:48+0900","lastModifiedAt":"2025-09-26 21:31:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/411/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0410-use-decimal-decimal-type-for-timestamps","title":"[Rejected] PEP 410 - Use decimal.Decimal type for timestamps","excerpt":"Python Enhancement Proposal 410: 'Use decimal.Decimal type for timestamps'에 대한 한국어 번역입니다.","date":"2025-09-26 21:31:14+0900","lastModifiedAt":"2025-09-26 21:31:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/410/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0409-suppressing-exception-context","title":"[Final] PEP 409 - Suppressing exception context","excerpt":"Python Enhancement Proposal 409: 'Suppressing exception context'에 대한 한국어 번역입니다.","date":"2025-09-26 21:30:26+0900","lastModifiedAt":"2025-09-26 21:30:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/409/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0408-standard-library-preview-package","title":"[Rejected] PEP 408 - Standard library __preview__ package","excerpt":"Python Enhancement Proposal 408: 'Standard library __preview__ package'에 대한 한국어 번역입니다.","date":"2025-09-26 21:30:04+0900","lastModifiedAt":"2025-09-26 21:30:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/408/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0407-new-release-cycle-and-introducing-long-term-support-versions","title":"[Deferred] PEP 407 - New release cycle and introducing long-term support versions","excerpt":"Python Enhancement Proposal 407: 'New release cycle and introducing long-term support versions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:29:25+0900","lastModifiedAt":"2025-09-26 21:29:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/407/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0406-improved-encapsulation-of-import-state","title":"[Withdrawn] PEP 406 - Improved Encapsulation of Import State","excerpt":"Python Enhancement Proposal 406: 'Improved Encapsulation of Import State'에 대한 한국어 번역입니다.","date":"2025-09-26 21:29:05+0900","lastModifiedAt":"2025-09-26 21:29:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/406/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0405-python-virtual-environments","title":"[Final] PEP 405 - Python Virtual Environments","excerpt":"Python Enhancement Proposal 405: 'Python Virtual Environments'에 대한 한국어 번역입니다.","date":"2025-09-26 21:28:37+0900","lastModifiedAt":"2025-09-26 21:28:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/405/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0404-python-2-8-un-release-schedule","title":"[Final] PEP 404 - Python 2.8 Un-release Schedule","excerpt":"Python Enhancement Proposal 404: 'Python 2.8 Un-release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:27:55+0900","lastModifiedAt":"2025-09-26 21:27:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/404/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0403-general-purpose-decorator-clause-aka-in-clause","title":"[Deferred] PEP 403 - General purpose decorator clause (aka “@in” clause)","excerpt":"Python Enhancement Proposal 403: 'General purpose decorator clause (aka “@in” clause)'에 대한 한국어 번역입니다.","date":"2025-09-26 21:27:31+0900","lastModifiedAt":"2025-09-26 21:27:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/403/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0402-simplified-package-layout-and-partitioning","title":"[Rejected] PEP 402 - Simplified Package Layout and Partitioning","excerpt":"Python Enhancement Proposal 402: 'Simplified Package Layout and Partitioning'에 대한 한국어 번역입니다.","date":"2025-09-26 21:26:40+0900","lastModifiedAt":"2025-09-26 21:26:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/402/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0401-bdfl-retirement","title":"[April Fool!] PEP 401 - BDFL Retirement","excerpt":"Python Enhancement Proposal 401: 'BDFL Retirement'에 대한 한국어 번역입니다.","date":"2025-09-26 21:25:14+0900","lastModifiedAt":"2025-09-26 21:25:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/401/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0400-deprecate-codecs-streamreader-and-codecs-streamwriter","title":"[Deferred] PEP 400 - Deprecate codecs.StreamReader and codecs.StreamWriter","excerpt":"Python Enhancement Proposal 400: 'Deprecate codecs.StreamReader and codecs.StreamWriter'에 대한 한국어 번역입니다.","date":"2025-09-26 21:24:56+0900","lastModifiedAt":"2025-09-26 21:24:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/400/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0399-pure-pythonc-accelerator-module-compatibility-requirements","title":"[Final] PEP 399 - Pure Python/C Accelerator Module Compatibility Requirements","excerpt":"Python Enhancement Proposal 399: 'Pure Python/C Accelerator Module Compatibility Requirements'에 대한 한국어 번역입니다.","date":"2025-09-26 21:24:25+0900","lastModifiedAt":"2025-09-26 21:24:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/399/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0398-python-3-3-release-schedule","title":"[Final] PEP 398 - Python 3.3 Release Schedule","excerpt":"Python Enhancement Proposal 398: 'Python 3.3 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:23:58+0900","lastModifiedAt":"2025-09-26 21:23:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/398/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0397-python-launcher-for-windows","title":"[Final] PEP 397 - Python launcher for Windows","excerpt":"Python Enhancement Proposal 397: 'Python launcher for Windows'에 대한 한국어 번역입니다.","date":"2025-09-26 21:23:27+0900","lastModifiedAt":"2025-09-26 21:23:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/397/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0396-module-version-numbers","title":"[Withdrawn] PEP 396 - Module Version Numbers","excerpt":"Python Enhancement Proposal 396: 'Module Version Numbers'에 대한 한국어 번역입니다.","date":"2025-09-26 21:22:37+0900","lastModifiedAt":"2025-09-26 21:22:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/396/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0395-qualified-names-for-modules","title":"[Withdrawn] PEP 395 - Qualified Names for Modules","excerpt":"Python Enhancement Proposal 395: 'Qualified Names for Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 21:22:00+0900","lastModifiedAt":"2025-09-26 21:22:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/395/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0394-the-python-command-on-unix-like-systems","title":"[Active] PEP 394 - The “python” Command on Unix-Like Systems","excerpt":"Python Enhancement Proposal 394: 'The “python” Command on Unix-Like Systems'에 대한 한국어 번역입니다.","date":"2025-09-26 21:21:20+0900","lastModifiedAt":"2025-09-26 21:21:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/394/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0393-flexible-string-representation","title":"[Final] PEP 393 - Flexible String Representation","excerpt":"Python Enhancement Proposal 393: 'Flexible String Representation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:20:21+0900","lastModifiedAt":"2025-09-26 21:20:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/393/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0392-python-3-2-release-schedule","title":"[Final] PEP 392 - Python 3.2 Release Schedule","excerpt":"Python Enhancement Proposal 392: 'Python 3.2 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:19:41+0900","lastModifiedAt":"2025-09-26 21:19:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/392/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0391-dictionary-based-configuration-for-logging","title":"[Final] PEP 391 - Dictionary-Based Configuration For Logging","excerpt":"Python Enhancement Proposal 391: 'Dictionary-Based Configuration For Logging'에 대한 한국어 번역입니다.","date":"2025-09-26 21:19:23+0900","lastModifiedAt":"2025-09-26 21:19:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/391/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0390-static-metadata-for-distutils","title":"[Rejected] PEP 390 - Static metadata for Distutils","excerpt":"Python Enhancement Proposal 390: 'Static metadata for Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 21:09:17+0900","lastModifiedAt":"2025-09-26 21:09:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/390/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0389-argparse-new-command-line-parsing-module","title":"[Final] PEP 389 - argparse - New Command Line Parsing Module","excerpt":"Python Enhancement Proposal 389: 'argparse - New Command Line Parsing Module'에 대한 한국어 번역입니다.","date":"2025-09-26 21:08:50+0900","lastModifiedAt":"2025-09-26 21:08:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/389/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0387-backwards-compatibility-policy","title":"[Active] PEP 387 - Backwards Compatibility Policy","excerpt":"Python Enhancement Proposal 387: 'Backwards Compatibility Policy'에 대한 한국어 번역입니다.","date":"2025-09-26 21:07:43+0900","lastModifiedAt":"2025-09-26 21:07:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/387/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0386-changing-the-version-comparison-module-in-distutils","title":"[Superseded] PEP 386 - Changing the version comparison module in Distutils","excerpt":"Python Enhancement Proposal 386: 'Changing the version comparison module in Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 21:07:16+0900","lastModifiedAt":"2025-09-26 21:07:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/386/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0385-migrating-from-subversion-to-mercurial","title":"[Final] PEP 385 - Migrating from Subversion to Mercurial","excerpt":"Python Enhancement Proposal 385: 'Migrating from Subversion to Mercurial'에 대한 한국어 번역입니다.","date":"2025-09-26 21:05:29+0900","lastModifiedAt":"2025-09-26 21:05:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/385/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0384-defining-a-stable-abi","title":"[Final] PEP 384 - Defining a Stable ABI","excerpt":"Python Enhancement Proposal 384: 'Defining a Stable ABI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:04:32+0900","lastModifiedAt":"2025-09-26 21:04:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/384/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0383-non-decodable-bytes-in-system-character-interfaces","title":"[Final] PEP 383 - Non-decodable Bytes in System Character Interfaces","excerpt":"Python Enhancement Proposal 383: 'Non-decodable Bytes in System Character Interfaces'에 대한 한국어 번역입니다.","date":"2025-09-26 21:03:51+0900","lastModifiedAt":"2025-09-26 21:03:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/383/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0382-namespace-packages","title":"[Rejected] PEP 382 - Namespace Packages","excerpt":"Python Enhancement Proposal 382: 'Namespace Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 21:03:27+0900","lastModifiedAt":"2025-09-26 21:03:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/382/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0381-mirroring-infrastructure-for-pypi","title":"[Withdrawn] PEP 381 - Mirroring infrastructure for PyPI","excerpt":"Python Enhancement Proposal 381: 'Mirroring infrastructure for PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:02:58+0900","lastModifiedAt":"2025-09-26 21:02:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/381/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0380-syntax-for-delegating-to-a-subgenerator","title":"[Final] PEP 380 - Syntax for Delegating to a Subgenerator","excerpt":"Python Enhancement Proposal 380: 'Syntax for Delegating to a Subgenerator'에 대한 한국어 번역입니다.","date":"2025-09-26 21:02:19+0900","lastModifiedAt":"2025-09-26 21:02:19+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/380/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0379-adding-an-assignment-expression","title":"[Withdrawn] PEP 379 - Adding an Assignment Expression","excerpt":"Python Enhancement Proposal 379: 'Adding an Assignment Expression'에 대한 한국어 번역입니다.","date":"2025-09-26 21:01:34+0900","lastModifiedAt":"2025-09-26 21:01:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/379/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0378-format-specifier-for-thousands-separator","title":"[Final] PEP 378 - Format Specifier for Thousands Separator","excerpt":"Python Enhancement Proposal 378: 'Format Specifier for Thousands Separator'에 대한 한국어 번역입니다.","date":"2025-09-26 21:01:13+0900","lastModifiedAt":"2025-09-26 21:01:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/378/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0377-allow-enter-methods-to-skip-the-statement-body","title":"[Rejected] PEP 377 - Allow __enter__() methods to skip the statement body","excerpt":"Python Enhancement Proposal 377: 'Allow __enter__() methods to skip the statement body'에 대한 한국어 번역입니다.","date":"2025-09-26 21:00:13+0900","lastModifiedAt":"2025-09-26 21:00:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/377/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0376-database-of-installed-python-distributions","title":"[Final] PEP 376 - Database of Installed Python Distributions","excerpt":"Python Enhancement Proposal 376: 'Database of Installed Python Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:42+0900","lastModifiedAt":"2025-09-26 20:59:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/376/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0375-python-3-1-release-schedule","title":"[Final] PEP 375 - Python 3.1 Release Schedule","excerpt":"Python Enhancement Proposal 375: 'Python 3.1 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:16+0900","lastModifiedAt":"2025-09-26 20:59:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/375/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0374-choosing-a-distributed-vcs-for-the-python-project","title":"[Final] PEP 374 - Choosing a distributed VCS for the Python project","excerpt":"Python Enhancement Proposal 374: 'Choosing a distributed VCS for the Python project'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:02+0900","lastModifiedAt":"2025-09-26 20:59:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/374/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0373-python-2-7-release-schedule","title":"[Final] PEP 373 - Python 2.7 Release Schedule","excerpt":"Python Enhancement Proposal 373: 'Python 2.7 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 20:56:29+0900","lastModifiedAt":"2025-09-26 20:56:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/373/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0372-adding-an-ordered-dictionary-to-collections","title":"[Final] PEP 372 - Adding an ordered dictionary to collections","excerpt":"Python Enhancement Proposal 372: 'Adding an ordered dictionary to collections'에 대한 한국어 번역입니다.","date":"2025-09-26 20:56:08+0900","lastModifiedAt":"2025-09-26 20:56:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/372/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0371-addition-of-the-multiprocessing-package-to-the-standard-library","title":"[Final] PEP 371 - Addition of the multiprocessing package to the standard library","excerpt":"Python Enhancement Proposal 371: 'Addition of the multiprocessing package to the standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 20:55:33+0900","lastModifiedAt":"2025-09-26 20:55:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/371/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0370-per-user-site-packages-directory","title":"[Final] PEP 370 - Per user site-packages directory","excerpt":"Python Enhancement Proposal 370: 'Per user site-packages directory'에 대한 한국어 번역입니다.","date":"2025-09-26 20:54:08+0900","lastModifiedAt":"2025-09-26 20:54:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/370/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0369-post-import-hooks","title":"[Withdrawn] PEP 369 - Post import hooks","excerpt":"Python Enhancement Proposal 369: 'Post import hooks'에 대한 한국어 번역입니다.","date":"2025-09-26 20:53:39+0900","lastModifiedAt":"2025-09-26 20:53:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/369/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0368-standard-image-protocol-and-class","title":"[Deferred] PEP 368 - Standard image protocol and class","excerpt":"Python Enhancement Proposal 368: 'Standard image protocol and class'에 대한 한국어 번역입니다.","date":"2025-09-26 20:53:03+0900","lastModifiedAt":"2025-09-26 20:53:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/368/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0367-new-super","title":"[Superseded] PEP 367 - New Super","excerpt":"Python Enhancement Proposal 367: 'New Super'에 대한 한국어 번역입니다.","date":"2025-09-26 20:50:02+0900","lastModifiedAt":"2025-09-26 20:50:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/367/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0366-main-module-explicit-relative-imports","title":"[Final] PEP 366 - Main module explicit relative imports","excerpt":"Python Enhancement Proposal 366: 'Main module explicit relative imports'에 대한 한국어 번역입니다.","date":"2025-09-26 20:49:35+0900","lastModifiedAt":"2025-09-26 20:49:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/366/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0329-treating-builtins-as-constants-in-the-standard-library","title":"[Rejected] PEP 329 - Treating Builtins as Constants in the Standard Library","excerpt":"Python Enhancement Proposal 329: 'Treating Builtins as Constants in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 20:37:36+0900","lastModifiedAt":"2025-09-26 20:37:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/329/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0326-a-case-for-top-and-bottom-values","title":"[Rejected] PEP 326 - A Case for Top and Bottom Values","excerpt":"Python Enhancement Proposal 326: 'A Case for Top and Bottom Values'에 대한 한국어 번역입니다.","date":"2025-09-26 20:37:04+0900","lastModifiedAt":"2025-09-26 20:37:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/326/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0365-adding-the-pkg-resources-module","title":"[Rejected] PEP 365 - Adding the pkg_resources module","excerpt":"Python Enhancement Proposal 365: 'Adding the pkg_resources module'에 대한 한국어 번역입니다.","date":"2025-09-26 19:09:15+0900","lastModifiedAt":"2025-09-26 19:09:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/365/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0364-transitioning-to-the-py3k-standard-library","title":"[Withdrawn] PEP 364 - Transitioning to the Py3K Standard Library","excerpt":"Python Enhancement Proposal 364: 'Transitioning to the Py3K Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:59+0900","lastModifiedAt":"2025-09-26 19:08:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/364/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0363-syntax-for-dynamic-attribute-access","title":"[Rejected] PEP 363 - Syntax For Dynamic Attribute Access","excerpt":"Python Enhancement Proposal 363: 'Syntax For Dynamic Attribute Access'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:27+0900","lastModifiedAt":"2025-09-26 19:08:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/363/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0362-function-signature-object","title":"[Final] PEP 362 - Function Signature Object","excerpt":"Python Enhancement Proposal 362: 'Function Signature Object'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:03+0900","lastModifiedAt":"2025-09-26 19:08:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/362/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0361-python-2-6-and-3-0-release-schedule","title":"[Final] PEP 361 - Python 2.6 and 3.0 Release Schedule","excerpt":"Python Enhancement Proposal 361: 'Python 2.6 and 3.0 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 19:07:30+0900","lastModifiedAt":"2025-09-26 19:07:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/361/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0360-externally-maintained-packages","title":"[Final] PEP 360 - Externally Maintained Packages","excerpt":"Python Enhancement Proposal 360: 'Externally Maintained Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 19:07:10+0900","lastModifiedAt":"2025-09-26 19:07:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/360/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0359-the-make-statement","title":"[Withdrawn] PEP 359 - The “make” Statement","excerpt":"Python Enhancement Proposal 359: 'The “make” Statement'에 대한 한국어 번역입니다.","date":"2025-09-26 19:06:53+0900","lastModifiedAt":"2025-09-26 19:06:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/359/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0358-the-bytes-object","title":"[Final] PEP 358 - The “bytes” Object","excerpt":"Python Enhancement Proposal 358: 'The “bytes” Object'에 대한 한국어 번역입니다.","date":"2025-09-26 19:04:13+0900","lastModifiedAt":"2025-09-26 19:04:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/358/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0357-allowing-any-object-to-be-used-for-slicing","title":"[Final] PEP 357 - Allowing Any Object to be Used for Slicing","excerpt":"Python Enhancement Proposal 357: 'Allowing Any Object to be Used for Slicing'에 대한 한국어 번역입니다.","date":"2025-09-26 19:03:31+0900","lastModifiedAt":"2025-09-26 19:03:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/357/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0356-python-2-5-release-schedule","title":"[Final] PEP 356 - Python 2.5 Release Schedule","excerpt":"Python Enhancement Proposal 356: 'Python 2.5 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 19:03:07+0900","lastModifiedAt":"2025-09-26 19:03:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/356/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0354-enumerations-in-python","title":"[Superseded] PEP 354 - Enumerations in Python","excerpt":"Python Enhancement Proposal 354: 'Enumerations in Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:58+0900","lastModifiedAt":"2025-09-26 18:59:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/354/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0353-using-ssize-t-as-the-index-type","title":"[Final] PEP 353 - Using ssize_t as the index type","excerpt":"Python Enhancement Proposal 353: 'Using ssize_t as the index type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:36+0900","lastModifiedAt":"2025-09-26 18:59:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/353/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0352-required-superclass-for-exceptions","title":"[Final] PEP 352 - Required Superclass for Exceptions","excerpt":"Python Enhancement Proposal 352: 'Required Superclass for Exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:06+0900","lastModifiedAt":"2025-09-26 18:59:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/352/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0351-the-freeze-protocol","title":"[Rejected] PEP 351 - The freeze protocol","excerpt":"Python Enhancement Proposal 351: 'The freeze protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 18:58:26+0900","lastModifiedAt":"2025-09-26 18:58:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/351/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0350-codetags","title":"[Rejected] PEP 350 - Codetags","excerpt":"Python Enhancement Proposal 350: 'Codetags'에 대한 한국어 번역입니다.","date":"2025-09-26 18:58:11+0900","lastModifiedAt":"2025-09-26 18:58:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/350/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0349-allow-str-to-return-unicode-strings","title":"[Rejected] PEP 349 - Allow str() to return unicode strings","excerpt":"Python Enhancement Proposal 349: 'Allow str() to return unicode strings'에 대한 한국어 번역입니다.","date":"2025-09-26 18:57:18+0900","lastModifiedAt":"2025-09-26 18:57:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/349/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0348-exception-reorganization-for-python-3-0","title":"[Rejected] PEP 348 - Exception Reorganization for Python 3.0","excerpt":"Python Enhancement Proposal 348: 'Exception Reorganization for Python 3.0'에 대한 한국어 번역입니다.","date":"2025-09-26 18:56:58+0900","lastModifiedAt":"2025-09-26 18:56:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/348/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0347-migrating-the-python-cvs-to-subversion","title":"[Final] PEP 347 - Migrating the Python CVS to Subversion","excerpt":"Python Enhancement Proposal 347: 'Migrating the Python CVS to Subversion'에 대한 한국어 번역입니다.","date":"2025-09-26 18:56:24+0900","lastModifiedAt":"2025-09-26 18:56:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/347/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0346-user-defined-with-statements","title":"[Withdrawn] PEP 346 - User Defined (”with”) Statements","excerpt":"Python Enhancement Proposal 346: 'User Defined (”with”) Statements'에 대한 한국어 번역입니다.","date":"2025-09-26 18:55:56+0900","lastModifiedAt":"2025-09-26 18:55:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/346/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0345-metadata-for-python-software-packages-1-2","title":"[Superseded] PEP 345 - Metadata for Python Software Packages 1.2","excerpt":"Python Enhancement Proposal 345: 'Metadata for Python Software Packages 1.2'에 대한 한국어 번역입니다.","date":"2025-09-26 18:54:36+0900","lastModifiedAt":"2025-09-26 18:54:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/345/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0344-exception-chaining-and-embedded-tracebacks","title":"[Superseded] PEP 344 - Exception Chaining and Embedded Tracebacks","excerpt":"Python Enhancement Proposal 344: 'Exception Chaining and Embedded Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:54:00+0900","lastModifiedAt":"2025-09-26 18:54:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/344/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0342-coroutines-via-enhanced-generators","title":"[Final] PEP 342 - Coroutines via Enhanced Generators","excerpt":"Python Enhancement Proposal 342: 'Coroutines via Enhanced Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:58+0900","lastModifiedAt":"2025-09-26 18:48:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/342/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0341-unifying-try-except-and-try-finally","title":"[Final] PEP 341 - Unifying try-except and try-finally","excerpt":"Python Enhancement Proposal 341: 'Unifying try-except and try-finally'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:18+0900","lastModifiedAt":"2025-09-26 18:48:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/341/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0340-anonymous-block-statements","title":"[Rejected] PEP 340 - Anonymous Block Statements","excerpt":"Python Enhancement Proposal 340: 'Anonymous Block Statements'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:04+0900","lastModifiedAt":"2025-09-26 18:48:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/340/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0339-design-of-the-cpython-compiler","title":"[Withdrawn] PEP 339 - Design of the CPython Compiler","excerpt":"Python Enhancement Proposal 339: 'Design of the CPython Compiler'에 대한 한국어 번역입니다.","date":"2025-09-26 18:46:48+0900","lastModifiedAt":"2025-09-26 18:46:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/339/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0338-executing-modules-as-scripts","title":"[Final] PEP 338 - Executing modules as scripts","excerpt":"Python Enhancement Proposal 338: 'Executing modules as scripts'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:56+0900","lastModifiedAt":"2025-09-26 18:45:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/338/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0337-logging-usage-in-the-standard-library","title":"[Deferred] PEP 337 - Logging Usage in the Standard Library","excerpt":"Python Enhancement Proposal 337: 'Logging Usage in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:28+0900","lastModifiedAt":"2025-09-26 18:45:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/337/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0336-make-none-callable","title":"[Rejected] PEP 336 - Make None Callable","excerpt":"Python Enhancement Proposal 336: 'Make None Callable'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:11+0900","lastModifiedAt":"2025-09-26 18:45:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/336/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0334-simple-coroutines-via-suspenditeration","title":"[Withdrawn] PEP 334 - Simple Coroutines via SuspendIteration","excerpt":"Python Enhancement Proposal 334: 'Simple Coroutines via SuspendIteration'에 대한 한국어 번역입니다.","date":"2025-09-26 18:43:20+0900","lastModifiedAt":"2025-09-26 18:43:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/334/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0333-python-web-server-gateway-interface-v1-0","title":"[Final] PEP 333 - Python Web Server Gateway Interface v1.0","excerpt":"Python Enhancement Proposal 333: 'Python Web Server Gateway Interface v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 18:42:49+0900","lastModifiedAt":"2025-09-26 18:42:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/333/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0332-byte-vectors-and-stringunicode-unification","title":"[Rejected] PEP 332 - Byte vectors and String/Unicode Unification","excerpt":"Python Enhancement Proposal 332: 'Byte vectors and String/Unicode Unification'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:55+0900","lastModifiedAt":"2025-09-26 18:38:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/332/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0331-locale-independent-floatstring-conversions","title":"[Final] PEP 331 - Locale-Independent Float/String Conversions","excerpt":"Python Enhancement Proposal 331: 'Locale-Independent Float/String Conversions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:41+0900","lastModifiedAt":"2025-09-26 18:38:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/331/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0330-python-bytecode-verification","title":"[Rejected] PEP 330 - Python Bytecode Verification","excerpt":"Python Enhancement Proposal 330: 'Python Bytecode Verification'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:18+0900","lastModifiedAt":"2025-09-26 18:38:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/330/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0328-imports-multi-line-and-absoluterelative","title":"[Final] PEP 328 - Imports: Multi-Line and Absolute/Relative","excerpt":"Python Enhancement Proposal 328: 'Imports: Multi-Line and Absolute/Relative'에 대한 한국어 번역입니다.","date":"2025-09-26 18:36:33+0900","lastModifiedAt":"2025-09-26 18:36:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/328/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0327-decimal-data-type","title":"[Final] PEP 327 - Decimal Data Type","excerpt":"Python Enhancement Proposal 327: 'Decimal Data Type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:35:48+0900","lastModifiedAt":"2025-09-26 18:35:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/327/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0325-resource-release-support-for-generators","title":"[Rejected] PEP 325 - Resource-Release Support for Generators","excerpt":"Python Enhancement Proposal 325: 'Resource-Release Support for Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:57+0900","lastModifiedAt":"2025-09-26 18:31:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/325/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0324-subprocess-new-process-module","title":"[Final] PEP 324 - subprocess - New process module","excerpt":"Python Enhancement Proposal 324: 'subprocess - New process module'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:27+0900","lastModifiedAt":"2025-09-26 18:31:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/324/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0323-copyable-iterators","title":"[Deferred] PEP 323 - Copyable Iterators","excerpt":"Python Enhancement Proposal 323: 'Copyable Iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:01+0900","lastModifiedAt":"2025-09-26 18:31:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/323/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0322-reverse-iteration","title":"[Final] PEP 322 - Reverse Iteration","excerpt":"Python Enhancement Proposal 322: 'Reverse Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 18:30:23+0900","lastModifiedAt":"2025-09-26 18:30:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/322/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0321-datetime-parsing-and-formatting","title":"[Withdrawn] PEP 321 - Date/Time Parsing and Formatting","excerpt":"Python Enhancement Proposal 321: 'Date/Time Parsing and Formatting'에 대한 한국어 번역입니다.","date":"2025-09-26 18:30:01+0900","lastModifiedAt":"2025-09-26 18:30:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/321/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0320-python-2-4-release-schedule","title":"[Final] PEP 320 - Python 2.4 Release Schedule","excerpt":"Python Enhancement Proposal 320: 'Python 2.4 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 18:29:42+0900","lastModifiedAt":"2025-09-26 18:29:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/320/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0319-python-synchronizeasynchronize-block","title":"[Rejected] PEP 319 - Python Synchronize/Asynchronize Block","excerpt":"Python Enhancement Proposal 319: 'Python Synchronize/Asynchronize Block'에 대한 한국어 번역입니다.","date":"2025-09-26 18:29:20+0900","lastModifiedAt":"2025-09-26 18:29:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/319/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0318-decorators-for-functions-and-methods","title":"[Final] PEP 318 - Decorators for Functions and Methods","excerpt":"Python Enhancement Proposal 318: 'Decorators for Functions and Methods'에 대한 한국어 번역입니다.","date":"2025-09-26 18:28:53+0900","lastModifiedAt":"2025-09-26 18:28:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/318/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0317-eliminate-implicit-exception-instantiation","title":"[Rejected] PEP 317 - Eliminate Implicit Exception Instantiation","excerpt":"Python Enhancement Proposal 317: 'Eliminate Implicit Exception Instantiation'에 대한 한국어 번역입니다.","date":"2025-09-26 18:28:16+0900","lastModifiedAt":"2025-09-26 18:28:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/317/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0316-programming-by-contract-for-python","title":"[Deferred] PEP 316 - Programming by Contract for Python","excerpt":"Python Enhancement Proposal 316: 'Programming by Contract for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:27:40+0900","lastModifiedAt":"2025-09-26 18:27:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/316/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0315-enhanced-while-loop","title":"[Rejected] PEP 315 - Enhanced While Loop","excerpt":"Python Enhancement Proposal 315: 'Enhanced While Loop'에 대한 한국어 번역입니다.","date":"2025-09-26 18:13:38+0900","lastModifiedAt":"2025-09-26 18:13:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/315/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0314-metadata-for-python-software-packages-1-1","title":"[Superseded] PEP 314 - Metadata for Python Software Packages 1.1","excerpt":"Python Enhancement Proposal 314: 'Metadata for Python Software Packages 1.1'에 대한 한국어 번역입니다.","date":"2025-09-26 18:13:21+0900","lastModifiedAt":"2025-09-26 18:13:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/314/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0313-adding-roman-numeral-literals-to-python","title":"[Rejected] PEP 313 - Adding Roman Numeral Literals to Python","excerpt":"Python Enhancement Proposal 313: 'Adding Roman Numeral Literals to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:44+0900","lastModifiedAt":"2025-09-26 18:12:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/313/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0312-simple-implicit-lambda","title":"[Deferred] PEP 312 - Simple Implicit Lambda","excerpt":"Python Enhancement Proposal 312: 'Simple Implicit Lambda'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:24+0900","lastModifiedAt":"2025-09-26 18:12:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/312/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0311-simplified-global-interpreter-lock-acquisition-for-extensions","title":"[Final] PEP 311 - Simplified Global Interpreter Lock Acquisition for Extensions","excerpt":"Python Enhancement Proposal 311: 'Simplified Global Interpreter Lock Acquisition for Extensions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:03+0900","lastModifiedAt":"2025-09-26 18:12:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/311/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0310-reliable-acquisitionrelease-pairs","title":"[Rejected] PEP 310 - Reliable Acquisition/Release Pairs","excerpt":"Python Enhancement Proposal 310: 'Reliable Acquisition/Release Pairs'에 대한 한국어 번역입니다.","date":"2025-09-26 18:11:37+0900","lastModifiedAt":"2025-09-26 18:11:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/310/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0309-partial-function-application","title":"[Final] PEP 309 - Partial Function Application","excerpt":"Python Enhancement Proposal 309: 'Partial Function Application'에 대한 한국어 번역입니다.","date":"2025-09-26 18:11:12+0900","lastModifiedAt":"2025-09-26 18:11:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/309/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0308-conditional-expressions","title":"[Final] PEP 308 - Conditional Expressions","excerpt":"Python Enhancement Proposal 308: 'Conditional Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:10:46+0900","lastModifiedAt":"2025-09-26 18:10:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/308/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0307-extensions-to-the-pickle-protocol","title":"[Final] PEP 307 - Extensions to the pickle protocol","excerpt":"Python Enhancement Proposal 307: 'Extensions to the pickle protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 18:10:28+0900","lastModifiedAt":"2025-09-26 18:10:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/307/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0306-how-to-change-pythons-grammar","title":"[Withdrawn] PEP 306 - How to Change Python’s Grammar","excerpt":"Python Enhancement Proposal 306: 'How to Change Python’s Grammar'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:57+0900","lastModifiedAt":"2025-09-26 18:09:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/306/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0305-csv-file-api","title":"[Final] PEP 305 - CSV File API","excerpt":"Python Enhancement Proposal 305: 'CSV File API'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:42+0900","lastModifiedAt":"2025-09-26 18:09:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/305/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0304-controlling-generation-of-bytecode-files","title":"[Withdrawn] PEP 304 - Controlling Generation of Bytecode Files","excerpt":"Python Enhancement Proposal 304: 'Controlling Generation of Bytecode Files'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:05+0900","lastModifiedAt":"2025-09-26 18:09:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/304/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0303-extend-divmod-for-multiple-divisors","title":"[Rejected] PEP 303 - Extend divmod() for Multiple Divisors","excerpt":"Python Enhancement Proposal 303: 'Extend divmod() for Multiple Divisors'에 대한 한국어 번역입니다.","date":"2025-09-26 18:08:34+0900","lastModifiedAt":"2025-09-26 18:08:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/303/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0302-new-import-hooks","title":"[Final] PEP 302 - New Import Hooks","excerpt":"Python Enhancement Proposal 302: 'New Import Hooks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:08:12+0900","lastModifiedAt":"2025-09-26 18:08:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/302/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0301-package-index-and-metadata-for-distutils","title":"[Final] PEP 301 - Package Index and Metadata for Distutils","excerpt":"Python Enhancement Proposal 301: 'Package Index and Metadata for Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 18:07:29+0900","lastModifiedAt":"2025-09-26 18:07:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/301/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0299-special-main-function-in-modules","title":"[Rejected] PEP 299 - Special __main__() function in modules","excerpt":"Python Enhancement Proposal 299: 'Special __main__() function in modules'에 대한 한국어 번역입니다.","date":"2025-09-26 18:07:00+0900","lastModifiedAt":"2025-09-26 18:07:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/299/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0298-the-locked-buffer-interface","title":"[Withdrawn] PEP 298 - The Locked Buffer Interface","excerpt":"Python Enhancement Proposal 298: 'The Locked Buffer Interface'에 대한 한국어 번역입니다.","date":"2025-09-26 18:06:42+0900","lastModifiedAt":"2025-09-26 18:06:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/298/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0297-support-for-system-upgrades","title":"[Rejected] PEP 297 - Support for System Upgrades","excerpt":"Python Enhancement Proposal 297: 'Support for System Upgrades'에 대한 한국어 번역입니다.","date":"2025-09-26 18:06:11+0900","lastModifiedAt":"2025-09-26 18:06:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/297/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0296-adding-a-bytes-object-type","title":"[Withdrawn] PEP 296 - Adding a bytes Object Type","excerpt":"Python Enhancement Proposal 296: 'Adding a bytes Object Type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:57+0900","lastModifiedAt":"2025-09-26 18:05:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/296/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0295-interpretation-of-multiline-string-constants","title":"[Rejected] PEP 295 - Interpretation of multiline string constants","excerpt":"Python Enhancement Proposal 295: 'Interpretation of multiline string constants'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:22+0900","lastModifiedAt":"2025-09-26 18:05:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/295/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0294-type-names-in-the-types-module","title":"[Rejected] PEP 294 - Type Names in the types Module","excerpt":"Python Enhancement Proposal 294: 'Type Names in the types Module'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:08+0900","lastModifiedAt":"2025-09-26 18:05:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/294/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0293-codec-error-handling-callbacks","title":"[Final] PEP 293 - Codec Error Handling Callbacks","excerpt":"Python Enhancement Proposal 293: 'Codec Error Handling Callbacks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:55+0900","lastModifiedAt":"2025-09-26 18:04:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/293/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0292-simpler-string-substitutions","title":"[Final] PEP 292 - Simpler String Substitutions","excerpt":"Python Enhancement Proposal 292: 'Simpler String Substitutions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:32+0900","lastModifiedAt":"2025-09-26 18:04:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/292/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0291-backward-compatibility-for-the-python-2-standard-library","title":"[Superseded] PEP 291 - Backward Compatibility for the Python 2 Standard Library","excerpt":"Python Enhancement Proposal 291: 'Backward Compatibility for the Python 2 Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:05+0900","lastModifiedAt":"2025-09-26 18:04:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/291/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0290-code-migration-and-modernization","title":"[Active] PEP 290 - Code Migration and Modernization","excerpt":"Python Enhancement Proposal 290: 'Code Migration and Modernization'에 대한 한국어 번역입니다.","date":"2025-09-26 18:00:45+0900","lastModifiedAt":"2025-09-26 18:00:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/290/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0289-generator-expressions","title":"[Final] PEP 289 - Generator Expressions","excerpt":"Python Enhancement Proposal 289: 'Generator Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:59+0900","lastModifiedAt":"2025-09-26 17:59:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/289/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0288-generators-attributes-and-exceptions","title":"[Withdrawn] PEP 288 - Generators Attributes and Exceptions","excerpt":"Python Enhancement Proposal 288: 'Generators Attributes and Exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:36+0900","lastModifiedAt":"2025-09-26 17:59:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/288/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0287-restructuredtext-docstring-format","title":"[Active] PEP 287 - reStructuredText Docstring Format","excerpt":"Python Enhancement Proposal 287: 'reStructuredText Docstring Format'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:15+0900","lastModifiedAt":"2025-09-26 17:59:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/287/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0286-enhanced-argument-tuples","title":"[Deferred] PEP 286 - Enhanced Argument Tuples","excerpt":"Python Enhancement Proposal 286: 'Enhanced Argument Tuples'에 대한 한국어 번역입니다.","date":"2025-09-26 17:58:16+0900","lastModifiedAt":"2025-09-26 17:58:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/286/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0285-adding-a-bool-type","title":"[Final] PEP 285 - Adding a bool type","excerpt":"Python Enhancement Proposal 285: 'Adding a bool type'에 대한 한국어 번역입니다.","date":"2025-09-26 17:58:00+0900","lastModifiedAt":"2025-09-26 17:58:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/285/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0284-integer-for-loops","title":"[Rejected] PEP 284 - Integer for-loops","excerpt":"Python Enhancement Proposal 284: 'Integer for-loops'에 대한 한국어 번역입니다.","date":"2025-09-26 17:57:40+0900","lastModifiedAt":"2025-09-26 17:57:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/284/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0283-python-2-3-release-schedule","title":"[Final] PEP 283 - Python 2.3 Release Schedule","excerpt":"Python Enhancement Proposal 283: 'Python 2.3 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 17:57:14+0900","lastModifiedAt":"2025-09-26 17:57:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/283/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0282-a-logging-system","title":"[Final] PEP 282 - A Logging System","excerpt":"Python Enhancement Proposal 282: 'A Logging System'에 대한 한국어 번역입니다.","date":"2025-09-26 17:56:46+0900","lastModifiedAt":"2025-09-26 17:56:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/282/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0281-loop-counter-iteration-with-range-and-xrange","title":"[Rejected] PEP 281 - Loop Counter Iteration with range and xrange","excerpt":"Python Enhancement Proposal 281: 'Loop Counter Iteration with range and xrange'에 대한 한국어 번역입니다.","date":"2025-09-26 17:56:00+0900","lastModifiedAt":"2025-09-26 17:56:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/281/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0280-optimizing-access-to-globals","title":"[Deferred] PEP 280 - Optimizing access to globals","excerpt":"Python Enhancement Proposal 280: 'Optimizing access to globals'에 대한 한국어 번역입니다.","date":"2025-09-26 17:55:42+0900","lastModifiedAt":"2025-09-26 17:55:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/280/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0279-the-enumerate-built-in-function","title":"[Final] PEP 279 - The enumerate() built-in function","excerpt":"Python Enhancement Proposal 279: 'The enumerate() built-in function'에 대한 한국어 번역입니다.","date":"2025-09-26 17:55:04+0900","lastModifiedAt":"2025-09-26 17:55:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/279/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0278-universal-newline-support","title":"[Final] PEP 278 - Universal Newline Support","excerpt":"Python Enhancement Proposal 278: 'Universal Newline Support'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:43+0900","lastModifiedAt":"2025-09-26 17:54:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/278/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0277-unicode-file-name-support-for-windows-nt","title":"[Final] PEP 277 - Unicode file name support for Windows NT","excerpt":"Python Enhancement Proposal 277: 'Unicode file name support for Windows NT'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:18+0900","lastModifiedAt":"2025-09-26 17:54:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/277/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0276-simple-iterator-for-ints","title":"[Rejected] PEP 276 - Simple Iterator for ints","excerpt":"Python Enhancement Proposal 276: 'Simple Iterator for ints'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:04+0900","lastModifiedAt":"2025-09-26 17:54:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/276/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0275-switching-on-multiple-values","title":"[Rejected] PEP 275 - Switching on Multiple Values","excerpt":"Python Enhancement Proposal 275: 'Switching on Multiple Values'에 대한 한국어 번역입니다.","date":"2025-09-26 17:53:28+0900","lastModifiedAt":"2025-09-26 17:53:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/275/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0274-dict-comprehensions","title":"[Final] PEP 274 - Dict Comprehensions","excerpt":"Python Enhancement Proposal 274: 'Dict Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:53:00+0900","lastModifiedAt":"2025-09-26 17:53:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/274/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0273-import-modules-from-zip-archives","title":"[Final] PEP 273 - Import Modules from Zip Archives","excerpt":"Python Enhancement Proposal 273: 'Import Modules from Zip Archives'에 대한 한국어 번역입니다.","date":"2025-09-26 17:52:44+0900","lastModifiedAt":"2025-09-26 17:52:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/273/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0272-api-for-block-encryption-algorithms-v1-0","title":"[Final] PEP 272 - API for Block Encryption Algorithms v1.0","excerpt":"Python Enhancement Proposal 272: 'API for Block Encryption Algorithms v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:52:18+0900","lastModifiedAt":"2025-09-26 17:52:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/272/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0271-prefixing-sys-path-by-command-line-option","title":"[Rejected] PEP 271 - Prefixing sys.path by command line option","excerpt":"Python Enhancement Proposal 271: 'Prefixing sys.path by command line option'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:56+0900","lastModifiedAt":"2025-09-26 17:51:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/271/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0270-uniq-method-for-list-objects","title":"[Rejected] PEP 270 - uniq method for list objects","excerpt":"Python Enhancement Proposal 270: 'uniq method for list objects'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:43+0900","lastModifiedAt":"2025-09-26 17:51:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/270/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0269-pgen-module-for-python","title":"[Deferred] PEP 269 - Pgen Module for Python","excerpt":"Python Enhancement Proposal 269: 'Pgen Module for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:28+0900","lastModifiedAt":"2025-09-26 17:51:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/269/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0268-extended-http-functionality-and-webdav","title":"[Rejected] PEP 268 - Extended HTTP functionality and WebDAV","excerpt":"Python Enhancement Proposal 268: 'Extended HTTP functionality and WebDAV'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:10+0900","lastModifiedAt":"2025-09-26 17:51:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/268/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0267-optimized-access-to-module-namespaces","title":"[Deferred] PEP 267 - Optimized Access to Module Namespaces","excerpt":"Python Enhancement Proposal 267: 'Optimized Access to Module Namespaces'에 대한 한국어 번역입니다.","date":"2025-09-26 17:50:38+0900","lastModifiedAt":"2025-09-26 17:50:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/267/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0266-optimizing-global-variableattribute-access","title":"[Withdrawn] PEP 266 - Optimizing Global Variable/Attribute Access","excerpt":"Python Enhancement Proposal 266: 'Optimizing Global Variable/Attribute Access'에 대한 한국어 번역입니다.","date":"2025-09-26 17:49:31+0900","lastModifiedAt":"2025-09-26 17:49:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/266/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0265-sorting-dictionaries-by-value","title":"[Rejected] PEP 265 - Sorting Dictionaries by Value","excerpt":"Python Enhancement Proposal 265: 'Sorting Dictionaries by Value'에 대한 한국어 번역입니다.","date":"2025-09-26 17:48:48+0900","lastModifiedAt":"2025-09-26 17:48:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/265/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0264-future-statements-in-simulated-shells","title":"[Final] PEP 264 - Future statements in simulated shells","excerpt":"Python Enhancement Proposal 264: 'Future statements in simulated shells'에 대한 한국어 번역입니다.","date":"2025-09-26 17:47:23+0900","lastModifiedAt":"2025-09-26 17:47:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/264/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0263-defining-python-source-code-encodings","title":"[Final] PEP 263 - Defining Python Source Code Encodings","excerpt":"Python Enhancement Proposal 263: 'Defining Python Source Code Encodings'에 대한 한국어 번역입니다.","date":"2025-09-26 17:46:12+0900","lastModifiedAt":"2025-09-26 17:46:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/263/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0262-a-database-of-installed-python-packages","title":"[Rejected] PEP 262 - A Database of Installed Python Packages","excerpt":"Python Enhancement Proposal 262: 'A Database of Installed Python Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 17:44:46+0900","lastModifiedAt":"2025-09-26 17:44:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/262/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0261-support-for-wide-unicode-characters","title":"[Final] PEP 261 - Support for “wide” Unicode characters","excerpt":"Python Enhancement Proposal 261: 'Support for “wide” Unicode characters'에 대한 한국어 번역입니다.","date":"2025-09-26 17:43:32+0900","lastModifiedAt":"2025-09-26 17:43:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/261/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0260-simplify-xrange","title":"[Final] PEP 260 - Simplify xrange()","excerpt":"Python Enhancement Proposal 260: 'Simplify xrange()'에 대한 한국어 번역입니다.","date":"2025-09-26 17:42:10+0900","lastModifiedAt":"2025-09-26 17:42:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/260/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0259-omit-printing-newline-after-newline","title":"[Rejected] PEP 259 - Omit printing newline after newline","excerpt":"Python Enhancement Proposal 259: 'Omit printing newline after newline'에 대한 한국어 번역입니다.","date":"2025-09-26 17:41:02+0900","lastModifiedAt":"2025-09-26 17:41:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/259/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0258-docutils-design-specification","title":"[Rejected] PEP 258 - Docutils Design Specification","excerpt":"Python Enhancement Proposal 258: 'Docutils Design Specification'에 대한 한국어 번역입니다.","date":"2025-09-26 17:39:51+0900","lastModifiedAt":"2025-09-26 17:39:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/258/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0257-docstring-conventions","title":"[Active] PEP 257 - Docstring Conventions","excerpt":"Python Enhancement Proposal 257: 'Docstring Conventions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:37:43+0900","lastModifiedAt":"2025-09-26 17:37:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/257/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0256-docstring-processing-system-framework","title":"[Rejected] PEP 256 - Docstring Processing System Framework","excerpt":"Python Enhancement Proposal 256: 'Docstring Processing System Framework'에 대한 한국어 번역입니다.","date":"2025-09-26 17:36:23+0900","lastModifiedAt":"2025-09-26 17:36:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/256/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0255-simple-generators","title":"[Final] PEP 255 - Simple Generators","excerpt":"Python Enhancement Proposal 255: 'Simple Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 17:35:06+0900","lastModifiedAt":"2025-09-26 17:35:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/255/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0254-making-classes-look-more-like-types","title":"[Rejected] PEP 254 - Making Classes Look More Like Types","excerpt":"Python Enhancement Proposal 254: 'Making Classes Look More Like Types'에 대한 한국어 번역입니다.","date":"2025-09-26 17:33:28+0900","lastModifiedAt":"2025-09-26 17:33:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/254/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0253-subtyping-built-in-types","title":"[Final] PEP 253 - Subtyping Built-in Types","excerpt":"Python Enhancement Proposal 253: 'Subtyping Built-in Types'에 대한 한국어 번역입니다.","date":"2025-09-26 17:32:18+0900","lastModifiedAt":"2025-09-26 17:32:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/253/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0252-making-types-look-more-like-classes","title":"[Final] PEP 252 - Making Types Look More Like Classes","excerpt":"Python Enhancement Proposal 252: 'Making Types Look More Like Classes'에 대한 한국어 번역입니다.","date":"2025-09-26 17:30:03+0900","lastModifiedAt":"2025-09-26 17:30:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/252/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0251-python-2-2-release-schedule","title":"[Final] PEP 251 - Python 2.2 Release Schedule","excerpt":"Python Enhancement Proposal 251: 'Python 2.2 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 17:27:59+0900","lastModifiedAt":"2025-09-26 17:27:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/251/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0250-using-site-packages-on-windows","title":"[Final] PEP 250 - Using site-packages on Windows","excerpt":"Python Enhancement Proposal 250: 'Using site-packages on Windows'에 대한 한국어 번역입니다.","date":"2025-09-26 17:26:48+0900","lastModifiedAt":"2025-09-26 17:26:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/250/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0249-python-database-api-specification-v2-0","title":"[Final] PEP 249 - Python Database API Specification v2.0","excerpt":"Python Enhancement Proposal 249: 'Python Database API Specification v2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:25:38+0900","lastModifiedAt":"2025-09-26 17:25:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/249/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0248-python-database-api-specification-v1-0","title":"[Final] PEP 248 - Python Database API Specification v1.0","excerpt":"Python Enhancement Proposal 248: 'Python Database API Specification v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:23:30+0900","lastModifiedAt":"2025-09-26 17:23:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/248/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0247-api-for-cryptographic-hash-functions","title":"[Final] PEP 247 - API for Cryptographic Hash Functions","excerpt":"Python Enhancement Proposal 247: 'API for Cryptographic Hash Functions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:22:10+0900","lastModifiedAt":"2025-09-26 17:22:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/247/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0246-object-adaptation","title":"[Rejected] PEP 246 - Object Adaptation","excerpt":"Python Enhancement Proposal 246: 'Object Adaptation'에 대한 한국어 번역입니다.","date":"2025-09-26 17:20:56+0900","lastModifiedAt":"2025-09-26 17:20:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/246/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0245-python-interface-syntax","title":"[Rejected] PEP 245 - Python Interface Syntax","excerpt":"Python Enhancement Proposal 245: 'Python Interface Syntax'에 대한 한국어 번역입니다.","date":"2025-09-26 17:16:57+0900","lastModifiedAt":"2025-09-26 17:16:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/245/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0244-thedirectivestatement","title":"[Rejected] PEP 244 - Thedirectivestatement","excerpt":"Python Enhancement Proposal 244: 'Thedirectivestatement'에 대한 한국어 번역입니다.","date":"2025-09-26 17:15:30+0900","lastModifiedAt":"2025-09-26 17:15:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/244/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0243-module-repository-upload-mechanism","title":"[Withdrawn] PEP 243 - Module Repository Upload Mechanism","excerpt":"Python Enhancement Proposal 243: 'Module Repository Upload Mechanism'에 대한 한국어 번역입니다.","date":"2025-09-26 17:14:15+0900","lastModifiedAt":"2025-09-26 17:14:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/243/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0242-numeric-kinds","title":"[Withdrawn] PEP 242 - Numeric Kinds","excerpt":"Python Enhancement Proposal 242: 'Numeric Kinds'에 대한 한국어 번역입니다.","date":"2025-09-26 17:13:00+0900","lastModifiedAt":"2025-09-26 17:13:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/242/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0241-metadata-for-python-software-packages","title":"[Superseded] PEP 241 - Metadata for Python Software Packages","excerpt":"Python Enhancement Proposal 241: 'Metadata for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 17:11:45+0900","lastModifiedAt":"2025-09-26 17:11:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/241/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0240-adding-a-rational-literal-to-python","title":"[Rejected] PEP 240 - Adding a Rational Literal to Python","excerpt":"Python Enhancement Proposal 240: 'Adding a Rational Literal to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:10:32+0900","lastModifiedAt":"2025-09-26 17:10:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/240/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0239-adding-a-rational-type-to-python","title":"[Rejected] PEP 239 - Adding a Rational Type to Python","excerpt":"Python Enhancement Proposal 239: 'Adding a Rational Type to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:09:22+0900","lastModifiedAt":"2025-09-26 17:09:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/239/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0238-changing-the-division-operator","title":"[Final] PEP 238 - Changing the Division Operator","excerpt":"Python Enhancement Proposal 238: 'Changing the Division Operator'에 대한 한국어 번역입니다.","date":"2025-09-26 17:08:05+0900","lastModifiedAt":"2025-09-26 17:08:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/238/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0237-unifying-long-integers-and-integers","title":"[Final] PEP 237 - Unifying Long Integers and Integers","excerpt":"Python Enhancement Proposal 237: 'Unifying Long Integers and Integers'에 대한 한국어 번역입니다.","date":"2025-09-26 17:06:31+0900","lastModifiedAt":"2025-09-26 17:06:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/237/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0236-back-to-the-future","title":"[Final] PEP 236 - Back to the __future__","excerpt":"Python Enhancement Proposal 236: 'Back to the __future__'에 대한 한국어 번역입니다.","date":"2025-09-26 17:05:02+0900","lastModifiedAt":"2025-09-26 17:05:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/236/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0235-import-on-case-insensitive-platforms","title":"[Final] PEP 235 - Import on Case-Insensitive Platforms","excerpt":"Python Enhancement Proposal 235: 'Import on Case-Insensitive Platforms'에 대한 한국어 번역입니다.","date":"2025-09-26 17:03:34+0900","lastModifiedAt":"2025-09-26 17:03:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/235/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0234-iterators","title":"[Final] PEP 234 - Iterators","excerpt":"Python Enhancement Proposal 234: 'Iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 17:02:22+0900","lastModifiedAt":"2025-09-26 17:02:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/234/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0233-python-online-help","title":"[Deferred] PEP 233 - Python Online Help","excerpt":"Python Enhancement Proposal 233: 'Python Online Help'에 대한 한국어 번역입니다.","date":"2025-09-26 17:00:50+0900","lastModifiedAt":"2025-09-26 17:00:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/233/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0232-function-attributes","title":"[Final] PEP 232 - Function Attributes","excerpt":"Python Enhancement Proposal 232: 'Function Attributes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:59:42+0900","lastModifiedAt":"2025-09-26 16:59:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/232/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0231-findattr","title":"[Rejected] PEP 231 - __findattr__()","excerpt":"Python Enhancement Proposal 231: '__findattr__()'에 대한 한국어 번역입니다.","date":"2025-09-26 16:58:10+0900","lastModifiedAt":"2025-09-26 16:58:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/231/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0230-warning-framework","title":"[Final] PEP 230 - Warning Framework","excerpt":"Python Enhancement Proposal 230: 'Warning Framework'에 대한 한국어 번역입니다.","date":"2025-09-26 16:53:00+0900","lastModifiedAt":"2025-09-26 16:53:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/230/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0229-using-distutils-to-build-python","title":"[Final] PEP 229 - Using Distutils to Build Python","excerpt":"Python Enhancement Proposal 229: 'Using Distutils to Build Python'에 대한 한국어 번역입니다.","date":"2025-09-26 16:51:00+0900","lastModifiedAt":"2025-09-26 16:51:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/229/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0228-reworking-pythons-numeric-model","title":"[Withdrawn] PEP 228 - Reworking Python’s Numeric Model","excerpt":"Python Enhancement Proposal 228: 'Reworking Python’s Numeric Model'에 대한 한국어 번역입니다.","date":"2025-09-26 16:49:50+0900","lastModifiedAt":"2025-09-26 16:49:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/228/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0227-statically-nested-scopes","title":"[Final] PEP 227 - Statically Nested Scopes","excerpt":"Python Enhancement Proposal 227: 'Statically Nested Scopes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:48:35+0900","lastModifiedAt":"2025-09-26 16:48:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/227/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0226-python-2-1-release-schedule","title":"[Final] PEP 226 - Python 2.1 Release Schedule","excerpt":"Python Enhancement Proposal 226: 'Python 2.1 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:47:09+0900","lastModifiedAt":"2025-09-26 16:47:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/226/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0225-elementwiseobjectwise-operators","title":"[Rejected] PEP 225 - Elementwise/Objectwise Operators","excerpt":"Python Enhancement Proposal 225: 'Elementwise/Objectwise Operators'에 대한 한국어 번역입니다.","date":"2025-09-26 16:45:47+0900","lastModifiedAt":"2025-09-26 16:45:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/225/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0224-attribute-docstrings","title":"[Rejected] PEP 224 - Attribute Docstrings","excerpt":"Python Enhancement Proposal 224: 'Attribute Docstrings'에 대한 한국어 번역입니다.","date":"2025-09-26 16:43:48+0900","lastModifiedAt":"2025-09-26 16:43:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/224/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0223-change-the-meaning-ofxescapes","title":"[Final] PEP 223 - Change the Meaning of '\\x' Escapes","excerpt":"Python Enhancement Proposal 223: 'Change the Meaning of '\\x' Escapes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:42:30+0900","lastModifiedAt":"2025-09-26 16:42:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/223/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0222-web-library-enhancements","title":"[Deferred] PEP 222 - Web Library Enhancements","excerpt":"Python Enhancement Proposal 222: 'Web Library Enhancements'에 대한 한국어 번역입니다.","date":"2025-09-26 16:41:15+0900","lastModifiedAt":"2025-09-26 16:41:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/222/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0221-import-as","title":"[Final] PEP 221 - Import As","excerpt":"Python Enhancement Proposal 221: 'Import As'에 대한 한국어 번역입니다.","date":"2025-09-26 16:39:52+0900","lastModifiedAt":"2025-09-26 16:39:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/221/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0220-coroutines-generators-continuations","title":"[Rejected] PEP 220 - Coroutines, Generators, Continuations","excerpt":"Python Enhancement Proposal 220: 'Coroutines, Generators, Continuations'에 대한 한국어 번역입니다.","date":"2025-09-26 16:38:41+0900","lastModifiedAt":"2025-09-26 16:38:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/220/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0219-stackless-python","title":"[Deferred] PEP 219 - Stackless Python","excerpt":"Python Enhancement Proposal 219: 'Stackless Python'에 대한 한국어 번역입니다.","date":"2025-09-26 16:37:30+0900","lastModifiedAt":"2025-09-26 16:37:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/219/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0218-adding-a-built-in-set-object-type","title":"[Final] PEP 218 - Adding a Built-In Set Object Type","excerpt":"Python Enhancement Proposal 218: 'Adding a Built-In Set Object Type'에 대한 한국어 번역입니다.","date":"2025-09-26 16:36:13+0900","lastModifiedAt":"2025-09-26 16:36:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/218/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0217-display-hook-for-interactive-use","title":"[Final] PEP 217 - Display Hook for Interactive Use","excerpt":"Python Enhancement Proposal 217: 'Display Hook for Interactive Use'에 대한 한국어 번역입니다.","date":"2025-09-26 16:34:58+0900","lastModifiedAt":"2025-09-26 16:34:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/217/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0216-docstring-format","title":"[Withdrawn] PEP 216 - Docstring Format","excerpt":"Python Enhancement Proposal 216: 'Docstring Format'에 대한 한국어 번역입니다.","date":"2025-09-26 16:33:50+0900","lastModifiedAt":"2025-09-26 16:33:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/216/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0215-string-interpolation","title":"[Superseded] PEP 215 - String Interpolation","excerpt":"Python Enhancement Proposal 215: 'String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 16:32:37+0900","lastModifiedAt":"2025-09-26 16:32:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/215/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0214-extended-print-statement","title":"[Final] PEP 214 - Extended Print Statement","excerpt":"Python Enhancement Proposal 214: 'Extended Print Statement'에 대한 한국어 번역입니다.","date":"2025-09-26 16:31:25+0900","lastModifiedAt":"2025-09-26 16:31:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/214/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0213-attribute-access-handlers","title":"[Deferred] PEP 213 - Attribute Access Handlers","excerpt":"Python Enhancement Proposal 213: 'Attribute Access Handlers'에 대한 한국어 번역입니다.","date":"2025-09-26 16:29:57+0900","lastModifiedAt":"2025-09-26 16:29:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/213/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0212-loop-counter-iteration","title":"[Rejected] PEP 212 - Loop Counter Iteration","excerpt":"Python Enhancement Proposal 212: 'Loop Counter Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:28:28+0900","lastModifiedAt":"2025-09-26 16:28:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/212/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0211-adding-a-new-outer-product-operator","title":"[Rejected] PEP 211 - Adding A New Outer Product Operator","excerpt":"Python Enhancement Proposal 211: 'Adding A New Outer Product Operator'에 대한 한국어 번역입니다.","date":"2025-09-26 16:27:14+0900","lastModifiedAt":"2025-09-26 16:27:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/211/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0210-decoupling-the-interpreter-loop","title":"[Rejected] PEP 210 - Decoupling the Interpreter Loop","excerpt":"Python Enhancement Proposal 210: 'Decoupling the Interpreter Loop'에 대한 한국어 번역입니다.","date":"2025-09-26 16:25:55+0900","lastModifiedAt":"2025-09-26 16:25:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/210/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0209-multi-dimensional-arrays","title":"[Withdrawn] PEP 209 - Multi-dimensional Arrays","excerpt":"Python Enhancement Proposal 209: 'Multi-dimensional Arrays'에 대한 한국어 번역입니다.","date":"2025-09-26 16:21:26+0900","lastModifiedAt":"2025-09-26 16:21:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/209/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0208-reworking-the-coercion-model","title":"[Final] PEP 208 - Reworking the Coercion Model","excerpt":"Python Enhancement Proposal 208: 'Reworking the Coercion Model'에 대한 한국어 번역입니다.","date":"2025-09-26 16:20:00+0900","lastModifiedAt":"2025-09-26 16:20:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/208/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0207-rich-comparisons","title":"[Final] PEP 207 - Rich Comparisons","excerpt":"Python Enhancement Proposal 207: 'Rich Comparisons'에 대한 한국어 번역입니다.","date":"2025-09-26 16:18:40+0900","lastModifiedAt":"2025-09-26 16:18:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/207/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0206-python-advanced-library","title":"[Withdrawn] PEP 206 - Python Advanced Library","excerpt":"Python Enhancement Proposal 206: 'Python Advanced Library'에 대한 한국어 번역입니다.","date":"2025-09-26 16:17:22+0900","lastModifiedAt":"2025-09-26 16:17:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/206/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0205-weak-references","title":"[Final] PEP 205 - Weak References","excerpt":"Python Enhancement Proposal 205: 'Weak References'에 대한 한국어 번역입니다.","date":"2025-09-26 16:16:10+0900","lastModifiedAt":"2025-09-26 16:16:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/205/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0204-range-literals","title":"[Rejected] PEP 204 - Range Literals","excerpt":"Python Enhancement Proposal 204: 'Range Literals'에 대한 한국어 번역입니다.","date":"2025-09-26 16:14:46+0900","lastModifiedAt":"2025-09-26 16:14:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/204/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0203-augmented-assignments","title":"[Final] PEP 203 - Augmented Assignments","excerpt":"Python Enhancement Proposal 203: 'Augmented Assignments'에 대한 한국어 번역입니다.","date":"2025-09-26 16:13:23+0900","lastModifiedAt":"2025-09-26 16:13:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/203/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0202-list-comprehensions","title":"[Final] PEP 202 - List Comprehensions","excerpt":"Python Enhancement Proposal 202: 'List Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 16:12:08+0900","lastModifiedAt":"2025-09-26 16:12:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/202/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0201-lockstep-iteration","title":"[Final] PEP 201 - Lockstep Iteration","excerpt":"Python Enhancement Proposal 201: 'Lockstep Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:10:52+0900","lastModifiedAt":"2025-09-26 16:10:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/201/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0200-python-2-0-release-schedule","title":"[Final] PEP 200 - Python 2.0 Release Schedule","excerpt":"Python Enhancement Proposal 200: 'Python 2.0 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:09:15+0900","lastModifiedAt":"2025-09-26 16:09:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/200/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0160-python-1-6-release-schedule","title":"[Final] PEP 160 - Python 1.6 Release Schedule","excerpt":"Python Enhancement Proposal 160: 'Python 1.6 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:07:53+0900","lastModifiedAt":"2025-09-26 16:07:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/160/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0103-collecting-information-about-git","title":"[Withdrawn] PEP 103 - Collecting information about git","excerpt":"Python Enhancement Proposal 103: 'Collecting information about git'에 대한 한국어 번역입니다.","date":"2025-09-26 16:06:44+0900","lastModifiedAt":"2025-09-26 16:06:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/103/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0102-doing-python-micro-releases","title":"[Superseded] PEP 102 - Doing Python Micro Releases","excerpt":"Python Enhancement Proposal 102: 'Doing Python Micro Releases'에 대한 한국어 번역입니다.","date":"2025-09-26 16:04:40+0900","lastModifiedAt":"2025-09-26 16:04:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/102/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0101-doing-python-releases-101","title":"[Active] PEP 101 - Doing Python Releases 101","excerpt":"Python Enhancement Proposal 101: 'Doing Python Releases 101'에 대한 한국어 번역입니다.","date":"2025-09-26 16:02:21+0900","lastModifiedAt":"2025-09-26 16:02:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/101/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0100-python-unicode-integration","title":"[Final] PEP 100 - Python Unicode Integration","excerpt":"Python Enhancement Proposal 100: 'Python Unicode Integration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:00:20+0900","lastModifiedAt":"2025-09-26 16:00:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/100/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0042-feature-requests","title":"[Withdrawn] PEP 42 - Feature Requests","excerpt":"Python Enhancement Proposal 42: 'Feature Requests'에 대한 한국어 번역입니다.","date":"2025-09-26 15:56:44+0900","lastModifiedAt":"2025-09-26 15:56:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/42/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0020-the-zen-of-python","title":"[Active] PEP 20 - The Zen of Python","excerpt":"Python Enhancement Proposal 20: 'The Zen of Python'에 대한 한국어 번역입니다.","date":"2025-09-26 15:55:10+0900","lastModifiedAt":"2025-09-26 15:55:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/20/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0013-python-language-governance","title":"[Active] PEP 13 - Python Language Governance","excerpt":"Python Enhancement Proposal 13: 'Python Language Governance'에 대한 한국어 번역입니다.","date":"2025-09-26 15:54:00+0900","lastModifiedAt":"2025-09-26 15:54:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/13/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0012-sample-restructuredtext-pep-template","title":"[Active] PEP 12 - Sample reStructuredText PEP Template","excerpt":"Python Enhancement Proposal 12: 'Sample reStructuredText PEP Template'에 대한 한국어 번역입니다.","date":"2025-09-26 15:52:34+0900","lastModifiedAt":"2025-09-26 15:52:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/12/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0011-cpython-platform-support","title":"[Active] PEP 11 - CPython platform support","excerpt":"Python Enhancement Proposal 11: 'CPython platform support'에 대한 한국어 번역입니다.","date":"2025-09-26 15:49:46+0900","lastModifiedAt":"2025-09-26 15:49:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/11/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0010-voting-guidelines","title":"[Active] PEP 10 - Voting Guidelines","excerpt":"Python Enhancement Proposal 10: 'Voting Guidelines'에 대한 한국어 번역입니다.","date":"2025-09-26 15:48:17+0900","lastModifiedAt":"2025-09-26 15:48:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/10/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0009-sample-plaintext-pep-template","title":"[Withdrawn] PEP 9 - Sample Plaintext PEP Template","excerpt":"Python Enhancement Proposal 9: 'Sample Plaintext PEP Template'에 대한 한국어 번역입니다.","date":"2025-09-26 15:47:09+0900","lastModifiedAt":"2025-09-26 15:47:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/9/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0008-style-guide-for-python-code","title":"[Active] PEP 8 - Style Guide for Python Code","excerpt":"Python Enhancement Proposal 8: 'Style Guide for Python Code'에 대한 한국어 번역입니다.","date":"2025-09-26 15:45:52+0900","lastModifiedAt":"2025-09-26 15:45:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0007-style-guide-for-c-code","title":"[Active] PEP 7 - Style Guide for C Code","excerpt":"Python Enhancement Proposal 7: 'Style Guide for C Code'에 대한 한국어 번역입니다.","date":"2025-09-26 15:43:52+0900","lastModifiedAt":"2025-09-26 15:43:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/7/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0006-bug-fix-releases","title":"[Superseded] PEP 6 - Bug Fix Releases","excerpt":"Python Enhancement Proposal 6: 'Bug Fix Releases'에 대한 한국어 번역입니다.","date":"2025-09-26 15:42:30+0900","lastModifiedAt":"2025-09-26 15:42:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/6/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0005-guidelines-for-language-evolution","title":"[Superseded] PEP 5 - Guidelines for Language Evolution","excerpt":"Python Enhancement Proposal 5: 'Guidelines for Language Evolution'에 대한 한국어 번역입니다.","date":"2025-09-26 15:40:01+0900","lastModifiedAt":"2025-09-26 15:40:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/5/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0004-deprecation-of-standard-modules","title":"[Active] PEP 4 - Deprecation of Standard Modules","excerpt":"Python Enhancement Proposal 4: 'Deprecation of Standard Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 15:38:53+0900","lastModifiedAt":"2025-09-26 15:38:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/4/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0003-guidelines-for-handling-bug-reports","title":"[Withdrawn] PEP 3 - Guidelines for Handling Bug Reports","excerpt":"Python Enhancement Proposal 3: 'Guidelines for Handling Bug Reports'에 대한 한국어 번역입니다.","date":"2025-09-26 15:37:46+0900","lastModifiedAt":"2025-09-26 15:37:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0002-procedure-for-adding-new-modules","title":"[Active] PEP 2 - Procedure for Adding New Modules","excerpt":"Python Enhancement Proposal 2: 'Procedure for Adding New Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 15:36:38+0900","lastModifiedAt":"2025-09-26 15:36:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/2/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-09-26-pep-0001-pep-purpose-and-guidelines","title":"[Active] PEP 1 - PEP Purpose and Guidelines","excerpt":"Python Enhancement Proposal 1: 'PEP Purpose and Guidelines'에 대한 한국어 번역입니다.","date":"2025-09-26 15:31:21+0900","lastModifiedAt":"2025-09-26 15:31:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/1/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity","title":"[논문리뷰] When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity","excerpt":"John P Dickerson이 [arXiv]에 게시한 'When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","LLM Judge","Benchmark Evaluation","Validity","Reliability","Psychometrics","Factor Analysis","Schema Adherence","ELO Ranking"],"permalink":"/ai/review/2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models","title":"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models","excerpt":"Yuewei Zhang이 [arXiv]에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Curriculum Learning","Large Language Models","Mathematical Reasoning","Variance-based Sampling","Replay Learning","Policy Optimization"],"permalink":"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models","title":"[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models","excerpt":"Shawn Guo이 [arXiv]에 게시한 'V-GameGym: Visual Game Generation for Code Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Code Large Language Models","Visual Game Generation","Benchmark","Pygame","Multimodal Evaluation","Software Engineering","AI-assisted Game Development"],"permalink":"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory","title":"[논문리뷰] Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory","excerpt":"Yanbin Fu이 [arXiv]에 게시한 'Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Cognitive Science","Schoenfeld's Episode Theory","Math Problem Solving","Chain-of-Thought","Behavioral Analysis","Dataset Annotation"],"permalink":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them","title":"[논문리뷰] TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them","excerpt":"Zhuohao Yu이 [arXiv]에 게시한 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","LLM-as-a-Judge","Evaluation Frameworks","Inconsistency Reduction","Probabilistic Scoring","Transitivity","Information Loss","Perplexity","Large Language Models"],"permalink":"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning","title":"[논문리뷰] Tree Search for LLM Agent Reinforcement Learning","excerpt":"Xiangxiang Chu이 [arXiv]에 게시한 'Tree Search for LLM Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Tree Search","Policy Optimization","Preference Learning","Sparse Rewards","Multi-turn Tasks"],"permalink":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification","title":"[논문리뷰] Thinking While Listening: Simple Test Time Scaling For Audio Classification","excerpt":"Mert Pilanci이 [arXiv]에 게시한 'Thinking While Listening: Simple Test Time Scaling For Audio Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Audio Classification","Test-Time Scaling","Reasoning Traces","Large Language Models (LLMs)","Transformer Architectures","Zero-shot Reasoning","Computational Efficiency"],"permalink":"/ai/review/2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Thinking-Augmented-Pre-training","title":"[논문리뷰] Thinking Augmented Pre-training","excerpt":"Furu Wei이 [arXiv]에 게시한 'Thinking Augmented Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Pre-training","Data Augmentation","Reasoning","Data Efficiency","Thinking Trajectories"],"permalink":"/ai/review/2025-9-26-Thinking-Augmented-Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment","title":"[논문리뷰] The Unanticipated Asymmetry Between Perceptual Optimization and Assessment","excerpt":"Du Chen이 [arXiv]에 게시한 'The Unanticipated Asymmetry Between Perceptual Optimization and Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Perceptual Optimization","Image Quality Assessment (IQA)","Adversarial Training","Discriminators","Super-Resolution","Fidelity Metrics","Deep Learning"],"permalink":"/ai/review/2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models","title":"[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models","excerpt":"Javad Lavaei이 [arXiv]에 게시한 'StyleBench: Evaluating thinking styles in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Large Language Models","Reasoning Strategies","Prompt Engineering","LLM Evaluation","Benchmark","Thinking Styles","Scaling Laws","Meta-Reasoning"],"permalink":"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation","title":"[논문리뷰] Seedream 4.0: Toward Next-generation Multimodal Image Generation","excerpt":"Yunpeng Chen이 [arXiv]에 게시한 'Seedream 4.0: Toward Next-generation Multimodal Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Multimodal Image Generation","Diffusion Transformer","VAE","Image Editing","Text-to-Image","Model Acceleration","Human Evaluation"],"permalink":"/ai/review/2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines","title":"[논문리뷰] SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines","excerpt":"Jiabei Xiao이 [arXiv]에 게시한 'SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Scientific Reasoning","Foundation Models","Multi-modal Learning","Cross-domain Generalization","Chain-of-Thought","Reinforcement Learning","Scientific Discovery","Molecular Design"],"permalink":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent","title":"[논문리뷰] SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent","excerpt":"Siyuan Huang이 [arXiv]에 게시한 'SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","3D Scene Synthesis","Agentic Framework","LLMs","Self-Reflection","Tool-Use","Physical Plausibility","Iterative Refinement","Embodied AI"],"permalink":"/ai/review/2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning","title":"[논문리뷰] ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning","excerpt":"Yu Li이 [arXiv]에 게시한 'ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Large Reasoning Models (LRMs)","Difficulty Scaling","Data Augmentation","Supervised Fine-Tuning (SFT)","Problem Generation","Solution Distillation"],"permalink":"/ai/review/2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows","title":"[논문리뷰] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows","excerpt":"Yi-Zhe Song이 [arXiv]에 게시한 'SD3.5-Flash: Distribution-Guided Distillation of Generative Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Generative AI","Image Generation","Diffusion Models","Rectified Flow","Model Distillation","Few-Step Generation","Computational Efficiency","Prompt Alignment"],"permalink":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies","title":"[논문리뷰] Residual Off-Policy RL for Finetuning Behavior Cloning Policies","excerpt":"Pieter Abbeel이 [arXiv]에 게시한 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Behavior Cloning (BC)","Residual Learning","Off-Policy RL","Robot Manipulation","Real-World Robotics","High-DoF Systems","Sample Efficiency"],"permalink":"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution","title":"[논문리뷰] Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution","excerpt":"Jinjie Gu이 [arXiv]에 게시한 'Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Browser Automation","Web Reconnaissance","Tool Generation","Task Execution","Self-Evolving AI","LLM/VLM","VisualWebArena"],"permalink":"/ai/review/2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer","title":"[논문리뷰] Quantized Visual Geometry Grounded Transformer","excerpt":"Yuqi Li이 [arXiv]에 게시한 'Quantized Visual Geometry Grounded Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Quantization","Post-Training Quantization","3D Reconstruction","Visual Transformer","Model Compression","Efficient Inference","Hadamard Rotation","Calibration Sampling"],"permalink":"/ai/review/2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning","title":"[논문리뷰] MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning","excerpt":"Junyan Zhang이 [arXiv]에 게시한 'MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Video Temporal Reasoning","Reinforcement Learning","Process Supervision","Dynamic Time Warping","Multimodal Large Language Models","Video State Prediction","Reward Hacking"],"permalink":"/ai/review/2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources","title":"[논문리뷰] MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources","excerpt":"Jing Wang이 [arXiv]에 게시한 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Reinforcement Learning","Variance-Aware Sampling","Gradient Vanishing","Data Curation","Chain-of-Thought","GRPO"],"permalink":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model","title":"[논문리뷰] MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model","excerpt":"Hung-yi Lee이 [arXiv]에 게시한 'MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Speech Emotion Recognition","Source-Free Unsupervised Domain Adaptation","Large Audio-Language Models","Label Fusion","Mutual Information","API-Only Models","Domain Mismatch"],"permalink":"/ai/review/2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands","title":"[논문리뷰] Interactive Recommendation Agent with Active User Commands","excerpt":"Xueyang Feng이 [arXiv]에 게시한 'Interactive Recommendation Agent with Active User Commands' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Interactive Recommendation","Large Language Models","Multi-Agent System","Natural Language Processing","Knowledge Distillation","User Control"],"permalink":"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets","title":"[논문리뷰] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets","excerpt":"Bowen Zhang이 [arXiv]에 게시한 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","3D Generation","Controllable Generation","Multi-modal Conditioning","Diffusion Models","Point Clouds","Voxels","Bounding Boxes","Skeletons","Hunyuan3D"],"permalink":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition","title":"[논문리뷰] Does FLUX Already Know How to Perform Physically Plausible Image Composition?","excerpt":"Chen Zhao이 [arXiv]에 게시한 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Image Composition","Diffusion Models","Training-Free","Physically Plausible","FLUX","Adapter","Guidance","Benchmark"],"permalink":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving","title":"[논문리뷰] Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving","excerpt":"Hang Zhao이 [arXiv]에 게시한 'Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Vision-Language-Action Models","Discrete Diffusion","Reflection Mechanism","Trajectory Generation","Safety Constraints","Imitation Learning"],"permalink":"/ai/review/2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling","title":"[논문리뷰] CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling","excerpt":"Yushi Bai이 [arXiv]에 게시한 'CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","3D Anime Hairstyle","Autoregressive Modeling","Control Points","Parametric Representation","Transformer","Generative AI","Dataset (AnimeHair)","Computer Graphics"],"permalink":"/ai/review/2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning","title":"[논문리뷰] CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning","excerpt":"Wenping Hu이 [arXiv]에 게시한 'CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Policy Optimization","PPO","Entropy Control","Gradient Clipping","Exploration-Exploitation"],"permalink":"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance","title":"[논문리뷰] Blueprints of Trust: AI System Cards for End to End Transparency and Governance","excerpt":"Roman Zhukov이 [arXiv]에 게시한 'Blueprints of Trust: AI System Cards for End to End Transparency and Governance' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","AI Governance","Transparency","AI System Card","Hazard-Aware System Card","Data Provenance","AI Safety","AI Risk Management","ISO/IEC 42001"],"permalink":"/ai/review/2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information","title":"[논문리뷰] Behind RoPE: How Does Causal Mask Encode Positional Information?","excerpt":"Yeyun Gong이 [arXiv]에 게시한 'Behind RoPE: How Does Causal Mask Encode Positional Information?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Transformer Decoder","Causal Mask","Positional Encoding","RoPE","Attention Mechanism","Length Generalization","Large Language Models"],"permalink":"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback","title":"[논문리뷰] BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback","excerpt":"Dongha Lee이 [arXiv]에 게시한 'BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Search-Augmented LLMs","Personalization","Benchmark","Diagnostic Feedback","User History","Evaluation Framework","RAG"],"permalink":"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-26-AutoIntent-AutoML-for-Text-Classification","title":"[논문리뷰] AutoIntent: AutoML for Text Classification","excerpt":"Denis Kuznetsov이 [arXiv]에 게시한 'AutoIntent: AutoML for Text Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","AutoML","Text Classification","Intent Classification","Transformer Embeddings","Out-of-Scope Detection","Multi-label Classification","Few-shot Learning","Sklearn-like Interface"],"permalink":"/ai/review/2025-9-26-AutoIntent-AutoML-for-Text-Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-Video-models-are-zero-shot-learners-and-reasoners","title":"[논문리뷰] Video models are zero-shot learners and reasoners","excerpt":"rgeirhos이 [arXiv]에 게시한 'Video models are zero-shot learners and reasoners' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Video Models","Zero-shot Learning","Visual Reasoning","Foundation Models","Generative AI","Perception","Manipulation","Modeling"],"permalink":"/ai/review/2025-9-25-Video-models-are-zero-shot-learners-and-reasoners/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought","title":"[논문리뷰] SIM-CoT: Supervised Implicit Chain-of-Thought","excerpt":"Yuhang Cao이 [arXiv]에 게시한 'SIM-CoT: Supervised Implicit Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Implicit Reasoning","Chain-of-Thought","LLM","Latent Space","Supervised Learning","Model Stability","Interpretability"],"permalink":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","title":"[논문리뷰] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation","excerpt":"Yiming Huang이 [arXiv]에 게시한 'PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Video Generation","Physics-Grounded","Controllable Generation","Diffusion Models","Point Cloud Trajectories","Material Simulation","Generative Physics"],"permalink":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub","title":"[논문리뷰] On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub","excerpt":"Hajimu Iida이 [arXiv]에 게시한 'On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Agentic Coding","AI Agents","Large Language Models","GitHub Pull Requests","Software Engineering","Empirical Study","Code Generation","Software Development"],"permalink":"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-Logics-Parsing-Technical-Report","title":"[논문리뷰] Logics-Parsing Technical Report","excerpt":"Fan Yang이 [arXiv]에 게시한 'Logics-Parsing Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Document Parsing","Large Vision-Language Models (LVLM)","Reinforcement Learning (RL)","Layout Analysis","Reading Order","Supervised Fine-Tuning (SFT)","HTML Annotation","Benchmarking"],"permalink":"/ai/review/2025-9-25-Logics-Parsing-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation","title":"[논문리뷰] Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation","excerpt":"Zhe Lin이 [arXiv]에 게시한 'Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Multimodal AI","Masked Diffusion Models","Image Understanding","Image Generation","Image Editing","Object Grounding","ElasticMoT","Self-reflection"],"permalink":"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines","title":"[논문리뷰] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines","excerpt":"Yanfang이 [arXiv]에 게시한 'LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Large Language Models","Generative AI","Academic Disciplines","LLM Applications","Review","Cross-disciplinary Research","Benchmarks"],"permalink":"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations","title":"[논문리뷰] EmbeddingGemma: Powerful and Lightweight Text Representations","excerpt":"Marksherwood이 [arXiv]에 게시한 'EmbeddingGemma: Powerful and Lightweight Text Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Text Embeddings","Lightweight Models","Encoder-Decoder","Knowledge Distillation","Model Souping","Quantization","Multilingual","Gemma"],"permalink":"/ai/review/2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning","title":"[논문리뷰] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning","excerpt":"Tianyu Wang이 [arXiv]에 게시한 'EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Unified Multimodal Model","In-Context Learning","Image and Video Editing","Video Generation","Full Self-Attention","Rotary Positional Embedding","Cross-Modal Knowledge Transfer"],"permalink":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO","title":"[논문리뷰] Advancing Speech Understanding in Speech-Aware Language Models with GRPO","excerpt":"Avihu이 [arXiv]에 게시한 'Advancing Speech Understanding in Speech-Aware Language Models with GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Speech-Aware Language Models","SALLMs","GRPO","Reinforcement Learning","Speech Understanding","Spoken Question Answering","Automatic Speech Translation","BLEU Metric"],"permalink":"/ai/review/2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications","title":"[논문리뷰] Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications","excerpt":"Genady Beryozkin이 [arXiv]에 게시한 'Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Remote Sensing","Zero-Shot Learning","Multimodal Models","Multi-spectral Imagery","Gemini 2.5","Prompt Engineering","Land Cover Classification","Pseudo-Image"],"permalink":"/ai/review/2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT","title":"[논문리뷰] What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT","excerpt":"Anthony Hartshorn이 [arXiv]에 게시한 'What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Chain-of-Thought","Reasoning Effectiveness","Large Reasoning Models","Failed-Step Fraction","Test-time Scaling","Reasoning Graph","Model Evaluation"],"permalink":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction","title":"[논문리뷰] VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction","excerpt":"Haoxiao Wang이 [arXiv]에 게시한 'VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting","Novel View Synthesis","Voxel-Aligned Prediction","Feed-Forward Reconstruction","Multi-View Consistency","Scene Representation","Computer Vision"],"permalink":"/ai/review/2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction","title":"[논문리뷰] VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction","excerpt":"So Fukuda이 [arXiv]에 게시한 'VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Video Understanding","Geospatial Reasoning","Temporal Reasoning","Travel Itinerary Reconstruction","Benchmark","Agent System","VLOG"],"permalink":"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-Reinforcement-Learning-on-Pre-Training-Data","title":"[논문리뷰] Reinforcement Learning on Pre-Training Data","excerpt":"Evander Yang이 [arXiv]에 게시한 'Reinforcement Learning on Pre-Training Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Pre-training","Large Language Models","Self-supervised Learning","Scaling Laws","Next-segment Reasoning","Reward Modeling"],"permalink":"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation","title":"[논문리뷰] OpenGVL - Benchmarking Visual Temporal Progress for Data Curation","excerpt":"Viktor Petrenko이 [arXiv]에 게시한 'OpenGVL - Benchmarking Visual Temporal Progress for Data Curation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Robotics Data Curation","Visual Temporal Progress","Generative Value Learning (GVL)","Vision-Language Models (VLMs)","Benchmark","Task Progress Prediction","Value-Order Correlation (VOC)"],"permalink":"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe","title":"[논문리뷰] MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe","excerpt":"Wenshuo Ma이 [arXiv]에 게시한 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","MLLM Efficiency","Multimodal Transformer","3D-Resampler","Document AI","Hybrid Reinforcement Learning","Video Understanding","Efficient Inference"],"permalink":"/ai/review/2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization","title":"[논문리뷰] MAPO: Mixed Advantage Policy Optimization","excerpt":"Xuankun Rong이 [arXiv]에 게시한 'MAPO: Mixed Advantage Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Foundation Models","Policy Optimization","Advantage Function","Trajectory Certainty","Multimodal Reasoning","GRPO"],"permalink":"/ai/review/2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation","title":"[논문리뷰] Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation","excerpt":"Yifeng Jiang이 [arXiv]에 게시한 'Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Generative AI","3D Scene Reconstruction","Video Diffusion Models","Self-Distillation","3D Gaussian Splatting","Dynamic 4D Generation","Monocular Input"],"permalink":"/ai/review/2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects","title":"[논문리뷰] Large Language Models Discriminate Against Speakers of German Dialects","excerpt":"Katharina von der Wense이 [arXiv]에 게시한 'Large Language Models Discriminate Against Speakers of German Dialects' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Large Language Models","Bias","German Dialects","Sociolinguistics","Stereotypes","Implicit Association Test","Decision Making"],"permalink":"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation","title":"[논문리뷰] Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation","excerpt":"Jianbin Zheng이 [arXiv]에 게시한 'Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Multimodal AI","Acceleration Framework","Speculative Decoding","Diffusion Distillation","Unified Models","Text-to-Image Generation","Image Editing","Computational Efficiency"],"permalink":"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis","title":"[논문리뷰] HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis","excerpt":"Dan Xu이 [arXiv]에 게시한 'HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Novel View Synthesis","3D Gaussian Splatting (3DGS)","Neural Radiance Fields (NeRF)","Memory Efficiency","High-Quality Rendering","Hybrid Representation","Real-time Rendering"],"permalink":"/ai/review/2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction","title":"[논문리뷰] GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction","excerpt":"Jin Zheng이 [arXiv]에 게시한 'GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Surface Reconstruction","Sparse Voxels","Geometric Accuracy","Neural Radiance Fields","3D Gaussian Splatting","Monocular Depth","Voxel Uncertainty"],"permalink":"/ai/review/2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies","title":"[논문리뷰] Do You Need Proprioceptive States in Visuomotor Policies?","excerpt":"Yushen Liang이 [arXiv]에 게시한 'Do You Need Proprioceptive States in Visuomotor Policies?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Visuomotor Policies","Spatial Generalization","Imitation Learning","Proprioception","State-free Policies","Robot Manipulation","End-Effector Control","Data Efficiency"],"permalink":"/ai/review/2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching","title":"[논문리뷰] CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching","excerpt":"Rui Qian이 [arXiv]에 게시한 'CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Flow Matching","Conditional Generative Models","Reparameterization","Mode Collapse","Image Generation","Latent Space Alignment","Diffusion Models"],"permalink":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR","title":"[논문리뷰] Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR","excerpt":"Zeina Aldallal이 [arXiv]에 게시한 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Arabic OCR","Vision-Language Model","Fine-tuning","Document Understanding","Markdown Conversion","Benchmark"],"permalink":"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs","title":"[논문리뷰] When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs","excerpt":"Anand Mishra이 [arXiv]에 게시한 'When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","VQA","Small VLMs","Large VLMs","Knowledge Transfer","Pseudo-labeling","Label-Free Learning","Model Parity Alignment","Computational Efficiency"],"permalink":"/ai/review/2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models","title":"[논문리뷰] VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models","excerpt":"Sunghyun Cho이 [arXiv]에 게시한 'VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Video Diffusion","Image Diffusion","Generative Models","Computer Graphics","Temporal Consistency","Sparse Anchor Views"],"permalink":"/ai/review/2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery","title":"[논문리뷰] VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery","excerpt":"Shiya Huang이 [arXiv]에 게시한 'VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Visual Question Answering","Reinforcement Learning","Cultural Heritage","Ancient Greek Pottery","Supervised Fine-Tuning","Benchmark"],"permalink":"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering","title":"[논문리뷰] Understanding Embedding Scaling in Collaborative Filtering","excerpt":"Yonghui Yang이 [arXiv]에 게시한 'Understanding Embedding Scaling in Collaborative Filtering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Collaborative Filtering","Embedding Scaling","Noise Robustness","Recommender Systems","Graph Neural Networks","Self-supervised Learning","Performance Degradation"],"permalink":"/ai/review/2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications","title":"[논문리뷰] Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications","excerpt":"Fatma Betül Terzioğlu이 [arXiv]에 게시한 'Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Hallucination Detection","Retrieval Augmented Generation","Large Language Models","Turkish NLP","Token Classification","ModernBERT","Low-Resource Languages"],"permalink":"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs","title":"[논문리뷰] TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs","excerpt":"Shaohui Jiao이 [arXiv]에 게시한 'TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Video LLMs","Temporal Grounding","Reinforcement Learning","Off-policy Learning","Reward Shaping","Chain-of-Thought","Multimodal LLMs"],"permalink":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Synthetic-bootstrapped-pretraining","title":"[논문리뷰] Synthetic bootstrapped pretraining","excerpt":"Emmanuel Candès이 [arXiv]에 게시한 'Synthetic bootstrapped pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Language Model Pretraining","Synthetic Data","Inter-document Correlation","Data Augmentation","Transformer","Bootstrapping","Concept Learning"],"permalink":"/ai/review/2025-9-23-Synthetic-bootstrapped-pretraining/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks","title":"[논문리뷰] SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?","excerpt":"Yannis Yiming He이 [arXiv]에 게시한 'SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","AI Agents","Software Engineering","LLMs","Code Generation","Benchmark","Contamination Resistance","Long-Horizon Tasks","Enterprise Software"],"permalink":"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning","title":"[논문리뷰] SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning","excerpt":"Zhaopeng Tu이 [arXiv]에 게시한 'SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Process Reward Models","Monte Carlo Annotation","Noise Denoising","Robust Learning","Self-Supervision","Mathematical Reasoning","Large Language Models"],"permalink":"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning","title":"[논문리뷰] Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning","excerpt":"Damien Sileo이 [arXiv]에 게시한 'Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Symbolic AI","Reinforcement Learning","Procedural Content Generation","Verifiable Rewards","Adaptive Curricula","First-Order Logic","PDDL Planning"],"permalink":"/ai/review/2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Qwen3-Omni-Technical-Report","title":"[논문리뷰] Qwen3-Omni Technical Report","excerpt":"Lhma-aslp이 [arXiv]에 게시한 'Qwen3-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Model","Thinker-Talker Architecture","Mixture-of-Experts","Low-latency","Audio Understanding","Cross-modal Reasoning","State-of-the-Art","Real-time Interaction"],"permalink":"/ai/review/2025-9-23-Qwen3-Omni-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models","title":"[논문리뷰] QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models","excerpt":"Jae-Joon Kim이 [arXiv]에 게시한 'QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","LLM Fine-tuning","Quantization-Aware PEFT","Walsh-Hadamard Transform","Sparse Adaptation","Low-bit Quantization","Parameter-Efficient Learning"],"permalink":"/ai/review/2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","title":"[논문리뷰] OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models","excerpt":"Pengze Zhang이 [arXiv]에 게시한 'OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Video Insertion","Diffusion Models","Diffusion Transformers","Mask-Free","Data Augmentation","Progressive Training","Preference Optimization","Video Generation"],"permalink":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction","title":"[논문리뷰] MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction","excerpt":"Xintao Chen이 [arXiv]에 게시한 'MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Retrieval","Late Interaction","Meta Tokens","Matryoshka Representation Learning","Test-Time Scaling","Vision-Language Models","Dense Retrieval","Efficiency"],"permalink":"/ai/review/2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Mano-Report","title":"[논문리뷰] Mano Report","excerpt":"Minghui Wu이 [arXiv]에 게시한 'Mano Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","GUI Agent","Multi-modal Foundation Model","Reinforcement Learning","Supervised Fine-tuning","Simulated Environment","Data Generation","Error Recovery","Web Automation"],"permalink":"/ai/review/2025-9-23-Mano-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-LIMI-Less-is-More-for-Agency","title":"[논문리뷰] LIMI: Less is More for Agency","excerpt":"happyZYM이 [arXiv]에 게시한 'LIMI: Less is More for Agency' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","AI Agency","Data Curation","Less Is More","Agentic Intelligence","Foundation Models","Evaluation Benchmark","Efficiency Principle","Large Language Models"],"permalink":"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning","title":"[논문리뷰] GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning","excerpt":"Hou Pong Chan이 [arXiv]에 게시한 'GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Geometric Reasoning","Visual Perception","Reinforcement Learning (RL)","Two-stage Training","GeoPQA Benchmark","Perceptual Bottleneck"],"permalink":"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature","title":"[논문리뷰] From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature","excerpt":"Bin Cui이 [arXiv]에 게시한 'From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Policy Optimization","Token Heterogeneity","Adaptive Sampling","Advantage Redistribution","Asymmetric Clipping","Entropy-based RL"],"permalink":"/ai/review/2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem","title":"[논문리뷰] From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem","excerpt":"Ahmed E. Hassan이 [arXiv]에 게시한 'From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Open-Source AI","License Compliance","License Drift","AI Supply Chain","Hugging Face","GitHub","LicenseRec","Legal Risk"],"permalink":"/ai/review/2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions","title":"[논문리뷰] FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions","excerpt":"tengdai722이 [arXiv]에 게시한 'FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","LLM Evaluation","Multimodal AI","Reasoning Behaviors","Hallucination","Contamination-Free","AI Safety","Instruction Following"],"permalink":"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering","title":"[논문리뷰] EpiCache: Episodic KV Cache Management for Long Conversational Question Answering","excerpt":"Minsik Cho이 [arXiv]에 게시한 'EpiCache: Episodic KV Cache Management for Long Conversational Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","KV Cache Management","Long Conversational QA","LLMs","Memory Efficiency","Episodic Clustering","Block Prefill Eviction","Sensitivity-aware Allocation"],"permalink":"/ai/review/2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process","title":"[논문리뷰] DiffusionNFT: Online Diffusion Reinforcement with Forward Process","excerpt":"Qinsheng Zhang이 [arXiv]에 게시한 'DiffusionNFT: Online Diffusion Reinforcement with Forward Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Diffusion Models","Reinforcement Learning","Online RL","Flow Matching","Forward Process","CFG-free","Image Generation","Negative-Aware FineTuning"],"permalink":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context","title":"[논문리뷰] DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context","excerpt":"Maunendra Sankar Desarkar이 [arXiv]에 게시한 'DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Cultural Adaptation","Large Language Models","Indian Culture","Dataset Creation","CSI","Human Evaluation","LLM Evaluation","Cultural Bias"],"permalink":"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models","title":"[논문리뷰] Cross-Attention is Half Explanation in Speech-to-Text Models","excerpt":"Luisa Bentivogli이 [arXiv]에 게시한 'Cross-Attention is Half Explanation in Speech-to-Text Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Cross-attention","Speech-to-Text (S2T)","Explainable AI (XAI)","Saliency Maps","Feature Attribution","Transformer","Context Mixing","Correlation"],"permalink":"/ai/review/2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment","title":"[논문리뷰] ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment","excerpt":"Yue Ma이 [arXiv]에 게시한 'ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Video Object Editing","Training-Free","Diffusion Transformers","Rectified Flow","Adaptive Context Enrichment","Guidance Responsiveness","Temporal Consistency","Image-to-Video"],"permalink":"/ai/review/2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects","title":"[논문리뷰] CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects","excerpt":"Hang Yu이 [arXiv]에 게시한 'CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Code Review","LLMs","Benchmark","Python Projects","End-to-End Evaluation","Context-Awareness","Software Engineering","LLM-as-a-Judge"],"permalink":"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces","title":"[논문리뷰] ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces","excerpt":"Jiafeng Xu이 [arXiv]에 게시한 'ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Robotics","Parallel Manipulator","Robotic Wrist","Confined Space Manipulation","Kinematics","Anthropomorphic Robot","Robot Design"],"permalink":"/ai/review/2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing","title":"[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?","excerpt":"Jaeho Lee이 [arXiv]에 게시한 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Auditory Knowledge","Large Language Models","Multimodal Reasoning","Benchmark","Chain-of-Thought","Auditory Imagination","Text-only Reasoning"],"permalink":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels","title":"[논문리뷰] Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels","excerpt":"Qi Zhang이 [arXiv]에 게시한 'Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Supervised Fine-Tuning (SFT)","Large Language Models (LLMs)","Model Knowledge","Closed-Book Question Answering (CBQA)","Parameter Restoration","Kullback-Leibler Divergence","Knowledge Forgetting"],"permalink":"/ai/review/2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations","title":"[논문리뷰] ARE: Scaling Up Agent Environments and Evaluations","excerpt":"Matteo Bettini이 [arXiv]에 게시한 'ARE: Scaling Up Agent Environments and Evaluations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Agent Environments","Agent Evaluation","LLM Agents","Asynchronous Systems","Reinforcement Learning","Tool Use","Multi-agent Collaboration","Benchmark"],"permalink":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers","title":"[논문리뷰] WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers","excerpt":"Karun Kumar이 [arXiv]에 게시한 'WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","ASR","Domain Adaptation","Text-Only Training","Transformer","Variational Autoencoder","Deep Supervision","Whisper","Encoder-Decoder Models"],"permalink":"/ai/review/2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents","title":"[논문리뷰] Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents","excerpt":"Chao Zhang이 [arXiv]에 게시한 'Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Role-playing Agents (RPAs)","Multimodal AI","Video Understanding","Large Language Models (LLMs)","Dataset Creation","Dynamic Role Profiles","Adaptive Temporal Sampling","Fine-tuning"],"permalink":"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation","title":"[논문리뷰] SPATIALGEN: Layout-guided 3D Indoor Scene Generation","excerpt":"Yongsen Mao이 [arXiv]에 게시한 'SPATIALGEN: Layout-guided 3D Indoor Scene Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Layout Guidance","Diffusion Models","Multi-view Synthesis","Synthetic Dataset","Indoor Environments","Gaussian Splatting","Semantic Consistency"],"permalink":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation","title":"[논문리뷰] RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation","excerpt":"Steven Liu이 [arXiv]에 게시한 'RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Code Generation","LLMs","Repository Planning","Graph-based Representation","Software Engineering","Agent Frameworks","Scalable Codebase"],"permalink":"/ai/review/2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes","title":"[논문리뷰] RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes","excerpt":"Narendra Ahuja이 [arXiv]에 게시한 'RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Camera Parameter Optimization","Dynamic Scenes","RGB-Only Supervision","Structure from Motion","Outlier Robustness","3D Gaussian Splatting","Two-stage Optimization","Point Tracking"],"permalink":"/ai/review/2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer","title":"[논문리뷰] MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer","excerpt":"jialingt이 [arXiv]에 게시한 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Hybrid Tokenizer","Text-to-Image Generation","Visual Question Answering","Autoregressive Model","Diffusion Decoder","Unified Architecture","Model Scaling"],"permalink":"/ai/review/2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation","title":"[논문리뷰] Lynx: Towards High-Fidelity Personalized Video Generation","excerpt":"Linjie Luo이 [arXiv]에 게시한 'Lynx: Towards High-Fidelity Personalized Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Personalized Video Generation","Diffusion Transformer","Identity Preservation","Video Synthesis","Adapter Networks","Facial Recognition","Cross-Attention"],"permalink":"/ai/review/2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification","title":"[논문리뷰] Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification","excerpt":"Wenyu Wang이 [arXiv]에 게시한 'Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Generative Modeling","Representation Learning","Classification","Unified Framework","Latent Space","Flow Matching","Deep Learning","Image Generation"],"permalink":"/ai/review/2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems","title":"[논문리뷰] Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems","excerpt":"Hung-yi Lee이 [arXiv]에 게시한 'Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Instruction-Guided TTS","Expressive Speech Synthesis","Human Perception","Subjective Evaluation","Controllability","Instruction Following","Evaluation Metrics"],"permalink":"/ai/review/2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model","title":"[논문리뷰] BaseReward: A Strong Baseline for Multimodal Reward Model","excerpt":"jianfeipan이 [arXiv]에 게시한 'BaseReward: A Strong Baseline for Multimodal Reward Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Multimodal Reward Model","MLLM Alignment","RLHF","Reward Head Architecture","Data Curation","Ensemble Methods","BaseReward"],"permalink":"/ai/review/2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent","title":"[논문리뷰] BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent","excerpt":"Jiahui Yang이 [arXiv]에 게시한 'BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","GUI Agent","Human-GUI Interaction","Cognitive Modeling","Reinforcement Learning","Multimodal Large Language Models","Attention Mechanisms","Action Planning"],"permalink":"/ai/review/2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue","title":"[논문리뷰] Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue","excerpt":"Hui Zhang이 [arXiv]에 게시한 'Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Embodied AI","Human-Robot Interaction","Multi-turn Dialogue","Instruction Following","Vision-Language Models","Diffusion Models","Ambiguity Resolution","Low-level Actions"],"permalink":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning","title":"[논문리뷰] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning","excerpt":"Jiangmiao이 [arXiv]에 게시한 'A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Robotics","Reinforcement Learning (RL)","Vision-Language-Action (VLA) Models","Reward Modeling","Human-in-the-Loop","Dense Rewards","Generalization","Autoregressive Models"],"permalink":"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance","title":"[논문리뷰] WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance","excerpt":"Ruibo Li이 [arXiv]에 게시한 'WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Video Diffusion Models","3D/4D Generation","Training-Free Guidance","Camera Trajectory Control","Novel View Synthesis","Geometric Consistency","Inference-Time Optimization"],"permalink":"/ai/review/2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding","title":"[논문리뷰] Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding","excerpt":"Rynson W. H. Lau이 [arXiv]에 게시한 'Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Spatio-Temporal Video Grounding","Multimodal Large Language Models","Zero-Shot Learning","Visual Grounding","Decomposed Spatio-Temporal Highlighting","Logit-Guided Re-attention","Temporal-Augmented Assembling"],"permalink":"/ai/review/2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation","title":"[논문리뷰] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation","excerpt":"Xihui Liu이 [arXiv]에 게시한 'Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Image Generation","Self-Supervised Learning","Visual Understanding","Masked Image Modeling","Contrastive Learning","Next-Token Prediction","LlamaGen"],"permalink":"/ai/review/2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data","title":"[논문리뷰] ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data","excerpt":"Zehao Li이 [arXiv]에 게시한 'ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Computer Use Agents","Vision-Language Models","Cross-Platform Data","GUI Automation","Data Scaling","Open-Source","Task Completion","GUI Grounding"],"permalink":"/ai/review/2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation","title":"[논문리뷰] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation","excerpt":"SpaceProduct이 [arXiv]에 게시한 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA) Model","Robot Manipulation","Human Demonstrations","Video Generative Pretraining","Ego-Centric Video","Trajectory Prediction","ActionVAE","Transformer"],"permalink":"/ai/review/2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems","title":"[논문리뷰] RecoWorld: Building Simulated Environments for Agentic Recommender Systems","excerpt":"Mingyuan Wu이 [arXiv]에 게시한 'RecoWorld: Building Simulated Environments for Agentic Recommender Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Agentic Recommender Systems","Simulated Environments","LLM-driven Simulation","Multi-turn Interaction","Reinforcement Learning","User Retention","Instruction Following","Multi-agent Systems"],"permalink":"/ai/review/2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration","title":"[논문리뷰] Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration","excerpt":"Zhilin Wang이 [arXiv]에 게시한 'Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","LLMs","Specification Alignment","Test-Time Deliberation","Safety-Behavior Trade-off","ALIGN3","SPECBENCH","Prompt Engineering"],"permalink":"/ai/review/2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks","title":"[논문리뷰] MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks","excerpt":"Xijun Gu이 [arXiv]에 게시한 'MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Instruction-based Image Editing","Dataset","Multi-modal LLM","Image Generation","Style Transfer","Multi-task Learning","Fine-tuning"],"permalink":"/ai/review/2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs","title":"[논문리뷰] Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs","excerpt":"Katharina von der Wense이 [arXiv]에 게시한 'Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Multiple-Choice QA","Tokenization","Prompt Sensitivity","Accuracy","Calibration","Model Ranking"],"permalink":"/ai/review/2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning","title":"[논문리뷰] FlowRL: Matching Reward Distributions for LLM Reasoning","excerpt":"Hengli Li이 [arXiv]에 게시한 'FlowRL: Matching Reward Distributions for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reward Distribution Matching","GFlowNets","Mode Collapse","Diverse Reasoning","Flow-Balanced Optimization"],"permalink":"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning","title":"[논문리뷰] FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning","excerpt":"Jiashuo Liu이 [arXiv]에 게시한 'FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Financial LLMs","Agent Benchmarking","Open-domain Search","Financial Reasoning","Time-Sensitive Data","Multi-hop QA","Tool Use"],"permalink":"/ai/review/2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection","title":"[논문리뷰] FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection","excerpt":"Zhewei Zhang이 [arXiv]에 게시한 'FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Change Detection","Remote Sensing","Frequency-Spatial Analysis","Wavelet Transform","Attention Mechanism","Gated Fusion","Deep Learning"],"permalink":"/ai/review/2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation","title":"[논문리뷰] Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation","excerpt":"Kishan Panaganti이 [arXiv]에 게시한 'Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Label-free Reinforcement Learning","LLMs","Self-improvement","Entropy Collapse","Novelty Reward","Test-Time RL","GRPO","Evolutionary Computing Principles"],"permalink":"/ai/review/2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence","title":"[논문리뷰] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence","excerpt":"Qinghua Huang이 [arXiv]에 게시한 'EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Ultrasound Imaging","Medical Diagnosis","Mixture-of-Experts (MoE)","Instruction Tuning","Multimodal AI","Report Generation","VQA"],"permalink":"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-19-AToken-A-Unified-Tokenizer-for-Vision","title":"[논문리뷰] AToken: A Unified Tokenizer for Vision","excerpt":"Mingze Xu이 [arXiv]에 게시한 'AToken: A Unified Tokenizer for Vision' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Unified Visual Tokenizer","Multimodal AI","Transformer Architecture","4D Representation","Adversarial-free Training","Reconstruction","Semantic Understanding","Generative Models"],"permalink":"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication","title":"[논문리뷰] Wan-Animate: Unified Character Animation and Replacement with Holistic Replication","excerpt":"Mingyang Huang이 [arXiv]에 게시한 'Wan-Animate: Unified Character Animation and Replacement with Holistic Replication' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Character Animation","Video Replacement","Diffusion Models","Transformer","DiT","Relighting LoRA","Holistic Replication","Open-Source"],"permalink":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning","title":"[논문리뷰] THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning","excerpt":"Yicheng Pan이 [arXiv]에 게시한 'THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Tool-Integrated Reasoning","Reinforcement Learning","Hierarchical Optimization","Self-Correction","Large Language Models","Code Generation"],"permalink":"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs","title":"[논문리뷰] SteeringControl: Holistic Evaluation of Alignment Steering in LLMs","excerpt":"Zhun Wang이 [arXiv]에 게시한 'SteeringControl: Holistic Evaluation of Alignment Steering in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","LLM Alignment","Representation Steering","Benchmark","Behavioral Entanglement","Bias Mitigation","Harmful Generation","Hallucination Control","Modular Framework"],"permalink":"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning","title":"[논문리뷰] Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning","excerpt":"Zhou Yang이 [arXiv]에 게시한 'Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Code Language Models","Machine Unlearning","Sensitive Memorization","Privacy","Gradient Ascent","Model Utility","Code Generation"],"permalink":"/ai/review/2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-SAIL-VL2-Technical-Report","title":"[논문리뷰] SAIL-VL2 Technical Report","excerpt":"Zijian Kang이 [arXiv]에 게시한 'SAIL-VL2 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Multimodal Understanding","Mixture-of-Experts","Progressive Training","Data Curation","Supervised Fine-tuning","Reinforcement Learning","SAIL-ViT"],"permalink":"/ai/review/2025-9-18-SAIL-VL2-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era","title":"[논문리뷰] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era","excerpt":"Zihao Dongfang이 [arXiv]에 게시한 'PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Omnidirectional Vision","Embodied AI","Panoramic Perception","Multi-modal Learning","Dataset Development","Robot Navigation","Spatial Reasoning","System Architecture"],"permalink":"/ai/review/2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook","title":"[논문리뷰] MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook","excerpt":"Bowen Zhou이 [arXiv]에 게시한 'MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Large Language Models (LLMs)","Multimodal Large Language Models (MLLMs)","Visual Grounding","Visual Question Answering","Advertisement Video Analysis","Real-world Scenarios","Challenge Benchmark"],"permalink":"/ai/review/2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning","title":"[논문리뷰] Improving Context Fidelity via Native Retrieval-Augmented Reasoning","excerpt":"Xiangru Tang이 [arXiv]에 게시한 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Context Fidelity","Retrieval-Augmented Generation (RAG)","Large Language Models (LLMs)","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Hallucination","Question Answering","In-context Retrieval","Curriculum Learning"],"permalink":"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale","title":"[논문리뷰] Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale","excerpt":"Bernard Ghanem이 [arXiv]에 게시한 'Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Arabic NLP","Instruction Tuning","Machine Translation","Large Language Models","FP8 Quantization","Data Bootstrapping","Model Merging","Language-Centric AI"],"permalink":"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam","title":"[논문리뷰] GenExam: A Multidisciplinary Text-to-Image Exam","excerpt":"Yu Qiao이 [arXiv]에 게시한 'GenExam: A Multidisciplinary Text-to-Image Exam' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Multidisciplinary","Benchmark","Evaluation","AGI","Reasoning","Scoring System","Visual Question Answering"],"permalink":"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research","title":"[논문리뷰] WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research","excerpt":"Houquan Zhou이 [arXiv]에 게시한 'WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Open-Ended Deep Research","LLM Agents","Dynamic Outline","Evidence Acquisition","Hierarchical Writing","Memory Bank","State-of-the-Art","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning","title":"[논문리뷰] WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning","excerpt":"Huifeng Yin이 [arXiv]에 게시한 'WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Web Agents","Reinforcement Learning","Synthetic Data","Knowledge Graphs","LLMs","Supervised Fine-Tuning","Sim-to-Real Transfer","Agentic AI"],"permalink":"/ai/review/2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents","title":"[논문리뷰] WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents","excerpt":"Wenbiao Yin이 [arXiv]에 게시한 'WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Agentic AI","Deep Research","Iterative Reasoning","Long-Horizon Tasks","Context Management","Data Synthesis","Tool-Augmented LLMs","Markov Decision Process"],"permalink":"/ai/review/2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling","title":"[논문리뷰] Towards General Agentic Intelligence via Environment Scaling","excerpt":"Guangyu Li이 [arXiv]에 게시한 'Towards General Agentic Intelligence via Environment Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Agentic AI","Environment Scaling","Function Calling","Tool Use","Large Language Models","Synthetic Data Generation","Supervised Fine-tuning"],"permalink":"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Single-stream-Policy-Optimization","title":"[논문리뷰] Single-stream Policy Optimization","excerpt":"Zihan Ding이 [arXiv]에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Optimization","Policy Gradient","Variance Reduction","Adaptive Sampling","Scalability","Agentic Systems","RLVR"],"permalink":"/ai/review/2025-9-17-Single-stream-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Scaling-Agents-via-Continual-Pre-training","title":"[논문리뷰] Scaling Agents via Continual Pre-training","excerpt":"Guangyu Li이 [arXiv]에 게시한 'Scaling Agents via Continual Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Agentic LLMs","Continual Pre-training","Deep Research Agents","Tool Use","Multi-step Reasoning","Data Synthesis","Scaling Laws"],"permalink":"/ai/review/2025-9-17-Scaling-Agents-via-Continual-Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization","title":"[논문리뷰] ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization","excerpt":"Litu Ou이 [arXiv]에 게시한 'ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","LLM Agents","Context Management","Summarization","ReAct","Reinforcement Learning","Web Search","Long-Horizon Reasoning"],"permalink":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs","title":"[논문리뷰] Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs","excerpt":"Luca Benini이 [arXiv]에 게시한 'Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","LLM Compression","Quantization","Sparsification","Post-training Quantization","Hessian-based Optimization","Error Compensation","Low-bit LLMs"],"permalink":"/ai/review/2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis","title":"[논문리뷰] Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis","excerpt":"Bo Liu이 [arXiv]에 게시한 'Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Multiple Instance Learning","Hard Instance Mining","Computational Pathology","Whole Slide Images","Masked Learning","Siamese Network","Medical Image Analysis"],"permalink":"/ai/review/2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge","title":"[논문리뷰] Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge","excerpt":"Wentao Zhang이 [arXiv]에 게시한 'Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Science AI","Caption-assisted Reasoning","SeePhys Challenge","Large Language Models","Visual Question Answering","Physics Problems","Cross-modal Alignment"],"permalink":"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation","title":"[논문리뷰] Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation","excerpt":"Lixin Xu이 [arXiv]에 게시한 'Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","3D Asset Generation","AI Pipeline","Generative AI","Game Development","Diffusion Models","Neural Modules","Retopology","UV Unwrapping"],"permalink":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms","title":"[논문리뷰] Exact Coset Sampling for Quantum Lattice Algorithms","excerpt":"Yifan Zhang이 [arXiv]에 게시한 'Exact Coset Sampling for Quantum Lattice Algorithms' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Quantum Algorithms","Lattice Problems","Coset Sampling","Quantum Fourier Transform (QFT)","Modular Arithmetic","Quantum Cryptography","Exact Sampling"],"permalink":"/ai/review/2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving","title":"[논문리뷰] EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving","excerpt":"Shansan Gong이 [arXiv]에 게시한 'EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","LLM","Test-Time Scaling","Chain-of-Thought","Reinforcement Learning","Efficiency Optimization","Token Cost","Sampling Cost","Dynamic CoT Switching"],"permalink":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model","title":"[논문리뷰] 3D Aware Region Prompted Vision Language Model","excerpt":"Xiaolong Li이 [arXiv]에 게시한 '3D Aware Region Prompted Vision Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","3D Vision","Vision-Language Models","Spatial Reasoning","Region Prompting","Multi-view Learning","Depth Estimation","Unified Representation","Generative AI"],"permalink":"/ai/review/2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning","title":"[논문리뷰] UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning","excerpt":"Yongliang Shen이 [arXiv]에 게시한 'UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","GUI Automation","Reinforcement Learning","Semi-online RL","Offline RL","Online RL","Patch Module","Multi-turn Interaction","Large Language Models"],"permalink":"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation","title":"[논문리뷰] SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation","excerpt":"Heshaam Faili이 [arXiv]에 게시한 'SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","LLM","Instruction Tuning","Domain Adaptation","Retrieval-Augmented Generation","Dataset Creation","Model Editing","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits","title":"[논문리뷰] PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits","excerpt":"Zhenhao Chen이 [arXiv]에 게시한 'PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Multimodal Dataset","LLM Inference","Behavioral Traits","Causal Representation Learning","Big Five","Multimodal AI","Causal Discovery","Human-Computer Interaction"],"permalink":"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling","title":"[논문리뷰] OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling","excerpt":"Yang Zhou이 [arXiv]에 게시한 'OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","4D World Modeling","Multi-Modal Dataset","Multi-Domain Data","Geometric Foundation Models","Video Generation","Spatio-Temporal Data","Dataset Benchmark"],"permalink":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models","title":"[논문리뷰] Measuring Epistemic Humility in Multimodal Large Language Models","excerpt":"Kaiyang Zhou이 [arXiv]에 게시한 'Measuring Epistemic Humility in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Hallucination","Epistemic Humility","Benchmark","False-Option Rejection","Visual Question Answering","Scene Graph"],"permalink":"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models","title":"[논문리뷰] Lost in Embeddings: Information Loss in Vision-Language Models","excerpt":"Ivan Vulić이 [arXiv]에 게시한 'Lost in Embeddings: Information Loss in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Information Loss","Embeddings","Connectors","k-NN Overlap Ratio","Embedding Reconstruction","Multimodal AI"],"permalink":"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models","title":"[논문리뷰] Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models","excerpt":"Shuo Ren이 [arXiv]에 게시한 'Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Visual Reasoning","Reflection","Reinforcement Learning","Visual Attention","Slow Thinking","Multimodal Agents"],"permalink":"/ai/review/2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics","title":"[논문리뷰] Locality in Image Diffusion Models Emerges from Data Statistics","excerpt":"Vincent Sitzmann이 [arXiv]에 게시한 'Locality in Image Diffusion Models Emerges from Data Statistics' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Diffusion Models","Locality","Data Statistics","Optimal Denoiser","Wiener Filter","Sensitivity Fields","Generative Models","Inductive Bias"],"permalink":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting","title":"[논문리뷰] Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting","excerpt":"Changlong Yu이 [arXiv]에 게시한 'Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Multi-objective Reinforcement Learning","LLM Alignment","Dynamic Reward Weighting","Pareto Front Optimization","Hypervolume Indicator","Gradient-based Optimization","Online RL"],"permalink":"/ai/review/2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence","title":"[논문리뷰] LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence","excerpt":"Lionel M. Ni이 [arXiv]에 게시한 'LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Multi-Modal Transformers","Drag-based Editing","Explicit Correspondence","Attention Control","Identity Preservation","Training-Free"],"permalink":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts","title":"[논문리뷰] InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts","excerpt":"Wenzhe Cai이 [arXiv]에 게시한 'InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Embodied AI","3D Scene Dataset","Simulation Environment","Scene Generation","Point-Goal Navigation","Realistic Layouts","Object Interaction","Real-to-Sim"],"permalink":"/ai/review/2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings","title":"[논문리뷰] GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings","excerpt":"Yixuan Tang이 [arXiv]에 게시한 'GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Model Pruning","Domain Adaptation","Embedding Models","Gradient Alignment","Fisher Information","Model Compression","LLMs"],"permalink":"/ai/review/2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI","title":"[논문리뷰] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI","excerpt":"UVSKKR이 [arXiv]에 게시한 'EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Ethical Reasoning","Mental Health AI","Benchmark Dataset","Large Language Models","AI Ethics","Clinical Decision Support","Human-in-the-loop"],"permalink":"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding","title":"[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding","excerpt":"Li Zheng이 [arXiv]에 게시한 'Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Video Hallucination","Large Video Models (LVMs)","Hierarchical Reasoning","Spatial-Temporal Grounding","Diagnostic Framework","Benchmark Dataset","Multimodal AI"],"permalink":"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media","title":"[논문리뷰] CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media","excerpt":"Subasish Das이 [arXiv]에 게시한 'CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Sentiment Analysis","Narrative Analysis","Decentralized Social Media","Bluesky","Transformer Models","Topic Modeling","Real-time Processing","Data Visualization"],"permalink":"/ai/review/2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition","title":"[논문리뷰] X-Part: high fidelity and structure coherent shape decomposition","excerpt":"Yunhan Yang이 [arXiv]에 게시한 'X-Part: high fidelity and structure coherent shape decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","3D Shape Decomposition","Diffusion Models","Part-level Generation","Controllable Generation","Bounding Box Prompts","Semantic Features","Interactive Editing","Generative AI"],"permalink":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-Virtual-Agent-Economies","title":"[논문리뷰] Virtual Agent Economies","excerpt":"William A. Cunningham이 [arXiv]에 게시한 'Virtual Agent Economies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","AI Agents","Virtual Economy","Multi-Agent Systems","Economic Mechanisms","Governance","Blockchain","Resource Allocation","Agent Alignment"],"permalink":"/ai/review/2025-9-15-Virtual-Agent-Economies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions","title":"[논문리뷰] VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions","excerpt":"Dong Zhang이 [arXiv]에 게시한 'VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Voice Style Adaptation","Spoken Language Models","Benchmark","LALM-as-a-Judge","Speech Generation","Multilingual","Evaluation Framework"],"permalink":"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs","title":"[논문리뷰] The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs","excerpt":"Jonas Geiping이 [arXiv]에 게시한 'The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Long-Horizon Tasks","Execution Capability","Scaling Laws","Self-Conditioning","Thinking Models","Agentic AI"],"permalink":"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading","title":"[논문리뷰] QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading","excerpt":"Chenyu You이 [arXiv]에 게시한 'QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","High-Frequency Trading","Multi-Agent Systems","Large Language Models","Technical Analysis","Algorithmic Trading","Financial Reasoning","Price-Driven Signals"],"permalink":"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools","title":"[논문리뷰] MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools","excerpt":"Xiaorui Wang이 [arXiv]에 게시한 'MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Language Agents","Tool Use","Benchmarks","Model Context Protocol (MCP)","LLM Evaluation","Agentic AI","Real-World Performance"],"permalink":"/ai/review/2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios","title":"[논문리뷰] LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios","excerpt":"Bing Su이 [arXiv]에 게시한 'LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Long-tailed Learning","Semi-Supervised Learning","Parameter-Efficient Fine-Tuning","Foundation Models","Open-World Scenarios","OOD Detection","Confidence Calibration"],"permalink":"/ai/review/2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations","title":"[논문리뷰] IntrEx: A Dataset for Modeling Engagement in Educational Conversations","excerpt":"Gabriele Pergola이 [arXiv]에 게시한 'IntrEx: A Dataset for Modeling Engagement in Educational Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Educational Dialogue","Engagement Modeling","Dataset Annotation","Second Language Learning","Human Feedback","LLM Alignment","Readability Metrics"],"permalink":"/ai/review/2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models","title":"[논문리뷰] Inpainting-Guided Policy Optimization for Diffusion Large Language Models","excerpt":"Chenyu Wang이 [arXiv]에 게시한 'Inpainting-Guided Policy Optimization for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Reinforcement Learning","Inpainting","Policy Optimization","Exploration","Mathematical Reasoning","GRPO"],"permalink":"/ai/review/2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis","title":"[논문리뷰] InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis","excerpt":"Song Guo이 [arXiv]에 게시한 'InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Image Synthesis","Resolution-Agnostic","Diffusion Models","Latent Space","VAE Decoder","High-Resolution Image Generation","Generative AI","Transformer Architecture"],"permalink":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering","title":"[논문리뷰] HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering","excerpt":"Zhehao Tan이 [arXiv]에 게시한 'HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation","Multi-hop QA","Noise Resistance","LLM","Query Decomposition","Adaptive Retrieval","Heuristic Framework","Revelator"],"permalink":"/ai/review/2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies","title":"[논문리뷰] FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies","excerpt":"Fabian Otto이 [arXiv]에 게시한 'FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Generalist Robot Policies","Vision-Language-Action Models","Efficient AI","Imitation Learning","Diffusion Models","Intermediate Fusion","Robotics"],"permalink":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China","title":"[논문리뷰] CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China","excerpt":"XU Han이 [arXiv]에 게시한 'CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Headline Generation","Minority Languages","Low-Resource NLP","Dataset","Benchmark","Natural Language Generation","Chinese Minority Languages"],"permalink":"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding","title":"[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding","excerpt":"Ethan Chern이 [arXiv]에 게시한 'Visual Programmability: A Guide for Code-as-Thought in Chart Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Visual Programmability","Code-as-Thought (CaT)","Chart Understanding","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Adaptive Reasoning","Dual-Reward System","Multimodal AI"],"permalink":"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model","title":"[논문리뷰] VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model","excerpt":"Zirui Ge이 [arXiv]에 게시한 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","Multimodal Learning","Efficient AI","Model Adaptation","Bridge Attention","Low-resource Training"],"permalink":"/ai/review/2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward","title":"[논문리뷰] The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward","excerpt":"Xiaoyu Tan이 [arXiv]에 게시한 'The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models (LLMs)","Diversity Collapse","f-divergence","Forward-KL","JS-divergence","Pass@k","Catastrophic Forgetting"],"permalink":"/ai/review/2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations","title":"[논문리뷰] SpatialVID: A Large-Scale Video Dataset with Spatial Annotations","excerpt":"Jian Gao이 [arXiv]에 게시한 'SpatialVID: A Large-Scale Video Dataset with Spatial Annotations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Video Dataset","Spatial Annotation","Camera Pose Estimation","Depth Map","Structured Caption","Motion Instruction","3D Vision","World Modeling"],"permalink":"/ai/review/2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning","title":"[논문리뷰] SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning","excerpt":"Zhaohui Yang이 [arXiv]에 게시한 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Vision-Language-Action (VLA) Models","Robotic Manipulation","Data Scarcity","Generalization","Sim-to-Real Transfer","Online RL","Long-Horizon Planning"],"permalink":"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated","title":"[논문리뷰] Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated","excerpt":"Jamie Hayes이 [arXiv]에 게시한 'Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","LLM Security","Data Poisoning","Chain-of-Thought","Reasoning Models","Backdoor Attacks","CoT Unfaithfulness","Emergent Robustness"],"permalink":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning","title":"[논문리뷰] OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning","excerpt":"Yuzheng Zhuang이 [arXiv]에 게시한 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Embodied AI","Multimodal LLMs","3D Grounding","Task-Adaptive Reasoning","Embodiment-Aware Planning","Robotics","Spatial Reasoning"],"permalink":"/ai/review/2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation","title":"[논문리뷰] Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation","excerpt":"Dong-Ho Lee이 [arXiv]에 게시한 'Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Multimodal Recommendation","Modality Alignment","Attention Mechanism","Dilated Convolution","Maximum Mean Discrepancy","Contrastive Learning","Dimensionality Reduction"],"permalink":"/ai/review/2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering","title":"[논문리뷰] LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering","excerpt":"Jianguo Zhang이 [arXiv]에 게시한 'LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Long-Context LLMs","Software Engineering","Code Evaluation","Benchmark","Multi-file Reasoning","Architectural Understanding","Context Length","Software Development Lifecycle","Metrics"],"permalink":"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis","title":"[논문리뷰] Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis","excerpt":"Wentao Hu이 [arXiv]에 게시한 'Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Avatar Animation","Multimodal Instructions","Long-Duration Video Generation","MLLM Director","Cascaded Framework","Lip Synchronization","Instruction Grounding","Video Diffusion Transformers"],"permalink":"/ai/review/2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning","title":"[논문리뷰] HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning","excerpt":"Zhuowei Chen이 [arXiv]에 게시한 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Human-Centric Video Generation","Multimodal Conditioning","Text-to-Video","Image-to-Video","Audio-to-Video","Diffusion Models","Subject Preservation","Audio-Visual Synchronization","Progressive Training"],"permalink":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents","title":"[논문리뷰] Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents","excerpt":"Xintao Wang이 [arXiv]에 게시한 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Policy Gradients","Entropy Modulation","Credit Assignment","Uncertainty","Long-Horizon Tasks","Self-Calibrating Gradient Scaling"],"permalink":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval","title":"[논문리뷰] Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval","excerpt":"Kaicheng Yang이 [arXiv]에 게시한 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Text-based Person Retrieval","CLIP","MLLM","Data Curation","Dual-Masking","Gradient-Attention","WebPerson Dataset"],"permalink":"/ai/review/2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark","title":"[논문리뷰] FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark","excerpt":"Shuai Bai이 [arXiv]에 게시한 'FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reasoning Dataset","Benchmark","Generation Chain-of-Thought","Vision-Language Model","Image Aesthetics","Prompt Alignment"],"permalink":"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs","title":"[논문리뷰] EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs","excerpt":"Kaiqi Kou이 [arXiv]에 게시한 'EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Speech-to-Speech LLMs","Acoustic-Semantic Gap","Echo Training","Unit Language","Streaming Inference","Knowledge-based QA"],"permalink":"/ai/review/2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist","title":"[논문리뷰] Can Understanding and Generation Truly Benefit Together -- or Just Coexist?","excerpt":"Hui Han이 [arXiv]에 게시한 'Can Understanding and Generation Truly Benefit Together -- or Just Coexist?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Multimodal Understanding","Multimodal Generation","Unified Models","Auto-Encoder","Reinforcement Learning","Image-to-Text","Text-to-Image","Reconstruction Fidelity"],"permalink":"/ai/review/2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting","title":"[논문리뷰] 2D Gaussian Splatting with Semantic Alignment for Image Inpainting","excerpt":"Guangming Lu이 [arXiv]에 게시한 '2D Gaussian Splatting with Semantic Alignment for Image Inpainting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Image Inpainting","2D Gaussian Splatting","Semantic Alignment","DINO Features","Patch-level Rasterization","Continuous Representation","Generative Models"],"permalink":"/ai/review/2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs","title":"[논문리뷰] <think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs","excerpt":"Alexander Panchenko이 [arXiv]에 게시한 '<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Toxic Text Generation","LLMs","Text Detoxification","Lexical Diversity","Synthetic Data","Human Annotation","Style Transfer"],"permalink":"/ai/review/2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation","title":"[논문리뷰] RewardDance: Reward Scaling in Visual Generation","excerpt":"Liang Li이 [arXiv]에 게시한 'RewardDance: Reward Scaling in Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Reward Model","Visual Generation","RLHF","VLM","Reward Scaling","Reward Hacking","Generative Paradigm","Context Scaling","Text-to-Image","Text-to-Video"],"permalink":"/ai/review/2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-P3-SAM-Native-3D-Part-Segmentation","title":"[논문리뷰] P3-SAM: Native 3D Part Segmentation","excerpt":"Yunhan Yang이 [arXiv]에 게시한 'P3-SAM: Native 3D Part Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","3D Part Segmentation","Point Cloud Segmentation","Prompt-based Segmentation","Deep Learning","Transformer","Interactive Segmentation","Automatic Segmentation","Native 3D"],"permalink":"/ai/review/2025-9-11-P3-SAM-Native-3D-Part-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-Hunyuan-MT-Technical-Report","title":"[논문리뷰] Hunyuan-MT Technical Report","excerpt":"Yang Du이 [arXiv]에 게시한 'Hunyuan-MT Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Machine Translation","Large Language Model","Multilingual","Low-Resource Languages","Reinforcement Learning","Weak-to-Strong Learning","Slow Thinking"],"permalink":"/ai/review/2025-9-11-Hunyuan-MT-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants","title":"[논문리뷰] HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants","excerpt":"Jacy Reese Anthis이 [arXiv]에 게시한 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Human Agency","AI Assistants","LLM Evaluation","Benchmark","Sociotechnical AI","AI Alignment","Scalable Evaluation"],"permalink":"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI","title":"[논문리뷰] EnvX: Agentize Everything with Agentic AI","excerpt":"Wenzheng Tom Tang이 [arXiv]에 게시한 'EnvX: Agentize Everything with Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Agentic AI","Multi-Agent Systems","Code Repository","Agentization","Natural Language Interaction","Agent-to-Agent Protocol","LLM-based Agents"],"permalink":"/ai/review/2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","title":"[논문리뷰] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning","excerpt":"Honglin Guo이 [arXiv]에 게시한 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Multi-Turn Interaction","Long-Horizon Decision Making","Agent Framework","Exploration-Exploitation","Progressive Scaling"],"permalink":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models","title":"[논문리뷰] A Survey of Reinforcement Learning for Large Reasoning Models","excerpt":"Runze Liu이 [arXiv]에 게시한 'A Survey of Reinforcement Learning for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Reasoning Models","LLMs","Reward Design","Policy Optimization","Verifiable Rewards","Agentic AI","Multimodal AI"],"permalink":"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-11-3D-and-4D-World-Modeling-A-Survey","title":"[논문리뷰] 3D and 4D World Modeling: A Survey","excerpt":"Ao Liang이 [arXiv]에 게시한 '3D and 4D World Modeling: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","3D World Modeling","4D World Modeling","Generative Models","Predictive Models","LiDAR","Occupancy Grids","Video Generation","Autonomous Driving","Robotics"],"permalink":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR","title":"[논문리뷰] ΔL Normalization: Rethink Loss Aggregation in RLVR","excerpt":"Lili Qiu이 [arXiv]에 게시한 'ΔL Normalization: Rethink Loss Aggregation in RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Gradient Variance","Loss Aggregation","Unbiased Estimator","RLVR","Policy Gradient","Normalization"],"permalink":"/ai/review/2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models","title":"[논문리뷰] Visual Representation Alignment for Multimodal Large Language Models","excerpt":"Heeseong Shin이 [arXiv]에 게시한 'Visual Representation Alignment for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Visual Representation Alignment","Foundation Models","Regularization","Fine-grained Visual Understanding","Spatial Reasoning","Object Counting","Vision-Language Models"],"permalink":"/ai/review/2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward","title":"[논문리뷰] UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward","excerpt":"Fei Ding이 [arXiv]에 게시한 'UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Image Customization","Multi-Identity Generation","Identity Consistency","Identity Confusion","Reinforcement Learning","Diffusion Models","Matching Reward","Global Assignment"],"permalink":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding","title":"[논문리뷰] Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding","excerpt":"Yongcheng Zeng이 [arXiv]에 게시한 'Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","RLVR","LLM Reasoning","Adaptive Learning","Hint Scaffolding","Item Response Theory","Exploration Efficiency","Problem Difficulty","Policy Optimization"],"permalink":"/ai/review/2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge","title":"[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge","excerpt":"Dipanjan Das이 [arXiv]에 게시한 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","LLM Factuality","Parametric Knowledge","Benchmark","Question Answering","Data Curation","Evaluation Metrics","Hallucination Mitigation","Large Language Models"],"permalink":"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models","title":"[논문리뷰] Reconstruction Alignment Improves Unified Multimodal Models","excerpt":"XuDong Wang이 [arXiv]에 게시한 'Reconstruction Alignment Improves Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Image Generation","Image Editing","Post-training","Self-supervised Learning","Reconstruction Alignment","Visual Embeddings"],"permalink":"/ai/review/2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling","title":"[논문리뷰] Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling","excerpt":"Diana Marculescu이 [arXiv]에 게시한 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Quantization","Few-Step Generation","Model Compression","Noise Scheduling","Post-Training Quantization","Image Quality Metrics","Latent Consistency Models"],"permalink":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning","title":"[논문리뷰] Parallel-R1: Towards Parallel Thinking via Reinforcement Learning","excerpt":"Xinyu Yang이 [arXiv]에 게시한 'Parallel-R1: Towards Parallel Thinking via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Large Language Models","Parallel Thinking","Reinforcement Learning","Mathematical Reasoning","Progressive Curriculum","Reward Design","Exploration Scaffold"],"permalink":"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search","title":"[논문리뷰] Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search","excerpt":"Tianjian Li이 [arXiv]에 게시한 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Visual Search","Multi-Turn Reasoning","Reinforcement Learning","Tool-Integrated Agents","Exploratory Reasoning","Data Augmentation","Over-turn Masking","Visual Language Models"],"permalink":"/ai/review/2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Language-Self-Play-For-Data-Free-Training","title":"[논문리뷰] Language Self-Play For Data-Free Training","excerpt":"Vijai Mohan이 [arXiv]에 게시한 'Language Self-Play For Data-Free Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Self-Play","Data-Free Training","Instruction Following","Adversarial Training","Reward Modeling"],"permalink":"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions","title":"[논문리뷰] F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions","excerpt":"Zherui Qiu이 [arXiv]에 게시한 'F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Vision-Language-Action","Embodied AI","Visual Foresight","Predictive Inverse Dynamics","Mixture-of-Transformer","Robot Manipulation","Multi-stage Training","Generalization"],"permalink":"/ai/review/2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference","title":"[논문리뷰] Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference","excerpt":"Yingfang Zhang이 [arXiv]에 게시한 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Reinforcement Learning","Human Preference","Text-to-Image Generation","Reward Hacking","Direct-Align","SRPO","Fine-Grained Control","Flow Matching Models"],"permalink":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology","title":"[논문리뷰] Curia: A Multi-Modal Foundation Model for Radiology","excerpt":"Elodie Ferreres이 [arXiv]에 게시한 'Curia: A Multi-Modal Foundation Model for Radiology' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Foundation Model","Radiology","Computed Tomography (CT)","Magnetic Resonance Imaging (MRI)","Self-supervised Learning","Vision Transformer","Cross-Modality Generalization"],"permalink":"/ai/review/2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-10-Causal-Attention-with-Lookahead-Keys","title":"[논문리뷰] Causal Attention with Lookahead Keys","excerpt":"Quanquan Gu이 [arXiv]에 게시한 'Causal Attention with Lookahead Keys' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Causal Attention","Lookahead Keys","Autoregressive Modeling","Language Models","Transformer","Perplexity Reduction","Parallel Training","Efficient Inference"],"permalink":"/ai/review/2025-9-10-Causal-Attention-with-Lookahead-Keys/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents","title":"[논문리뷰] WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents","excerpt":"Aili Chen이 [arXiv]에 게시한 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Web Agents","Long-Horizon Reasoning","Large Language Models (LLMs)","Data Generation","Reinforcement Learning (RL)","Supervised Fine-tuning (SFT)","Web Navigation","Information Retrieval"],"permalink":"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts","title":"[논문리뷰] UniVerse-1: Unified Audio-Video Generation via Stitching of Experts","excerpt":"Xinyao Liao이 [arXiv]에 게시한 'UniVerse-1: Unified Audio-Video Generation via Stitching of Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Unified Audio-Video Generation","Stitching of Experts (SoE)","Multimodal Diffusion","Online Annotation","Cross-modal Noise Correlation","Foundation Models","Verse-Bench"],"permalink":"/ai/review/2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet","title":"[논문리뷰] Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet","excerpt":"See-Kiong Ng이 [arXiv]에 게시한 'Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Test-Time Scaling","Reasoning Models","Knowledge-Intensive Tasks","Hallucinations","Factual Accuracy","Chain-of-Thought","Large Language Models"],"permalink":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers","title":"[논문리뷰] Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers","excerpt":"Xia Xiao이 [arXiv]에 게시한 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","LLM Step-Provers","Reinforcement Learning (RL)","Off-Policy RL","Multi-Agent Systems","Tree Search","Automated Theorem Proving (ATP)","Formal Mathematics","AlphaZero"],"permalink":"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem","title":"[논문리뷰] Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem","excerpt":"Damien Sileo이 [arXiv]에 게시한 'Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","LLM","Mathematical Reasoning","Synthetic Data Generation","TPTP Ecosystem","Saturation Proving","Proof Graph Reconstruction","Data Augmentation"],"permalink":"/ai/review/2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World","title":"[논문리뷰] R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World","excerpt":"Bowen Zhou이 [arXiv]에 게시한 'R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","AI Safety","Resistant AI","Resilient AI","Coevolution","Fast-Slow Models","Adversarial Training","Continual Learning","AGI Alignment"],"permalink":"/ai/review/2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models","title":"[논문리뷰] Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models","excerpt":"Ke Shen이 [arXiv]에 게시한 'Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Reinforcement Learning","Trajectory-aware RL","Value Model","Masked Diffusion Models","Large Language Models","Reasoning Tasks","Code Generation"],"permalink":"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation","title":"[논문리뷰] Reverse-Engineered Reasoning for Open-Ended Generation","excerpt":"Wangchunshu Zhou이 [arXiv]에 게시한 'Reverse-Engineered Reasoning for Open-Ended Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Deep Reasoning","Open-Ended Generation","Reverse-Engineered Reasoning (REER)","LLMs","Synthetic Data","Iterative Refinement","Perplexity Minimization","DeepWriting-20K"],"permalink":"/ai/review/2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey","title":"[논문리뷰] Reinforcement Learning Foundations for Deep Research Systems: A Survey","excerpt":"Wei Han이 [arXiv]에 게시한 'Reinforcement Learning Foundations for Deep Research Systems: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Deep Research Systems","Agentic AI","Tool Use","Hierarchical Agents","Reward Design","Multimodal AI","RL Frameworks"],"permalink":"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Reinforced-Visual-Perception-with-Tools","title":"[논문리뷰] Reinforced Visual Perception with Tools","excerpt":"Mingyang Fu이 [arXiv]에 게시한 'Reinforced Visual Perception with Tools' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Visual Reasoning","Multimodal LLMs","Reinforcement Learning","Tool Usage","Perception-heavy Benchmarks","GRPO","Vision Tools"],"permalink":"/ai/review/2025-9-9-Reinforced-Visual-Perception-with-Tools/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents","title":"[논문리뷰] Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents","excerpt":"James Zou이 [arXiv]에 게시한 'Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","AI Agents","Research Reproducibility","Scientific Communication","Model Context Protocol (MCP)","Natural Language Interaction","Genomics","Single-Cell Analysis","Spatial Transcriptomics"],"permalink":"/ai/review/2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents","title":"[논문리뷰] MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents","excerpt":"Zhengxi Lu이 [arXiv]에 게시한 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Mobile GUI Agents","Hybrid Automation","Shortcut Generation","Benchmark","Task Efficiency","LLM-based Agents","Mobile Robotics"],"permalink":"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian","title":"[논문리뷰] Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian","excerpt":"Hoi-Fong Mak이 [arXiv]에 게시한 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Multilingual LLM","Low-Resource Language","German","Bavarian Dialect","Cross-Lingual Transfer","Continuous Pretraining","Llama-3.1","Model Expansion"],"permalink":"/ai/review/2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation","title":"[논문리뷰] Interleaving Reasoning for Better Text-to-Image Generation","excerpt":"Shixiang Tang이 [arXiv]에 게시한 'Interleaving Reasoning for Better Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Interleaving Reasoning","Multimodal Learning","Visual Quality","Fine-grained Detail","Diffusion Models","Self-Correction"],"permalink":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning","title":"[논문리뷰] Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning","excerpt":"Baolong Bi이 [arXiv]에 게시한 'Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Visual Reasoning","Attention Mechanisms","Contrastive Learning","Noise Suppression","Visual Complexity","Training-Free"],"permalink":"/ai/review/2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play","title":"[논문리뷰] Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?","excerpt":"Rui Chen이 [arXiv]에 게시한 'Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","T2I Benchmarking","Compositional Reasoning","Deductive Inference","Inductive Inference","Abductive Inference","MLLM Evaluation"],"permalink":"/ai/review/2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard","title":"[논문리뷰] Does DINOv3 Set a New Medical Vision Standard?","excerpt":"Bailiang Jian이 [arXiv]에 게시한 'Does DINOv3 Set a New Medical Vision Standard?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Medical Imaging","Foundation Models","DINOv3","Self-Supervised Learning","Vision Transformer","2D/3D Classification","Segmentation","Domain Adaptation","Scaling Laws"],"permalink":"/ai/review/2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning","title":"[논문리뷰] D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning","excerpt":"Dhanvin Sanjay Namboodiri이 [arXiv]에 게시한 'D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Dark Humor Detection","Multimodal Reasoning","Vision-Language Models (VLMs)","Iterative Reasoning Refinement","Meme Analysis","Content Moderation","Cross-Modal Attention","Dataset Annotation"],"permalink":"/ai/review/2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool","title":"[논문리뷰] WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool","excerpt":"Wenzheng Chang이 [arXiv]에 게시한 'WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Online 3D Reconstruction","Camera Pose Estimation","Streaming Reconstruction","Sliding Window","Camera Token Pool","Real-time Performance","Computer Vision"],"permalink":"/ai/review/2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning","title":"[논문리뷰] WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning","excerpt":"Amit Namburi이 [arXiv]에 게시한 'WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Symbolic Music Reasoning","Music Score Analysis","Benchmarking","Visual Question Answering","In-the-Wild Data","Music Theory"],"permalink":"/ai/review/2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-Why-Language-Models-Hallucinate","title":"[논문리뷰] Why Language Models Hallucinate","excerpt":"Edwin Zhang이 [arXiv]에 게시한 'Why Language Models Hallucinate' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Language Models","Hallucination","Pretraining","Post-training","Evaluation Metrics","Binary Classification","Uncertainty Quantification","Calibration"],"permalink":"/ai/review/2025-9-8-Why-Language-Models-Hallucinate/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation","title":"[논문리뷰] U-ARM : Ultra low-cost general teleoperation interface for robot manipulation","excerpt":"Junda Huang이 [arXiv]에 게시한 'U-ARM : Ultra low-cost general teleoperation interface for robot manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Teleoperation","Robot Manipulation","Low-Cost Hardware","3D Printing","Leader-Follower System","Data Collection","Robotics Interface","Open Source"],"permalink":"/ai/review/2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models","title":"[논문리뷰] Symbolic Graphics Programming with Large Language Models","excerpt":"Kaipeng Zhang이 [arXiv]에 게시한 'Symbolic Graphics Programming with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Symbolic Graphics Programming","Large Language Models","Reinforcement Learning","SVG Generation","Text-to-Image Synthesis","Cross-Modal Alignment","Program Synthesis"],"permalink":"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator","title":"[논문리뷰] Set Block Decoding is a Language Model Inference Accelerator","excerpt":"Jeremy Reizenstein이 [arXiv]에 게시한 'Set Block Decoding is a Language Model Inference Accelerator' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Language Model Inference","Acceleration","Set Block Decoding","Next Token Prediction","Masked Token Prediction","Parallel Decoding","KV-caching","Diffusion Models"],"permalink":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs","title":"[논문리뷰] On Robustness and Reliability of Benchmark-Based Evaluation of LLMs","excerpt":"Kevin Roitero이 [arXiv]에 게시한 'On Robustness and Reliability of Benchmark-Based Evaluation of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Model Robustness","Benchmark Reliability","Paraphrasing","Linguistic Variability","Generalization","Question Answering"],"permalink":"/ai/review/2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting","title":"[논문리뷰] MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting","excerpt":"Vanessa Wildman이 [arXiv]에 게시한 'MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","3D CT","Vision-Language Model","Medical Imaging","Diagnostic Error Reduction","Multi-scale Alignment","Semantic Enrichment","Radiology Reporting","Zero-shot Learning"],"permalink":"/ai/review/2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer","title":"[논문리뷰] LuxDiT: Lighting Estimation with Video Diffusion Transformer","excerpt":"Sanja Fidler이 [arXiv]에 게시한 'LuxDiT: Lighting Estimation with Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Lighting Estimation","HDR Environment Map","Diffusion Models","Video Transformer","Low-Rank Adaptation","Generative Models","Synthetic Data"],"permalink":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation","title":"[논문리뷰] LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation","excerpt":"Zhan Zhao이 [arXiv]에 게시한 'LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Multimodal LLM","3D World Generation","Unreal Engine 5","Procedural Content Generation","Interactive Environments","Sim-to-Real","Spatial Understanding","Multimodal Input"],"permalink":"/ai/review/2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement","title":"[논문리뷰] Bootstrapping Task Spaces for Self-Improvement","excerpt":"Yoram Bachrach이 [arXiv]에 게시한 'Bootstrapping Task Spaces for Self-Improvement' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Self-Improvement","Autocurriculum","Task-Space Exploration","Inference-Time Iteration","Policy Optimization"],"permalink":"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models","title":"[논문리뷰] Behavioral Fingerprinting of Large Language Models","excerpt":"Xing Li이 [arXiv]에 게시한 'Behavioral Fingerprinting of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Behavioral Evaluation","Model Alignment","Sycophancy","World Model Brittleness","Metacognition","Personality Profiling"],"permalink":"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding","title":"[논문리뷰] Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding","excerpt":"Lionel Ni이 [arXiv]에 게시한 'Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Long Video Understanding","Reinforcement Learning","Multi-Turn Reasoning","MLLMs","Video Segment Selection","Bi-level Reward","Question Answering"],"permalink":"/ai/review/2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective","title":"[논문리뷰] Transition Models: Rethinking the Generative Learning Objective","excerpt":"Yangguang Li이 [arXiv]에 게시한 'Transition Models: Rethinking the Generative Learning Objective' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Generative Models","Diffusion Models","Training Objective","Continuous-Time Dynamics","State Transition","Few-Step Generation","Scalable Training","Image Generation"],"permalink":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training","title":"[논문리뷰] Towards a Unified View of Large Language Model Post-Training","excerpt":"Hongyi Liu이 [arXiv]에 게시한 'Towards a Unified View of Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Post-Training","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Policy Gradient","Unified Framework","Hybrid Algorithms","Bias-Variance Tradeoff"],"permalink":"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings","title":"[논문리뷰] NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings","excerpt":"Oren Glickman이 [arXiv]에 게시한 'NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Named Entity Retrieval","Zero-Shot Learning","Type-Aware Embeddings","Large Language Models (LLMs)","Contrastive Learning","Internal Representations","Information Retrieval"],"permalink":"/ai/review/2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions","title":"[논문리뷰] Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?","excerpt":"Yu Fu이 [arXiv]에 게시한 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLMs","Instruction Following","Benchmark","Cognitive Inertia","Out-of-Distribution","Supervised Fine-Tuning","Evaluation","Robustness"],"permalink":"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-From-Editor-to-Dense-Geometry-Estimator","title":"[논문리뷰] From Editor to Dense Geometry Estimator","excerpt":"Lang Nie이 [arXiv]에 게시한 'From Editor to Dense Geometry Estimator' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Dense Geometry Estimation","Diffusion Transformer","Image Editing","Zero-shot Learning","Depth Estimation","Normal Estimation","Flow Matching","Logarithmic Quantization"],"permalink":"/ai/review/2025-9-5-From-Editor-to-Dense-Geometry-Estimator/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation","title":"[논문리뷰] Few-step Flow for 3D Generation via Marginal-Data Transport Distillation","excerpt":"Lingxi Xie이 [arXiv]에 게시한 'Few-step Flow for 3D Generation via Marginal-Data Transport Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","3D Generation","Flow-based Models","Model Distillation","Few-step Sampling","Marginal-Data Transport","Velocity Matching","Velocity Distillation"],"permalink":"/ai/review/2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize","title":"[논문리뷰] False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize","excerpt":"Muhao Chen이 [arXiv]에 게시한 'False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Safety","Malicious Input Detection","Probing Classifiers","Out-of-Distribution Generalization","Superficial Patterns","Instructional Patterns","Trigger Words","AI Safety"],"permalink":"/ai/review/2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer","title":"[논문리뷰] Durian: Dual Reference-guided Portrait Animation with Attribute Transfer","excerpt":"Hanbyul Joo이 [arXiv]에 게시한 'Durian: Dual Reference-guided Portrait Animation with Attribute Transfer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Portrait Animation","Attribute Transfer","Diffusion Models","Dual Reference Networks","Zero-shot Learning","Self-Reconstruction","Facial Editing"],"permalink":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth","title":"[논문리뷰] Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth","excerpt":"Chi-Li Chen이 [arXiv]에 게시한 'Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Large Language Models","Pragmatic Understanding","Drivelology","Benchmark Dataset","Multilingual NLP","Semantic Reasoning","Contextual Inference"],"permalink":"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings","title":"[논문리뷰] Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings","excerpt":"Meie Fang이 [arXiv]에 게시한 'Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","CAD Generation","Vector Graphics","Sequence-to-Sequence Learning","Transformer Architecture","Engineering Drawings","Multi-modal Learning","Soft Target Loss","Dual Decoder"],"permalink":"/ai/review/2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models","title":"[논문리뷰] Delta Activations: A Representation for Finetuned Large Language Models","excerpt":"Ser-Nam Lim이 [arXiv]에 게시한 'Delta Activations: A Representation for Finetuned Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Embedding","Delta Activations","Finetuned Models","Model Representation","Model Clustering","Additive Property","Task Embedding","Model Merging"],"permalink":"/ai/review/2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks","title":"[논문리뷰] DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks","excerpt":"Jiaxuan Lu이 [arXiv]에 게시한 'DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Research Agents","Benchmark","Multi-Agent System","Seminar-Grounded Tasks","Data Leakage Prevention","Ill-Structured Problems"],"permalink":"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning","title":"[논문리뷰] Robix: A Unified Model for Robot Interaction, Reasoning and Planning","excerpt":"Zixuan Wang이 [arXiv]에 게시한 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Robot Learning","Vision-Language Models (VLMs)","Embodied AI","Human-Robot Interaction (HRI)","Task Planning","Reinforcement Learning (RL)","Chain-of-Thought (CoT) Reasoning","Robotics"],"permalink":"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-4-Open-Data-Synthesis-For-Deep-Research","title":"[논문리뷰] Open Data Synthesis For Deep Research","excerpt":"Zheng Liu이 [arXiv]에 게시한 'Open Data Synthesis For Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Data Synthesis","Deep Research","Hierarchical Constraint Satisfaction Problems","Large Language Models","Agentic AI","Reinforcement Learning","Question Answering"],"permalink":"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation","title":"[논문리뷰] Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation","excerpt":"Kai Li이 [arXiv]에 게시한 'Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Diffusion Transformer","Mixture of Experts","Controllable Generation","Face Generation","Multimodal Synthesis","Semantic Control","Image Generation"],"permalink":"/ai/review/2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement","title":"[논문리뷰] MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement","excerpt":"Hualiang Wang이 [arXiv]에 게시한 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Multi-Subject Generation","Personalized Image Synthesis","Semantic Correspondence","Attention Disentanglement","Diffusion Models","Identity Preservation","Dataset"],"permalink":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations","title":"[논문리뷰] LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations","excerpt":"Yoav Gur-Arieh이 [arXiv]에 게시한 'LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Language Models","Knowledge Acquisition","Pretraining Data","Entity Linking","Coreference Resolution","Information Retrieval","Model Analysis","Checkpoints"],"permalink":"/ai/review/2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association","title":"[논문리뷰] ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association","excerpt":"Daniel Cremers이 [arXiv]에 게시한 'ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Monocular SLAM","Dense Reconstruction","Neural Networks","Pose Graph Optimization","Intrinsics-free","Real-time","Two-view Association"],"permalink":"/ai/review/2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use","title":"[논문리뷰] VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use","excerpt":"Zhiheng Lyu이 [arXiv]에 게시한 'VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Tool Use","Large Language Models","Reinforcement Learning from Verifiable Rewards (RLVR)","Asynchronous Execution","Multi-modal AI","Framework"],"permalink":"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy","title":"[논문리뷰] Universal Deep Research: Bring Your Own Model and Strategy","excerpt":"Pavlo Molchanov이 [arXiv]에 게시한 'Universal Deep Research: Bring Your Own Model and Strategy' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Agentic Systems","Language Models (LLMs)","Research Automation","Customizable Strategies","Code Generation","Deep Research","User-Defined Agents","Sandboxed Execution"],"permalink":"/ai/review/2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning","title":"[논문리뷰] UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning","excerpt":"Haoyang Zou이 [arXiv]에 게시한 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","GUI Agent","Multi-Turn RL","Reinforcement Learning","Data Flywheel","Agent Framework","Hybrid Environments","Parameter Interpolation"],"permalink":"/ai/review/2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views","title":"[논문리뷰] Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views","excerpt":"Junchi Yan이 [arXiv]에 게시한 'Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Point Cloud Learning","Self-Supervised Learning","Cross Reconstruction","Decoupled Views","Generative Models","Positional Encoding","3D Vision"],"permalink":"/ai/review/2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey","title":"[논문리뷰] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","excerpt":"Hejia Geng이 [arXiv]에 게시한 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Large Language Models","LLM Agents","Sequential Decision Making","Policy Optimization","Tool Use","Dynamic Environments","Autonomous AI"],"permalink":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang","title":"[논문리뷰] The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang","excerpt":"Solomon Tsai이 [arXiv]에 게시한 'The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","LLMs","Metalinguistic Reasoning","Constructed Language","Camlang","Second Language Acquisition","Zero-shot Learning","Natural Language Understanding","Commonsense Reasoning"],"permalink":"/ai/review/2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning","title":"[논문리뷰] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning","excerpt":"Qian Liu이 [arXiv]에 게시한 'SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Tool-Integrated Reasoning","Multi-turn Reasoning","Gradient Explosion","Training Stability","Trajectory Filtering","Zero RL"],"permalink":"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction","title":"[논문리뷰] SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction","excerpt":"bindsch이 [arXiv]에 게시한 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Text-to-SQL","Multi-agent Systems","Chain-of-Thought","Error Correction","Large Language Models","Query Planning","Database Interaction"],"permalink":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic","title":"[논문리뷰] Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic","excerpt":"Bernard Ghanem이 [arXiv]에 게시한 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reasoning Vectors","Task Arithmetic","Chain-of-Thought","LLMs","Reinforcement Learning","Model Merging","Parameter Transfer"],"permalink":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion","title":"[논문리뷰] POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion","excerpt":"Haicheng Wang이 [arXiv]에 게시한 'POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","문서 변환","시각-언어 모델","자가 개선","합성 데이터","증류 없는 학습","OCR","멀티모달 AI","데이터 필터링"],"permalink":"/ai/review/2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning","title":"[논문리뷰] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning","excerpt":"Zirui Wang이 [arXiv]에 게시한 'OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Multimodal Learning","Vision Encoder","Generative Pretraining","Captioning Loss","Training Efficiency","Image-Text Models","Large Language Models"],"permalink":"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents","title":"[논문리뷰] MobiAgent: A Systematic Framework for Customizable Mobile Agents","excerpt":"Wangbo Gong이 [arXiv]에 게시한 'MobiAgent: A Systematic Framework for Customizable Mobile Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Mobile Agents","GUI Agents","Vision-Language Models","Agent Acceleration","Benchmarking","Reinforcement Learning","Data Collection"],"permalink":"/ai/review/2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization","title":"[논문리뷰] Metis: Training Large Language Models with Advanced Low-Bit Quantization","excerpt":"Hengjie Cao이 [arXiv]에 게시한 'Metis: Training Large Language Models with Advanced Low-Bit Quantization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Low-Bit Quantization","LLMs","Spectral Decomposition","Anisotropy","Adaptive Learning Rate","Regularization","FP8 Training","FP4 Training"],"permalink":"/ai/review/2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation","title":"[논문리뷰] MedDINOv3: How to adapt vision foundation models for medical image segmentation?","excerpt":"Xiaofeng Yang이 [arXiv]에 게시한 'MedDINOv3: How to adapt vision foundation models for medical image segmentation?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Medical Image Segmentation","Vision Foundation Models","Self-supervised Learning","Vision Transformers (ViT)","Domain Adaptation","DINOv3","CT Imaging"],"permalink":"/ai/review/2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision","title":"[논문리뷰] M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision","excerpt":"Yan-Jie Zhou이 [arXiv]에 게시한 'M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Medical Image Retrieval","Self-Supervised Learning","Multimodal","Zero-shot","Foundation Models","MAE","SimDINO","Vision Transformer"],"permalink":"/ai/review/2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model","title":"[논문리뷰] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model","excerpt":"Jianwei Yang이 [arXiv]에 게시한 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Critic Models","Policy Models","Reinforcement Learning (RL)","Self-Criticism","Multimodal Reasoning","Preference Learning","Generative Models"],"permalink":"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Kwai-Keye-VL-1-5-Technical-Report","title":"[논문리뷰] Kwai Keye-VL 1.5 Technical Report","excerpt":"SXxtyz이 [arXiv]에 게시한 'Kwai Keye-VL 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Video Understanding","Slow-Fast Encoding","Long Context","Chain-of-Thought","Reinforcement Learning","Human Alignment","Native-Resolution Vision Encoder"],"permalink":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations","title":"[논문리뷰] Jointly Reinforcing Diversity and Quality in Language Model Generations","excerpt":"Tianlu이 [arXiv]에 게시한 'Jointly Reinforcing Diversity and Quality in Language Model Generations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Language Models","Diversity Optimization","Quality Enhancement","Semantic Clustering","Post-training","Generative AI"],"permalink":"/ai/review/2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers","title":"[논문리뷰] Improving Large Vision and Language Models by Learning from a Panel of Peers","excerpt":"Simon Jenni이 [arXiv]에 게시한 'Improving Large Vision and Language Models by Learning from a Panel of Peers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Large Vision and Language Models (LVLMs)","Self-Improvement","Peer Learning","Preference Alignment","Reward Modeling","Multimodal Learning","Knowledge Transfer"],"permalink":"/ai/review/2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR","title":"[논문리뷰] Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR","excerpt":"Lu Wang이 [arXiv]에 게시한 'Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","RLVR","Large Language Models","Actor-Critic","Supervised Learning","Mathematical Reasoning","Policy Optimization","Cross-Entropy Loss"],"permalink":"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer","title":"[논문리뷰] GenCompositor: Generative Video Compositing with Diffusion Transformer","excerpt":"Lingen Li이 [arXiv]에 게시한 'GenCompositor: Generative Video Compositing with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Video Compositing","Diffusion Transformer","Generative Models","Video Editing","Position Embedding","Diffusion Models","Masked Token Injection","Video Harmonization"],"permalink":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games","title":"[논문리뷰] FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games","excerpt":"Dongmin Park이 [arXiv]에 게시한 'FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","GUI Agents","Adventure Games","Benchmark","Full Story Arc","Observation-Behavior Gap","LLMs","Automated Evaluation"],"permalink":"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models","title":"[논문리뷰] FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models","excerpt":"Zhen Wang이 [arXiv]에 게시한 'FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Diffusion Models","Cacheable Architecture","Multi-Reference","Semi-Attention","Efficiency","Image Synthesis"],"permalink":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them","title":"[논문리뷰] Fantastic Pretraining Optimizers and Where to Find Them","excerpt":"Percy Liang이 [arXiv]에 게시한 'Fantastic Pretraining Optimizers and Where to Find Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Deep Learning Optimizers","Large Language Models","Hyperparameter Tuning","Pretraining Speedup","Scaling Laws","AdamW","Matrix-based Optimizers","Data-to-Model Ratio"],"permalink":"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding","title":"[논문리뷰] ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding","excerpt":"Xuanyu Zheng이 [arXiv]에 게시한 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Long Video Understanding","Hallucination","Semantic Aggregation","Video MLLM","Benchmark","DPO","Positional Encoding","VideoQA"],"permalink":"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing","title":"[논문리뷰] Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing","excerpt":"Amin Heyrani Nobar이 [arXiv]에 게시한 'Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Image Editing","Autoregressive Models","Noise Inversion","Text-to-Image","Gumbel-max Trick","Training-free","Location-aware Argmax Inversion"],"permalink":"/ai/review/2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization","title":"[논문리뷰] DCPO: Dynamic Clipping Policy Optimization","excerpt":"Kai Lu이 [arXiv]에 게시한 'DCPO: Dynamic Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM","Policy Optimization","Dynamic Clipping","Advantage Standardization","RLVR","Reasoning"],"permalink":"/ai/review/2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection","title":"[논문리뷰] C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection","excerpt":"Vito Renó이 [arXiv]에 게시한 'C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Object Detection","Diffusion Model","Global Scene Context","Context-Aware Fusion","Fine-grained Detection","Automotive Damage Assessment","Generative Denoising","Cross-Attention"],"permalink":"/ai/review/2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining","title":"[논문리뷰] Benchmarking Optimizers for Large Language Model Pretraining","excerpt":"mjaggi이 [arXiv]에 게시한 'Benchmarking Optimizers for Large Language Model Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","LLM Optimizers","Benchmarking","Hyperparameter Tuning","AdamW","AdEMAMix","MARS","Mixture of Experts (MoE)","Weight Decay"],"permalink":"/ai/review/2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System","title":"[논문리뷰] Baichuan-M2: Scaling Medical Capability with Large Verifier System","excerpt":"Jayok6이 [arXiv]에 게시한 'Baichuan-M2: Scaling Medical Capability with Large Verifier System' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Medical AI","LLM","Reinforcement Learning","Verifier System","Patient Simulator","Clinical Rubrics","Baichuan-M2","HealthBench"],"permalink":"/ai/review/2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation","title":"[논문리뷰] Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation","excerpt":"Xiaolei Huang이 [arXiv]에 게시한 'Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Synthetic Data Generation","Large Language Models (LLMs)","Genetic Algorithms","Textual Data Augmentation","Active Learning","NLP","Data Diversity"],"permalink":"/ai/review/2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models","title":"[논문리뷰] AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models","excerpt":"Rahul Karthikeyan이 [arXiv]에 게시한 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Bias Mitigation","Large Language Models","Speculative Decoding","Constitutional AI","Fairness","Inference-Time Control","Indian Sociocultural Context"],"permalink":"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat","title":"[논문리뷰] UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat","excerpt":"Omartificial-Intelligence-Space이 [arXiv]에 게시한 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Arabic LLM","UI-level Evaluation","ALLaM 34B","HUMAIN Chat","Dialectal Arabic","LLM as a Judge","Safety Evaluation"],"permalink":"/ai/review/2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables","title":"[논문리뷰] T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables","excerpt":"Yu Zhao이 [arXiv]에 게시한 'T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Table-to-Report Generation","Large Language Models (LLMs)","Benchmark Dataset","Industrial Applications","Table Reasoning","Evaluation Metrics","Real-world Data"],"permalink":"/ai/review/2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning","title":"[논문리뷰] PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning","excerpt":"Yuewei Zhang이 [arXiv]에 게시한 'PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Critic-Free RL","Agentic Reasoning","Policy Optimization","Large Language Models (LLMs)","Advantage Estimation","Group Sampling","Static Value Estimation"],"permalink":"/ai/review/2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes","title":"[논문리뷰] No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes","excerpt":"Danijel Skočaj이 [arXiv]에 게시한 'No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Surface Defect Detection","Anomaly Detection","Mixed Supervision","Deep Learning","Industrial Inspection","Unified Model"],"permalink":"/ai/review/2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench","title":"[논문리뷰] How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench","excerpt":"Jayanth Srinivasa이 [arXiv]에 게시한 'How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","LLM Agents","Tool Use","Function Calling","Input Reformulation","Dynamic Environments","τ-bench","Context Engineering","Multi-Agent Framework"],"permalink":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents","title":"[논문리뷰] From reactive to cognitive: brain-inspired spatial intelligence for embodied agents","excerpt":"Songming Liu이 [arXiv]에 게시한 'From reactive to cognitive: brain-inspired spatial intelligence for embodied agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Spatial Cognition","Embodied Agents","Brain-inspired AI","Cognitive Map","Spatial Memory","MLLMs","Navigation"],"permalink":"/ai/review/2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning","title":"[논문리뷰] UItron: Foundational GUI Agent with Advanced Perception and Planning","excerpt":"Yufeng Zhong이 [arXiv]에 게시한 'UItron: Foundational GUI Agent with Advanced Perception and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","GUI Agent","Foundational Model","Multimodal LLM","Perception","Planning","Reinforcement Learning","Data Engineering","Chinese App Scenarios"],"permalink":"/ai/review/2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training","title":"[논문리뷰] TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training","excerpt":"Jiyao Deng이 [arXiv]에 게시한 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Language Model Pre-training","Dynamic Data Mixing","Data Influence","Group Influence","Optimization","Regression Model","LLM Training"],"permalink":"/ai/review/2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models","title":"[논문리뷰] Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models","excerpt":"Yifan Lu이 [arXiv]에 게시한 'Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Game AI","Procedural Knowledge","Declarative Knowledge","Explainable AI","Strategic Decision-Making"],"permalink":"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis","title":"[논문리뷰] TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis","excerpt":"Pengcheng Chen이 [arXiv]에 게시한 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Audio-Driven Talking Head Synthesis","Large-Scale Dataset","Data Diversity","Data Curation","Evaluation Benchmark","Generalization Gap","Algorithmic Fairness"],"permalink":"/ai/review/2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning","title":"[논문리뷰] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning","excerpt":"Han Hu이 [arXiv]에 게시한 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Auto-Thinking","Reinforcement Learning (RL)","Bi-mode Annealing","Bi-mode Policy Optimization (BPO)","General-Purpose AI","Reasoning","Efficiency"],"permalink":"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices","title":"[논문리뷰] Morae: Proactively Pausing UI Agents for User Choices","excerpt":"Amy Pavel이 [arXiv]에 게시한 'Morae: Proactively Pausing UI Agents for User Choices' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","UI Agents","Accessibility","Human-Agent Interaction","Mixed-Initiative AI","Large Multimodal Models","Proactive AI","User Choice","Blind and Low-Vision Users"],"permalink":"/ai/review/2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery","title":"[논문리뷰] Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery","excerpt":"Wenjie Zhou이 [arXiv]에 게시한 'Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Physics Formula Discovery","Multimodal AI","Vision-Language Models","Symbolic Regression","Causal Chain of Thought","Reinforcement Learning","Agentic AI"],"permalink":"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation","title":"[논문리뷰] HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation","excerpt":"Tianhai Liang이 [arXiv]에 게시한 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Dexterous Manipulation","Mobile Manipulation","Human-to-Robot Learning","Sim2Real","Reinforcement Learning","Depth Image","Visual Localization","Bimanual Control"],"permalink":"/ai/review/2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control","title":"[논문리뷰] EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control","excerpt":"Zhaoqing Chen이 [arXiv]에 게시한 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Embodied AI","Robot Control","Vision-Language-Action Models","Multimodal Pretraining","Flow Matching","Foundation Models","Generalization","Real-world Robotics"],"permalink":"/ai/review/2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models","title":"[논문리뷰] Efficient Code Embeddings from Code Generation Models","excerpt":"Han Xiao이 [arXiv]에 게시한 'Efficient Code Embeddings from Code Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Code Embeddings","Code Generation Models","Autoregressive Backbones","Last-Token Pooling","Instruction Tuning","Contrastive Learning","Retrieval-Augmented Generation","MTEB Benchmark"],"permalink":"/ai/review/2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation","title":"[논문리뷰] Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation","excerpt":"Qi Jia이 [arXiv]에 게시한 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","3D Generation","Video Diffusion Models","Spatial Consistency","Semantic Knowledge","Multi-view Synthesis","Large-scale Dataset","Image-to-3D","Text-to-3D"],"permalink":"/ai/review/2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP","title":"[논문리뷰] CLIPSym: Delving into Symmetry Detection with CLIP","excerpt":"Raymond A. Yeh이 [arXiv]에 게시한 'CLIPSym: Delving into Symmetry Detection with CLIP' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Symmetry Detection","Vision-Language Models","CLIP","Equivariant Networks","Prompt Engineering","Geometric Deep Learning"],"permalink":"/ai/review/2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models","title":"[논문리뷰] AHELM: A Holistic Evaluation of Audio-Language Models","excerpt":"Siwei Yang이 [arXiv]에 게시한 'AHELM: A Holistic Evaluation of Audio-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Audio-Language Models","Holistic Evaluation","Benchmarking","Multimodality","Fairness","Robustness","Reasoning","Bias Detection"],"permalink":"/ai/review/2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers","title":"[논문리뷰] A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers","excerpt":"Jiamin Wu이 [arXiv]에 게시한 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Scientific LLMs","AI for Science","Scientific Data","Agentic AI","Multimodal Integration","Knowledge Representation","Autonomous Discovery","Data Ecosystems"],"permalink":"/ai/review/2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code","title":"[논문리뷰] A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code","excerpt":"Libo Chen이 [arXiv]에 게시한 'A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","AI-Generated Code Security","LLM Evaluation","Repository-Level Benchmark","Code Security","Vulnerability Detection","Static Analysis","Reproducibility","Context-Awareness"],"permalink":"/ai/review/2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report","title":"[논문리뷰] rStar2-Agent: Agentic Reasoning Technical Report","excerpt":"Weijiang Xu이 [arXiv]에 게시한 'rStar2-Agent: Agentic Reasoning Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Math Reasoning","Code Interpreter","Tool Use","GRPO-RoC","LLM Training Efficiency","Self-Reflection"],"permalink":"/ai/review/2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning","title":"[논문리뷰] USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning","excerpt":"Jiahe Tian이 [arXiv]에 게시한 'USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Style-Driven Generation","Subject-Driven Generation","Disentangled Representation","Reward Learning","Cross-Task Learning","Diffusion Models","Image Customization","Unified Framework"],"permalink":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection","title":"[논문리뷰] Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection","excerpt":"Bernard Ghanem이 [arXiv]에 게시한 'Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","LLM Safety","Alignment Amplification","Rank-One Update","Mechanistic Interpretability","Weight Steering","Jailbreak Robustness","Fine-tuning-free","Safety Injection"],"permalink":"/ai/review/2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning","title":"[논문리뷰] TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning","excerpt":"Simin Ma이 [arXiv]에 게시한 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Instruction Augmentation","Fine-tuning","Large Language Models","Task-Centric","Data Diversity","Task Alignment","Breadth-First Search","Constraint Generation"],"permalink":"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos","title":"[논문리뷰] ROSE: Remove Objects with Side Effects in Videos","excerpt":"Hantang Liu이 [arXiv]에 게시한 'ROSE: Remove Objects with Side Effects in Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Video Object Removal","Side Effects","3D Rendering","Diffusion Transformer","Video Inpainting","Synthetic Data","Difference Mask"],"permalink":"/ai/review/2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models","title":"[논문리뷰] Provable Benefits of In-Tool Learning for Large Language Models","excerpt":"Vivien Cabannes이 [arXiv]에 게시한 'Provable Benefits of In-Tool Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Large Language Models","In-Tool Learning","In-Weight Learning","Factual Recall","Retrieval-Augmented Generation","Scaling Laws","Parameter Efficiency","Catastrophic Forgetting"],"permalink":"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning","title":"[논문리뷰] Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning","excerpt":"Jiazi Bu이 [arXiv]에 게시한 'Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Text-to-Image Generation","GRPO","Reward Hacking","Pairwise Preference","Reward Model","Stable Optimization","UniGenBench"],"permalink":"/ai/review/2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD","title":"[논문리뷰] Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD","excerpt":"Roy Ka-Wei Lee이 [arXiv]에 게시한 'Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Persuasion Dynamics","Large Language Models (LLMs)","Robustness","Gullibility","Receptiveness","Direct Preference Optimization (DPO)","Safety Alignment","Multi-turn Dialogue"],"permalink":"/ai/review/2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning","title":"[논문리뷰] OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning","excerpt":"Yitong Wang이 [arXiv]에 게시한 'OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Image Generation","Mask-Guided Editing","Reinforcement Learning","Human Preference Learning","Vision-Language Models","Multi-Task Learning","Flow Matching"],"permalink":"/ai/review/2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models","title":"[논문리뷰] OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models","excerpt":"Alex Endert이 [arXiv]에 게시한 'OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Human-Computer Interaction (HCI)","Conversational AI","Goal Tracking","Visualization","Multi-Turn Dialogue","User Interface Design","Sensemaking"],"permalink":"/ai/review/2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-Multi-View-3D-Point-Tracking","title":"[논문리뷰] Multi-View 3D Point Tracking","excerpt":"Irem Demir이 [arXiv]에 게시한 'Multi-View 3D Point Tracking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","3D Point Tracking","Multi-View","Transformer","kNN Correlation","Depth Estimation","Dynamic Scenes","Occlusion Handling","Feature Fusion"],"permalink":"/ai/review/2025-8-29-Multi-View-3D-Point-Tracking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation","title":"[논문리뷰] Mixture of Contexts for Long Video Generation","excerpt":"Junfei Xiao이 [arXiv]에 게시한 'Mixture of Contexts for Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Long Video Generation","Diffusion Transformers (DiT)","Sparse Attention","Context Routing","Memory Management","Generative Models","Video Synthesis"],"permalink":"/ai/review/2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers","title":"[논문리뷰] MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers","excerpt":"Shashank Biju이 [arXiv]에 게시한 'MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","LLM Agents","Tool Use","Benchmarking","Model Context Protocol (MCP)","Cross-Domain Orchestration","Fuzzy Instructions","Multi-Step Tasks","Real-World Scenarios"],"permalink":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes","title":"[논문리뷰] FakeParts: a New Family of AI-Generated DeepFakes","excerpt":"Xi Wang이 [arXiv]에 게시한 'FakeParts: a New Family of AI-Generated DeepFakes' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Deepfake Detection","Partial Deepfakes","AI-Generated Video","Benchmark Dataset","Video Forensics","Generative Models","Manipulation Detection","Human Perception"],"permalink":"/ai/review/2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview","title":"[논문리뷰] Dress&Dance: Dress up and Dance as You Like It - Technical Preview","excerpt":"Yu-Xiong Wang이 [arXiv]에 게시한 'Dress&Dance: Dress up and Dance as You Like It - Technical Preview' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Video Diffusion","Multi-modal Conditioning","Garment Transfer","Pose Animation","Generative AI","Fashion Tech","CondNet"],"permalink":"/ai/review/2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation","title":"[논문리뷰] Collaborative Multi-Modal Coding for High-Quality 3D Generation","excerpt":"Ziwei Liu이 [arXiv]에 게시한 'Collaborative Multi-Modal Coding for High-Quality 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","3D Generation","Multi-modal Learning","Diffusion Models","Triplane Representation","Collaborative Coding","Image-to-3D","Latent Space"],"permalink":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification","title":"[논문리뷰] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification","excerpt":"Liqiang Nie이 [arXiv]에 게시한 'CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Model","Sparsification","Instruction-Driven Routing","Cognition-Aligned AI","Robotics","Computational Efficiency","Multimodal AI"],"permalink":"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI","title":"[논문리뷰] AWorld: Orchestrating the Training Recipe for Agentic AI","excerpt":"Qintong Wu이 [arXiv]에 게시한 'AWorld: Orchestrating the Training Recipe for Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Agentic AI","Reinforcement Learning","Distributed Systems","Experience Generation","LLM Fine-tuning","GAIA Benchmark","Scalability","AWORLD Framework"],"permalink":"/ai/review/2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference","title":"[논문리뷰] Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference","excerpt":"Chunlei Han이 [arXiv]에 게시한 'Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","LLM Inference","Autoscaling","Disaggregated Architecture","Heterogeneous Hardware","Resource Management","Topology-aware Scheduling","GPU Utilization"],"permalink":"/ai/review/2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning","title":"[논문리뷰] StepWiser: Stepwise Generative Judges for Wiser Reasoning","excerpt":"Olga Golovneva이 [arXiv]에 게시한 'StepWiser: Stepwise Generative Judges for Wiser Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Process Reward Models","Reinforcement Learning","Generative Judges","Stepwise Feedback","Chain-of-Thought","Meta-Reasoning"],"permalink":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition","title":"[논문리뷰] Self-Rewarding Vision-Language Model via Reasoning Decomposition","excerpt":"Zhenwen Liang이 [arXiv]에 게시한 'Self-Rewarding Vision-Language Model via Reasoning Decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Reinforcement Learning","Self-Rewarding","Reasoning Decomposition","Visual Perception","Language Reasoning","Hallucinations","Language Shortcuts"],"permalink":"/ai/review/2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling","title":"[논문리뷰] Predicting the Order of Upcoming Tokens Improves Language Modeling","excerpt":"Alham Fikri Aji이 [arXiv]에 게시한 'Predicting the Order of Upcoming Tokens Improves Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Language Modeling","Next-Token Prediction","Multi-Token Prediction","Token Order Prediction","Auxiliary Objective","Learning-to-Rank","Transformer","Large Language Models"],"permalink":"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment","title":"[논문리뷰] MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment","excerpt":"An-An Liu이 [arXiv]에 게시한 'MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Text-Guided Motion Generation","Rectified Flow Matching","Preference Alignment","Human Motion Synthesis","Real-time AI","Transformer Architecture","Self-supervised Learning"],"permalink":"/ai/review/2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents","title":"[논문리뷰] Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents","excerpt":"Yue Yao이 [arXiv]에 게시한 'Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Multimodal LLMs (MLLMs)","Smartphone Agents","Privacy Awareness","Benchmarking","Sensitive Data Detection","Risk Assessment","UI Automation"],"permalink":"/ai/review/2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation","title":"[논문리뷰] MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation","excerpt":"Yan Zhou이 [arXiv]에 게시한 'MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Multimodal Generation","Digital Human Synthesis","Real-time Video Generation","Autoregressive LLM","Diffusion Models","Deep Compression Autoencoder","Exposure Bias Mitigation","Streaming Inference"],"permalink":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation","title":"[논문리뷰] Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation","excerpt":"Anton Ivaschenko이 [arXiv]에 게시한 'Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","rPPG","Multi-View Video Dataset","Health Biomarkers","Physiological Monitoring","Deep Learning","Telemedicine","Biosignals"],"permalink":"/ai/review/2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies","title":"[논문리뷰] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies","excerpt":"Sitong Mao이 [arXiv]에 게시한 'Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Discrete Diffusion","Action Decoding","Transformer","Robot Control","Masked Modeling","Adaptive Decoding","Reinforcement Learning"],"permalink":"/ai/review/2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding","title":"[논문리뷰] Diffusion Language Models Know the Answer Before Decoding","excerpt":"Shilin Yan이 [arXiv]에 게시한 'Diffusion Language Models Know the Answer Before Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","DLM Acceleration","Early Answer Convergence","Early Commit Decoding","Confidence Gap","Inference Speedup","Training-Free"],"permalink":"/ai/review/2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis","title":"[논문리뷰] DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis","excerpt":"Ion Stoica이 [arXiv]에 게시한 'DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Generative Research Synthesis","Live Benchmark","Automated Evaluation","LLM-as-a-judge","Related Work Generation","Retrieval-Augmented Generation","Verifiability"],"permalink":"/ai/review/2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning","title":"[논문리뷰] CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning","excerpt":"Jianze Liang이 [arXiv]에 게시한 'CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","GUI Agents","Reinforcement Learning","Planner-Executor Architecture","Decoupled Training","Large Vision-Language Models","Specialization","Generalization","Computer Use Agent"],"permalink":"/ai/review/2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR","title":"[논문리뷰] Beyond Transcription: Mechanistic Interpretability in ASR","excerpt":"Aviv Shamsian이 [arXiv]에 게시한 'Beyond Transcription: Mechanistic Interpretability in ASR' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","ASR","Mechanistic Interpretability","Logit Lens","Linear Probing","Activation Patching","Hallucinations","Repetitions","Encoder-Decoder"],"permalink":"/ai/review/2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models","title":"[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models","excerpt":"Yixiao Ge이 [arXiv]에 게시한 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Text-to-Audio","Long-Form Audio Generation","Large Language Models","Narrative Reasoning","Diffusion Models","Multimodal AI","Progressive Training"],"permalink":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation","title":"[논문리뷰] Wan-S2V: Audio-Driven Cinematic Video Generation","excerpt":"Chaonan Ji이 [arXiv]에 게시한 'Wan-S2V: Audio-Driven Cinematic Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Audio-Driven Video Generation","Cinematic Video","Diffusion Models","Transformer Architecture","Long Video Consistency","Human Animation","Multimodal Control","Data Curation"],"permalink":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space","title":"[논문리뷰] VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space","excerpt":"Rui Chen이 [arXiv]에 게시한 'VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Editing","Training-Free","Diffusion Models","Latent Space","3D Inversion","Contextual Feature Replacement","3D Consistency","Edit3D-Bench"],"permalink":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-VibeVoice-Technical-Report","title":"[논문리뷰] VibeVoice Technical Report","excerpt":"Yaoyao Chang이 [arXiv]에 게시한 'VibeVoice Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Speech Synthesis","Long-form Audio","Multi-speaker","Next-token Diffusion","Speech Tokenizer","Large Language Model","Variational Autoencoder","Audio Compression"],"permalink":"/ai/review/2025-8-27-VibeVoice-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities","title":"[논문리뷰] Unraveling the cognitive patterns of Large Language Models through module communities","excerpt":"Jianxi Gao이 [arXiv]에 게시한 'Unraveling the cognitive patterns of Large Language Models through module communities' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Network Community Structure","Cognitive Skills","AI Interpretability","Module Communities","Fine-tuning","Neural Plasticity"],"permalink":"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning","title":"[논문리뷰] UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning","excerpt":"Ran Guo이 [arXiv]에 게시한 'UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Memory Networks","Mixture of Experts (MoE)","Long-Context Learning","Sparse Models","Transformer Architecture","LLMs","Efficient Inference"],"permalink":"/ai/review/2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling","title":"[논문리뷰] TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling","excerpt":"Zhoufutu Wen이 [arXiv]에 게시한 'TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Policy Optimization","Large Language Models","Inference Efficiency","Tree Search","Segment-level Decoding","Advantage Estimation","Reasoning"],"permalink":"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo","title":"[논문리뷰] Training Language Model Agents to Find Vulnerabilities with CTF-Dojo","excerpt":"Zijian Wang이 [arXiv]에 게시한 'Training Language Model Agents to Find Vulnerabilities with CTF-Dojo' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","LLM Agents","Cybersecurity","CTF Challenges","Vulnerability Detection","Execution Environments","Docker","Automated Training","Verifiable Feedback"],"permalink":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models","title":"[논문리뷰] ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models","excerpt":"Jiangjie Chen이 [arXiv]에 게시한 'ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","LLMs","Controllable Reasoning","Computational Efficiency","Reinforcement Learning","Supervised Fine-tuning","Reasoning Compression","Budget-Aware Training"],"permalink":"/ai/review/2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration","title":"[논문리뷰] Spacer: Towards Engineered Scientific Inspiration","excerpt":"zerojun48이 [arXiv]에 게시한 'Spacer: Towards Engineered Scientific Inspiration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Scientific Discovery","Large Language Models (LLMs)","Decontextualization","Keyword Graph","Multi-Agent System","Scientific Ideation","Research Automation","Inspiration Engine"],"permalink":"/ai/review/2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks","title":"[논문리뷰] ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks","excerpt":"Kai Jia이 [arXiv]에 게시한 'ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Deep Research Agents","LLM Evaluation","Academic Survey","Factual Accuracy","Citation Verification","Report Generation","Benchmark","Hallucination"],"permalink":"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting","title":"[논문리뷰] QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting","excerpt":"Manuela Veloso이 [arXiv]에 게시한 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Hallucination Mitigation","Large Language Models","Contextual Bandits","Query Rewriting","Semantic Features","No-Regret Learning"],"permalink":"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels","title":"[논문리뷰] Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels","excerpt":"Dinesh Jayaraman이 [arXiv]에 게시한 'Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Physics Prediction","Supervised Learning","CLIP Features","Neural Radiance Fields","Material Point Method","PIXIEVERSE Dataset","Zero-Shot Generalization"],"permalink":"/ai/review/2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks","title":"[논문리뷰] Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks","excerpt":"Daisuke Nohara이 [arXiv]에 게시한 'Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Sparsity","Scaling Laws","Reasoning Tasks","Memorization","Large Language Models","Generalization Gap","Top-k Routing"],"permalink":"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation","title":"[논문리뷰] OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation","excerpt":"Jiaqi Yang이 [arXiv]에 게시한 'OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Video Avatar Generation","Cognitive Simulation","Multimodal Large Language Models (MLLMs)","Diffusion Transformers (DiT)","Multimodal Fusion","Human Motion Synthesis","Contextual Animation"],"permalink":"/ai/review/2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models","title":"[논문리뷰] ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models","excerpt":"Beiqi Chen이 [arXiv]에 게시한 'ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Inpainting","Multi-view Consistency","Video Diffusion Models","3D Object Completion","Generative Models","LoRA","3D Gaussian Splatting"],"permalink":"/ai/review/2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies","title":"[논문리뷰] MovieCORE: COgnitive REasoning in Movies","excerpt":"Hung-Ting Su이 [arXiv]에 게시한 'MovieCORE: COgnitive REasoning in Movies' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Video Question Answering (VQA)","Cognitive Reasoning","System-2 Thinking","Multi-agent LLMs","Dataset Creation","Movie Understanding","Cinematic Content","Agentic Enhancement"],"permalink":"/ai/review/2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling","title":"[논문리뷰] FastMesh:Efficient Artistic Mesh Generation via Component Decoupling","excerpt":"Xingang Pan이 [arXiv]에 게시한 'FastMesh:Efficient Artistic Mesh Generation via Component Decoupling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Mesh Generation","Component Decoupling","Autoregressive Models","Bidirectional Transformer","Fidelity Enhancement","Prediction Filtering","Token Efficiency","Artistic Meshes"],"permalink":"/ai/review/2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning","title":"[논문리뷰] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning","excerpt":"Arman Cohan이 [arXiv]에 게시한 'Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Scientific Reasoning","Knowledge Retrieval","Reasoning Probing","Benchmarks","Chain-of-Thought","Fine-tuning"],"permalink":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation","title":"[논문리뷰] ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation","excerpt":"Kun Kuang이 [arXiv]에 게시한 'ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Legal AI","Natural Language Processing","Claim Generation","Chinese Legal Dataset","Factuality","Clarity","Large Language Models","Zero-shot Evaluation"],"permalink":"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","title":"[논문리뷰] CineScale: Free Lunch in High-Resolution Cinematic Visual Generation","excerpt":"Ziwei Liu이 [arXiv]에 게시한 'CineScale: Free Lunch in High-Resolution Cinematic Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Diffusion Models","High-Resolution Generation","Image Generation","Video Generation","UNet Architecture","DiT Architecture","Scale Fusion","LoRA Fine-tuning"],"permalink":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics","title":"[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics","excerpt":"Dongchen Huang이 [arXiv]에 게시한 'CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Condensed Matter Physics","Benchmark","Scientific Reasoning","Evaluation Metric","Expression Edit Distance","Problem Solving"],"permalink":"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-27-Autoregressive-Universal-Video-Segmentation-Model","title":"[논문리뷰] Autoregressive Universal Video Segmentation Model","excerpt":"Albert Gu이 [arXiv]에 게시한 'Autoregressive Universal Video Segmentation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Video Segmentation","Autoregressive Model","Universal Model","State Space Models","Mamba","Parallel Training","Streaming Video","Deep Learning"],"permalink":"/ai/review/2025-8-27-Autoregressive-Universal-Video-Segmentation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation","title":"[논문리뷰] Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation","excerpt":"Haoxiang Shi이 [arXiv]에 게시한 'Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reinforcement Learning","Chain of Thought","Multimodal LLMs","Stage-Aware Rewards","Semantic Reasoning","Generative AI"],"permalink":"/ai/review/2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions","title":"[논문리뷰] UQ: Assessing Language Models on Unsolved Questions","excerpt":"Wei Liu이 [arXiv]에 게시한 'UQ: Assessing Language Models on Unsolved Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Unsolved Questions","AI Benchmark","Oracle-Free Validation","Generator-Validator Gap","Community Evaluation","Stack Exchange"],"permalink":"/ai/review/2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling","title":"[논문리뷰] TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling","excerpt":"Jiaqi Li이 [arXiv]에 게시한 'TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Speech Tokenizer","Diffusion Model","Text-to-Speech","Speech Language Modeling","Low Bitrate Codec","End-to-End Training","Binary Spherical Quantization"],"permalink":"/ai/review/2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation","title":"[논문리뷰] T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation","excerpt":"Xihui Liu이 [arXiv]에 게시한 'T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reasoning Benchmark","Idiom Interpretation","Textual Image Design","Entity Reasoning","Scientific Reasoning","Multimodal LLM Evaluation"],"permalink":"/ai/review/2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods","title":"[논문리뷰] SpotEdit: Evaluating Visually-Guided Image Editing Methods","excerpt":"Ersin Yumer이 [arXiv]에 게시한 'SpotEdit: Evaluating Visually-Guided Image Editing Methods' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Visually-Guided Image Editing","Multimodal Models","Benchmark","Hallucination","Diffusion Models","Autoregressive Models","Evaluation Metrics"],"permalink":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering","title":"[논문리뷰] ST-Raptor: LLM-Powered Semi-Structured Table Question Answering","excerpt":"Wei Zhou이 [arXiv]에 게시한 'ST-Raptor: LLM-Powered Semi-Structured Table Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Semi-structured Tables","Question Answering","LLMs","Hierarchical Orthogonal Tree","Table Layout Understanding","Pipeline Generation","Verification Mechanism"],"permalink":"/ai/review/2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs","title":"[논문리뷰] PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs","excerpt":"Chenyu You이 [arXiv]에 게시한 'PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Multi-Agent LLMs","Academic Poster Generation","Aesthetic Design","Layout Optimization","Typography","Color Palette","VLM-as-Judge","Content Fidelity"],"permalink":"/ai/review/2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges","title":"[논문리뷰] Neither Valid nor Reliable? Investigating the Use of LLMs as Judges","excerpt":"Golnoosh Farnadi이 [arXiv]에 게시한 'Neither Valid nor Reliable? Investigating the Use of LLMs as Judges' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","LLMs as Judges","NLG Evaluation","Measurement Theory","Validity","Reliability","Evaluation Bias","Scalability","Responsible AI"],"permalink":"/ai/review/2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting","title":"[논문리뷰] MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting","excerpt":"Yanzhe Liang이 [arXiv]에 게시한 'MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Sparse-View","Surface Reconstruction","Gaussian Splatting","2DGS","Novel View Synthesis","Generalizable","Mesh Extraction","3D Vision"],"permalink":"/ai/review/2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion","title":"[논문리뷰] MV-RAG: Retrieval Augmented Multiview Diffusion","excerpt":"sagiebenaim이 [arXiv]에 게시한 'MV-RAG: Retrieval Augmented Multiview Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Retrieval Augmented Generation","Multiview Diffusion","Text-to-3D Generation","Out-of-Domain","Image Retrieval","3D Consistency","Diffusion Models","Hybrid Training"],"permalink":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment","title":"[논문리뷰] MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment","excerpt":"Doratossadat Dastgheib이 [arXiv]에 게시한 'MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Multimodal Language Models","Multilingual Benchmarking","Persian Language","Educational Assessment","Vision-Language Models","Cultural Nuance","Reasoning Tasks"],"permalink":"/ai/review/2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism","title":"[논문리뷰] Limitations of Normalization in Attention Mechanism","excerpt":"Radu State이 [arXiv]에 게시한 'Limitations of Normalization in Attention Mechanism' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Attention Mechanism","Normalization","Softmax","Transformer Models","Gradient Sensitivity","Token Separability","Context Length","GPT-2"],"permalink":"/ai/review/2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency","title":"[논문리뷰] InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency","excerpt":"jinglinglin이 [arXiv]에 게시한 'InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Reinforcement Learning","Inference Efficiency","Vision-Language Models","Open-Source","Versatility","Reasoning"],"permalink":"/ai/review/2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German","title":"[논문리뷰] German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German","excerpt":"Cristian-George Craciun이 [arXiv]에 게시한 'German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Text Simplification","Paraphrasing","Readability Control","German NLP","Dataset Generation","LLM Distillation","Multi-level Text Generation","Accessibility"],"permalink":"/ai/review/2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning","title":"[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning","excerpt":"Xin Zheng이 [arXiv]에 게시한 'Explain Before You Answer: A Survey on Compositional Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Compositional Visual Reasoning","Multimodal AI","Vision-Language Models","Large Language Models","Chain-of-Thought","Tool Learning","Agentic AI","Survey"],"permalink":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning","title":"[논문리뷰] Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning","excerpt":"Jiale Zhao이 [arXiv]에 게시한 'Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Exploration Bottleneck","Instructional Scaffolding","Rubric-based Rewards","General Reasoning","RL with Verifiable Rewards","Policy Optimization"],"permalink":"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling","title":"[논문리뷰] Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling","excerpt":"Daniil Orel이 [arXiv]에 게시한 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Reasoning Depth","Cellular Automata","Transformer Architectures","Recurrence","Adaptive Computation Time","Chain-of-Thought","Reinforcement Learning","Generalization"],"permalink":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference","title":"[논문리뷰] TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference","excerpt":"Di Yin이 [arXiv]에 게시한 'TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Inference","Tensor Parallelism","KV Cache Optimization","Latent Attention","Memory Efficiency","Decoding Speedup","Prefill/Decode Separation","Reparameterization"],"permalink":"/ai/review/2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding","title":"[논문리뷰] Selective Contrastive Learning for Weakly Supervised Affordance Grounding","excerpt":"Jae-Pil Heo이 [arXiv]에 게시한 'Selective Contrastive Learning for Weakly Supervised Affordance Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Weakly Supervised Learning","Affordance Grounding","Contrastive Learning","CLIP","Part Discovery","Object Localization","DINO","Generative Models"],"permalink":"/ai/review/2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics","title":"[논문리뷰] Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics","excerpt":"Xiao Sun이 [arXiv]에 게시한 'Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Inverse Kinematics","Human Pose Estimation","SMPL Model","Neural Networks","Optimization-Free","Residual Learning","Data-Driven"],"permalink":"/ai/review/2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts","title":"[논문리뷰] Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts","excerpt":"Liming Fang이 [arXiv]에 게시한 'Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Jailbreaking","Red Teaming","Malicious Content Detection","Developer Messages","D-Attack","DH-CoT","Adversarial Attacks","Dataset Cleaning"],"permalink":"/ai/review/2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles","title":"[논문리뷰] InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles","excerpt":"Diping Song이 [arXiv]에 게시한 'InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Human Reasoning Styles","Social Deduction Games","Theory of Mind","Adaptive Reasoning","Avalon Game","Cognitive Grounding"],"permalink":"/ai/review/2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning","title":"[논문리뷰] End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning","excerpt":"Pengcheng Qiu이 [arXiv]에 게시한 'End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Agentic RAG","Medical Diagnosis","Reinforcement Learning","Traceable AI","Large Language Models","Clinical Decision Support","Out-of-Distribution Generalization","Reward Design"],"permalink":"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person","title":"[논문리뷰] EgoTwin: Dreaming Body and View in First Person","excerpt":"Wentao Wang이 [arXiv]에 게시한 'EgoTwin: Dreaming Body and View in First Person' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Egocentric Video Generation","Human Motion Synthesis","Diffusion Transformers","Multimodal Generation","Viewpoint Alignment","Causal Interplay","First-Person Vision"],"permalink":"/ai/review/2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible","title":"[논문리뷰] Do What? Teaching Vision-Language-Action Models to Reject the Impossible","excerpt":"Roei Herzig이 [arXiv]에 게시한 'Do What? Teaching Vision-Language-Action Models to Reject the Impossible' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","False Premise Detection","Instruction Following","Human-Robot Interaction","Clarification","Instruction Tuning"],"permalink":"/ai/review/2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders","title":"[논문리뷰] CRISP: Persistent Concept Unlearning via Sparse Autoencoders","excerpt":"Yonatan Belinkov이 [arXiv]에 게시한 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Concept Unlearning","Sparse Autoencoders (SAEs)","LLMs","Parameter-Efficient Fine-Tuning","Model Interpretability","Safety-Critical AI","Feature Suppression","WMDP Benchmark"],"permalink":"/ai/review/2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning","title":"[논문리뷰] CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning","excerpt":"Yulun Zhang이 [arXiv]에 게시한 'CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Contrastive Learning","Reinforcement Learning","Fine-tuning","Chain-of-Thought (CoT)","Annotated Data","Model Stability"],"permalink":"/ai/review/2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR","title":"[논문리뷰] Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR","excerpt":"Ying Nian Wu이 [arXiv]에 게시한 'Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Self-Play","Variational Problem Synthesis","Policy Entropy","Pass@k","Reasoning Benchmarks"],"permalink":"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications","title":"[논문리뷰] AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications","excerpt":"Liuyi Yao이 [arXiv]에 게시한 'AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Agents","Agentic Applications","ReAct Paradigm","Framework","Tool Use","Multi-Agent Systems","Developer Experience","Evaluation"],"permalink":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions","title":"[논문리뷰] AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions","excerpt":"Yidi Du이 [arXiv]에 게시한 'AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Competitive Programming","LLM Evaluation","Code Reasoning","Benchmark","Test Case Generation","Programming Competitions","Algorithmic Problems"],"permalink":"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists","title":"[논문리뷰] aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists","excerpt":"Heng Zhang이 [arXiv]에 게시한 'aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","AI Agents","Open Access","Scientific Discovery","Peer Review","LLMs","Multi-agent Systems","Prompt Injection","Iterative Refinement"],"permalink":"/ai/review/2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding","title":"[논문리뷰] When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding","excerpt":"Rui Guo이 [arXiv]에 게시한 'When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Video-LLM","Diffusion Model","Temporal Grounding","Object Segmentation","Long Video Understanding","Multimodal AI","Video Question Answering"],"permalink":"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation","title":"[논문리뷰] Waver: Wave Your Way to Lifelike Video Generation","excerpt":"Yifu Zhang이 [arXiv]에 게시한 'Waver: Wave Your Way to Lifelike Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Video Generation","Foundation Model","Diffusion Model","Transformer","Text-to-Video","Image-to-Video","Super-Resolution","Data Curation"],"permalink":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds","title":"[논문리뷰] Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds","excerpt":"Chuiyun Wu이 [arXiv]에 게시한 'Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","3D Human Reconstruction","Gaussian Splatting","Sparse View","Two-Image Input","Real-time Inference","Point Cloud Prediction","Feed-forward Network"],"permalink":"/ai/review/2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass","title":"[논문리뷰] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass","excerpt":"Ya Zhang이 [arXiv]에 게시한 'SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Single-Image Input","Feedforward Networks","Diffusion Models","Geometric Modeling","Texture Synthesis","Transformer","Feature Aggregation"],"permalink":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation","title":"[논문리뷰] Mobile-Agent-v3: Foundamental Agents for GUI Automation","excerpt":"Haowei Liu이 [arXiv]에 게시한 'Mobile-Agent-v3: Foundamental Agents for GUI Automation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","GUI Automation","Multimodal Agents","Foundational Models","Reinforcement Learning","Large Language Models","Cross-Platform","Self-Supervised Learning"],"permalink":"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries","title":"[논문리뷰] LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries","excerpt":"huuuyeah이 [arXiv]에 게시한 'LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","AI Agents","Tool Use","Model Context Protocol (MCP)","Benchmarking","Large Language Models (LLMs)","Real-world Tasks","Evaluation","Error Analysis"],"permalink":"/ai/review/2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model","title":"[논문리뷰] Intern-S1: A Scientific Multimodal Foundation Model","excerpt":"xuhuang87이 [arXiv]에 게시한 'Intern-S1: A Scientific Multimodal Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Multimodal Foundation Model","Scientific AI","Reinforcement Learning","Mixture-of-Experts (MoE)","Dynamic Tokenizer","Data Curation","Low-Resource Learning"],"permalink":"/ai/review/2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior","title":"[논문리뷰] INTIMA: A Benchmark for Human-AI Companionship Behavior","excerpt":"Yacine Jernite이 [arXiv]에 게시한 'INTIMA: A Benchmark for Human-AI Companionship Behavior' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","AI Companionship","Benchmark","Language Models (LLMs)","Human-AI Interaction","Emotional AI","Boundary Setting","Psychological Frameworks","Evaluation Metrics"],"permalink":"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models","title":"[논문리뷰] Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models","excerpt":"Lifan Guo이 [arXiv]에 게시한 'Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Large Language Models","Process Reward Models","Financial Reasoning","Domain Specialization","RLHF","Best-of-N Selection","Data Curation"],"permalink":"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries","title":"[논문리뷰] 'Does the cafe entrance look accessible? Where is the door?' Towards Geospatial AI Agents for Visual Inquiries","excerpt":"Xia Su이 [arXiv]에 게시한 'Does the cafe entrance look accessible? Where is the door? Towards Geospatial AI Agents for Visual Inquiries' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Geospatial AI","Multimodal AI Agents","Visual Question Answering","Accessibility","Street View Imagery","Spatial Reasoning","Human-Computer Interaction"],"permalink":"/ai/review/2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-Deep-Think-with-Confidence","title":"[논문리뷰] Deep Think with Confidence","excerpt":"Xuewei Wang이 [arXiv]에 게시한 'Deep Think with Confidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Confidence Filtering","Self-Consistency","Test-Time Optimization","Computational Efficiency","Adaptive Sampling","Early Stopping","Majority Voting"],"permalink":"/ai/review/2025-8-22-Deep-Think-with-Confidence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling","title":"[논문리뷰] ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling","excerpt":"Shunsuke Saito이 [arXiv]에 게시한 'ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Parametric Human Model","3D Human Modeling","Shape-Skeleton Decoupling","Pose Correctives","Single Image Mesh Fitting","Expressive Modeling","Goliath Dataset"],"permalink":"/ai/review/2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks","title":"[논문리뷰] A Survey on Large Language Model Benchmarks","excerpt":"Siyi Li이 [arXiv]에 게시한 'A Survey on Large Language Model Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","LLM Benchmarks","Evaluation","Systematic Review","General Capabilities","Domain-Specific Benchmarks","Target-Specific Benchmarks","Data Contamination","AI Ethics"],"permalink":"/ai/review/2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning","title":"[논문리뷰] mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning","excerpt":"anoperson이 [arXiv]에 게시한 'mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Multilingual Benchmark","Commonsense Reasoning","LLM Evaluation","Reasoning Taxonomy","Benchmark Scaling","Data Synthesis","Cultural Nuances"],"permalink":"/ai/review/2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions","title":"[논문리뷰] ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?","excerpt":"Daeyoung Kim이 [arXiv]에 게시한 'ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Vision Language Models","Multimodal AI","Vietnamese Language","Educational Assessment","Low-Resource Languages","Cross-Lingual Reasoning","ViExam","Human-in-the-Loop"],"permalink":"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization","title":"[논문리뷰] Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization","excerpt":"Hao Chen이 [arXiv]에 게시한 'Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","3D Editing","Multi-View Consistency","Diffusion Models","Sparse Input","Zero-Shot Learning","Scene Completion","Gaussian Splatting"],"permalink":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World","title":"[논문리뷰] RynnEC: Bringing MLLMs into Embodied World","excerpt":"jiangpinliu이 [arXiv]에 게시한 'RynnEC: Bringing MLLMs into Embodied World' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Multi-modal Large Language Models","Embodied AI","Embodied Cognition","Video Understanding","Instance Segmentation","Spatial Reasoning","Robotics"],"permalink":"/ai/review/2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation","title":"[논문리뷰] Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation","excerpt":"Shiqing Wu이 [arXiv]에 게시한 'Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Multi-modal Recommendation","Contrastive Learning","Graph Neural Network","Homography Relations","Meta-network","Orthogonal Constraint","Data Sparsity"],"permalink":"/ai/review/2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs","title":"[논문리뷰] Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs","excerpt":"Haobo Xu이 [arXiv]에 게시한 'Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Post-training Quantization (PTQ)","Model Compression","Activation Outliers","Quantization Methods","Efficient Deployment","Large Language Models"],"permalink":"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting","title":"[논문리뷰] On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting","excerpt":"Guoyin Wang이 [arXiv]에 게시한 'On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Supervised Fine-Tuning","On-Policy RL","Off-Policy Experts","Dynamic Weighting","LLM Alignment","Reasoning"],"permalink":"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model","title":"[논문리뷰] NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model","excerpt":"abercovich이 [arXiv]에 게시한 'NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Hybrid Architecture","Mamba-Transformer","Reasoning LLM","Model Compression","Knowledge Distillation","Long Context","High Throughput","FP8 Training","Instruction Following"],"permalink":"/ai/review/2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds","title":"[논문리뷰] MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds","excerpt":"Jiangmiao이 [arXiv]에 게시한 'MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","LLM","Point Clouds","3D Reconstruction","Structured Mesh","Blender Python","Shape Editing","Part-based Representation","Large Language Model"],"permalink":"/ai/review/2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers","title":"[논문리뷰] MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers","excerpt":"Prathyusha Jwalapuram이 [arXiv]에 게시한 'MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Large Language Models","Benchmarking","Model Context Protocol","Tool Use","Real-World Applications","Agent Evaluation","Long Context","Unknown Tools"],"permalink":"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer","title":"[논문리뷰] Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer","excerpt":"Jeremiah Jiang이 [arXiv]에 게시한 'Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Scale Equivariance","Deep Equilibrium Models","Canonicalization","Computer Vision","Image Classification","Semantic Segmentation","Latent Representation","Monotone Scaling"],"permalink":"/ai/review/2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell","title":"[논문리뷰] Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell","excerpt":"Ingrid Verbauwhede이 [arXiv]에 게시한 'Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Fully Homomorphic Encryption (FHE)","TFHE","Levenshtein Distance","Programmable Bootstrapping (PBS)","Privacy-Preserving Computation","String Similarity"],"permalink":"/ai/review/2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction","title":"[논문리뷰] FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction","excerpt":"tianlecai이 [arXiv]에 게시한 'FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","LLM Agents","Future Prediction","Live Benchmark","Dynamic Evaluation","Data Contamination","Tool Use","Web Search","Financial Forecasting","Misinformation"],"permalink":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models","title":"[논문리뷰] From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models","excerpt":"Ziyan Kuang이 [arXiv]에 게시한 'From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Financial LLMs","Cognitive Diagnosis Model","LLM Evaluation","Knowledge Assessment","Matrix Factorization","CPA-QKA","Interpretability"],"permalink":"/ai/review/2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery","title":"[논문리뷰] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery","excerpt":"zijieqiu이 [arXiv]에 게시한 'From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Agentic AI","Autonomous Scientific Discovery","AI for Science","Large Language Models","Multi-agent Systems","Scientific Workflow Automation","Natural Sciences"],"permalink":"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization","title":"[논문리뷰] DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization","excerpt":"Yu Lu이 [arXiv]에 게시한 'DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","LLM Optimization","Self-Verification","Dual Learning","Preference Optimization","Self-Supervised Learning","Mathematical Reasoning","Multilingual Translation","RLHF"],"permalink":"/ai/review/2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents","title":"[논문리뷰] ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents","excerpt":"Flora D. Salim이 [arXiv]에 게시한 'ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Zero-shot HAR","LLM Agents","Time-Series Analysis","Knowledge Base","Retrieval-Augmented Generation","Multi-sensor Fusion","Interpretability"],"permalink":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer","title":"[논문리뷰] Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer","excerpt":"Deyu Zhou이 [arXiv]에 게시한 'Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Text-Guided Editing","Color Editing","Diffusion Transformers","Training-Free","Multi-Modal AI","Attention Control","Image Manipulation"],"permalink":"/ai/review/2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models","title":"[논문리뷰] TempFlow-GRPO: When Timing Matters for GRPO in Flow Models","excerpt":"Jian Yang이 [arXiv]에 게시한 'TempFlow-GRPO: When Timing Matters for GRPO in Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Flow Matching","Reinforcement Learning","Human Preference Alignment","GRPO","Temporal Credit Assignment","Generative AI","Text-to-Image"],"permalink":"/ai/review/2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation","title":"[논문리뷰] Semantic IDs for Joint Generative Search and Recommendation","excerpt":"Enrico Palumbo이 [arXiv]에 게시한 'Semantic IDs for Joint Generative Search and Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Generative Models","Search and Recommendation","Semantic IDs","Bi-Encoder","Quantization","Multi-Task Learning","Retrieval Augmented Generation"],"permalink":"/ai/review/2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research","title":"[논문리뷰] Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research","excerpt":"Susanne Schmidt이 [arXiv]에 게시한 'Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Radiance Fields","XR","NeRF","3D Gaussian Splatting","View Synthesis","Systematic Review","Immersive Technology"],"permalink":"/ai/review/2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Prompt-Orchestration-Markup-Language","title":"[논문리뷰] Prompt Orchestration Markup Language","excerpt":"Yuqing Yang이 [arXiv]에 게시한 'Prompt Orchestration Markup Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Prompt Engineering","Large Language Models","Markup Language","Structured Prompting","IDE Support","Multimodal Data","Styling System","Development Toolkit"],"permalink":"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks","title":"[논문리뷰] OmniTry: Virtual Try-On Anything without Masks","excerpt":"Xiaoduan Feng이 [arXiv]에 게시한 'OmniTry: Virtual Try-On Anything without Masks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Diffusion Model","Mask-Free","Image Inpainting","ID Consistency","Wearable Objects","Generative AI"],"permalink":"/ai/review/2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References","title":"[논문리뷰] MultiRef: Controllable Image Generation with Multiple Visual References","excerpt":"Shiyun Lang이 [arXiv]에 게시한 'MultiRef: Controllable Image Generation with Multiple Visual References' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Controllable Image Generation","Multi-modal Generation","Visual References","Image-to-Image","Benchmark","Dataset","MLLM-as-a-Judge"],"permalink":"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence","title":"[논문리뷰] Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence","excerpt":"Xin Chen이 [arXiv]에 게시한 'Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Motion Transfer","Cross-topology","Sparse Correspondence","Motion Matching","Animation","Training-free","Few-shot Learning"],"permalink":"/ai/review/2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation","title":"[논문리뷰] Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation","excerpt":"Xinyi Wang이 [arXiv]에 게시한 'Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","LLMs","Confidence Estimation","Fine-Grained","Generation Process","Calibration","Monte Carlo Sampling","Backward Confidence Integration"],"permalink":"/ai/review/2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation","title":"[논문리뷰] MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation","excerpt":"Jonas Geiping이 [arXiv]에 게시한 'MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Medical Image Segmentation","Model Merging","Training-Free","SAM","Generalization","Zero-Order Optimization","Bayesian Optimization"],"permalink":"/ai/review/2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence","title":"[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence","excerpt":"Fernando López이 [arXiv]에 게시한 'MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Audio Intelligence","Multimodal AI","Benchmark","Audio-Language Models","Holistic Evaluation","Reasoning","Long-Form Audio","Multicultural Music"],"permalink":"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents","title":"[논문리뷰] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents","excerpt":"Jun Dong이 [arXiv]에 게시한 'MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Multimodal Browsing","AI Agents","Benchmark","Vision-Language Models","Reasoning","Tool Use","Deep Search"],"permalink":"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos","title":"[논문리뷰] LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos","excerpt":"Yen-Yu Lin이 [arXiv]에 게시한 'LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Novel View Synthesis","3D Gaussian Splatting","Unposed Reconstruction","Camera Pose Estimation","Incremental Optimization","Octree","Long Videos"],"permalink":"/ai/review/2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery","title":"[논문리뷰] Leveraging Large Language Models for Predictive Analysis of Human Misery","excerpt":"Abhilash Nandy이 [arXiv]에 게시한 'Leveraging Large Language Models for Predictive Analysis of Human Misery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Affective Computing","Misery Score Prediction","Prompt Engineering","Few-shot Learning","Gamified Evaluation","Feedback-driven Adaptation"],"permalink":"/ai/review/2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge","title":"[논문리뷰] Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge","excerpt":"Alice Wang이 [arXiv]에 게시한 'Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Podcast Recommendation","LLM-as-a-Judge","Offline Evaluation","User Profiling","Recommender Systems","Natural Language Processing"],"permalink":"/ai/review/2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation","title":"[논문리뷰] Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation","excerpt":"Fei Ni이 [arXiv]에 게시한 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Embodied AI","Robotic Manipulation","Reinforcement Learning","Vision-Language Model","Pointing","Zero-shot Generalization"],"permalink":"/ai/review/2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","title":"[논문리뷰] Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations","excerpt":"Mounia Lalmas이 [arXiv]에 게시한 'Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Video Recommendation","Zero-Shot Learning","Content-Based Filtering","Natural Language Processing","Foundation Models"],"permalink":"/ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection","title":"[논문리뷰] CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection","excerpt":"Adriano Koshiyama이 [arXiv]에 게시한 'CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Sparse Autoencoders","LLM Steering","Feature Selection","Correlation Analysis","AI Safety","Bias Mitigation","Mechanistic Interpretability"],"permalink":"/ai/review/2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends","title":"[논문리뷰] Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends","excerpt":"Xixiang Zhao이 [arXiv]에 게시한 'Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","LLM Copyright Protection","Model Fingerprinting","Text Watermarking","Invasive Fingerprinting","Intrinsic Fingerprinting","Intellectual Property","Digital Rights Management","Backdoor Watermarking"],"permalink":"/ai/review/2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL","title":"[논문리뷰] Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL","excerpt":"Liam-Liu이 [arXiv]에 게시한 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Chain-of-Agents","Agent Foundation Models","Multi-Agent Systems","Tool-Integrated Reasoning","Multi-agent Distillation","Agentic Reinforcement Learning","LLMs","End-to-End Learning"],"permalink":"/ai/review/2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing","title":"[논문리뷰] CAMAR: Continuous Actions Multi-Agent Routing","excerpt":"Alexey Skrynnik이 [arXiv]에 게시한 'CAMAR: Continuous Actions Multi-Agent Routing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Multi-Agent Reinforcement Learning","Continuous Control","Pathfinding","MARL Benchmark","GPU Acceleration","Robotics Simulation","Scalability","Heterogeneous Agents"],"permalink":"/ai/review/2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding","title":"[논문리뷰] Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding","excerpt":"Alina Landowska이 [arXiv]에 게시한 'Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Large Language Models","Moral Reasoning","Bayesian Evaluation","Uncertainty Quantification","Natural Language Processing","Soft Labels"],"permalink":"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends","title":"[논문리뷰] Advances in Speech Separation: Techniques, Challenges, and Future Trends","excerpt":"Zhuo Chen이 [arXiv]에 게시한 'Advances in Speech Separation: Techniques, Challenges, and Future Trends' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Speech Separation","Deep Neural Networks","Cocktail Party Problem","Transformer Architecture","Unsupervised Learning","Supervised Learning","Evaluation Metrics","Datasets"],"permalink":"/ai/review/2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models","title":"[논문리뷰] A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models","excerpt":"Zishang Jiang이 [arXiv]에 게시한 'A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Self-Refinement","Language Models","Reinforcement Learning","Proactive AI","Generation Process","Markov Decision Process","Adaptive Learning","LLM Efficiency"],"permalink":"/ai/review/2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs","title":"[논문리뷰] When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs","excerpt":"Elena Tutubalina이 [arXiv]에 게시한 'When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","LLM Robustness","Prompt Sensitivity","In-Context Learning","Fine-Tuning","Batch Calibration","Template Ensembles","Distribution Shift"],"permalink":"/ai/review/2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models","title":"[논문리뷰] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models","excerpt":"Jusen Du이 [arXiv]에 게시한 'Speed Always Wins: A Survey on Efficient Architectures for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Large Language Models","Efficient Architectures","Transformer Optimization","Linear Attention","State Space Models","Mixture-of-Experts","Sparse Attention","Diffusion LLMs"],"permalink":"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models","title":"[논문리뷰] S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models","excerpt":"Meiqi Wu이 [arXiv]에 게시한 'S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Classifier-free Guidance","Self-Guidance","Training-Free","Stochastic Block-Dropping","Generative Models","Text-to-Image"],"permalink":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens","title":"[논문리뷰] Representing Speech Through Autoregressive Prediction of Cochlear Tokens","excerpt":"Daniel L. K. Yamins이 [arXiv]에 게시한 'Representing Speech Through Autoregressive Prediction of Cochlear Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Speech Representation Learning","Autoregressive Models","Cochlear Tokens","Biologically Inspired AI","Self-Supervised Learning","Audio Processing","Transformer Networks"],"permalink":"/ai/review/2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Reinforcement-Learning-with-Rubric-Anchors","title":"[논문리뷰] Reinforcement Learning with Rubric Anchors","excerpt":"Haokai Xu이 [arXiv]에 게시한 'Reinforcement Learning with Rubric Anchors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Rubric-based Reward","RLVR Extension","Human-centric AI","Controllable Generation","Reward Hacking Mitigation"],"permalink":"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts","title":"[논문리뷰] Precise Action-to-Video Generation Through Visual Action Prompts","excerpt":"Minghan Qin이 [arXiv]에 게시한 'Precise Action-to-Video Generation Through Visual Action Prompts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Action-to-Video Generation","Visual Action Prompts","Skeleton Representation","Human-Object Interaction","Robotic Manipulation","Cross-Domain Transfer","Diffusion Models"],"permalink":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Ovis2-5-Technical-Report","title":"[논문리뷰] Ovis2.5 Technical Report","excerpt":"Yang Li이 [arXiv]에 게시한 'Ovis2.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Native Resolution Vision","Deep Reasoning","Chart Analysis","OCR","Visual Grounding","Training Efficiency","Preference Optimization"],"permalink":"/ai/review/2025-8-19-Ovis2-5-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Next-Visual-Granularity-Generation","title":"[논문리뷰] Next Visual Granularity Generation","excerpt":"Kang Liao이 [arXiv]에 게시한 'Next Visual Granularity Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Image Generation","Granularity Control","Structured Representation","Hierarchical Generation","Coarse-to-fine","Visual Tokenization","Latent Space"],"permalink":"/ai/review/2025-8-19-Next-Visual-Granularity-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model","title":"[논문리뷰] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model","excerpt":"Yifan Zhang이 [arXiv]에 게시한 'Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","World Model","Interactive Video Generation","Real-Time AI","Diffusion Models","Auto-Regressive Generation","Data Pipeline","Self-Forcing","KV Caching"],"permalink":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models","title":"[논문리뷰] Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models","excerpt":"Zixiang Gao이 [arXiv]에 게시한 'Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Video Relighting","Background Replacement","Generative Models","Diffusion Models","Temporal Consistency","Dataset Generation","Video Editing"],"permalink":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping","title":"[논문리뷰] Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping","excerpt":"Tyler Derr이 [arXiv]에 게시한 'Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Multimodal Learning","Vision-Language Models","Alignment Pre-training","Text-to-Vision Mapping","Continuous Representations","Computational Efficiency","LLM"],"permalink":"/ai/review/2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds","title":"[논문리뷰] HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds","excerpt":"Artyom Sorokin이 [arXiv]에 게시한 'HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Long-Horizon Planning","Structured Reasoning","LLM Evaluation","Virtual Worlds","RPG","Benchmark","Agent Systems","Combat Simulation"],"permalink":"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study","title":"[논문리뷰] Has GPT-5 Achieved Spatial Intelligence? An Empirical Study","excerpt":"Ruisi Wang이 [arXiv]에 게시한 'Has GPT-5 Achieved Spatial Intelligence? An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Spatial Intelligence","Multimodal LLMs","Benchmark Evaluation","GPT-5","Cognitive AI","AGI"],"permalink":"/ai/review/2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration","title":"[논문리뷰] G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration","excerpt":"Evgeny Burnaev이 [arXiv]에 게시한 'G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Deep Learning","Multi-Modal Fusion","Camera Pose Estimation","Depth Estimation","Transformer Networks","Prior Information"],"permalink":"/ai/review/2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning","title":"[논문리뷰] ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning","excerpt":"Yufeng Wang이 [arXiv]에 게시한 'ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Cognitive-Inspired RAG","Stateful Reasoning","Long Narrative Comprehension","Dynamic Memory","Metacognitive Regulation","Multi-step Retrieval","Hierarchical Knowledge Source"],"permalink":"/ai/review/2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information","title":"[논문리뷰] Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information","excerpt":"Xi Yang이 [arXiv]에 게시한 'Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Large Reasoning Models (LRMs)","Information Seeking","Incomplete Problems","Mathematical Reasoning","Supervised Fine-tuning (SFT)","Overthinking","Hallucination","CRITIC-math"],"permalink":"/ai/review/2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy","title":"[논문리뷰] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy","excerpt":"Zeng Tao이 [arXiv]에 게시한 '4DNeX: Feed-Forward 4D Generative Modeling Made Easy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","4D Generation","Dynamic 3D","Generative Models","Diffusion Models","Single Image Input","Video Synthesis","Point Clouds","Dataset"],"permalink":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-X-Node-Self-Explanation-is-All-We-Need","title":"[논문리뷰] X-Node: Self-Explanation is All We Need","excerpt":"Islem Rekik이 [arXiv]에 게시한 'X-Node: Self-Explanation is All We Need' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Graph Neural Networks","Explainable AI","Self-Explanation","Node Classification","Medical Imaging","Natural Language Processing","Interpretability"],"permalink":"/ai/review/2025-8-18-X-Node-Self-Explanation-is-All-We-Need/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-Thyme-Think-Beyond-Images","title":"[논문리뷰] Thyme: Think Beyond Images","excerpt":"Wei Chen이 [arXiv]에 게시한 'Thyme: Think Beyond Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Code Generation","Image Processing","Reinforcement Learning","Supervised Fine-Tuning","Visual Reasoning","Sandbox"],"permalink":"/ai/review/2025-8-18-Thyme-Think-Beyond-Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures","title":"[논문리뷰] TexVerse: A Universe of 3D Objects with High-Resolution Textures","excerpt":"Nan Cao이 [arXiv]에 게시한 'TexVerse: A Universe of 3D Objects with High-Resolution Textures' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","3D Dataset","High-Resolution Textures","Physically Based Rendering (PBR)","3D Animation","Data Curation","GPT-5 Annotations","Sketchfab"],"permalink":"/ai/review/2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation","title":"[논문리뷰] StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation","excerpt":"Junyong Noh이 [arXiv]에 게시한 'StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","3D Morphable Model","Face Stylization","Text-to-Image Translation","Diffusion Model","Attribute Preservation","Generative AI","Computer Graphics"],"permalink":"/ai/review/2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-SSRL-Self-Search-Reinforcement-Learning","title":"[논문리뷰] SSRL: Self-Search Reinforcement Learning","excerpt":"Yanxu Chen이 [arXiv]에 게시한 'SSRL: Self-Search Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Self-Search","Sim-to-Real Transfer","Agentic AI","Knowledge Retrieval","Reward Modeling"],"permalink":"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation","title":"[논문리뷰] SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation","excerpt":"Paolo Soda이 [arXiv]에 게시한 'SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Semi-supervised Learning","Few-shot Learning","Medical Imaging","GAN-based Methods","Image-to-image Translation","Pseudo-labeling","Ensemble Learning"],"permalink":"/ai/review/2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing","title":"[논문리뷰] PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing","excerpt":"Xianpei Han이 [arXiv]에 게시한 'PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","논문 검색","계층적 인덱싱","유연한 검색","대규모 언어 모델","정보 추출","뷰 인식","강화 학습"],"permalink":"/ai/review/2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data","title":"[논문리뷰] MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data","excerpt":"Nicolas Gonthier이 [arXiv]에 게시한 'MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Masked Autoencoder","Earth Observation","Multimodal","Multitemporal","Multispectral","Fusion Strategies","Target Normalization"],"permalink":"/ai/review/2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation","title":"[논문리뷰] FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation","excerpt":"Mu Xu이 [arXiv]에 게시한 'FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Audio-Driven Animation","Preference Optimization","Diffusion Models","Reward Modeling","Human Feedback","Multi-Objective Optimization","Timestep-Layer Adaptive"],"permalink":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-DINOv3","title":"[논문리뷰] DINOv3","excerpt":"Maxime Oquab이 [arXiv]에 게시한 'DINOv3' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Foundation Models","Vision Transformer","Dense Feature Maps","Gram Anchoring","Model Distillation","Geospatial AI"],"permalink":"/ai/review/2025-8-18-DINOv3/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding","title":"[논문리뷰] Controlling Multimodal LLMs via Reward-guided Decoding","excerpt":"Michal Drozdzal이 [arXiv]에 게시한 'Controlling Multimodal LLMs via Reward-guided Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Reward Models","Guided Decoding","Visual Grounding","Hallucination Mitigation","Object Precision","Object Recall","Inference-time Control"],"permalink":"/ai/review/2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning","title":"[논문리뷰] We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning","excerpt":"Xiaowan Wang이 [arXiv]에 게시한 'We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Visual Mathematical Reasoning","MLLMs","Knowledge System","Reinforcement Learning","Curriculum Learning","Dataset Construction","Mathematical Benchmark"],"permalink":"/ai/review/2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT","title":"[논문리뷰] UI-Venus Technical Report: Building High-performance UI Agents with RFT","excerpt":"Shuheng Shen이 [arXiv]에 게시한 'UI-Venus Technical Report: Building High-performance UI Agents with RFT' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","UI Agent","MLLM","RFT","UI Grounding","UI Navigation","GRPO","Data Cleaning","Self-Evolving Trajectory"],"permalink":"/ai/review/2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing","title":"[논문리뷰] ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing","excerpt":"Xiaoyu Li이 [arXiv]에 게시한 'ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Cartoon Generation","Video Diffusion Models","DiT","Post-Keyframing","Low-Rank Adaptation","Sparse Control","Generative AI","Animation"],"permalink":"/ai/review/2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer","title":"[논문리뷰] STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer","excerpt":"Honghua Chen이 [arXiv]에 게시한 'STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Causal Transformer","Sequential Modeling","Streaming Data","Pointmap Prediction","Online Perception","KVCache"],"permalink":"/ai/review/2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera","title":"[논문리뷰] Processing and acquisition traces in visual encoders: What does CLIP know about your camera?","excerpt":"Giorgos Tolias이 [arXiv]에 게시한 'Processing and acquisition traces in visual encoders: What does CLIP know about your camera?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Visual Encoders","Metadata","Image Processing","Image Acquisition","Robustness","CLIP","Foundation Models","Distribution Shift"],"permalink":"/ai/review/2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models","title":"[논문리뷰] Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models","excerpt":"Qinghao Ye이 [arXiv]에 게시한 'Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Exploration-Exploitation","Reward Design","Reasoning Tasks","Pass@k","Policy Optimization"],"permalink":"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts","title":"[논문리뷰] PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts","excerpt":"Rui Lu이 [arXiv]에 게시한 'PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Long-Context Understanding","Reasoning Benchmark","LLMs Evaluation","Natural Language Processing","Global Comprehension","Fluid Intelligence","Prequel Entailment","RAG"],"permalink":"/ai/review/2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale","title":"[논문리뷰] NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale","excerpt":"Quan Sun이 [arXiv]에 게시한 'NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Text-to-Image Generation","Continuous Latent Tokens","Flow Matching","Image Editing","Multimodal Learning","Transformer Architecture"],"permalink":"/ai/review/2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs","title":"[논문리뷰] HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs","excerpt":"Yi Yuan이 [arXiv]에 게시한 'HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Human-Centered AI","Empathy","Context-Awareness","MLLM Benchmark","Reinforcement Learning","Reasoning"],"permalink":"/ai/review/2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms","title":"[논문리뷰] From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms","excerpt":"Ziyin Zhang이 [arXiv]에 게시한 'From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Automated Interpreting Assessment","Explainable AI","Data Augmentation","Variational Autoencoder","SHAP","Interpreting Quality","Natural Language Processing"],"permalink":"/ai/review/2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-A-Survey-on-Diffusion-Language-Models","title":"[논문리뷰] A Survey on Diffusion Language Models","excerpt":"Zhiqiang Shen이 [arXiv]에 게시한 'A Survey on Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Generative AI","Parallel Decoding","Text Generation","Multimodal AI","Model Compression","Reinforcement Learning from Human Feedback","Inference Optimization"],"permalink":"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP","title":"[논문리뷰] When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing","excerpt":"Gjergji Kasneci이 [arXiv]에 게시한 'When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Natural Language Processing (NLP)","Explainable AI (XAI)","Post-hoc Explainability","Differential Privacy (DP)","Privacy-Utility Trade-off","Model Faithfulness","Text Privatization"],"permalink":"/ai/review/2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models","title":"[논문리뷰] VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models","excerpt":"Dongdong Zhang이 [arXiv]에 게시한 'VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Code Generation","Model Merging","Task Vectors","Vision-Language Model","Coding LLM","Instruction Tuning","Benchmark"],"permalink":"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation","title":"[논문리뷰] Story2Board: A Training-Free Approach for Expressive Storyboard Generation","excerpt":"Dani Lischinski이 [arXiv]에 게시한 'Story2Board: A Training-Free Approach for Expressive Storyboard Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Storyboard Generation","Text-to-Image","Diffusion Models","Training-Free","Character Consistency","Scene Diversity","Visual Storytelling"],"permalink":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","title":"[논문리뷰] Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation","excerpt":"Chen Li이 [arXiv]에 게시한 'Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Video Generation","Identity Preservation","Plug-and-Play","Diffusion Models","Self-Attention","Lightweight AI","Conditional Image Branch"],"permalink":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory","title":"[논문리뷰] Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory","excerpt":"Yuan Lin이 [arXiv]에 게시한 'Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multimodal Agent","Long-Term Memory","Episodic Memory","Semantic Memory","Reinforcement Learning","Video Question Answering","Entity-Centric Memory"],"permalink":"/ai/review/2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models","title":"[논문리뷰] Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models","excerpt":"Zeynep Akata이 [arXiv]에 게시한 'Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Diffusion Models","Hypernetworks","Test-Time Optimization","Reward-Guided Generation","Latent Space Optimization","LoRA","Generative AI"],"permalink":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery","title":"[논문리뷰] Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery","excerpt":"Di Zhang이 [arXiv]에 게시한 'Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Molecule Discovery","Chain-of-Thought","Large Language Models","Reinforcement Learning","Supervised Fine-tuning","Molecular Generation","Explainable AI"],"permalink":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models","title":"[논문리뷰] MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models","excerpt":"Zhihan Zhou이 [arXiv]에 게시한 'MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Math Reasoning","Real-World Benchmark","Visual Perception","Robustness","K-12 Education","Dataset"],"permalink":"/ai/review/2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment","title":"[논문리뷰] Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment","excerpt":"Lei Fan이 [arXiv]에 게시한 'Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","LLM Alignment","Reinforcement Learning from Human Feedback","Preference Learning","Group Relative Alignment Optimization","Self-Optimization","Mixture-of-Experts","Imitation Learning"],"permalink":"/ai/review/2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding","title":"[논문리뷰] IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding","excerpt":"Di Zhang이 [arXiv]에 게시한 'IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Backdoor Attack","Vision-Language Models (VLMs)","Visual Grounding","Input-aware Trigger","Adversarial Attack","Security","U-Net","Open-vocabulary"],"permalink":"/ai/review/2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors","title":"[논문리뷰] GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors","excerpt":"Qingnan Fan이 [arXiv]에 게시한 'GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting","Novel View Synthesis","Diffusion Model","Artifact Restoration","Sparse-view 3D Reconstruction","Reference-Guided"],"permalink":"/ai/review/2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation","title":"[논문리뷰] Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation","excerpt":"Zhenghao Hu이 [arXiv]에 게시한 'Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Synthetic Data","Image Generation","GPT-4o","Multimodal Models","Instruction Following","Surreal Image Generation","Dataset","Benchmarking"],"permalink":"/ai/review/2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing","title":"[논문리뷰] Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing","excerpt":"Hao Zhang이 [arXiv]에 게시한 'Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Faster Inference","Discrete Diffusion Forcing (D2F)","Autoregressive Generation","KV Cache Optimization","Parallel Decoding","Text Generation","Model Distillation"],"permalink":"/ai/review/2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models","title":"[논문리뷰] Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models","excerpt":"Guiyang Hou이 [arXiv]에 게시한 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reward Model","Policy Optimization","Reward Hacking","Hybrid Annotation","Mathematical Reasoning","Verifiable Rewards"],"permalink":"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study","title":"[논문리뷰] Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study","excerpt":"Gjergji Kasneci이 [arXiv]에 게시한 'Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Explainable NLP","Natural Language Explanations","Large Language Models","Pre-trained Language Models","Natural Language Inference","Model Performance Enhancement","Text Generation"],"permalink":"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving","title":"[논문리뷰] AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving","excerpt":"Jinjie Gu이 [arXiv]에 게시한 'AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Agent Stability","LLM","Tool Use","GAIA Benchmark","Robustness","Dynamic Supervision","Maneuvering"],"permalink":"/ai/review/2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance","title":"[논문리뷰] AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance","excerpt":"Yong Li이 [arXiv]에 게시한 'AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Large Language Models","Fine-tuning","Reinforcement Learning","Meta-learning","Adaptive Control","Imitation Learning","Exploration","Reasoning"],"permalink":"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion","title":"[논문리뷰] WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion","excerpt":"Rachid Nedjai이 [arXiv]에 게시한 'WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Spatio-Temporal Fusion","Land Surface Temperature","Generative Adversarial Network","Weakly-Supervised Learning","Remote Sensing","Deep Learning"],"permalink":"/ai/review/2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail","title":"[논문리뷰] VertexRegen: Mesh Generation with Continuous Level of Detail","excerpt":"Jakob Engel이 [arXiv]에 게시한 'VertexRegen: Mesh Generation with Continuous Level of Detail' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Mesh Generation","Level of Detail (LOD)","Progressive Meshes","Vertex Split","Autoregressive Models","Transformer","3D Graphics"],"permalink":"/ai/review/2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation","title":"[논문리뷰] UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation","excerpt":"Kevin Galim이 [arXiv]에 게시한 'UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Masked Generative Transformers","Compositional Generation","Attention Guidance","Unmasking Strategy","Contrastive Learning","Training-Free","Attribute Binding"],"permalink":"/ai/review/2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning","title":"[논문리뷰] Train Long, Think Short: Curriculum Learning for Efficient Reasoning","excerpt":"Marzyeh Ghassemi이 [arXiv]에 게시한 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Curriculum Learning","Reinforcement Learning","Large Language Models","Reasoning Efficiency","Token Budget Control","Group Relative Policy Optimization","Chain-of-Thought"],"permalink":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors","title":"[논문리뷰] Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors","excerpt":"Haoran Xu이 [arXiv]에 게시한 'Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Robotic Dexterous Grasping","Affordance-Aware","Human-like Priors","Reinforcement Learning","Vision-Language Models","Two-Stage Training","Manipulation"],"permalink":"/ai/review/2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation","title":"[논문리뷰] TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation","excerpt":"Rachel Bawden이 [arXiv]에 게시한 'TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Low-Resource MT","Data Augmentation","Large Language Models (LLMs)","Back-Translation","In-Context Learning (ICL)","Fine-Tuning","Topic-Guided Generation","Parallel Data Synthesis"],"permalink":"/ai/review/2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models","title":"[논문리뷰] Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models","excerpt":"Chenchen Jing이 [arXiv]에 게시한 'Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Temporal Oscillation","Self-Consistency Voting","Reinforcement Learning","Temporal Semantic Entropy","Text Generation"],"permalink":"/ai/review/2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency","title":"[논문리뷰] Test-Time Reinforcement Learning for GUI Grounding via Region Consistency","excerpt":"Zhengxi Lu이 [arXiv]에 게시한 'Test-Time Reinforcement Learning for GUI Grounding via Region Consistency' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","GUI Grounding","Test-Time Scaling","Reinforcement Learning","Region Consistency","Spatial Voting","Self-Supervised Learning","Vision-Language Models"],"permalink":"/ai/review/2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents","title":"[논문리뷰] OpenCUA: Open Foundations for Computer-Use Agents","excerpt":"Tianbao Xie이 [arXiv]에 게시한 'OpenCUA: Open Foundations for Computer-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Computer-Use Agents","Vision-Language Models","Chain-of-Thought Reasoning","Large-scale Dataset","Open-source Framework","Desktop Automation","Agent Evaluation"],"permalink":"/ai/review/2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations","title":"[논문리뷰] NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations","excerpt":"Haoyue Zhan이 [arXiv]에 게시한 'NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Paralinguistic Vocalizations","Speech Recognition","Text-to-Speech","Speech Synthesis","Data Annotation","Mandarin Speech","Expressive Speech"],"permalink":"/ai/review/2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation","title":"[논문리뷰] Matrix-3D: Omnidirectional Explorable 3D World Generation","excerpt":"Yuqi Li이 [arXiv]에 게시한 'Matrix-3D: Omnidirectional Explorable 3D World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","3D World Generation","Panoramic Video Generation","3D Reconstruction","Diffusion Models","Gaussian Splatting","Dataset","Camera Control"],"permalink":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches","title":"[논문리뷰] HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches","excerpt":"Qiang Ju이 [arXiv]에 게시한 'HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Hierarchical Reinforcement Learning","Deep Search","Multi-source RAG","Agentic AI","Knowledge Integration","Enterprise Search","Large Reasoning Models"],"permalink":"/ai/review/2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay","title":"[논문리뷰] GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay","excerpt":"Yang Fan이 [arXiv]에 게시한 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Continual Learning","Large Language Models (LLMs)","Catastrophic Forgetting","Replay","Knowledge Distillation","Activation States","Anti-forgetting","Threshold-based Margin Loss"],"permalink":"/ai/review/2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments","title":"[논문리뷰] Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments","excerpt":"Xuesong Yao이 [arXiv]에 게시한 'Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Tool Use","Reinforcement Learning (RL)","Automated Environment Generation","Feedback-Driven Training","Reward Mechanism","Contextual Understanding"],"permalink":"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy","title":"[논문리뷰] Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy","excerpt":"Elizabeth Karpinski이 [arXiv]에 게시한 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Large Language Models","Diplomacy Game","Multi-agent Systems","Strategic Reasoning","LLM Evaluation","Prompt Engineering","Behavioral Analysis","Game AI"],"permalink":"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition","title":"[논문리뷰] DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition","excerpt":"Lukáš Burget이 [arXiv]에 게시한 'DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Speech Recognition","Encoder-Decoder","Regularization","Decoder-Centric","Intermediate Supervision","Out-of-Domain Generalization","Internal Language Model"],"permalink":"/ai/review/2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning","title":"[논문리뷰] Cut2Next: Generating Next Shot via In-Context Tuning","excerpt":"Yu Qiao이 [arXiv]에 게시한 'Cut2Next: Generating Next Shot via In-Context Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Next Shot Generation","In-Context Tuning","Diffusion Transformer","Cinematic Continuity","Hierarchical Prompting","Video Generation","Shot Editing"],"permalink":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation","title":"[논문리뷰] CharacterShot: Controllable and Consistent 4D Character Animation","excerpt":"Fei Shen이 [arXiv]에 게시한 'CharacterShot: Controllable and Consistent 4D Character Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","4D Character Animation","Diffusion Models","Gaussian Splatting","Pose Control","Multi-view Synthesis","Temporal Consistency","Character Dataset"],"permalink":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware","title":"[논문리뷰] Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware","excerpt":"Jhon Alejandro Andrade이 [arXiv]에 게시한 'Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Quantum Game Theory","NISQ Hardware","Error Mitigation","Battle of the Sexes","Qiskit","Quantum Computing","Strategic Coordination","Payoff Maximization"],"permalink":"/ai/review/2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them","title":"[논문리뷰] BiasGym: Fantastic Biases and How to Find (and Remove) Them","excerpt":"Arnav Arora이 [arXiv]에 게시한 'BiasGym: Fantastic Biases and How to Find (and Remove) Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Bias Mitigation","LLMs","Mechanistic Interpretability","Fine-tuning","Attention Steering","Stereotype Analysis","Safety Alignment"],"permalink":"/ai/review/2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL","title":"[논문리뷰] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL","excerpt":"Chuyi He이 [arXiv]에 게시한 'Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Agents","Agentic Search","Asynchronous RL","Long-Horizon Planning","Tool Use","Data Synthesis"],"permalink":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators","title":"[논문리뷰] AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators","excerpt":"Tao Zhang이 [arXiv]에 게시한 'AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","코드 생성","대규모 언어 모델","코드 벤치마크","다국어 프로그래밍","자동화된 데이터 생성","샌드박스 평가","멀티모달 AI"],"permalink":"/ai/review/2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math","title":"[논문리뷰] Aryabhata: An exam-focused language model for JEE Math","excerpt":"Sandeep Varma이 [arXiv]에 게시한 'Aryabhata: An exam-focused language model for JEE Math' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Language Model","Math Reasoning","JEE","Supervised Fine-Tuning","Reinforcement Learning","Model Merging","Chain-of-Thought","Curriculum Learning"],"permalink":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval","title":"[논문리뷰] Adversarial Video Promotion Against Text-to-Video Retrieval","excerpt":"Shuai Liu이 [arXiv]에 게시한 'Adversarial Video Promotion Against Text-to-Video Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Adversarial Attack","Video Promotion","Text-to-Video Retrieval","Modality Refinement","Black-box Attack","Video Manipulation","Transferability"],"permalink":"/ai/review/2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking","title":"[논문리뷰] WideSearch: Benchmarking Agentic Broad Info-Seeking","excerpt":"Yan Gao이 [arXiv]에 게시한 'WideSearch: Benchmarking Agentic Broad Info-Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Agentic Search","LLM","Benchmark","Information Seeking","Structured Output","Evaluation Metrics","Multi-agent Systems"],"permalink":"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs","title":"[논문리뷰] When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs","excerpt":"Dasol Choi이 [arXiv]에 게시한 'When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Audio-Language Models","Jailbreak Attack","Adversarial Audio","Reinforcement Learning","Projected Gradient Descent","Native Payload Discovery","Multimodal AI Safety"],"permalink":"/ai/review/2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding","title":"[논문리뷰] VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding","excerpt":"Tong Yu이 [arXiv]에 게시한 'VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Multimodal Retrieval","Retrieval-Augmented Generation","Long Document Understanding","Multilingual NLP","Visual QA","Benchmark","MLLMs","Table Understanding"],"permalink":"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents","title":"[논문리뷰] UserBench: An Interactive Gym Environment for User-Centric Agents","excerpt":"Jianguo Zhang이 [arXiv]에 게시한 'UserBench: An Interactive Gym Environment for User-Centric Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","User-Centric AI","LLM Evaluation","Interactive Agents","Gym Environment","Preference Elicitation","Multi-turn Dialogue","Tool Use"],"permalink":"/ai/review/2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future","title":"[논문리뷰] Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future","excerpt":"Qiufeng Wang이 [arXiv]에 게시한 'Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Self-Rewarding LLMs","Direct Preference Optimization (DPO)","Preference Learning","Generative AI","Gradient Collapse","LLM Alignment","Iterative Optimization"],"permalink":"/ai/review/2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences","title":"[논문리뷰] Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences","excerpt":"Matvey Skripkin이 [arXiv]에 게시한 'Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Speech-to-LaTeX","ASR","Language Models","Multimodal AI","Dataset Creation","Mathematical Expression Recognition","LaTeX Generation"],"permalink":"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation","title":"[논문리뷰] Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation","excerpt":"Hengtao Shen이 [arXiv]에 게시한 'Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Robot Learning","Generalization","Shortcut Learning","Dataset Diversity","Dataset Fragmentation","Data Augmentation","Imitation Learning"],"permalink":"/ai/review/2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Reinforcement-Learning-in-Vision-A-Survey","title":"[논문리뷰] Reinforcement Learning in Vision: A Survey","excerpt":"Qingwei Meng이 [arXiv]에 게시한 'Reinforcement Learning in Vision: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Computer Vision (CV)","Multimodal Large Language Models (MLLMs)","Visual Generation","Vision-Language-Action (VLA) Models","Policy Optimization","Reward Modeling"],"permalink":"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability","title":"[논문리뷰] ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability","excerpt":"Yuchen Li이 [arXiv]에 게시한 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Passage Ranking","Reasoning Models","Large Language Models","Data Synthesis","Reinforcement Learning","Listwise Reranking","Information Retrieval"],"permalink":"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning","title":"[논문리뷰] Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning","excerpt":"Jiaheng Liu이 [arXiv]에 게시한 'Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","LLM Reasoning","Policy Optimization","Normalization","Clipping","Loss Aggregation","Overlong Filtering"],"permalink":"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks","title":"[논문리뷰] OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks","excerpt":"Hongxing Li이 [arXiv]에 게시한 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Embodied AI","Agent Reasoning","LLM","Benchmarking","Tool Use","Multi-Agent Systems","Physical Interaction","Constraint Reasoning"],"permalink":"/ai/review/2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","title":"[논문리뷰] Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation","excerpt":"Xiaokun Feng이 [arXiv]에 게시한 'Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Visual Effects","Video Generation","LoRA","Mixture of Experts","Spatial Control","Diffusion Models","Multi-VFX"],"permalink":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space","title":"[논문리뷰] MolmoAct: Action Reasoning Models that can Reason in Space","excerpt":"Shuo Liu이 [arXiv]에 게시한 'MolmoAct: Action Reasoning Models that can Reason in Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Robotics","Action Reasoning","Vision-Language Models","Spatial Planning","Depth Perception","Trajectory Generation","Explainable AI"],"permalink":"/ai/review/2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs","title":"[논문리뷰] MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs","excerpt":"Jianguo Li이 [arXiv]에 게시한 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","LLM Compression","Matrix Decomposition","Parameter Efficiency","Deep Learning","Memory Optimization"],"permalink":"/ai/review/2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning","title":"[논문리뷰] Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning","excerpt":"Baihong Yuan이 [arXiv]에 게시한 'Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Sparse Attention","LLMs","Reasoning Tasks","Efficiency","Training-Free","Global Locality","KV Cache Optimization"],"permalink":"/ai/review/2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization","title":"[논문리뷰] Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization","excerpt":"Guanting Dong이 [arXiv]에 게시한 'Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Reasoning LLMs","Reinforcement Learning","PPO","Gradient Clipping","Supervised Fine-tuning","Math Reasoning","Code Generation","Policy Optimization"],"permalink":"/ai/review/2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts","title":"[논문리뷰] Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts","excerpt":"Tieyuan Chen이 [arXiv]에 게시한 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Mixture of Experts","LLMs","MoE Architecture","Dynamic Activation","Adjugate Experts","Upcycling Strategy","Load Balancing"],"permalink":"/ai/review/2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks","title":"[논문리뷰] GLiClass: Generalist Lightweight Model for Sequence Classification Tasks","excerpt":"Alexander Yavorskyi이 [arXiv]에 게시한 'GLiClass: Generalist Lightweight Model for Sequence Classification Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Sequence Classification","Zero-shot Learning","Few-shot Learning","Transformer","Multi-label Classification","PPO","GLiNER","Computational Efficiency"],"permalink":"/ai/review/2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control","title":"[논문리뷰] Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control","excerpt":"Hongyu Liu이 [arXiv]에 게시한 'Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Image Editing","Shape Transformation","Rectified Flow","Trajectory Divergence Map","Region Control","Generative Models","Diffusion Models"],"permalink":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System","title":"[논문리뷰] Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System","excerpt":"Reynold Cheng이 [arXiv]에 게시한 'Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Adversarial Attack","Poisoning Attack","Fact-checking","LLM Agent","Retrieval Augmented Generation","Misinformation","System Security"],"permalink":"/ai/review/2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs","title":"[논문리뷰] Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs","excerpt":"Robert Kirk이 [arXiv]에 게시한 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","LLMs","데이터 필터링","사전 학습","변조 저항성","바이오위협","AI 안전","서킷 브레이킹","머신 언러닝"],"permalink":"/ai/review/2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy","title":"[논문리뷰] Compressing Chain-of-Thought in LLMs via Step Entropy","excerpt":"Zhijian Xu이 [arXiv]에 게시한 'Compressing Chain-of-Thought in LLMs via Step Entropy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","LLM","Chain-of-Thought","CoT Compression","Step Entropy","Reinforcement Learning","SFT","GRPO"],"permalink":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent","title":"[논문리뷰] BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent","excerpt":"Kai Zou이 [arXiv]에 게시한 'BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Benchmarking","Deep-Research Agents","LLMs","Retrieval","Curated Corpus","Evaluation","Fairness","Transparency","Reproducibility"],"permalink":"/ai/review/2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents","title":"[논문리뷰] Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents","excerpt":"Mohit Bansal이 [arXiv]에 게시한 'Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Diffusion Model","CLIP Latent","Image Generation","Multimodal Understanding","ControlNet","Training Efficiency"],"permalink":"/ai/review/2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems","title":"[논문리뷰] A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems","excerpt":"Xinhao Yi이 [arXiv]에 게시한 'A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Self-Evolving AI Agents","Lifelong Learning","Foundation Models","Multi-Agent Systems","Agent Optimization","Prompt Engineering","Tool Use","AI Safety","Survey"],"permalink":"/ai/review/2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off","title":"[논문리뷰] Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off","excerpt":"jgkwak이 [arXiv]에 게시한 'Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Virtual Try-Off","Diffusion Transformer","Bidirectional Learning","Generative AI","Fashion Synthesis","Attention Mechanism","Self-Correction"],"permalink":"/ai/review/2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding","title":"[논문리뷰] UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding","excerpt":"Bingqi Chen이 [arXiv]에 게시한 'UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","GUI Agents","Reinforcement Learning","Grounding","MLLMs","Reward Function","Resampling","Visual Noise Reduction"],"permalink":"/ai/review/2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal","title":"[논문리뷰] Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal","excerpt":"Chengcheng Wan이 [arXiv]에 게시한 'Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Code Reasoning","CoT Compression","LLMs","Efficiency","Surprisal","Pruning","Fine-tuning","Large Reasoning Models"],"permalink":"/ai/review/2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh","title":"[논문리뷰] MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh","excerpt":"Yi Yang이 [arXiv]에 게시한 'MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","3D Mesh Generation","LLMs","Mesh Understanding","Text-to-3D","Primitive-Mesh Decomposition","Progressive Training","Multimodal AI"],"permalink":"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-Memp-Exploring-Agent-Procedural-Memory","title":"[논문리뷰] Memp: Exploring Agent Procedural Memory","excerpt":"Shuofei Qiao이 [arXiv]에 게시한 'Memp: Exploring Agent Procedural Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Procedural Memory","LLM Agents","Memory Management","Task Automation","Lifelong Learning","Experience Replay","Agent Learning"],"permalink":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs","title":"[논문리뷰] MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs","excerpt":"Guohang Yan이 [arXiv]에 게시한 'MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Low-Resource Languages","Cultural Groundedness","Linguistic Capability","Dataset Creation","Multilingual AI"],"permalink":"/ai/review/2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion","title":"[논문리뷰] LightSwitch: Multi-view Relighting with Material-guided Diffusion","excerpt":"Shubham Tulsiani이 [arXiv]에 게시한 'LightSwitch: Multi-view Relighting with Material-guided Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Multi-view Relighting","Diffusion Models","Material-guided","Inverse Rendering","3D Scene Reconstruction","Image Synthesis","Consistent Relighting"],"permalink":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization","title":"[논문리뷰] InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization","excerpt":"Pengxiang Li이 [arXiv]에 게시한 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","GUI Grounding","MLLMs","Reinforcement Learning","Policy Optimization","Exploration Strategy","Semantic Alignment","Adaptive Exploration Reward","Human-Computer Interaction"],"permalink":"/ai/review/2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models","title":"[논문리뷰] GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models","excerpt":"GLM-4. 5 Team이 [arXiv]에 게시한 'GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Large Language Model","Mixture-of-Experts","Agentic AI","Reasoning","Code Generation","Reinforcement Learning","Foundation Model"],"permalink":"/ai/review/2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing","title":"[논문리뷰] GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing","excerpt":"Przemysław Spurek이 [arXiv]에 게시한 'GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Neural Radiance Fields (NeRF)","Gaussian Splatting (GS)","Interactive Editing","3D Scene Representation","Physics Simulation","Hybrid Model","Real-time Rendering","Ray Tracing"],"permalink":"/ai/review/2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey","title":"[논문리뷰] Adapting Vision-Language Models Without Labels: A Comprehensive Survey","excerpt":"Eleni Chatzi이 [arXiv]에 게시한 'Adapting Vision-Language Models Without Labels: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Unsupervised Adaptation","Test-Time Adaptation (TTA)","Domain Transfer","Multimodal Learning","Label-Free Learning","Zero-Shot Learning"],"permalink":"/ai/review/2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling","title":"[논문리뷰] Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling","excerpt":"Ruolin Shen이 [arXiv]에 게시한 'Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Visual Document Understanding","Visual Question Answering","Multi-Agent System","Test-Time Scaling","Self-Correction","Mixed Reward Modeling","Large Language Models"],"permalink":"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance","title":"[논문리뷰] StrandDesigner: Towards Practical Strand Generation with Sketch Guidance","excerpt":"Xiaobin Hu이 [arXiv]에 게시한 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Strand Generation","Sketch Guidance","Diffusion Models","Multi-scale Learning","Adaptive Conditioning","3D Hair Modeling","Computer Graphics"],"permalink":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression","title":"[논문리뷰] Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression","excerpt":"Yifei Ji이 [arXiv]에 게시한 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Image Compression","Diffusion Models","One-Step Decoding","Fidelity Guidance","Rate Annealing","VAE","Perceptual Quality"],"permalink":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation","title":"[논문리뷰] RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation","excerpt":"Jian Yang이 [arXiv]에 게시한 'RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Robust PCA","Deep Unfolding","Sparse Segmentation","Interpretability","Image Decomposition","Computer Vision"],"permalink":"/ai/review/2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation","title":"[논문리뷰] REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation","excerpt":"Xiao Yu이 [arXiv]에 게시한 'REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Simultaneous Speech Translation","Adaptive Policy","Entropy-based Loss","Mutual Information","Latency-Quality Trade-off","Speech-to-Text Translation","REINA"],"permalink":"/ai/review/2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data","title":"[논문리뷰] R-Zero: Self-Evolving Reasoning LLM from Zero Data","excerpt":"Zongxia Li이 [arXiv]에 게시한 'R-Zero: Self-Evolving Reasoning LLM from Zero Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Self-Evolving LLM","Reinforcement Learning","Curriculum Learning","Reasoning","Large Language Models","Self-Play","Zero-Data Training"],"permalink":"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction","title":"[논문리뷰] PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction","excerpt":"Prajit Das이 [arXiv]에 게시한 'PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","PII Redaction","Large Language Models","Instruction Tuning","Retrieval-Augmented Generation","Privacy Preservation","Model Evaluation","Cross-Domain Generalization","Open-Source LLMs"],"permalink":"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification","title":"[논문리뷰] On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification","excerpt":"Xinyu Ye이 [arXiv]에 게시한 'On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Generalization","Reward Rectification","Dynamic Fine-Tuning (DFT)","LLM","Policy Gradient","Mathematical Reasoning"],"permalink":"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Marco-Voice-Technical-Report","title":"[논문리뷰] Marco-Voice Technical Report","excerpt":"Qingjuan Li이 [arXiv]에 게시한 'Marco-Voice Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Speech Synthesis","Voice Cloning","Emotion Control","Text-to-Speech","Disentanglement","Contrastive Learning","Flow Matching","Emotional Speech Dataset"],"permalink":"/ai/review/2025-8-8-Marco-Voice-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes","title":"[논문리뷰] MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes","excerpt":"Xudong Jiang이 [arXiv]에 게시한 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Video Object Segmentation","Dataset","Complex Scenes","Benchmark","Object Tracking","Computer Vision","Dataset Challenges"],"permalink":"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities","title":"[논문리뷰] InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities","excerpt":"Zhijie Sang이 [arXiv]에 게시한 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","LLM Alignment","Reasoning","Data Curation","Supervised Fine-tuning (SFT)","Direct Preference Optimization (DPO)","Sample Efficiency","Scalability","Multi-dimensional Filtering"],"permalink":"/ai/review/2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking","title":"[논문리뷰] I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking","excerpt":"Chao Wang이 [arXiv]에 게시한 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Multimodal Entity Linking","Large Language Models","Collaborative Reflection","Iterative Reasoning","Visual Information","Text-centric"],"permalink":"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations","title":"[논문리뷰] I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations","excerpt":"Chirag Shah이 [arXiv]에 게시한 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","LLM Bias","Hiring Evaluation","Linguistic Shibboleth","Hedging Language","Fairness","Benchmarking","Sociolinguistics"],"permalink":"/ai/review/2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis","title":"[논문리뷰] Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis","excerpt":"Reshmi Ghosh이 [arXiv]에 게시한 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Multi-hop Question Answering","Large Language Models","Reasoning Errors","Error Taxonomy","Human Evaluation","Automated Evaluation","Overthinking"],"permalink":"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity","title":"[논문리뷰] Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity","excerpt":"Zhibing Li이 [arXiv]에 게시한 'Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","3D Generation Evaluation","Hierarchical Evaluation","Material Properties","Multi-Agent Annotation","Hybrid Scoring System","Video-based Evaluation","Part-level Analysis"],"permalink":"/ai/review/2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation","title":"[논문리뷰] Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation","excerpt":"Shengcong Chen이 [arXiv]에 게시한 'Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Robotic Manipulation","World Model","Video Generation","Diffusion Model","Embodied AI","Foundation Model","Robotics Simulation","Policy Learning"],"permalink":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation","title":"[논문리뷰] Evaluating, Synthesizing, and Enhancing for Customer Support Conversation","excerpt":"Feng Chen이 [arXiv]에 게시한 'Evaluating, Synthesizing, and Enhancing for Customer Support Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Customer Support","Dialogue Generation","Large Language Models","Role-Playing","COPC Framework","Synthetic Data","Strategy Prediction","Empathetic AI"],"permalink":"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models","title":"[논문리뷰] Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models","excerpt":"Fangzhou Yao이 [arXiv]에 게시한 'Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Efficient Reasoning","Chain-of-Thought","Model Optimization","Model Collaboration","Overthinking Problem","LLM Efficiency"],"permalink":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning","title":"[논문리뷰] DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning","excerpt":"Ziming Wang이 [arXiv]에 게시한 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Vision Language Models (VLMs)","Agentic AI","Physical Reasoning","Benchmark","Simulation Environments","Action Planning","Interactive AI"],"permalink":"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions","title":"[논문리뷰] CoAct-1: Computer-using Agents with Coding as Actions","excerpt":"Taiwei Shi이 [arXiv]에 게시한 'CoAct-1: Computer-using Agents with Coding as Actions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","AI Agent","Multi-agent System","GUI Automation","Programmatic Control","Code Generation","OSWorld Benchmark","Hybrid AI"],"permalink":"/ai/review/2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability","title":"[논문리뷰] Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability","excerpt":"Yuan Wu이 [arXiv]에 게시한 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Large Multimodal Models","Input Scrutiny","Error Detection","Faulty Inputs","Evaluation Framework","Modality Preference","Cross-Modal Inconsistency"],"permalink":"/ai/review/2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation","title":"[논문리뷰] Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?","excerpt":"Junjie Yang이 [arXiv]에 게시한 'Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation","Multimodal LLMs","Benchmark Evaluation","Document Understanding","Multi-hop Reasoning","Information Retrieval","Evaluation Dataset"],"permalink":"/ai/review/2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts","title":"[논문리뷰] Are Today's LLMs Ready to Explain Well-Being Concepts?","excerpt":"Huan Liu이 [arXiv]에 게시한 'Are Today's LLMs Ready to Explain Well-Being Concepts?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Large Language Models","Well-being Concepts","LLM Evaluation","Principle-Guided Evaluation","LLM-as-a-Judge","Supervised Fine-Tuning (SFT)","Direct Preference Optimization (DPO)","Explanation Generation"],"permalink":"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents","title":"[논문리뷰] Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents","excerpt":"Xinyu Yang이 [arXiv]에 게시한 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Web Agent","Cognitive Reasoning","Knowledge-Induced","Large Multimodal Models (LMMs)","Bloom's Taxonomy","Chain-of-Thought (CoT)","Web-CogDataset","Web-CogBench"],"permalink":"/ai/review/2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning","title":"[논문리뷰] Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning","excerpt":"Maksim Nekrashevich이 [arXiv]에 게시한 'Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Software Engineering","Multi-Turn Interaction","Long Context","DAPO","Autonomous Agents","SWE-BENCH"],"permalink":"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models","title":"[논문리뷰] The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models","excerpt":"Elisabetta Rocchetti이 [arXiv]에 게시한 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Diffusion Models","Cross-Attention Analysis","Content-Style Disentanglement","Artistic Style Transfer","Explainable AI","SDXL"],"permalink":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence","title":"[논문리뷰] Sotopia-RL: Reward Design for Social Intelligence","excerpt":"Keyang Xuan이 [arXiv]에 게시한 'Sotopia-RL: Reward Design for Social Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Social Intelligence","Reinforcement Learning","Reward Design","Large Language Models","Utterance-level Rewards","Multi-dimensional Rewards","Partial Observability","SOTOPIA"],"permalink":"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering","title":"[논문리뷰] SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering","excerpt":"Ambuj Mehrish이 [arXiv]에 게시한 'SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Music Restoration","Audio Mastering","Generative Models","Flow Matching","Text-to-Audio","Audio Quality Enhancement","Multi-task Learning","Dataset Creation"],"permalink":"/ai/review/2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation","title":"[논문리뷰] Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation","excerpt":"Hao Huang이 [arXiv]에 게시한 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Text-to-3D Generation","Prompt Engineering","Visual Analytics","Human-Computer Interaction","Multi-modal Large Language Models","3D Model Evaluation"],"permalink":"/ai/review/2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management","title":"[논문리뷰] Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management","excerpt":"Yunxin Liu이 [arXiv]에 게시한 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Large Language Models","Active Context Management","Proactive Interference","Tool Augmentation","Working Memory","Context Curation","Long Context"],"permalink":"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience","title":"[논문리뷰] SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience","excerpt":"Xiaoyi Dong이 [arXiv]에 게시한 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Computer Use Agent","Self-Evolving","Reinforcement Learning","Curriculum Learning","Vision-Language Models","Experiential Learning","Specialist-to-Generalist"],"permalink":"/ai/review/2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks","title":"[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks","excerpt":"Haozhe Zhang이 [arXiv]에 게시한 'Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Root Cause Analysis","Large Language Models","5G Wireless Networks","Supervised Fine-Tuning","Reinforcement Learning","Chain-of-Thought","TeleLogs Dataset"],"permalink":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","title":"[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization","excerpt":"Kechi Zhang이 [arXiv]에 게시한 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Capability Collapse","Hybrid Policy Optimization","Multiple Importance Sampling","Exploration","Math Reasoning","Out-of-Distribution"],"permalink":"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference","title":"[논문리뷰] Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference","excerpt":"Jiaying Wu이 [arXiv]에 게시한 'Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","AI Conferences","Sustainability","Peer Review","Community Building","Environmental Impact","Mental Health","Centralized Model","Decentralized Model"],"permalink":"/ai/review/2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets","title":"[논문리뷰] OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets","excerpt":"MaziyarPanahi이 [arXiv]에 게시한 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Biomedical NER","Transformer","Domain Adaptation","LoRA","Open-Source","Named Entity Recognition","Healthcare AI"],"permalink":"/ai/review/2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions","title":"[논문리뷰] MiDashengLM: Efficient Audio Understanding with General Audio Captions","excerpt":"Yadong Niu이 [arXiv]에 게시한 'MiDashengLM: Efficient Audio Understanding with General Audio Captions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Audio-Language Model","General Audio Captions","Audio Understanding","Speech Recognition","Efficient Inference","Public Datasets","Multimodality","Data Curation"],"permalink":"/ai/review/2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following","title":"[논문리뷰] Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following","excerpt":"Liang Xu이 [arXiv]에 게시한 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","LLMs","Instruction Following","Reasoning","Reinforcement Learning","Supervised Fine-tuning","Entropy Regularization","Self-Checking","Previewing"],"permalink":"/ai/review/2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding","title":"[논문리뷰] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding","excerpt":"Yuqing Yang이 [arXiv]에 게시한 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","LLM","KV Cache Optimization","Model Pruning","Efficient Decoding","Memory Optimization","Static Sparsity","Transformer"],"permalink":"/ai/review/2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought","title":"[논문리뷰] LaTCoder: Converting Webpage Design to Code with Layout-as-Thought","excerpt":"Tianpeng Lv이 [arXiv]에 게시한 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Design-to-Code","Webpage Generation","Multimodal Large Language Models (MLLMs)","Layout Preservation","Chain-of-Thought (CoT)","UI Automation","Code Generation"],"permalink":"/ai/review/2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens","title":"[논문리뷰] Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens","excerpt":"Zhen Tan이 [arXiv]에 게시한 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Chain-of-Thought","LLMs","OOD Generalization","Data Distribution Shift","Reasoning","Pattern Matching","DataAlchemy"],"permalink":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards","title":"[논문리뷰] IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards","excerpt":"Ling-I Wu이 [arXiv]에 게시한 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Instruction Following","Reinforcement Learning","Reward Hacking","LLMs","Curriculum Learning","Data Flywheel","Verifiable Rewards"],"permalink":"/ai/review/2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-IAUNet-Instance-Aware-U-Net","title":"[논문리뷰] IAUNet: Instance-Aware U-Net","excerpt":"Dmytro Fishman이 [arXiv]에 게시한 'IAUNet: Instance-Aware U-Net' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Instance Segmentation","U-Net","Query-based Model","Transformer Decoder","Biomedical Imaging","Cell Segmentation","Deep Learning"],"permalink":"/ai/review/2025-8-7-IAUNet-Instance-Aware-U-Net/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score","title":"[논문리뷰] HPSv3: Towards Wide-Spectrum Human Preference Score","excerpt":"Hongsheng Li이 [arXiv]에 게시한 'HPSv3: Towards Wide-Spectrum Human Preference Score' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Human Preference Score","Text-to-Image Generation","Image Evaluation","Vision-Language Models (VLMs)","Uncertainty-Aware Ranking Loss","Dataset","Iterative Refinement","Chain-of-Thought"],"permalink":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis","title":"[논문리뷰] Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis","excerpt":"Feng Zhao이 [arXiv]에 게시한 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","4D Generation","Video-to-3D Synthesis","Gaussian Splatting","Diffusion Models","Latent Space Modeling","Variational Autoencoder","Temporal Coherence"],"permalink":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success","title":"[논문리뷰] Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success","excerpt":"Ruslan Rakhimov이 [arXiv]에 게시한 'Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Vision-Language Models","Synthetic Worlds","Transfer Learning","PPO","Actor-Critic","Embodied AI"],"permalink":"/ai/review/2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost","title":"[논문리뷰] Efficient Agents: Building Effective Agents While Reducing Cost","excerpt":"Yue Hou이 [arXiv]에 게시한 'Efficient Agents: Building Effective Agents While Reducing Cost' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","LLM Agents","Cost Efficiency","Performance-Cost Trade-off","Agent Frameworks","GAIA Benchmark","Optimization","Resource Management"],"permalink":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation","title":"[논문리뷰] EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation","excerpt":"Dong Chen이 [arXiv]에 게시한 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","C-to-Rust Conversion","Project-Level Translation","Large Language Models","Code Synthesis","Memory Safety","Software Migration","Hybrid Translation"],"permalink":"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework","title":"[논문리뷰] DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework","excerpt":"Chao Liang이 [arXiv]에 게시한 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Video Virtual Try-On","Diffusion Transformers","Stage-Wise Framework","Vision-Language Models","LoRA","Temporal Consistency","Garment Preservation"],"permalink":"/ai/review/2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction","title":"[논문리뷰] CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction","excerpt":"Donghyeon Lee이 [arXiv]에 게시한 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Toxicity Prediction","Large Language Model","Chain-of-Thought","Drug Development","Cheminformatics","Interpretable AI","IUPAC Nomenclature"],"permalink":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor","title":"[논문리뷰] C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor","excerpt":"Jinbao Wang이 [arXiv]에 게시한 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","3D Anomaly Detection","Continual Learning","Kernel Attention","Learnable Advisor","Parameter Perturbation","Point Cloud","Industrial AI"],"permalink":"/ai/review/2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning","title":"[논문리뷰] Agent Lightning: Train ANY AI Agents with Reinforcement Learning","excerpt":"Zilong Wang이 [arXiv]에 게시한 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","AI Agents","Framework","Markov Decision Process","Hierarchical RL","Training-Agent Disaggregation","Observability"],"permalink":"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding","title":"[논문리뷰] A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding","excerpt":"Jianke Zhu이 [arXiv]에 게시한 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","3D Occupancy Grounding","Multi-modal Learning","Natural Language Understanding","Autonomous Driving","Voxel-based Prediction","Benchmark Dataset","Coarse-to-Fine"],"permalink":"/ai/review/2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search","title":"[논문리뷰] Tool-integrated Reinforcement Learning for Repo Deep Search","excerpt":"Yanzhen Zou이 [arXiv]에 게시한 'Tool-integrated Reinforcement Learning for Repo Deep Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Issue Localization","Large Language Models (LLMs)","Reinforcement Learning (RL)","Supervised Fine-tuning (SFT)","Tool-integrated Agents","Software Engineering","Code Search"],"permalink":"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs","title":"[논문리뷰] TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs","excerpt":"Aman Chadha이 [arXiv]에 게시한 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","LLM Alignment","Alignment Drift","Training Data Provenance","Belief Conflict Index (BCI)","Suffix Array","Safety Interventions","Reinforcement Learning from Human Feedback","Explainable AI"],"permalink":"/ai/review/2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation","title":"[논문리뷰] Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation","excerpt":"Tianyidan Xie이 [arXiv]에 게시한 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Multimodal AI","Image Generation","Image Editing","Visual Understanding","Unified Architecture","Parameter Efficiency"],"permalink":"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference","title":"[논문리뷰] Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference","excerpt":"Fan Xia이 [arXiv]에 게시한 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Diffusion Models","Language Models","Code Generation","Non-Autoregressive Inference","High-Speed Inference","Discrete Diffusion","LLM Inference"],"permalink":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-Multi-human-Interactive-Talking-Dataset","title":"[논문리뷰] Multi-human Interactive Talking Dataset","excerpt":"Mike Zheng Shou이 [arXiv]에 게시한 'Multi-human Interactive Talking Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Multi-human Video Generation","Interactive Talking","Dataset","Audio-driven Animation","Pose Control","Speech Interaction","Diffusion Models"],"permalink":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation","title":"[논문리뷰] LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation","excerpt":"Chenyang Si이 [arXiv]에 게시한 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Ultra-long Video Generation","Multimodal Guidance","Controllable Video Generation","Diffusion Models","Temporal Consistency","Visual Quality","Autoregressive Generation","Degradation-aware Training"],"permalink":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools","title":"[논문리뷰] LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?","excerpt":"Yaojie Lu이 [arXiv]에 게시한 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","LLM Agent","Tool-use","MCP","Benchmark","Large-scale","Real-world tasks","Automated Evaluation","Meta-tool-learning"],"permalink":"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer","title":"[논문리뷰] LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer","excerpt":"Shunyu Yao이 [arXiv]에 게시한 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Multi-Image Composition","Layout Control","Diffusion Models","Transformer","Attention Mechanisms","Training-Free","Zero-Shot Generalization"],"permalink":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction","title":"[논문리뷰] Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction","excerpt":"Jui-Hui Chung이 [arXiv]에 게시한 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Formal Verification","Language Models","Self-Correction","Data Synthesis","Reinforcement Learning","Model Averaging","Lean"],"permalink":"/ai/review/2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward","title":"[논문리뷰] CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward","excerpt":"Songyang Gao이 [arXiv]에 게시한 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Answer Verification","Reward Model","Benchmarking","Data Augmentation","Reinforcement Learning","Formula Verification","Hallucination Detection"],"permalink":"/ai/review/2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning","title":"[논문리뷰] ChartCap: Mitigating Hallucination of Dense Chart Captioning","excerpt":"Gunhee Kim이 [arXiv]에 게시한 'ChartCap: Mitigating Hallucination of Dense Chart Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Chart Captioning","Hallucination Mitigation","Dataset Generation","Visual Language Models","Cycle Consistency","Reference-Free Metric","Data Visualization"],"permalink":"/ai/review/2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search","title":"[논문리뷰] CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search","excerpt":"Jiwei Li이 [arXiv]에 게시한 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Approximate Nearest Neighbor Search","Reinforcement Learning","Large Language Models","Code Optimization","HNSW","Retrieval-Augmented Generation","Contrastive Learning"],"permalink":"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization","title":"[논문리뷰] AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization","excerpt":"Aman Chadha이 [arXiv]에 게시한 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Alignment Preservation","Fine-Tuning","LoRA","Fisher Information Matrix","Catastrophic Forgetting","LLM Safety","Riemannian Geometry","Parameter-Efficient Learning"],"permalink":"/ai/review/2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo","title":"[논문리뷰] VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo","excerpt":"Bin Jia이 [arXiv]에 게시한 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Omni-modal LLMs","Distributed Training","Model-centric","Parallelism","FSDP","Sequence Parallelism","Expert Parallelism","Mixture-of-Experts"],"permalink":"/ai/review/2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension","title":"[논문리뷰] SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension","excerpt":"Liyan Xu이 [arXiv]에 게시한 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Dense Retrieval","Context-Aware Embedding","RAG","Long Document Comprehension","Residual Learning","Semantic Association","Text Embedding"],"permalink":"/ai/review/2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems","title":"[논문리뷰] RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems","excerpt":"Junkun Hong이 [arXiv]에 게시한 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Brain-inspired AI","Lifelong Learning","Embodied AI","Multi-memory Systems","Knowledge Graph","Robotics","Closed-Loop Planning"],"permalink":"/ai/review/2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-Qwen-Image-Technical-Report","title":"[논문리뷰] Qwen-Image Technical Report","excerpt":"Kaiyuan Gao이 [arXiv]에 게시한 'Qwen-Image Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Image Generation","Text-to-Image","Image Editing","Text Rendering","Multimodal Diffusion Transformer","Curriculum Learning","Reinforcement Learning","Foundation Model"],"permalink":"/ai/review/2025-8-5-Qwen-Image-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models","title":"[논문리뷰] Personalized Safety Alignment for Text-to-Image Diffusion Models","excerpt":"Kaidong Yu이 [arXiv]에 게시한 'Personalized Safety Alignment for Text-to-Image Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Personalized Safety Alignment","Text-to-Image Diffusion Models","DPO","User Preferences","Content Moderation","Generative AI","Cross-Attention","Safety Alignment"],"permalink":"/ai/review/2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report","title":"[논문리뷰] Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report","excerpt":"Anu Vellore이 [arXiv]에 게시한 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Large Language Model","Cybersecurity","Instruction Tuning","Direct Preference Optimization","Cyber Threat Intelligence","Foundation Model","Chatbot"],"permalink":"/ai/review/2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation","title":"[논문리뷰] InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation","excerpt":"Yang Tian이 [arXiv]에 게시한 'InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Instruction Tuning","Multimodal Reasoning","Robotic Manipulation","Catastrophic Forgetting","Mixture-of-Experts (MoE)","Flow Matching"],"permalink":"/ai/review/2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-Exploitation-Is-All-You-Need-for-Exploration","title":"[논문리뷰] Exploitation Is All You Need... for Exploration","excerpt":"Jesse Roberts이 [arXiv]에 게시한 'Exploitation Is All You Need... for Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Exploration-Exploitation","Meta-RL","Transformer Architecture","Emergent Behavior","Multi-Armed Bandits","Gridworlds","Pseudo-Thompson Sampling"],"permalink":"/ai/review/2025-8-5-Exploitation-Is-All-You-Need-for-Exploration/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime","title":"[논문리뷰] Cyber-Zero: Training Cybersecurity Agents without Runtime","excerpt":"Zijian Wang이 [arXiv]에 게시한 'Cyber-Zero: Training Cybersecurity Agents without Runtime' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Cybersecurity Agents","LLM Training","Trajectory Synthesis","Runtime-Free Training","CTF Challenges","LLM Simulation"],"permalink":"/ai/review/2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models","title":"[논문리뷰] CellForge: Agentic Design of Virtual Cell Models","excerpt":"Daniel Shao이 [arXiv]에 게시한 'CellForge: Agentic Design of Virtual Cell Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","AI Scientist","Multi-Agent System","Virtual Cell Modeling","Single-Cell Perturbation Prediction","Deep Learning","Automated Model Design","Code Generation","Retrieval-Augmented Generation"],"permalink":"/ai/review/2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following","title":"[논문리뷰] Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following","excerpt":"Jiaqing Liang이 [arXiv]에 게시한 'Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Self-Supervised RL","Instruction Following","Reasoning Models","Large Language Models","Reward Modeling","Curriculum Learning"],"permalink":"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks","title":"[논문리뷰] AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks","excerpt":"Zhiwei Zhang이 [arXiv]에 게시한 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Large Language Models","LLM Agents","Test-time Scaling","Compute Optimization","Multi-stage Tasks","Resource Allocation","Search Efficiency"],"permalink":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models","title":"[논문리뷰] A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models","excerpt":"Zuxuan Wu이 [arXiv]에 게시한 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Large Vision-Language Models (LVLMs)","Visual Token Pruning","Dynamic Compression","GlimpsePrune","Computational Efficiency","VQA","Reinforcement Learning"],"permalink":"/ai/review/2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation","title":"[논문리뷰] SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation","excerpt":"Long Chen이 [arXiv]에 게시한 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Audio-driven Video Generation","Spatial Auditory Cues","Video Scene Layout","MLLM","Diffusion Models","Training-free"],"permalink":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution","title":"[논문리뷰] SWE-Exp: Experience-Driven Software Issue Resolution","excerpt":"Heng Lian이 [arXiv]에 게시한 'SWE-Exp: Experience-Driven Software Issue Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Software Issue Resolution","LLM Agents","Experience-Driven Learning","Automated Program Repair","Multi-Agent Systems","Knowledge Management","Continuous Learning"],"permalink":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution","title":"[논문리뷰] SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution","excerpt":"Heng Lian이 [arXiv]에 게시한 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Software Engineering","Fault Localization","Issue Resolution","Large Language Models","Competitive Debate","Graph Traversal"],"permalink":"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion","title":"[논문리뷰] PixNerd: Pixel Neural Field Diffusion","excerpt":"Limin Wang이 [arXiv]에 게시한 'PixNerd: Pixel Neural Field Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Neural Fields","Pixel Space","Generative Models","Image Synthesis","Transformer Architecture","End-to-End Learning"],"permalink":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-Multimodal-Referring-Segmentation-A-Survey","title":"[논문리뷰] Multimodal Referring Segmentation: A Survey","excerpt":"Zuxuan Wu이 [arXiv]에 게시한 'Multimodal Referring Segmentation: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Multimodal Learning","Referring Segmentation","Vision-Language Models","Image Segmentation","Video Segmentation","3D Vision","Survey"],"permalink":"/ai/review/2025-8-4-Multimodal-Referring-Segmentation-A-Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges","title":"[논문리뷰] Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges","excerpt":"Chengfei Lv이 [arXiv]에 게시한 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Multi-Turn Dialogue Evaluation","LLM-as-a-Judge","Multi-Judge Aggregation","Preference Learning","Dialogue Quality Assessment","Maximum Likelihood Estimation","Computational Efficiency"],"permalink":"/ai/review/2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages","title":"[논문리뷰] Investigating Hallucination in Conversations for Low Resource Languages","excerpt":"Fatemeh Jamshidi이 [arXiv]에 게시한 'Investigating Hallucination in Conversations for Low Resource Languages' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","LLM Hallucination","Low-resource Languages","Conversational AI","ROUGE Score","Cross-lingual Evaluation","Factual Consistency"],"permalink":"/ai/review/2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation","title":"[논문리뷰] IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation","excerpt":"Jianjiang Feng이 [arXiv]에 게시한 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Image-goal Navigation","3D Gaussian Splatting (3DGS)","Incremental Scene Representation","Coarse-to-fine Localization","Embodied AI","Robotics","Differentiable Rendering"],"permalink":"/ai/review/2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models","title":"[논문리뷰] Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models","excerpt":"Jiaqi Wang이 [arXiv]에 게시한 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Diffusion Large Language Models","Variable-Length Generation","Dynamic Length Adaptation","Denoising Strategy","Inference Optimization","Computational Efficiency"],"permalink":"/ai/review/2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding","title":"[논문리뷰] 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding","excerpt":"Hao Tang이 [arXiv]에 게시한 '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","3D Vision-Language Models","Reasoning","Scene Understanding","Reinforcement Learning","Chain-of-Thought","Dynamic View Selection","Multi-task Learning"],"permalink":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models","title":"[논문리뷰] villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models","excerpt":"Kaixin Wang이 [arXiv]에 게시한 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Latent Actions","Robot Manipulation","Pre-training","Diffusion Models","Proprioceptive Feedback","Foundation Models"],"permalink":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model","title":"[논문리뷰] iLRM: An Iterative Large 3D Reconstruction Model","excerpt":"Abdelrahman Mohamed이 [arXiv]에 게시한 'iLRM: An Iterative Large 3D Reconstruction Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Gaussian Splatting","Iterative Refinement","Transformer Architecture","Multi-view Learning","Scalability","Feed-forward Models"],"permalink":"/ai/review/2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs","title":"[논문리뷰] TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs","excerpt":"Jiasheng Tang이 [arXiv]에 게시한 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","MLLMs","Hallucination Reduction","Preference Optimization","Min-Max Optimization","Token-Adaptive Strategy","Spectral Regularization","Visual Grounding"],"permalink":"/ai/review/2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving","title":"[논문리뷰] Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving","excerpt":"Zhicheng Jiang이 [arXiv]에 게시한 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Large Language Models","Formal Verification","Reinforcement Learning","Lean","Geometry Reasoning","Chain-of-Thought","Lemma-Style Proving"],"permalink":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents","title":"[논문리뷰] Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents","excerpt":"Anji Liu이 [arXiv]에 게시한 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Multi-Task Learning","Visuomotor Agents","Spatial Reasoning","Generalization","Minecraft","Cross-View Goal Specification","Automated Task Synthesis"],"permalink":"/ai/review/2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-RecGPT-Technical-Report","title":"[논문리뷰] RecGPT Technical Report","excerpt":"Jian Wu이 [arXiv]에 게시한 'RecGPT Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Recommender Systems","Large Language Models (LLMs)","User Intent Modeling","Multi-Stage Training","Human-in-the-Loop","E-commerce","Filter Bubble Mitigation","Matthew Effect"],"permalink":"/ai/review/2025-8-3-RecGPT-Technical-Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding","title":"[논문리뷰] Phi-Ground Tech Report: Advancing Perception in GUI Grounding","excerpt":"Kai Qiu이 [arXiv]에 게시한 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","GUI grounding","AI agent","Large Multi-modal Model","Perception","Data Augmentation","Direct Preference Optimization","Computational Efficiency"],"permalink":"/ai/review/2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models","title":"[논문리뷰] Persona Vectors: Monitoring and Controlling Character Traits in Language Models","excerpt":"Jack Lindsey이 [arXiv]에 게시한 'Persona Vectors: Monitoring and Controlling Character Traits in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Persona Control","Activation Steering","Finetuning","Behavioral Shift Detection","Interpretability","Data Filtering"],"permalink":"/ai/review/2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective","title":"[논문리뷰] On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective","excerpt":"Eric C. Larson이 [arXiv]에 게시한 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Softmax Attention","Linear Attention","Recurrent Neural Networks (RNNs)","Taylor Series Expansion","Attention Mechanisms","Expressiveness","Transformer Architectures"],"permalink":"/ai/review/2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting","title":"[논문리뷰] NeRF Is a Valuable Assistant for 3D Gaussian Splatting","excerpt":"ZeSheng Wang이 [arXiv]에 게시한 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","NeRF","3D Gaussian Splatting","Hybrid Model","Joint Optimization","Scene Representation","Neural Rendering","Residual Learning","Sparse View"],"permalink":"/ai/review/2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks","title":"[논문리뷰] Flow Equivariant Recurrent Neural Networks","excerpt":"T. Anderson Keller이 [arXiv]에 게시한 'Flow Equivariant Recurrent Neural Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Flow Equivariance","Recurrent Neural Networks","Sequence Models","Group Equivariance","Lie Subgroups","Generalization","Time-Parameterized Symmetries"],"permalink":"/ai/review/2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring","title":"[논문리뷰] Enhanced Arabic Text Retrieval with Attentive Relevance Scoring","excerpt":"Abdenour Hadid이 [arXiv]에 게시한 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Arabic NLP","Dense Passage Retrieval","Attentive Relevance Scoring","Information Retrieval","Question Answering","Transformer Models","Semantic Matching"],"permalink":"/ai/review/2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation","title":"[논문리뷰] Efficient Machine Unlearning via Influence Approximation","excerpt":"Enhong Chen이 [arXiv]에 게시한 'Efficient Machine Unlearning via Influence Approximation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Machine Unlearning","Influence Function","Incremental Learning","Privacy Protection","Gradient Optimization","Model Editing","Computational Efficiency"],"permalink":"/ai/review/2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations","title":"[논문리뷰] C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations","excerpt":"Yiwen Guo이 [arXiv]에 게시한 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Spoken Dialogue Models","Bilingual Benchmark","Complex Conversations","Ambiguity Resolution","Context Understanding","LLM Evaluation","Human-Computer Interaction"],"permalink":"/ai/review/2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification","title":"[논문리뷰] Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification","excerpt":"Abdelmalik Taleb-Ahmed이 [arXiv]에 게시한 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Kolmogorov-Arnold Networks","Knowledge Distillation","Art Style Classification","Self-Supervised Learning","Spline-Based Activation","Dual-Teacher","Gram Matrix"],"permalink":"/ai/review/2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture","title":"[논문리뷰] AgroBench: Vision-Language Model Benchmark in Agriculture","excerpt":"Yoshitaka Ushiku이 [arXiv]에 게시한 'AgroBench: Vision-Language Model Benchmark in Agriculture' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Agriculture","Benchmarking","Disease Identification","Pest Management","Crop Management","Agronomy"],"permalink":"/ai/review/2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-03-08-Pass-Certified-Solutions-Architect-Associate","title":"[AWS] AWS Solutions Architect Associate 자격증 취득 후기","excerpt":"AWS Solutions Architect Associate(SAA) 자격증 공부 방법, 시험 후기를 정리했습니다.","date":"2025-03-08 23:10:11+0900","lastModifiedAt":"2025-03-08 23:10:14+0900","categories":["Me"],"tags":[["Me"]],"permalink":"/me/pass-certified-solutions-architect-associate/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-25-brain-hack-productivity","title":"뇌과학적으로 게으름과 무기력을 극복하는 방법","excerpt":"게으름과 무기력은 단순한 의지 부족이 아니라, 뇌의 작동 방식과 깊은 관련이 있습니다. 전두엽을 활성화하고, 불필요한 정보를 차단하며, 긍정적인 자기 암시를 활용하는 방법을 통해 효율적으로 극복하는 법을 알아보세요.","date":"2025-02-25 23:10:11+0900","lastModifiedAt":"2025-02-25 23:10:14+0900","categories":["Me"],"tags":[["Me"]],"permalink":"/me/brain-hack-productivity/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-25-aws-lambda-eventbridge","title":"[AWS] AWS Lambda와 Notion API를 활용한 15분 단위 자동 기록 시스템","excerpt":"AWS Lambda, Notion API, Slack Webhook을 활용하여 15분마다 자동으로 시간을 기록하고 Slack 알림을 받는 시스템을 구축하는 방법을 설명합니다.","date":"2025-02-25 18:57:00+0900","lastModifiedAt":"2025-02-25 19:36:00+0900","categories":["AWS"],"tags":[["AWS","Lambda","EventBridge"]],"permalink":"/devops/aws/aws-lambda-eventbridge/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-25-block-youtube-extention","title":"[Chrome Extention] YouTube 차단 확장 프로그램 개발하기","excerpt":"YouTube 접속을 차단하고 생산성을 높이는 크롬 확장 프로그램을 만들어보세요. 개발 과정과 설치 방법을 자세히 설명합니다.","date":"2025-02-25 18:01:00+0900","lastModifiedAt":"2025-02-25 18:10:00+0900","categories":["Chrome Extension"],"tags":[["Chrome Extension"]],"permalink":"/etc/chrome-extension/block-youtube-with-chrome-extension/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-15-django-makefile","title":"[Django] Makefile 사용하기","excerpt":"Django 프로젝트에서 Makefile을 사용하는 방법을 공유합니다.","date":"2025-02-15 04:20:00+0900","lastModifiedAt":"2025-02-15 04:40:00+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/makefile/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-15-docker-fast-build","title":"[Docker] 빠른 빌드 방법","excerpt":"Docker 빌드 속도를 높이는 방법을 공유합니다.","date":"2025-02-14 11:30:11+0900","lastModifiedAt":"2025-02-15 00:11:11+0900","categories":["Docker"],"tags":[["Docker"]],"permalink":"/devops/docker/fast-build/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-14-django-optimistic-locking","title":"[Django] 낙관적 락","excerpt":"Django에서 낙관적 락에 대해 알아보자","date":"2025-02-14 08:33:11+0900","lastModifiedAt":"2025-02-14 08:49:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/optimistic-locking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-14-django-pessimistic-locking","title":"[Django] 비관적 락","excerpt":"Django에서 비관적 락에 대해 알아보자","date":"2025-02-14 08:03:11+0900","lastModifiedAt":"2025-02-14 08:31:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/pessimistic-locking/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-14-django-index","title":"[Django] index","excerpt":"Django에서 index의 사용법을 공유합니다.","date":"2025-02-14 07:30:00+0900","lastModifiedAt":"2025-02-14 07:30:00+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/index/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-14-django-prefetch-related","title":"[Django] prefetch_related","excerpt":"Django에서 prefetch_related의 사용법을 공유합니다.","date":"2025-02-14 06:03:11+0900","lastModifiedAt":"2025-02-14 06:03:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/prefetch-related/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-02-05-see-log-in-django","title":"[Django] DB 로그 확인하기","excerpt":"Django에서 ORM을 사용하여 데이터베이스에 접근하는 경우, 쿼리 로그를 확인하는 방법을 공유합니다.","date":"2025-02-05 19:03:11+0900","lastModifiedAt":"2025-02-05 19:03:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/see-log-in-django/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-18-zero-downtime-deploy-with-jenkins","title":"[Jenkins] 무중단 배포를 위한 파이프라인 구성","excerpt":"Jenkins를 이용한 무중단 배포 방법을 공유합니다.","date":"2025-01-18 19:03:11+0900","lastModifiedAt":"2025-01-18 19:03:11+0900","categories":["Jenkins"],"tags":[["Jenkins"]],"permalink":"/jenkins/zero-downtime-deploy-with-jenkins/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-17-get-ssl-auth-with-letsencrypt","title":"[Nginx] Let's Encrypt로 SSL 인증서 발급받기","excerpt":"Let's Encrypt로 SSL 인증서를 발급받는 방법을 공유합니다.","date":"2025-01-17 23:10:11+0900","lastModifiedAt":"2025-01-17 23:10:14+0900","categories":["Nginx"],"tags":[["Nginx"]],"permalink":"/nginx/get-ssl-auth-with-letsencrypt/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-14-automating-update-github-profile","title":"[GitHub Actions] 깃허브 프로필 자동 업데이트하기","excerpt":"GitHub Actions를 이용해 깃허브 프로필을 자동으로 업데이트하는 방법을 공유합니다.","date":"2025-01-14 23:10:11+0900","lastModifiedAt":"2025-01-14 23:10:14+0900","categories":["GitHub Actions"],"tags":[["GitHub Actions"]],"permalink":"/github-actions/automating-update-github-profile/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-07-create-pipeline-with-jenkins","title":"[Jenkins] Jenkins 파이프라인 생성","excerpt":"Jenkins 파이프라인 생성 방법을 공유합니다.","date":"2025-01-08 03:03:11+0900","lastModifiedAt":"2025-01-14 03:43:11+0900","categories":["Jenkins"],"tags":[["Jenkins"]],"permalink":"/jenkins/create-pipeline-with-jenkins/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-07-start-jenkins","title":"[Jenkins] Jenkins 초기 설정","excerpt":"Jenkins 초기 설정 방법을 공유합니다.","date":"2025-01-07 19:03:11+0900","lastModifiedAt":"2025-01-07 19:03:11+0900","categories":["Jenkins"],"tags":[["Jenkins"]],"permalink":"/jenkins/start-jenkins/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-03-code-review-with-llm","title":"[LLM] LLM을 활용한 코드 리뷰","excerpt":"github workflow를 활용해 코드 리뷰를 자동화하기","date":"2025-01-03 01:03:11+0900","lastModifiedAt":"2025-01-03 02:10:14+0900","categories":["LLM"],"tags":[["LLM"]],"permalink":"/llm/code-review-with-llm/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-02-loguru","title":"[Python] logging 사용법","excerpt":"Loguru 을 이용한 logging 간단하게 사용해보자","date":"2025-01-03 01:03:11+0900","lastModifiedAt":"2025-01-03 02:10:14+0900","categories":["Logging"],"tags":[["Logging"]],"permalink":"/logging/loguru/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-02-start-safeline","title":"[SafeLine] SafeLine 초기 설정","excerpt":"SafeLine 초기 설정 방법을 공유합니다.","date":"2025-01-02 13:03:11+0900","lastModifiedAt":"2025-01-02 14:10:14+0900","categories":["SafeLine"],"tags":[["SafeLine"]],"permalink":"/safeline/start-safeline/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0},{"id":"2025-01-01-first-post","title":"블로그 개발 완료","excerpt":"새해 첫날, 기술 블로그 작성을 시작하다","date":"2025-01-01 23:10:11+0900","lastModifiedAt":"2025-01-01 23:10:14+0900","categories":["Me"],"tags":[["Me"]],"permalink":"/me/first-post/","toc":true,"tocSticky":true,"published":true,"contentHtml":"","comments":[],"commentCount":0}]