[{"id":"2025-11-21-Video-as-Answer_Predict_and_Generate_Next_Video_Event_with_Joint-GRPO","title":"[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO","excerpt":"이 [arXiv]에 게시한 'Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Next Event Prediction","Reinforcement Learning","Vision-Language Model","Video Diffusion Model","Joint Optimization","Multimodal AI","Procedural Learning"],"permalink":"/ai/review/2025-11-21-Video-as-Answer_Predict_and_Generate_Next_Video_Event_with_Joint-GRPO/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-V-ReasonBench_Toward_Unified_Reasoning_Benchmark_Suite_for_Video_Generation_Models","title":"[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models","excerpt":"Baijiong Lin이 [arXiv]에 게시한 'V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Reasoning Benchmark","Chain-of-Frame","Evaluation","Multimodal AI","Physical Dynamics","Spatial Cognition","Pattern Inference"],"permalink":"/ai/review/2025-11-21-V-ReasonBench_Toward_Unified_Reasoning_Benchmark_Suite_for_Video_Generation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-TurkColBERT_A_Benchmark_of_Dense_and_Late-Interaction_Models_for_Turkish_Information_Retrieval","title":"[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval","excerpt":"이 [arXiv]에 게시한 'TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Information Retrieval","Turkish Language","Late-Interaction Models","ColBERT","Dense Retrieval","MUVERA","Benchmarking","Low-Resource NLP","Fine-tuning"],"permalink":"/ai/review/2025-11-21-TurkColBERT_A_Benchmark_of_Dense_and_Late-Interaction_Models_for_Turkish_Information_Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-TimeViper_A_Hybrid_Mamba-Transformer_Vision-Language_Model_for_Efficient_Long_Video_Understanding","title":"[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding","excerpt":"이 [arXiv]에 게시한 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long Video Understanding","Hybrid Mamba-Transformer","Vision-Language Model","Token Compression","Vision-to-Text Aggregation","Efficient LLM","Multimodal AI"],"permalink":"/ai/review/2025-11-21-TimeViper_A_Hybrid_Mamba-Transformer_Vision-Language_Model_for_Efficient_Long_Video_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-Thinking-while-Generating_Interleaving_Textual_Reasoning_throughout_Visual_Generation","title":"[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation","excerpt":"Xinyan Chen이 [arXiv]에 게시한 'Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Generation","Textual Reasoning","Interleaving","Large Multimodal Models (LMMs)","Chain-of-Thought (CoT)","Zero-shot Learning","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)"],"permalink":"/ai/review/2025-11-21-Thinking-while-Generating_Interleaving_Textual_Reasoning_throughout_Visual_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-Step-Audio-R1_Technical_Report","title":"[논문리뷰] Step-Audio-R1 Technical Report","excerpt":"이 [arXiv]에 게시한 'Step-Audio-R1 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio Reasoning","Multimodal LLMs","Modality-Grounded Reasoning Distillation (MGRD)","Chain-of-Thought","Reinforcement Learning","Audio Understanding","Self-Distillation"],"permalink":"/ai/review/2025-11-21-Step-Audio-R1_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-Scaling_Spatial_Intelligence_with_Multimodal_Foundation_Models","title":"[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models","excerpt":"이 [arXiv]에 게시한 'Scaling Spatial Intelligence with Multimodal Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Spatial Intelligence","Multimodal Foundation Models","Data Scaling","Perspective-taking","Visual Question Answering","Emergent Capabilities","Embodied AI","Benchmark Evaluation"],"permalink":"/ai/review/2025-11-21-Scaling_Spatial_Intelligence_with_Multimodal_Foundation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-SRPO_Self-Referential_Policy_Optimization_for_Vision-Language-Action_Models","title":"[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Vision-Language-Action Models","Reward Shaping","World Models","Self-Referential Learning","Robotics","Trajectory Optimization"],"permalink":"/ai/review/2025-11-21-SRPO_Self-Referential_Policy_Optimization_for_Vision-Language-Action_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-SAM_3D_3Dfy_Anything_in_Images","title":"[논문리뷰] SAM 3D: 3Dfy Anything in Images","excerpt":"이 [arXiv]에 게시한 'SAM 3D: 3Dfy Anything in Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Generative Models","Single Image 3D","Object Reconstruction","Scene Understanding","Data Engine","Model-in-the-Loop","Human Preference"],"permalink":"/ai/review/2025-11-21-SAM_3D_3Dfy_Anything_in_Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-SAM2S_Segment_Anything_in_Surgical_Videos_via_Semantic_Long-term_Tracking","title":"[논문리뷰] SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking","excerpt":"이 [arXiv]에 게시한 'SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Surgical Video Segmentation","Interactive Video Object Segmentation","Long-term Tracking","Foundation Models","Domain Adaptation","Semantic Learning","Prompt-based Segmentation"],"permalink":"/ai/review/2025-11-21-SAM2S_Segment_Anything_in_Surgical_Videos_via_Semantic_Long-term_Tracking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-PartUV_Part-Based_UV_Unwrapping_of_3D_Meshes","title":"[논문리뷰] PartUV: Part-Based UV Unwrapping of 3D Meshes","excerpt":"Hao Su이 [arXiv]에 게시한 'PartUV: Part-Based UV Unwrapping of 3D Meshes' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","UV Unwrapping","3D Meshes","Part-Based Decomposition","Neural Fields","Geometric Heuristics","Parameterization","Texture Mapping"],"permalink":"/ai/review/2025-11-21-PartUV_Part-Based_UV_Unwrapping_of_3D_Meshes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-Nemotron_Elastic_Towards_Efficient_Many-in-One_Reasoning_LLMs","title":"[논문리뷰] Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs","excerpt":"이 [arXiv]에 게시한 'Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Compression","Elastic Networks","Knowledge Distillation","Hybrid Mamba-Attention","Reasoning LLMs","Multi-Budget Training","Zero-Shot Deployment"],"permalink":"/ai/review/2025-11-21-Nemotron_Elastic_Towards_Efficient_Many-in-One_Reasoning_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-NaTex_Seamless_Texture_Generation_as_Latent_Color_Diffusion","title":"[논문리뷰] NaTex: Seamless Texture Generation as Latent Color Diffusion","excerpt":"이 [arXiv]에 게시한 'NaTex: Seamless Texture Generation as Latent Color Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Texture Generation","Latent Diffusion Model","Geometry-Aware VAE","Multi-Control DiT","Color Point Cloud","Texture Synthesis","3D Asset Creation"],"permalink":"/ai/review/2025-11-21-NaTex_Seamless_Texture_Generation_as_Latent_Color_Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-MiMo-Embodied_X-Embodied_Foundation_Model_Technical_Report","title":"[논문리뷰] MiMo-Embodied: X-Embodied Foundation Model Technical Report","excerpt":"이 [arXiv]에 게시한 'MiMo-Embodied: X-Embodied Foundation Model Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Model (VLM)","Embodied AI","Autonomous Driving","Foundation Model","Multimodal Learning","Task Planning","Affordance Prediction","Spatial Understanding","Reinforcement Learning"],"permalink":"/ai/review/2025-11-21-MiMo-Embodied_X-Embodied_Foundation_Model_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-First_Frame_Is_the_Place_to_Go_for_Video_Content_Customization","title":"[논문리뷰] First Frame Is the Place to Go for Video Content Customization","excerpt":"이 [arXiv]에 게시한 'First Frame Is the Place to Go for Video Content Customization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generation","Content Customization","Few-shot Learning","LoRA","Vision-Language Models (VLMs)","First Frame Conditioning","Reference-based Generation"],"permalink":"/ai/review/2025-11-21-First_Frame_Is_the_Place_to_Go_for_Video_Content_Customization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-21-Draft_and_Refine_with_Visual_Experts","title":"[논문리뷰] Draft and Refine with Visual Experts","excerpt":"이 [arXiv]에 게시한 'Draft and Refine with Visual Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","lastModifiedAt":"2025-11-21 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Vision-Language Models (LVLMs)","Visual Grounding","Hallucination Mitigation","Agent Framework","Visual Question Answering (VQA)","Expert Coordination","Relevance Map","Multi-modal Reasoning"],"permalink":"/ai/review/2025-11-21-Draft_and_Refine_with_Visual_Experts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity","title":"[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity","excerpt":"이 [arXiv]에 게시한 'What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Research Agents","Ideation Diversity","MLE-bench","LLM Backbones","Agentic Scaffolds","Shannon Entropy","Machine Learning Engineering","Performance Metrics"],"permalink":"/ai/review/2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images","title":"[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images","excerpt":"이 [arXiv]에 게시한 'VisPlay: Self-Evolving Vision-Language Models from Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-Evolving","Vision-Language Models","Reinforcement Learning","Self-Play","Unlabeled Data","Multimodal Reasoning","Group Relative Policy Optimization","Hallucination Mitigation"],"permalink":"/ai/review/2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks","title":"[논문리뷰] Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks","excerpt":"Yiran Peng이 [arXiv]에 게시한 'Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Models","Spatial Reasoning","Maze Solving","Video Generation","Benchmark","Supervised Fine-tuning","Test-Time Scaling","Multimodal Reasoning"],"permalink":"/ai/review/2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation","title":"[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation","excerpt":"이 [arXiv]에 게시한 'Mixture of States: Routing Token-Level Dynamics for Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Diffusion","Mixture of States (MoS)","Token-Level Routing","Dynamic Conditional Fusion","Text-to-Image Generation","Image Editing","Transformer Architecture"],"permalink":"/ai/review/2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-Medal_S_Spatio-Textual_Prompt_Model_for_Medical_Segmentation","title":"[논문리뷰] Medal S: Spatio-Textual Prompt Model for Medical Segmentation","excerpt":"Tao Chen이 [arXiv]에 게시한 'Medal S: Spatio-Textual Prompt Model for Medical Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Medical Segmentation","Foundation Model","Spatio-Textual Prompts","3D Convolution","Multi-modal Imaging","Dynamic Resampling","Parallel Inference","Iterative Refinement"],"permalink":"/ai/review/2025-11-20-Medal_S_Spatio-Textual_Prompt_Model_for_Medical_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-MHR_Momentum_Human_Rig","title":"[논문리뷰] MHR: Momentum Human Rig","excerpt":"Chris Twigg이 [arXiv]에 게시한 'MHR: Momentum Human Rig' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Parametric Body Model","Human Animation","Character Rigging","Pose Correctives","Skeletal Decoupling","Computer Graphics","AR/VR"],"permalink":"/ai/review/2025-11-20-MHR_Momentum_Human_Rig/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-Kandinsky_5.0_A_Family_of_Foundation_Models_for_Image_and_Video_Generation","title":"[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation","excerpt":"Vladimir Arkhipkin이 [arXiv]에 게시한 'Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Generation","Video Generation","Diffusion Models","Flow Matching","Diffusion Transformer","NABLA","RLHF","Supervised Fine-tuning"],"permalink":"/ai/review/2025-11-20-Kandinsky_5.0_A_Family_of_Foundation_Models_for_Image_and_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-Instruction-Guided_Lesion_Segmentation_for_Chest_X-rays_with_Automatically_Generated_Large-Scale_Dataset","title":"[논문리뷰] Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset","excerpt":"이 [arXiv]에 게시한 'Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Medical Imaging","Chest X-ray","Lesion Segmentation","Vision-Language Models","Instruction Following","Data Generation","MIMIC-CXR"],"permalink":"/ai/review/2025-11-20-Instruction-Guided_Lesion_Segmentation_for_Chest_X-rays_with_Automatically_Generated_Large-Scale_Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-FreeAskWorld_An_Interactive_and_Closed-Loop_Simulator_for_Human-Centric_Embodied_AI","title":"[논문리뷰] FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI","excerpt":"Xinyu Yin이 [arXiv]에 게시한 'FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Embodied AI","Vision-and-Language Navigation (VLN)","LLM-driven Simulation","Human-Agent Interaction","Closed-Loop","Benchmark Dataset","Social Cognition"],"permalink":"/ai/review/2025-11-20-FreeAskWorld_An_Interactive_and_Closed-Loop_Simulator_for_Human-Centric_Embodied_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-Aligning_Generative_Music_AI_with_Human_Preferences_Methods_and_Challenges","title":"[논문리뷰] Aligning Generative Music AI with Human Preferences: Methods and Challenges","excerpt":"Abhinaba Roy이 [arXiv]에 게시한 'Aligning Generative Music AI with Human Preferences: Methods and Challenges' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Music AI","Preference Alignment","Reinforcement Learning from Human Feedback (RLHF)","Direct Preference Optimization (DPO)","Inference-Time Optimization","Music Generation","Human-Computer Interaction"],"permalink":"/ai/review/2025-11-20-Aligning_Generative_Music_AI_with_Human_Preferences_Methods_and_Challenges/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-20-ARC-Chapter_Structuring_Hour-Long_Videos_into_Navigable_Chapters_and_Hierarchical_Summaries","title":"[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries","excerpt":"이 [arXiv]에 게시한 'ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","lastModifiedAt":"2025-11-20 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Chaptering","Long-form Video Understanding","Large Language Models","Multimodal Learning","Hierarchical Summarization","Video Segmentation","Reinforcement Learning","Dataset Creation"],"permalink":"/ai/review/2025-11-20-ARC-Chapter_Structuring_Hour-Long_Videos_into_Navigable_Chapters_and_Hierarchical_Summaries/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Φeat_Physically-Grounded_Feature_Representation","title":"[논문리뷰] Φeat: Physically-Grounded Feature Representation","excerpt":"이 [arXiv]에 게시한 'Φeat: Physically-Grounded Feature Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Physically-Grounded Features","Material Representation","Intrinsic Scene Understanding","Vision Transformer","Synthetic Data","Contrastive Learning"],"permalink":"/ai/review/2025-11-19-Φeat_Physically-Grounded_Feature_Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-VIDEOP2R_Video_Understanding_from_Perception_to_Reasoning","title":"[논문리뷰] VIDEOP2R: Video Understanding from Perception to Reasoning","excerpt":"이 [arXiv]에 게시한 'VIDEOP2R: Video Understanding from Perception to Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Understanding","Reinforcement Fine-Tuning (RFT)","Large Video Language Models (LVLMs)","Perception and Reasoning","Chain-of-Thought (CoT)","Process-Aware Learning","Policy Optimization","Credit Assignment"],"permalink":"/ai/review/2025-11-19-VIDEOP2R_Video_Understanding_from_Perception_to_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-TopoPerception_A_Shortcut-Free_Evaluation_of_Global_Visual_Perception_in_Large_Vision-Language_Models","title":"[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models","excerpt":"Rong Zhao이 [arXiv]에 게시한 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LVLM Evaluation","Global Visual Perception","Topological Properties","Shortcut-Free Benchmark","Visual Bottleneck","Multimodal AI","Synthetic Data"],"permalink":"/ai/review/2025-11-19-TopoPerception_A_Shortcut-Free_Evaluation_of_Global_Visual_Perception_in_Large_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-REVISOR_Beyond_Textual_Reflection_Towards_Multimodal_Introspective_Reasoning_in_Long-Form_Video_Understanding","title":"[논문리뷰] REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding","excerpt":"Jingyang Chen이 [arXiv]에 게시한 'REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Long-Form Video Understanding","Self-Reflection","Reinforcement Learning","Tool-Augmented MLLMs","Visual Rethinking","Video Question Answering","Causal Attribution"],"permalink":"/ai/review/2025-11-19-REVISOR_Beyond_Textual_Reflection_Towards_Multimodal_Introspective_Reasoning_in_Long-Form_Video_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Proactive_Hearing_Assistants_that_Isolate_Egocentric_Conversations","title":"[논문리뷰] Proactive Hearing Assistants that Isolate Egocentric Conversations","excerpt":"이 [arXiv]에 게시한 'Proactive Hearing Assistants that Isolate Egocentric Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Proactive Hearing Assistant","Egocentric Audio Processing","Speech Separation","Turn-taking Dynamics","Dual-Model Architecture","Real-time Inference","Wearable Devices","Dialogue Modeling"],"permalink":"/ai/review/2025-11-19-Proactive_Hearing_Assistants_that_Isolate_Egocentric_Conversations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Orion_A_Unified_Visual_Agent_for_Multimodal_Perception_Advanced_Visual_Reasoning_and_Execution","title":"[논문리뷰] Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution","excerpt":"Sudeep Pillai이 [arXiv]에 게시한 'Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Agent","Multimodal Perception","Tool-Augmented LLM","Agentic AI","Visual Reasoning","Computer Vision","Structured Outputs","ReAct Framework"],"permalink":"/ai/review/2025-11-19-Orion_A_Unified_Visual_Agent_for_Multimodal_Perception_Advanced_Visual_Reasoning_and_Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-OmniZip_Audio-Guided_Dynamic_Token_Compression_for_Fast_Omnimodal_Large_Language_Models","title":"[논문리뷰] OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models","excerpt":"Jian liu이 [arXiv]에 게시한 'OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Omnimodal LLMs","Token Compression","Audio-Video Understanding","Dynamic Pruning","Inference Acceleration","Spatio-Temporal Compression","Large Language Models"],"permalink":"/ai/review/2025-11-19-OmniZip_Audio-Guided_Dynamic_Token_Compression_for_Fast_Omnimodal_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Mitigating_Label_Length_Bias_in_Large_Language_Models","title":"[논문리뷰] Mitigating Label Length Bias in Large Language Models","excerpt":"Katharina von der Wense이 [arXiv]에 게시한 'Mitigating Label Length Bias in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Label Bias","Calibration","In-Context Learning","Text Classification","Multi-token Labels","Label Length Bias","Multiple Choice QA"],"permalink":"/ai/review/2025-11-19-Mitigating_Label_Length_Bias_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-MVI-Bench_A_Comprehensive_Benchmark_for_Evaluating_Robustness_to_Misleading_Visual_Inputs_in_LVLMs","title":"[논문리뷰] MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs","excerpt":"Kaijie Chen이 [arXiv]에 게시한 'MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LVLM Robustness","Misleading Visual Inputs","VQA Benchmark","Visual Perception","Visual Reasoning","MVI-Sensitivity","Multimodal AI"],"permalink":"/ai/review/2025-11-19-MVI-Bench_A_Comprehensive_Benchmark_for_Evaluating_Robustness_to_Misleading_Visual_Inputs_in_LVLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Large_Language_Models_Meet_Extreme_Multi-label_Classification_Scaling_and_Multi-modal_Framework","title":"[논문리뷰] Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework","excerpt":"이 [arXiv]에 게시한 'Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Extreme Multi-label Classification (XMC)","Large Language Models (LLMs)","Multi-modal Learning","Dual-decoder Learning","Vision Transformers","Contrastive Learning","Prompt Engineering"],"permalink":"/ai/review/2025-11-19-Large_Language_Models_Meet_Extreme_Multi-label_Classification_Scaling_and_Multi-modal_Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-LLM-Powered_Fully_Automated_Chaos_Engineering_Towards_Enabling_Anyone_to_Build_Resilient_Software_Systems_at_Low_Cost","title":"[논문리뷰] LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost","excerpt":"Kengo Tajiri이 [arXiv]에 게시한 'LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Chaos Engineering","Large Language Models","System Resilience","Kubernetes","Software Automation","AI Agents","Fault Injection"],"permalink":"/ai/review/2025-11-19-LLM-Powered_Fully_Automated_Chaos_Engineering_Towards_Enabling_Anyone_to_Build_Resilient_Software_Systems_at_Low_Cost/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Error-Driven_Scene_Editing_for_3D_Grounding_in_Large_Language_Models","title":"[논문리뷰] Error-Driven Scene Editing for 3D Grounding in Large Language Models","excerpt":"이 [arXiv]에 게시한 'Error-Driven Scene Editing for 3D Grounding in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Grounding","3D-LLMs","Scene Editing","Counterfactual Augmentation","Error-Driven Learning","Spatial Reasoning","Visual Grounding"],"permalink":"/ai/review/2025-11-19-Error-Driven_Scene_Editing_for_3D_Grounding_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Can_World_Simulators_Reason_Gen-ViRe_A_Generative_Visual_Reasoning_Benchmark","title":"[논문리뷰] Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark","excerpt":"Yuzhang Shang이 [arXiv]에 게시한 'Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Visual Reasoning","Chain-of-Frames (CoF)","Video Generation Models","World Simulators","AI Benchmarking","Cognitive Reasoning","VLM Evaluation"],"permalink":"/ai/review/2025-11-19-Can_World_Simulators_Reason_Gen-ViRe_A_Generative_Visual_Reasoning_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-AraLingBench_A_Human-Annotated_Benchmark_for_Evaluating_Arabic_Linguistic_Capabilities_of_Large_Language_Models","title":"[논문리뷰] AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models","excerpt":"이 [arXiv]에 게시한 'AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Arabic LLMs","Linguistic Benchmark","Human Annotation","Natural Language Understanding","Grammar Evaluation","Morphology Analysis","Syntax Assessment","Reading Comprehension"],"permalink":"/ai/review/2025-11-19-AraLingBench_A_Human-Annotated_Benchmark_for_Evaluating_Arabic_Linguistic_Capabilities_of_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Agent_READMEs_An_Empirical_Study_of_Context_Files_for_Agentic_Coding","title":"[논문리뷰] Agent READMEs: An Empirical Study of Context Files for Agentic Coding","excerpt":"Kundjanasith Thonglek이 [arXiv]에 게시한 'Agent READMEs: An Empirical Study of Context Files for Agentic Coding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic Coding","Context Files","READMEs for Agents","Empirical Study","Software Engineering","Documentation Maintenance","Non-functional Requirements","LLMs"],"permalink":"/ai/review/2025-11-19-Agent_READMEs_An_Empirical_Study_of_Context_Files_for_Agentic_Coding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-Agent-R1_Training_Powerful_LLM_Agents_with_End-to-End_Reinforcement_Learning","title":"[논문리뷰] Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning","excerpt":"Yucong Luo이 [arXiv]에 게시한 'Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Markov Decision Process","Tool Use","Multi-turn Interaction","Policy Optimization","Reward Shaping","Agent Framework"],"permalink":"/ai/review/2025-11-19-Agent-R1_Training_Powerful_LLM_Agents_with_End-to-End_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-A_Style_is_Worth_One_Code_Unlocking_Code-to-Style_Image_Generation_with_Discrete_Style_Space","title":"[논문리뷰] A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space","excerpt":"이 [arXiv]에 게시한 'A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Code-to-Style Generation","Discrete Style Space","Style Codebook","Autoregressive Model","Diffusion Models","Visual Stylization","Generative AI"],"permalink":"/ai/review/2025-11-19-A_Style_is_Worth_One_Code_Unlocking_Code-to-Style_Image_Generation_with_Discrete_Style_Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-A_Brain_Wave_Encodes_a_Thousand_Tokens_Modeling_Inter-Cortical_Neural_Interactions_for_Effective_EEG-based_Emotion_Recognition","title":"[논문리뷰] A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition","excerpt":"G. Maragatham이 [arXiv]에 게시한 'A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","EEG","Emotion Recognition","Transformer Architecture","Inter-Cortical Neural Interactions","Multi-Head Attention","Brain-Computer Interface","Affective Computing"],"permalink":"/ai/review/2025-11-19-A_Brain_Wave_Encodes_a_Thousand_Tokens_Modeling_Inter-Cortical_Neural_Interactions_for_Effective_EEG-based_Emotion_Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-19-ATLAS_A_High-Difficulty_Multidisciplinary_Benchmark_for_Frontier_Scientific_Reasoning","title":"[논문리뷰] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning","excerpt":"Yuqiang Li이 [arXiv]에 게시한 'ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","lastModifiedAt":"2025-11-19 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Benchmark","LLMs","Scientific Reasoning","Multidisciplinary","AI4S","Data Contamination","Evaluation","LRM-as-Judge"],"permalink":"/ai/review/2025-11-19-ATLAS_A_High-Difficulty_Multidisciplinary_Benchmark_for_Frontier_Scientific_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-Uni-MoE-2.0-Omni_Scaling_Language-Centric_Omnimodal_Large_Model_with_Advanced_MoE_Training_and_Data","title":"[논문리뷰] Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data","excerpt":"이 [arXiv]에 게시한 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Omnimodal Large Models","Mixture-of-Experts (MoE)","Language-Centric AI","Multimodal Understanding","Multimodal Generation","Progressive Training","Omni-Modality 3D RoPE"],"permalink":"/ai/review/2025-11-18-Uni-MoE-2.0-Omni_Scaling_Language-Centric_Omnimodal_Large_Model_with_Advanced_MoE_Training_and_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-UnSAMv2_Self-Supervised_Learning_Enables_Segment_Anything_at_Any_Granularity","title":"[논문리뷰] UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity","excerpt":"이 [arXiv]에 게시한 'UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Self-Supervised Learning","Segmentation","Granularity Control","SAM","Foundation Models","Unsupervised Learning","Image Segmentation","Video Segmentation"],"permalink":"/ai/review/2025-11-18-UnSAMv2_Self-Supervised_Learning_Enables_Segment_Anything_at_Any_Granularity/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-UFO3_Weaving_the_Digital_Agent_Galaxy","title":"[논문리뷰] UFO^3: Weaving the Digital Agent Galaxy","excerpt":"이 [arXiv]에 게시한 'UFO^3: Weaving the Digital Agent Galaxy' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Cross-Device Orchestration","LLM-Powered Agents","Task Constellation","Directed Acyclic Graph (DAG)","Agent Interaction Protocol (AIP)","Fault Tolerance","Asynchronous Execution"],"permalink":"/ai/review/2025-11-18-UFO3_Weaving_the_Digital_Agent_Galaxy/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-TiViBench_Benchmarking_Think-in-Video_Reasoning_for_Video_Generative_Models","title":"[논문리뷰] TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models","excerpt":"Qingyang Liu이 [arXiv]에 게시한 'TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Generative Models","Visual Reasoning","Benchmarking","Image-to-Video","TiViBench","VideoTPO","Prompt Optimization"],"permalink":"/ai/review/2025-11-18-TiViBench_Benchmarking_Think-in-Video_Reasoning_for_Video_Generative_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-Test-Time_Spectrum-Aware_Latent_Steering_for_Zero-Shot_Generalization_in_Vision-Language_Models","title":"[논문리뷰] Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Test-Time Adaptation","Zero-Shot Generalization","Spectral Decomposition","Latent Space Steering","SVD","Out-of-Distribution"],"permalink":"/ai/review/2025-11-18-Test-Time_Spectrum-Aware_Latent_Steering_for_Zero-Shot_Generalization_in_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-Souper-Model_How_Simple_Arithmetic_Unlocks_State-of-the-Art_LLM_Performance","title":"[논문리뷰] Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance","excerpt":"이 [arXiv]에 게시한 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Model Souping","Large Language Models","Weighted Averaging","Benchmark Optimization","State-of-the-Art","Category Experts","Parameter Averaging","Post-training"],"permalink":"/ai/review/2025-11-18-Souper-Model_How_Simple_Arithmetic_Unlocks_State-of-the-Art_LLM_Performance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-SafeGRPO_Self-Rewarded_Multimodal_Safety_Alignment_via_Rule-Governed_Policy_Optimization","title":"[논문리뷰] SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization","excerpt":"Bo Du이 [arXiv]에 게시한 'SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Safety Alignment","Rule-Governed RL","Self-Rewarded Learning","MLLM Safety","Policy Optimization","Safety Benchmarking","Compositional Robustness"],"permalink":"/ai/review/2025-11-18-SafeGRPO_Self-Rewarded_Multimodal_Safety_Alignment_via_Rule-Governed_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-Part-X-MLLM_Part-aware_3D_Multimodal_Large_Language_Model","title":"[논문리뷰] Part-X-MLLM: Part-aware 3D Multimodal Large Language Model","excerpt":"이 [arXiv]에 게시한 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Multimodal LLM","Part-aware","3D Generation","3D Editing","3D Understanding","Bounding Box","Structured Program","Dual-encoder"],"permalink":"/ai/review/2025-11-18-Part-X-MLLM_Part-aware_3D_Multimodal_Large_Language_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-P1_Mastering_Physics_Olympiads_with_Reinforcement_Learning","title":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning","excerpt":"Haiyuan Wan이 [arXiv]에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Physics Reasoning","Agentic AI","Olympiad Problems","Post-Training","Knowledge Transfer"],"permalink":"/ai/review/2025-11-18-P1_Mastering_Physics_Olympiads_with_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-OlmoEarth_Stable_Latent_Image_Modeling_for_Multimodal_Earth_Observation","title":"[논문리뷰] OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation","excerpt":"이 [arXiv]에 게시한 'OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Earth Observation","Foundation Model","Multimodal Learning","Self-supervised Learning","Latent Image Modeling","Vision Transformer","Spatio-temporal"],"permalink":"/ai/review/2025-11-18-OlmoEarth_Stable_Latent_Image_Modeling_for_Multimodal_Earth_Observation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-NORA-1.5_A_Vision-Language-Action_Model_Trained_using_World_Model-_and_Action-based_Preference_Rewards","title":"[논문리뷰] NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards","excerpt":"이 [arXiv]에 게시한 'NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Model","Direct Preference Optimization","World Model","Reward Learning","Robotics","Embodied AI","Flow-Matching"],"permalink":"/ai/review/2025-11-18-NORA-1.5_A_Vision-Language-Action_Model_Trained_using_World_Model-_and_Action-based_Preference_Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-MiroThinker_Pushing_the_Performance_Boundaries_of_Open-Source_Research_Agents_via_Model_Context_and_Interactive_Scaling","title":"[논문리뷰] MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling","excerpt":"cyyang822이 [arXiv]에 게시한 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Research Agent","Tool-Augmented Reasoning","Interaction Scaling","Large Language Models","Reinforcement Learning","Context Management","Open-Source AI"],"permalink":"/ai/review/2025-11-18-MiroThinker_Pushing_the_Performance_Boundaries_of_Open-Source_Research_Agents_via_Model_Context_and_Interactive_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-MicroVQA_High-Quality_Microscopy_Reasoning_Dataset_with_Weakly_Supervised_Graphs_for_Multimodal_Large_Language_Model","title":"[논문리뷰] MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model","excerpt":"Bo Yan이 [arXiv]에 게시한 'MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Microscopy VQA","Multimodal LLM","Weak Supervision","Graph Neural Networks","Dataset Generation","Biomedical Imaging","Scientific Reasoning","Cross-Modal Consistency"],"permalink":"/ai/review/2025-11-18-MicroVQA_High-Quality_Microscopy_Reasoning_Dataset_with_Weakly_Supervised_Graphs_for_Multimodal_Large_Language_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-LoCoBench-Agent_An_Interactive_Benchmark_for_LLM_Agents_in_Long-Context_Software_Engineering","title":"[논문리뷰] LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering","excerpt":"이 [arXiv]에 게시한 'LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Software Engineering","Long-Context","Interactive Benchmark","Tool Usage","Memory Management","Bias-Free Evaluation","Multi-Turn"],"permalink":"/ai/review/2025-11-18-LoCoBench-Agent_An_Interactive_Benchmark_for_LLM_Agents_in_Long-Context_Software_Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-Live-SWE-agent_Can_Software_Engineering_Agents_Self-Evolve_on_the_Fly","title":"[논문리뷰] Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?","excerpt":"Lingming Zhang이 [arXiv]에 게시한 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Software Engineering Agents","LLM Agents","Self-Evolution","On-the-Fly Learning","Tool Creation","SWE-bench","Autonomous Systems","Code Generation"],"permalink":"/ai/review/2025-11-18-Live-SWE-agent_Can_Software_Engineering_Agents_Self-Evolve_on_the_Fly/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-Genomic_Next-Token_Predictors_are_In-Context_Learners","title":"[논문리뷰] Genomic Next-Token Predictors are In-Context Learners","excerpt":"이 [arXiv]에 게시한 'Genomic Next-Token Predictors are In-Context Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","In-Context Learning (ICL)","Genomic Sequences","Next-Token Prediction","Large Language Models (LLMs)","Modality-Agnostic AI","Meta-Learning","Bitstring Program Synthesis","Evo2"],"permalink":"/ai/review/2025-11-18-Genomic_Next-Token_Predictors_are_In-Context_Learners/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-Assessing_LLMs_for_Serendipity_Discovery_in_Knowledge_Graphs_A_Case_for_Drug_Repurposing","title":"[논문리뷰] Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing","excerpt":"이 [arXiv]에 게시한 'Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Serendipity Discovery","Knowledge Graphs","Drug Repurposing","LLMs","KGQA","RNS Metric","Biomedical AI"],"permalink":"/ai/review/2025-11-18-Assessing_LLMs_for_Serendipity_Discovery_in_Knowledge_Graphs_A_Case_for_Drug_Repurposing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-A_Decentralized_Retrieval_Augmented_Generation_System_with_Source_Reliabilities_Secured_on_Blockchain","title":"[논문리뷰] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain","excerpt":"Meng Jiang이 [arXiv]에 게시한 'A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Decentralized RAG","Blockchain","Smart Contracts","Source Reliability","Large Language Models","Retrieval Augmented Generation","Trustworthy AI"],"permalink":"/ai/review/2025-11-18-A_Decentralized_Retrieval_Augmented_Generation_System_with_Source_Reliabilities_Secured_on_Blockchain/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-18-AI-Salesman_Towards_Reliable_Large_Language_Model_Driven_Telemarketing","title":"[논문리뷰] AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing","excerpt":"Hongyu Lin이 [arXiv]에 게시한 'AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","lastModifiedAt":"2025-11-18 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Telemarketing","Large Language Models","Persuasive Dialogue","Reinforcement Learning","Bayesian Optimization","Dynamic Prompting","Dialogue Systems"],"permalink":"/ai/review/2025-11-18-AI-Salesman_Towards_Reliable_Large_Language_Model_Driven_Telemarketing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-miniF2F-Lean_Revisited_Reviewing_Limitations_and_Charting_a_Path_Forward","title":"[논문리뷰] miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward","excerpt":"Farzan Farnia이 [arXiv]에 게시한 'miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Autoformalization","Benchmark Dataset","miniF2F","Lean Language","Large Language Models","Mathematical Reasoning","Formal Verification"],"permalink":"/ai/review/2025-11-17-miniF2F-Lean_Revisited_Reviewing_Limitations_and_Charting_a_Path_Forward/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-Workload_Schedulers_--_Genesis_Algorithms_and_Differences","title":"[논문리뷰] Workload Schedulers -- Genesis, Algorithms and Differences","excerpt":"Vladimir Getov이 [arXiv]에 게시한 'Workload Schedulers -- Genesis, Algorithms and Differences' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Workload Scheduling","Process Scheduling","Job Scheduling","Big Data Processing","Resource Management","Distributed Systems","Scheduling Algorithms","Performance Optimization"],"permalink":"/ai/review/2025-11-17-Workload_Schedulers_--_Genesis_Algorithms_and_Differences/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-Virtual_Width_Networks","title":"[논문리뷰] Virtual Width Networks","excerpt":"이 [arXiv]에 게시한 'Virtual Width Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Virtual Width Networks","Transformer","Mixture-of-Experts (MoE)","Scaling Laws","Representation Learning","Model Efficiency","Multi-Token Prediction","Hyper-Connections"],"permalink":"/ai/review/2025-11-17-Virtual_Width_Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-UI2CodeN_A_Visual_Language_Model_for_Test-Time_Scalable_Interactive_UI-to-Code_Generation","title":"[논문리뷰] UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation","excerpt":"Weihan Wang이 [arXiv]에 게시한 'UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Language Model","UI-to-Code Generation","Interactive UI","UI Editing","UI Polishing","Reinforcement Learning","Multimodal Coding","Test-Time Scaling"],"permalink":"/ai/review/2025-11-17-UI2CodeN_A_Visual_Language_Model_for_Test-Time_Scalable_Interactive_UI-to-Code_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-Simulating_the_Visual_World_with_Artificial_Intelligence_A_Roadmap","title":"[논문리뷰] Simulating the Visual World with Artificial Intelligence: A Roadmap","excerpt":"Pengfei Wan이 [arXiv]에 게시한 'Simulating the Visual World with Artificial Intelligence: A Roadmap' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","World Models","Video Generation","AI Simulation","Generative AI","Physical Plausibility","Interactive AI","Planning","Roadmap"],"permalink":"/ai/review/2025-11-17-Simulating_the_Visual_World_with_Artificial_Intelligence_A_Roadmap/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-MarsRL_Advancing_Multi-Agent_Reasoning_System_via_Reinforcement_Learning_with_Agentic_Pipeline_Parallelism","title":"[논문리뷰] MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism","excerpt":"이 [arXiv]에 게시한 'MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Reinforcement Learning","LLMs","Pipeline Parallelism","Reasoning","Reward Shaping","Agentic AI"],"permalink":"/ai/review/2025-11-17-MarsRL_Advancing_Multi-Agent_Reasoning_System_via_Reinforcement_Learning_with_Agentic_Pipeline_Parallelism/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-LiteAttention_A_Temporal_Sparse_Attention_for_Diffusion_Transformers","title":"[논문리뷰] LiteAttention: A Temporal Sparse Attention for Diffusion Transformers","excerpt":"이 [arXiv]에 게시한 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Transformers","Sparse Attention","Temporal Coherence","Video Generation","Computational Efficiency","FlashAttention","CUDA Kernels"],"permalink":"/ai/review/2025-11-17-LiteAttention_A_Temporal_Sparse_Attention_for_Diffusion_Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-Large_Language_Models_for_Scientific_Idea_Generation_A_Creativity-Centered_Survey","title":"[논문리뷰] Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey","excerpt":"Mohammad Hossein Rohban이 [arXiv]에 게시한 'Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Scientific Discovery","Idea Generation","Creativity","Survey","AI in Science","Prompt Engineering","Multi-agent Systems","Evaluation Metrics"],"permalink":"/ai/review/2025-11-17-Large_Language_Models_for_Scientific_Idea_Generation_A_Creativity-Centered_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-HI-TransPA_Hearing_Impairments_Translation_Personal_Assistant","title":"[논문리뷰] HI-TransPA: Hearing Impairments Translation Personal Assistant","excerpt":"이 [arXiv]에 게시한 'HI-TransPA: Hearing Impairments Translation Personal Assistant' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Hearing Impairment","Audio-Visual Speech Recognition","Curriculum Learning","Omni-Models","Assistive Technology","Lip Reading","Speech Translation"],"permalink":"/ai/review/2025-11-17-HI-TransPA_Hearing_Impairments_Translation_Personal_Assistant/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-GGBench_A_Geometric_Generative_Reasoning_Benchmark_for_Unified_Multimodal_Models","title":"[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models","excerpt":"Siyuan Li이 [arXiv]에 게시한 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal AI","Generative Reasoning","Geometric Construction","Benchmark","GeoGebra","Code-based Evaluation","Unified Models"],"permalink":"/ai/review/2025-11-17-GGBench_A_Geometric_Generative_Reasoning_Benchmark_for_Unified_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-From_Proof_to_Program_Characterizing_Tool-Induced_Reasoning_Hallucinations_in_Large_Language_Models","title":"[논문리뷰] From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models","excerpt":"이 [arXiv]에 게시한 'From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Tool-augmented LLMs","Reasoning Hallucinations","Tool-Induced Myopia (TIM)","Code Interpreter","Mathematical Reasoning","LLM Evaluation","Preference Optimization"],"permalink":"/ai/review/2025-11-17-From_Proof_to_Program_Characterizing_Tool-Induced_Reasoning_Hallucinations_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-Experience-Guided_Adaptation_of_Inference-Time_Reasoning_Strategies","title":"[논문리뷰] Experience-Guided Adaptation of Inference-Time Reasoning Strategies","excerpt":"이 [arXiv]에 게시한 'Experience-Guided Adaptation of Inference-Time Reasoning Strategies' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Adaptive AI","Inference-Time Adaptation","Reasoning Strategies","Meta-Learning","LLM-based Agents","Dynamic Strategy Generation","Continual Learning","Computational Efficiency"],"permalink":"/ai/review/2025-11-17-Experience-Guided_Adaptation_of_Inference-Time_Reasoning_Strategies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-EmoVid_A_Multimodal_Emotion_Video_Dataset_for_Emotion-Centric_Video_Understanding_and_Generation","title":"[논문리뷰] EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation","excerpt":"Zeyu Wang이 [arXiv]에 게시한 'EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Dataset","Emotion Recognition","Video Generation","Affective Computing","Stylized Media","Diffusion Models","Video Understanding","Text-to-Video"],"permalink":"/ai/review/2025-11-17-EmoVid_A_Multimodal_Emotion_Video_Dataset_for_Emotion-Centric_Video_Understanding_and_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-Dont_Waste_It_Guiding_Generative_Recommenders_with_Structured_Human_Priors_via_Multi-head_Decoding","title":"[논문리뷰] Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding","excerpt":"이 [arXiv]에 게시한 'Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generative Recommenders","Human Priors","Multi-head Decoding","Disentangled Representation Learning","Sequential Recommendation","Adapter Networks","Hierarchical Modeling"],"permalink":"/ai/review/2025-11-17-Dont_Waste_It_Guiding_Generative_Recommenders_with_Structured_Human_Priors_via_Multi-head_Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-DoPE_Denoising_Rotary_Position_Embedding","title":"[논문리뷰] DoPE: Denoising Rotary Position Embedding","excerpt":"Min Yang이 [arXiv]에 게시한 'DoPE: Denoising Rotary Position Embedding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Rotary Position Embedding","Transformer","Length Extrapolation","Attention Sink","Matrix Entropy","Denoising","Large Language Models"],"permalink":"/ai/review/2025-11-17-DoPE_Denoising_Rotary_Position_Embedding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-DiscoX_Benchmarking_Discourse-Level_Translation_task_in_Expert_Domains","title":"[논문리뷰] DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains","excerpt":"이 [arXiv]에 게시한 'DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Discourse-Level Translation","Expert Domains","Benchmarking","LLM Evaluation","Reference-Free Metric","Chinese-English Translation","Contextual Coherence","Domain-Specific Terminology"],"permalink":"/ai/review/2025-11-17-DiscoX_Benchmarking_Discourse-Level_Translation_task_in_Expert_Domains/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-CATS-V2V_A_Real-World_Vehicle-to-Vehicle_Cooperative_Perception_Dataset_with_Complex_Adverse_Traffic_Scenarios","title":"[논문리뷰] CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios","excerpt":"Juyoung Oh이 [arXiv]에 게시한 'CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Cooperative Perception","Vehicle-to-Vehicle (V2V)","Autonomous Driving","Dataset","Adverse Traffic Scenarios","Sensor Fusion","Temporal Alignment","3D Bounding Box Annotation"],"permalink":"/ai/review/2025-11-17-CATS-V2V_A_Real-World_Vehicle-to-Vehicle_Cooperative_Perception_Dataset_with_Complex_Adverse_Traffic_Scenarios/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-17-A_Meta-Heuristic_Load_Balancer_for_Cloud_Computing_Systems","title":"[논문리뷰] A Meta-Heuristic Load Balancer for Cloud Computing Systems","excerpt":"Vladimir Getov이 [arXiv]에 게시한 'A Meta-Heuristic Load Balancer for Cloud Computing Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","lastModifiedAt":"2025-11-17 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Cloud Computing","Load Balancing","Meta-Heuristic","Genetic Algorithm","Simulated Annealing","Tabu Search","Resource Management","Service Migration"],"permalink":"/ai/review/2025-11-17-A_Meta-Heuristic_Load_Balancer_for_Cloud_Computing_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-UniVA_Universal_Video_Agent_towards_Open-Source_Next-Generation_Video_Generalist","title":"[논문리뷰] UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist","excerpt":"이 [arXiv]에 게시한 'UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Agents","Multi-modal AI","Plan-Act Architecture","Tool-Use","Long-horizon Reasoning","Open-source","Video Generation","Video Understanding"],"permalink":"/ai/review/2025-11-14-UniVA_Universal_Video_Agent_towards_Open-Source_Next-Generation_Video_Generalist/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-Superpositional_Gradient_Descent_Harnessing_Quantum_Principles_for_Model_Training","title":"[논문리뷰] Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training","excerpt":"suayptalha이 [arXiv]에 게시한 'Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Quantum Computing","Optimization","Machine Learning","Transformers","Gradient Descent","Superposition","Large Language Models","Hybrid Quantum-Classical"],"permalink":"/ai/review/2025-11-14-Superpositional_Gradient_Descent_Harnessing_Quantum_Principles_for_Model_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-SliderEdit_Continuous_Image_Editing_with_Fine-Grained_Instruction_Control","title":"[논문리뷰] SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control","excerpt":"Ryan Rossi이 [arXiv]에 게시한 'SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Image Editing","Continuous Control","Fine-Grained Control","Instruction-based","Low-Rank Adaptation","Disentanglement","Generative Models"],"permalink":"/ai/review/2025-11-14-SliderEdit_Continuous_Image_Editing_with_Fine-Grained_Instruction_Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-Rubric-Based_Benchmarking_and_Reinforcement_Learning_for_Advancing_LLM_Instruction_Following","title":"[논문리뷰] Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following","excerpt":"Karishma Mandyam이 [arXiv]에 게시한 'Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","Instruction Following","Reinforcement Learning","Rubric-based Evaluation","Benchmarking","Reward Shaping","Rubric Verifier","AdvancedIF"],"permalink":"/ai/review/2025-11-14-Rubric-Based_Benchmarking_and_Reinforcement_Learning_for_Advancing_LLM_Instruction_Following/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-ResearchRubrics_A_Benchmark_of_Prompts_and_Rubrics_For_Evaluating_Deep_Research_Agents","title":"[논문리뷰] ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents","excerpt":"이 [arXiv]에 게시한 'ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Deep Research Agents","LLM Evaluation","Benchmark","Rubrics","Multi-step Reasoning","Cross-document Synthesis","AI Performance","Task Complexity"],"permalink":"/ai/review/2025-11-14-ResearchRubrics_A_Benchmark_of_Prompts_and_Rubrics_For_Evaluating_Deep_Research_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-One_Small_Step_in_Latent_One_Giant_Leap_for_Pixels_Fast_Latent_Upscale_Adapter_for_Your_Diffusion_Models","title":"[논문리뷰] One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models","excerpt":"Ilya Makarov이 [arXiv]에 게시한 'One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Latent Diffusion Models","Super-Resolution","Upscaling Adapter","Image Generation","Latent Space","Multi-scale Learning","Cross-VAE"],"permalink":"/ai/review/2025-11-14-One_Small_Step_in_Latent_One_Giant_Leap_for_Pixels_Fast_Latent_Upscale_Adapter_for_Your_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-Music_Flamingo_Scaling_Music_Understanding_in_Audio_Language_Models","title":"[논문리뷰] Music Flamingo: Scaling Music Understanding in Audio Language Models","excerpt":"이 [arXiv]에 게시한 'Music Flamingo: Scaling Music Understanding in Audio Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Audio Language Models","Music Understanding","Chain-of-Thought","Reinforcement Learning","Data Curation","Multimodal AI","Music Information Retrieval"],"permalink":"/ai/review/2025-11-14-Music_Flamingo_Scaling_Music_Understanding_in_Audio_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-MuSc-V2_Zero-Shot_Multimodal_Industrial_Anomaly_Classification_and_Segmentation_with_Mutual_Scoring_of_Unlabeled_Samples","title":"[논문리뷰] MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples","excerpt":"이 [arXiv]에 게시한 'MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Zero-Shot Learning","Anomaly Detection","Anomaly Segmentation","Multimodal","Industrial Inspection","Mutual Scoring","Unsupervised Learning","Transformer"],"permalink":"/ai/review/2025-11-14-MuSc-V2_Zero-Shot_Multimodal_Industrial_Anomaly_Classification_and_Segmentation_with_Mutual_Scoring_of_Unlabeled_Samples/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-MM-CRITIC_A_Holistic_Evaluation_of_Large_Multimodal_Models_as_Multimodal_Critique","title":"[논문리뷰] MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique","excerpt":"이 [arXiv]에 게시한 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LMMs","Multimodal Critique","Benchmark","Evaluation","Reward Model","GPT-4o","Scaling Law"],"permalink":"/ai/review/2025-11-14-MM-CRITIC_A_Holistic_Evaluation_of_Large_Multimodal_Models_as_Multimodal_Critique/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-Hail_to_the_Thief_Exploring_Attacks_and_Defenses_in_Decentralised_GRPO","title":"[논문리뷰] Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO","excerpt":"이 [arXiv]에 게시한 'Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Decentralized RL","GRPO","LLM Post-training","Adversarial Attacks","Data Poisoning","Defense Mechanisms","In-context Attack","Out-of-context Attack"],"permalink":"/ai/review/2025-11-14-Hail_to_the_Thief_Exploring_Attacks_and_Defenses_in_Decentralised_GRPO/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-Depth_Anything_3_Recovering_the_Visual_Space_from_Any_Views","title":"[논문리뷰] Depth Anything 3: Recovering the Visual Space from Any Views","excerpt":"이 [arXiv]에 게시한 'Depth Anything 3: Recovering the Visual Space from Any Views' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Depth Estimation","Multi-view Geometry","Transformer Architecture","Teacher-Student Learning","Pose Estimation","3D Reconstruction","Novel View Synthesis","Visual Space Recovery"],"permalink":"/ai/review/2025-11-14-Depth_Anything_3_Recovering_the_Visual_Space_from_Any_Views/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-CC30k_A_Citation_Contexts_Dataset_for_Reproducibility-Oriented_Sentiment_Analysis","title":"[논문리뷰] CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis","excerpt":"Jian Wu이 [arXiv]에 게시한 'CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Citation Contexts","Reproducibility","Sentiment Analysis","Large Language Models","Crowdsourcing","Dataset","Machine Learning","Science of Science"],"permalink":"/ai/review/2025-11-14-CC30k_A_Citation_Contexts_Dataset_for_Reproducibility-Oriented_Sentiment_Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-Black-Box_On-Policy_Distillation_of_Large_Language_Models","title":"[논문리뷰] Black-Box On-Policy Distillation of Large Language Models","excerpt":"이 [arXiv]에 게시한 'Black-Box On-Policy Distillation of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Knowledge Distillation (KD)","Black-box Distillation","Generative Adversarial Networks (GANs)","On-policy Learning","Reinforcement Learning","Minimax Game","Model Compression"],"permalink":"/ai/review/2025-11-14-Black-Box_On-Policy_Distillation_of_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-Benchmarking_Diversity_in_Image_Generation_via_Attribute-Conditional_Human_Evaluation","title":"[논문리뷰] Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation","excerpt":"이 [arXiv]에 게시한 'Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Models","Diversity Evaluation","Human Evaluation","Attribute-Conditional","Vendi Score","Generative AI","Benchmarking"],"permalink":"/ai/review/2025-11-14-Benchmarking_Diversity_in_Image_Generation_via_Attribute-Conditional_Human_Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-14-AffordBot_3D_Fine-grained_Embodied_Reasoning_via_Multimodal_Large_Language_Models","title":"[논문리뷰] AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models","excerpt":"Zhen Li이 [arXiv]에 게시한 'AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","lastModifiedAt":"2025-11-14 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Embodied Reasoning","Multimodal Large Language Models (MLLMs)","Chain-of-Thought (CoT)","Affordance Grounding","Motion Estimation","View Synthesis","Active Perception"],"permalink":"/ai/review/2025-11-14-AffordBot_3D_Fine-grained_Embodied_Reasoning_via_Multimodal_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-WebVIA_A_Web-based_Vision-Language_Agentic_Framework_for_Interactive_and_Verifiable_UI-to-Code_Generation","title":"[논문리뷰] WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation","excerpt":"이 [arXiv]에 게시한 'WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","UI-to-Code","Vision-Language Models","Agentic Framework","Interactive UI","Web Automation","Code Generation","UI Verification","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-11-13-WebVIA_A_Web-based_Vision-Language_Agentic_Framework_for_Interactive_and_Verifiable_UI-to-Code_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-WMPO_World_Model-based_Policy_Optimization_for_Vision-Language-Action_Models","title":"[논문리뷰] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Reinforcement Learning (RL)","Model-based RL","World Models","Policy Optimization","Robotics","Sample Efficiency","Self-correction"],"permalink":"/ai/review/2025-11-13-WMPO_World_Model-based_Policy_Optimization_for_Vision-Language-Action_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-Toward_the_Frontiers_of_Reliable_Diffusion_Sampling_via_Adversarial_Sinkhorn_Attention_Guidance","title":"[논문리뷰] Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance","excerpt":"Kwanyoung Kim이 [arXiv]에 게시한 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Guidance Sampling","Optimal Transport","Sinkhorn Algorithm","Self-Attention","Adversarial Perturbation","Image Generation","ControlNet"],"permalink":"/ai/review/2025-11-13-Toward_the_Frontiers_of_Reliable_Diffusion_Sampling_via_Adversarial_Sinkhorn_Attention_Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-TiDAR_Think_in_Diffusion_Talk_in_Autoregression","title":"[논문리뷰] TiDAR: Think in Diffusion, Talk in Autoregression","excerpt":"이 [arXiv]에 게시한 'TiDAR: Think in Diffusion, Talk in Autoregression' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hybrid LLM Architecture","Diffusion-Autoregressive","Parallel Token Generation","Speculative Decoding","Structured Attention Masks","LLM Inference Acceleration","KV Cache"],"permalink":"/ai/review/2025-11-13-TiDAR_Think_in_Diffusion_Talk_in_Autoregression/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-Stemming_Hallucination_in_Language_Models_Using_a_Licensing_Oracle","title":"[논문리뷰] Stemming Hallucination in Language Models Using a Licensing Oracle","excerpt":"Richard Ackermann이 [arXiv]에 게시한 'Stemming Hallucination in Language Models Using a Licensing Oracle' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hallucination Mitigation","Language Models","Knowledge Graphs","SHACL Validation","Epistemic Grounding","Retrieval-Augmented Generation","Neuro-symbolic AI"],"permalink":"/ai/review/2025-11-13-Stemming_Hallucination_in_Language_Models_Using_a_Licensing_Oracle/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-Motif_2_12.7B_technical_report","title":"[논문리뷰] Motif 2 12.7B technical report","excerpt":"이 [arXiv]에 게시한 'Motif 2 12.7B technical report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Model","LLM Efficiency","Grouped Differential Attention","Kernel Fusion","Parallel Muon","Supervised Fine-tuning","Architectural Scaling","Instruction Following"],"permalink":"/ai/review/2025-11-13-Motif_2_12.7B_technical_report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-MathSE_Improving_Multimodal_Mathematical_Reasoning_via_Self-Evolving_Iterative_Reflection_and_Reward-Guided_Fine-Tuning","title":"[논문리뷰] MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning","excerpt":"이 [arXiv]에 게시한 'MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Mathematical Problem Solving","Self-Evolving","Iterative Fine-Tuning","Reward Models","Reflection","Large Language Models (LLMs)"],"permalink":"/ai/review/2025-11-13-MathSE_Improving_Multimodal_Mathematical_Reasoning_via_Self-Evolving_Iterative_Reflection_and_Reward-Guided_Fine-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-MADD_Multi-Agent_Drug_Discovery_Orchestra","title":"[논문리뷰] MADD: Multi-Agent Drug Discovery Orchestra","excerpt":"이 [arXiv]에 게시한 'MADD: Multi-Agent Drug Discovery Orchestra' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Drug Discovery","LLM","Hit Identification","Virtual Screening","Generative AI","Property Prediction","Automated Machine Learning"],"permalink":"/ai/review/2025-11-13-MADD_Multi-Agent_Drug_Discovery_Orchestra/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-Lumine_An_Open_Recipe_for_Building_Generalist_Agents_in_3D_Open_Worlds","title":"[논문리뷰] Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds","excerpt":"이 [arXiv]에 게시한 'Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Generalist Agent","3D Open World","Vision-Language Model","Imitation Learning","Real-time Inference","Hybrid Thinking","Action Chunking","Genshin Impact"],"permalink":"/ai/review/2025-11-13-Lumine_An_Open_Recipe_for_Building_Generalist_Agents_in_3D_Open_Worlds/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-LoopTool_Closing_the_Data-Training_Loop_for_Robust_LLM_Tool_Calls","title":"[논문리뷰] LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls","excerpt":"이 [arXiv]에 게시한 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Tool Learning","Data Generation","Model Training","Closed-Loop Framework","Reinforcement Learning (RL)","Data Refinement","Self-Correction"],"permalink":"/ai/review/2025-11-13-LoopTool_Closing_the_Data-Training_Loop_for_Robust_LLM_Tool_Calls/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-Agentic_Refactoring_An_Empirical_Study_of_AI_Coding_Agents","title":"[논문리뷰] Agentic Refactoring: An Empirical Study of AI Coding Agents","excerpt":"Hajimu Iida이 [arXiv]에 게시한 'Agentic Refactoring: An Empirical Study of AI Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","AI Agents","Code Refactoring","Software Engineering","Empirical Study","Large Language Models","Code Quality","Agentic Software Development","Maintainability"],"permalink":"/ai/review/2025-11-13-Agentic_Refactoring_An_Empirical_Study_of_AI_Coding_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-13-Adapting_Web_Agents_with_Synthetic_Supervision","title":"[논문리뷰] Adapting Web Agents with Synthetic Supervision","excerpt":"Siwei Han이 [arXiv]에 게시한 'Adapting Web Agents with Synthetic Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","lastModifiedAt":"2025-11-13 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Web Agents","Synthetic Data Generation","LLM","Task Refinement","Trajectory Refinement","Supervised Fine-tuning","Web Automation","Environment Adaptation"],"permalink":"/ai/review/2025-11-13-Adapting_Web_Agents_with_Synthetic_Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Wasm_A_Pipeline_for_Constructing_Structured_Arabic_Interleaved_Multimodal_Corpora","title":"[논문리뷰] Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora","excerpt":"Mohamed Motasim Hamed이 [arXiv]에 게시한 'Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Arabic Language","Multimodal Corpus","Data Curation","Web Scraping","Large Language Models","Document Structure","Markdown","Perplexity Filtering"],"permalink":"/ai/review/2025-11-12-Wasm_A_Pipeline_for_Constructing_Structured_Arabic_Interleaved_Multimodal_Corpora/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Walking_the_Tightrope_of_LLMs_for_Software_Development_A_Practitioners_Perspective","title":"[논문리뷰] Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective","excerpt":"Christoph Treude이 [arXiv]에 게시한 'Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Software Engineering","Developer Productivity","Socio-Technical Grounded Theory","Practitioner Insights","AI Adoption","Benefits and Risks","Balanced Use"],"permalink":"/ai/review/2025-11-12-Walking_the_Tightrope_of_LLMs_for_Software_Development_A_Practitioners_Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-VideoSSR_Video_Self-Supervised_Reinforcement_Learning","title":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'VideoSSR: Video Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Understanding","Self-Supervised Learning","Reinforcement Learning","MLLMs","Pretext Tasks","Verifiable Rewards","Data Generation","Temporal Grounding"],"permalink":"/ai/review/2025-11-12-VideoSSR_Video_Self-Supervised_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Tiny_Model_Big_Logic_Diversity-Driven_Optimization_Elicits_Large-Model_Reasoning_Ability_in_VibeThinker-1.5B","title":"[논문리뷰] Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B","excerpt":"이 [arXiv]에 게시한 'Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Small Language Models","Reasoning","Diversity Optimization","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Spectrum-to-Signal Principle (SSP)","Mathematical Reasoning","Code Generation"],"permalink":"/ai/review/2025-11-12-Tiny_Model_Big_Logic_Diversity-Driven_Optimization_Elicits_Large-Model_Reasoning_Ability_in_VibeThinker-1.5B/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-TimeSearch-R_Adaptive_Temporal_Search_for_Long-Form_Video_Understanding_via_Self-Verification_Reinforcement_Learning","title":"[논문리뷰] TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long-form Video Understanding","Temporal Search","Reinforcement Learning","Self-Verification","Video-Language Models","Adaptive Search","Interleaved Reasoning"],"permalink":"/ai/review/2025-11-12-TimeSearch-R_Adaptive_Temporal_Search_for_Long-Form_Video_Understanding_via_Self-Verification_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-The_Path_Not_Taken_RLVR_Provably_Learns_Off_the_Principals","title":"[논문리뷰] The Path Not Taken: RLVR Provably Learns Off the Principals","excerpt":"이 [arXiv]에 게시한 'The Path Not Taken: RLVR Provably Learns Off the Principals' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Parameter-Efficient Fine-Tuning","Optimization Bias","Spectral Geometry","Model Sparsity","LoRA"],"permalink":"/ai/review/2025-11-12-The_Path_Not_Taken_RLVR_Provably_Learns_Off_the_Principals/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Optimizing_Diversity_and_Quality_through_Base-Aligned_Model_Collaboration","title":"[논문리뷰] Optimizing Diversity and Quality through Base-Aligned Model Collaboration","excerpt":"Jonathan May이 [arXiv]에 게시한 'Optimizing Diversity and Quality through Base-Aligned Model Collaboration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Generative AI","Diversity-Quality Trade-off","Model Collaboration","Inference Optimization","Routing Strategy","Text Generation"],"permalink":"/ai/review/2025-11-12-Optimizing_Diversity_and_Quality_through_Base-Aligned_Model_Collaboration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-KLASS_KL-Guided_Fast_Inference_in_Masked_Diffusion_Models","title":"[논문리뷰] KLASS: KL-Guided Fast Inference in Masked Diffusion Models","excerpt":"이 [arXiv]에 게시한 'KLASS: KL-Guided Fast Inference in Masked Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Masked Diffusion Models","Fast Inference","Adaptive Sampling","KL Divergence","Confidence Score","Generative AI","Efficient Sampling"],"permalink":"/ai/review/2025-11-12-KLASS_KL-Guided_Fast_Inference_in_Masked_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Intelligence_per_Watt_Measuring_Intelligence_Efficiency_of_Local_AI","title":"[논문리뷰] Intelligence per Watt: Measuring Intelligence Efficiency of Local AI","excerpt":"이 [arXiv]에 게시한 'Intelligence per Watt: Measuring Intelligence Efficiency of Local AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Local AI","LLM Inference","Intelligence per Watt","Edge Computing","Hybrid Cloud","AI Efficiency","Hardware Benchmarking","Query Routing"],"permalink":"/ai/review/2025-11-12-Intelligence_per_Watt_Measuring_Intelligence_Efficiency_of_Local_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Grounding_Computer_Use_Agents_on_Human_Demonstrations","title":"[논문리뷰] Grounding Computer Use Agents on Human Demonstrations","excerpt":"이 [arXiv]에 게시한 'Grounding Computer Use Agents on Human Demonstrations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Computer Use Agents","UI Grounding","Desktop Applications","Human Demonstrations","Large-Scale Dataset","Vision-Language Models","Supervised Fine-tuning","Reinforcement Learning"],"permalink":"/ai/review/2025-11-12-Grounding_Computer_Use_Agents_on_Human_Demonstrations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-DynaAct_Large_Language_Model_Reasoning_with_Dynamic_Action_Spaces","title":"[논문리뷰] DynaAct: Large Language Model Reasoning with Dynamic Action Spaces","excerpt":"Lingpeng Kong이 [arXiv]에 게시한 'DynaAct: Large Language Model Reasoning with Dynamic Action Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Sequential Reasoning","Action Space Construction","Submodular Optimization","Markov Decision Process","Monte Carlo Tree Search","Utility-Diversity Trade-off"],"permalink":"/ai/review/2025-11-12-DynaAct_Large_Language_Model_Reasoning_with_Dynamic_Action_Spaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-BiCA_Effective_Biomedical_Dense_Retrieval_with_Citation-Aware_Hard_Negatives","title":"[논문리뷰] BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives","excerpt":"이 [arXiv]에 게시한 'BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Dense Retrieval","Biomedical IR","Hard Negative Mining","Citation Networks","PubMed","Zero-shot Retrieval","Transformer Models"],"permalink":"/ai/review/2025-11-12-BiCA_Effective_Biomedical_Dense_Retrieval_with_Citation-Aware_Hard_Negatives/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Beyond_Fact_Retrieval_Episodic_Memory_for_RAG_with_Generative_Semantic_Workspaces","title":"[논문리뷰] Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces","excerpt":"Vwani Roychowdhury이 [arXiv]에 게시한 'Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation (RAG)","Episodic Memory","Generative Semantic Workspaces (GSW)","Large Language Models (LLMs)","Question Answering (QA)","Semantic Modeling","Knowledge Graph"],"permalink":"/ai/review/2025-11-12-Beyond_Fact_Retrieval_Episodic_Memory_for_RAG_with_Generative_Semantic_Workspaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Beyond_English_Toward_Inclusive_and_Scalable_Multilingual_Machine_Translation_with_LLMs","title":"[논문리뷰] Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs","excerpt":"이 [arXiv]에 게시한 'Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multilingual Machine Translation","Large Language Models","Directional Degeneration","Strategic Downsampling","Parallel Multilingual Prompting","Chinese-centric MT","Cross-lingual Transfer","Instruction Tuning"],"permalink":"/ai/review/2025-11-12-Beyond_English_Toward_Inclusive_and_Scalable_Multilingual_Machine_Translation_with_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-12-Adaptive_Multi-Agent_Response_Refinement_in_Conversational_Systems","title":"[논문리뷰] Adaptive Multi-Agent Response Refinement in Conversational Systems","excerpt":"이 [arXiv]에 게시한 'Adaptive Multi-Agent Response Refinement in Conversational Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","lastModifiedAt":"2025-11-12 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Large Language Models","Multi-Agent Systems","Conversational AI","Response Refinement","Dynamic Agent Selection","Persona Alignment","Factual Grounding","Coherence"],"permalink":"/ai/review/2025-11-12-Adaptive_Multi-Agent_Response_Refinement_in_Conversational_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-VADER_Towards_Causal_Video_Anomaly_Understanding_with_Relation-Aware_Large_Language_Models","title":"[논문리뷰] VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models","excerpt":"이 [arXiv]에 게시한 'VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Video Anomaly Understanding","Large Language Models","Causal Reasoning","Relation-Aware","Keyframe Sampling","Multimodal LLMs","Scene Graphs"],"permalink":"/ai/review/2025-11-11-VADER_Towards_Causal_Video_Anomaly_Understanding_with_Relation-Aware_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-The_Station_An_Open-World_Environment_for_AI-Driven_Discovery","title":"[논문리뷰] The Station: An Open-World Environment for AI-Driven Discovery","excerpt":"wydu이 [arXiv]에 게시한 'The Station: An Open-World Environment for AI-Driven Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Open-World Environment","Scientific Discovery","AI-Driven Research","Large Language Models","Emergent Behavior","State-of-the-Art (SOTA)"],"permalink":"/ai/review/2025-11-11-The_Station_An_Open-World_Environment_for_AI-Driven_Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Teaching_Pretrained_Language_Models_to_Think_Deeper_with_Retrofitted_Recurrence","title":"[논문리뷰] Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence","excerpt":"이 [arXiv]에 게시한 'Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Recurrent Language Models","Pretrained Models","Model Surgery","Curriculum Learning","Test-Time Compute Scaling","Mathematics Reasoning","Efficient Training","Depth Recurrence"],"permalink":"/ai/review/2025-11-11-Teaching_Pretrained_Language_Models_to_Think_Deeper_with_Retrofitted_Recurrence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-SofT-GRPO_Surpassing_Discrete-Token_LLM_Reinforcement_Learning_via_Gumbel-Reparameterized_Soft-Thinking_Policy_Optimization","title":"[논문리뷰] SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization","excerpt":"이 [arXiv]에 게시한 'SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","Reinforcement Learning","Soft-Thinking","Gumbel Reparameterization","Policy Optimization","Chain-of-Thought (CoT)","GRPO"],"permalink":"/ai/review/2025-11-11-SofT-GRPO_Surpassing_Discrete-Token_LLM_Reinforcement_Learning_via_Gumbel-Reparameterized_Soft-Thinking_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-SWE-fficiency_Can_Language_Models_Optimize_Real-World_Repositories_on_Real_Workloads","title":"[논문리뷰] SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?","excerpt":"Ofir Press이 [arXiv]에 게시한 'SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","소프트웨어 성능 최적화","언어 모델","저장소 수준 추론","벤치마크","실제 워크로드","코드 정확성","속도 향상","코드 최적화"],"permalink":"/ai/review/2025-11-11-SWE-fficiency_Can_Language_Models_Optimize_Real-World_Repositories_on_Real_Workloads/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Routing_Manifold_Alignment_Improves_Generalization_of_Mixture-of-Experts_LLMs","title":"[논문리뷰] Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs","excerpt":"Ziyue Li이 [arXiv]에 게시한 'Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Large Language Models (LLMs)","Router Optimization","Manifold Regularization","Generalization","Post-training Fine-tuning","Task Embedding Alignment"],"permalink":"/ai/review/2025-11-11-Routing_Manifold_Alignment_Improves_Generalization_of_Mixture-of-Experts_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Robot_Learning_from_a_Physical_World_Model","title":"[논문리뷰] Robot Learning from a Physical World Model","excerpt":"이 [arXiv]에 게시한 'Robot Learning from a Physical World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Robot Learning","Video Generation","Physical World Model","Reinforcement Learning","Zero-shot Manipulation","Object-Centric Learning","Sim-to-Real"],"permalink":"/ai/review/2025-11-11-Robot_Learning_from_a_Physical_World_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Reinforcement_Learning_Improves_Traversal_of_Hierarchical_Knowledge_in_LLMs","title":"[논문리뷰] Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs","excerpt":"이 [arXiv]에 게시한 'Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Hierarchical Knowledge","Knowledge Traversal","Structured Prompting","Internal Representations","Alignment Tax"],"permalink":"/ai/review/2025-11-11-Reinforcement_Learning_Improves_Traversal_of_Hierarchical_Knowledge_in_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-RedOne_2.0_Rethinking_Domain-specific_LLM_Post-Training_in_Social_Networking_Services","title":"[논문리뷰] RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services","excerpt":"Zijie Meng이 [arXiv]에 게시한 'RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Post-Training","Domain Adaptation","Social Networking Services","Reinforcement Learning","Supervised Fine-Tuning","Catastrophic Forgetting","Data Efficiency"],"permalink":"/ai/review/2025-11-11-RedOne_2.0_Rethinking_Domain-specific_LLM_Post-Training_in_Social_Networking_Services/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Reasoning_with_Confidence_Efficient_Verification_of_LLM_Reasoning_Steps_via_Uncertainty_Heads","title":"[논문리뷰] Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads","excerpt":"Jiaheng Zhang이 [arXiv]에 게시한 'Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Reasoning Verification","Uncertainty Quantification (UQ)","UHeads","Process Reward Models (PRMs)","Chain-of-Thought (CoT)","Self-Supervised Learning","Computational Efficiency","Domain Generalization"],"permalink":"/ai/review/2025-11-11-Reasoning_with_Confidence_Efficient_Verification_of_LLM_Reasoning_Steps_via_Uncertainty_Heads/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-RLoop_An_Self-Improving_Framework_for_Reinforcement_Learning_with_Iterative_Policy_Initialization","title":"[논문리뷰] RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization","excerpt":"Wenhao Huang이 [arXiv]에 게시한 'RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Generalization","Overfitting","Catastrophic Forgetting","Iterative Policy Optimization","Policy Diversity"],"permalink":"/ai/review/2025-11-11-RLoop_An_Self-Improving_Framework_for_Reinforcement_Learning_with_Iterative_Policy_Initialization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-RLVE_Scaling_Up_Reinforcement_Learning_for_Language_Models_with_Adaptive_Verifiable_Environments","title":"[논문리뷰] RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments","excerpt":"Shuyue Stella Li이 [arXiv]에 게시한 'RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Language Models","Adaptive Environments","Verifiable Environments","Procedural Generation","Curriculum Learning","Generalization"],"permalink":"/ai/review/2025-11-11-RLVE_Scaling_Up_Reinforcement_Learning_for_Language_Models_with_Adaptive_Verifiable_Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Omni-AVSR_Towards_Unified_Multimodal_Speech_Recognition_with_Large_Language_Models","title":"[논문리뷰] Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models","excerpt":"이 [arXiv]에 게시한 'Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Speech Recognition","Large Language Models","Audio-Visual Speech Recognition","LoRA","Matryoshka Representation Learning","Elastic Inference","Parameter-Efficient Adaptation"],"permalink":"/ai/review/2025-11-11-Omni-AVSR_Towards_Unified_Multimodal_Speech_Recognition_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-NURBGen_High-Fidelity_Text-to-CAD_Generation_through_LLM-Driven_NURBS_Modeling","title":"[논문리뷰] NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling","excerpt":"이 [arXiv]에 게시한 'NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-CAD","NURBS Modeling","Large Language Models","Geometric Deep Learning","Boundary Representation","Hybrid Representation","CAD Generation"],"permalink":"/ai/review/2025-11-11-NURBGen_High-Fidelity_Text-to-CAD_Generation_through_LLM-Driven_NURBS_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-MVU-Eval_Towards_Multi-Video_Understanding_Evaluation_for_Multimodal_LLMs","title":"[논문리뷰] MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs","excerpt":"이 [arXiv]에 게시한 'MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Multi-Video Understanding","Evaluation Benchmark","Video Perception","Video Reasoning","Sports Analytics","Autonomous Driving"],"permalink":"/ai/review/2025-11-11-MVU-Eval_Towards_Multi-Video_Understanding_Evaluation_for_Multimodal_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-MPJudge_Towards_Perceptual_Assessment_of_Music-Induced_Paintings","title":"[논문리뷰] MPJudge: Towards Perceptual Assessment of Music-Induced Paintings","excerpt":"이 [arXiv]에 게시한 'MPJudge: Towards Perceptual Assessment of Music-Induced Paintings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Music-Painting Cross-Modal","Perceptual Assessment","Modality-Adaptive Normalization","Direct Preference Optimization","Cross-Modal Fusion","Dataset Annotation","Affective Computing"],"permalink":"/ai/review/2025-11-11-MPJudge_Towards_Perceptual_Assessment_of_Music-Induced_Paintings/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Long_Grounded_Thoughts_Distilling_Compositional_Visual_Reasoning_Chains_at_Scale","title":"[논문리뷰] Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale","excerpt":"이 [arXiv]에 게시한 'Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Visual Reasoning","Compositional AI","Vision-Language Models","Data Synthesis","Chain-of-Thought","Reinforcement Learning","Multimodal Transfer","Grounded Reasoning"],"permalink":"/ai/review/2025-11-11-Long_Grounded_Thoughts_Distilling_Compositional_Visual_Reasoning_Chains_at_Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Llama-Embed-Nemotron-8B_A_Universal_Text_Embedding_Model_for_Multilingual_and_Cross-Lingual_Tasks","title":"[논문리뷰] Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks","excerpt":"이 [arXiv]에 게시한 'Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text Embedding","Multilingual","Cross-Lingual","Contrastive Learning","Model Merging","Synthetic Data Generation","Instruction-Tuning","LLM"],"permalink":"/ai/review/2025-11-11-Llama-Embed-Nemotron-8B_A_Universal_Text_Embedding_Model_for_Multilingual_and_Cross-Lingual_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-LUT-LLM_Efficient_Large_Language_Model_Inference_with_Memory-based_Computations_on_FPGAs","title":"[논문리뷰] LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs","excerpt":"Jason Cong이 [arXiv]에 게시한 'LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","FPGA","Large Language Models (LLM)","Inference Optimization","Memory-based Computation","Vector Quantization","Table Lookup","Hardware Acceleration"],"permalink":"/ai/review/2025-11-11-LUT-LLM_Efficient_Large_Language_Model_Inference_with_Memory-based_Computations_on_FPGAs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-IterResearch_Rethinking_Long-Horizon_Agents_via_Markovian_State_Reconstruction","title":"[논문리뷰] IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction","excerpt":"Haotian Xu이 [arXiv]에 게시한 'IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Long-Horizon Agents","Markov Decision Process","Workspace Reconstruction","Reinforcement Learning","Context Management","Iterative Deep Research","LLM Agents","Efficiency-Aware Policy Optimization"],"permalink":"/ai/review/2025-11-11-IterResearch_Rethinking_Long-Horizon_Agents_via_Markovian_State_Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-HaluMem_Evaluating_Hallucinations_in_Memory_Systems_of_Agents","title":"[논문리뷰] HaluMem: Evaluating Hallucinations in Memory Systems of Agents","excerpt":"이 [arXiv]에 게시한 'HaluMem: Evaluating Hallucinations in Memory Systems of Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Memory Systems","AI Agents","Hallucination Detection","Evaluation Benchmark","Long-term Memory","Memory Extraction","Memory Updating","Question Answering"],"permalink":"/ai/review/2025-11-11-HaluMem_Evaluating_Hallucinations_in_Memory_Systems_of_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Generating_an_Image_From_1000_Words_Enhancing_Text-to-Image_With_Structured_Captions","title":"[논문리뷰] Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions","excerpt":"이 [arXiv]에 게시한 'Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Structured Captions","LLM Fusion","Controllability","Image Generation Evaluation","Diffusion Models","DimFusion","TaBR"],"permalink":"/ai/review/2025-11-11-Generating_an_Image_From_1000_Words_Enhancing_Text-to-Image_With_Structured_Captions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-FLEX_Continuous_Agent_Evolution_via_Forward_Learning_from_Experience","title":"[논문리뷰] FLEX: Continuous Agent Evolution via Forward Learning from Experience","excerpt":"Jiangjie Chen이 [arXiv]에 게시한 'FLEX: Continuous Agent Evolution via Forward Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Agents","Continuous Learning","Experience Library","Forward Learning","Meta-MDP","Knowledge Distillation","Non-parametric Adaptation"],"permalink":"/ai/review/2025-11-11-FLEX_Continuous_Agent_Evolution_via_Forward_Learning_from_Experience/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Do_LLMs_Feel_Teaching_Emotion_Recognition_with_Prompts_Retrieval_and_Curriculum_Learning","title":"[논문리뷰] Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning","excerpt":"이 [arXiv]에 게시한 'Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Emotion Recognition in Conversation","Large Language Models","Prompt Engineering","Demonstration Retrieval","Curriculum Learning","Fine-tuning","Affective Computing","SOTA"],"permalink":"/ai/review/2025-11-11-Do_LLMs_Feel_Teaching_Emotion_Recognition_with_Prompts_Retrieval_and_Curriculum_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-DigiData_Training_and_Evaluating_General-Purpose_Mobile_Control_Agents","title":"[논문리뷰] DigiData: Training and Evaluating General-Purpose Mobile Control Agents","excerpt":"이 [arXiv]에 게시한 'DigiData: Training and Evaluating General-Purpose Mobile Control Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Mobile Control Agents","User Interface Automation","Large-Scale Dataset","Benchmarking","LLM Judges","Data Diversity","Task Success Rate"],"permalink":"/ai/review/2025-11-11-DigiData_Training_and_Evaluating_General-Purpose_Mobile_Control_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Diffusion-SDPO_Safeguarded_Direct_Preference_Optimization_for_Diffusion_Models","title":"[논문리뷰] Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models","excerpt":"Zhao Xu이 [arXiv]에 게시한 'Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Diffusion Models","Direct Preference Optimization (DPO)","Safeguarded Learning","Text-to-Image Generation","Preference Alignment","Generative Models","Stable Diffusion"],"permalink":"/ai/review/2025-11-11-Diffusion-SDPO_Safeguarded_Direct_Preference_Optimization_for_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-DRIVE_Data_Curation_Best_Practices_for_Reinforcement_Learning_with_Verifiable_Reward_in_Competitive_Code_Generation","title":"[논문리뷰] DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation","excerpt":"이 [arXiv]에 게시한 'DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Reward","Competitive Programming","Code Generation","Data Curation","Curriculum Learning","Supervised Fine-tuning","Entropy Expansion"],"permalink":"/ai/review/2025-11-11-DRIVE_Data_Curation_Best_Practices_for_Reinforcement_Learning_with_Verifiable_Reward_in_Competitive_Code_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects","title":"[논문리뷰] DIMO: Diverse 3D Motion Generation for Arbitrary Objects","excerpt":"Kostas Daniilidis이 [arXiv]에 게시한 'DIMO: Diverse 3D Motion Generation for Arbitrary Objects' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Motion Generation","Generative Models","Arbitrary Objects","Neural Key Points","Latent Space","4D Content Generation","Diffusion Models","3D Gaussian Splatting"],"permalink":"/ai/review/2025-11-11-DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-Ariadne_A_Controllable_Framework_for_Probing_and_Extending_VLM_Reasoning_Boundaries","title":"[논문리뷰] Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries","excerpt":"Zhengzhong Tu이 [arXiv]에 게시한 'Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Spatial Reasoning","Controllable Framework","RLVR","GRPO","Maze Navigation","Generalization Boundaries"],"permalink":"/ai/review/2025-11-11-Ariadne_A_Controllable_Framework_for_Probing_and_Extending_VLM_Reasoning_Boundaries/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-11-10_Open_Challenges_Steering_the_Future_of_Vision-Language-Action_Models","title":"[논문리뷰] 10 Open Challenges Steering the Future of Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 '10 Open Challenges Steering the Future of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","lastModifiedAt":"2025-11-11 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Embodied AI","Robotics","Multimodal Perception","Cross-Robot Generalization","Hierarchical Planning","World Models","Robot Safety"],"permalink":"/ai/review/2025-11-11-10_Open_Challenges_Steering_the_Future_of_Vision-Language-Action_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-Visual_Spatial_Tuning","title":"[논문리뷰] Visual Spatial Tuning","excerpt":"이 [arXiv]에 게시한 'Visual Spatial Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Spatial Reasoning","Spatial Perception","Dataset Creation","Reinforcement Learning","Visuospatial AI","Robotics"],"permalink":"/ai/review/2025-11-10-Visual_Spatial_Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-VeriCoT_Neuro-symbolic_Chain-of-Thought_Validation_via_Logical_Consistency_Checks","title":"[논문리뷰] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks","excerpt":"이 [arXiv]에 게시한 'VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Neuro-symbolic AI","Chain-of-Thought","Large Language Models","Logical Consistency","Automated Verification","Fine-tuning","SMT Solvers","Self-Reflection"],"permalink":"/ai/review/2025-11-10-VeriCoT_Neuro-symbolic_Chain-of-Thought_Validation_via_Logical_Consistency_Checks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-Towards_Mitigating_Hallucinations_in_Large_Vision-Language_Models_by_Refining_Textual_Embeddings","title":"[논문리뷰] Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings","excerpt":"Jiaxin Yuan이 [arXiv]에 게시한 'Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Hallucination Mitigation","Large Vision-Language Models","Textual Embeddings","Multimodal Reasoning","Attention Mechanism","Visual Grounding","Modality Imbalance"],"permalink":"/ai/review/2025-11-10-Towards_Mitigating_Hallucinations_in_Large_Vision-Language_Models_by_Refining_Textual_Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-Too_Good_to_be_Bad_On_the_Failure_of_LLMs_to_Role-Play_Villains","title":"[논문리뷰] Too Good to be Bad: On the Failure of LLMs to Role-Play Villains","excerpt":"이 [arXiv]에 게시한 'Too Good to be Bad: On the Failure of LLMs to Role-Play Villains' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM","Role-playing","Safety Alignment","Villain","Persona Simulation","Moral Alignment","Benchmark","Character Fidelity"],"permalink":"/ai/review/2025-11-10-Too_Good_to_be_Bad_On_the_Failure_of_LLMs_to_Role-Play_Villains/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-Real-Time_Reasoning_Agents_in_Evolving_Environments","title":"[논문리뷰] Real-Time Reasoning Agents in Evolving Environments","excerpt":"이 [arXiv]에 게시한 'Real-Time Reasoning Agents in Evolving Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Real-time Reasoning","LLM Agents","Dynamic Environments","Dual-System AI","AgileThinker","Reactive Planning","Cognitive Load","Time Pressure"],"permalink":"/ai/review/2025-11-10-Real-Time_Reasoning_Agents_in_Evolving_Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-Jailbreaking_in_the_Haystack","title":"[논문리뷰] Jailbreaking in the Haystack","excerpt":"Alexander Robey이 [arXiv]에 게시한 'Jailbreaking in the Haystack' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Jailbreaking","LLM Safety","Long-Context Models","Positional Bias","Attack Success Rate (ASR)","Prompt Engineering","Compute Efficiency","AI Agents"],"permalink":"/ai/review/2025-11-10-Jailbreaking_in_the_Haystack/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-HAFixAgent_History-Aware_Automated_Program_Repair_Agent","title":"[논문리뷰] HAFixAgent: History-Aware Automated Program Repair Agent","excerpt":"Ahmed E. Hassan이 [arXiv]에 게시한 'HAFixAgent: History-Aware Automated Program Repair Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Automated Program Repair","AI Agent","Large Language Models","Repository Mining","Historical Context","Bug Fixing","Defects4J"],"permalink":"/ai/review/2025-11-10-HAFixAgent_History-Aware_Automated_Program_Repair_Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-Dense_Motion_Captioning","title":"[논문리뷰] Dense Motion Captioning","excerpt":"Paolo Rota이 [arXiv]에 게시한 'Dense Motion Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","3D Human Motion","Dense Captioning","Large Language Models","Motion Understanding","Temporal Localization","Human-Language Datasets","Motion Generation"],"permalink":"/ai/review/2025-11-10-Dense_Motion_Captioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-DeepEyesV2_Toward_Agentic_Multimodal_Model","title":"[논문리뷰] DeepEyesV2: Toward Agentic Multimodal Model","excerpt":"Guohai Xu이 [arXiv]에 게시한 'DeepEyesV2: Toward Agentic Multimodal Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","Agentic AI","Multimodal Models","Tool Use","Reinforcement Learning","Supervised Fine-tuning","Multimodal Reasoning","Web Search","Code Execution"],"permalink":"/ai/review/2025-11-10-DeepEyesV2_Toward_Agentic_Multimodal_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-10-CritiCal_Can_Critique_Help_LLM_Uncertainty_or_Confidence_Calibration","title":"[논문리뷰] CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?","excerpt":"Baixuan Xu이 [arXiv]에 게시한 'CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","lastModifiedAt":"2025-11-10 00:00:00+0900+0900","categories":["Review"],"tags":["Review","LLM Calibration","Confidence Calibration","Uncertainty Estimation","Critique Learning","Supervised Fine-Tuning","Natural Language Processing","Self-Critique"],"permalink":"/ai/review/2025-11-10-CritiCal_Can_Critique_Help_LLM_Uncertainty_or_Confidence_Calibration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-V-Thinker_Interactive_Thinking_with_Images","title":"[논문리뷰] V-Thinker: Interactive Thinking with Images","excerpt":"Peiqing Yang이 [arXiv]에 게시한 'V-Thinker: Interactive Thinking with Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Large Multimodal Models","Interactive Reasoning","Vision-Centric Thinking","Reinforcement Learning","Data Synthesis","Visual Tools","Curriculum Learning","Multimodal AI"],"permalink":"/ai/review/2025-11-7-V-Thinker_Interactive_Thinking_with_Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-Thinking_with_Video_Video_Generation_as_a_Promising_Multimodal_Reasoning_Paradigm","title":"[논문리뷰] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm","excerpt":"이 [arXiv]에 게시한 'Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Video Generation","Multimodal Reasoning","Temporal Understanding","Spatial Reasoning","Foundation Models","AI Benchmarking","In-Context Learning","Self-Consistency"],"permalink":"/ai/review/2025-11-7-Thinking_with_Video_Video_Generation_as_a_Promising_Multimodal_Reasoning_Paradigm/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-The_Strong_Lottery_Ticket_Hypothesis_for_Multi-Head_Attention_Mechanisms","title":"[논문리뷰] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms","excerpt":"Susumu Takeuchi이 [arXiv]에 게시한 'The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Strong Lottery Ticket Hypothesis","Multi-Head Attention","Transformers","Neural Network Pruning","Overparameterization","Weight Initialization","Model Compression"],"permalink":"/ai/review/2025-11-7-The_Strong_Lottery_Ticket_Hypothesis_for_Multi-Head_Attention_Mechanisms/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-Scaling_Agent_Learning_via_Experience_Synthesis","title":"[논문리뷰] Scaling Agent Learning via Experience Synthesis","excerpt":"이 [arXiv]에 게시한 'Scaling Agent Learning via Experience Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Agents","Experience Synthesis","World Models","Curriculum Learning","Sim-to-Real Transfer","Web Agents"],"permalink":"/ai/review/2025-11-7-Scaling_Agent_Learning_via_Experience_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-SIMS-V_Simulated_Instruction-Tuning_for_Spatial_Video_Understanding","title":"[논문리뷰] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding","excerpt":"이 [arXiv]에 게시한 'SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Spatial Reasoning","Video Understanding","Simulated Data","Instruction Tuning","Multimodal LLMs","Sim-to-Real Transfer","AI2-THOR"],"permalink":"/ai/review/2025-11-7-SIMS-V_Simulated_Instruction-Tuning_for_Spatial_Video_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-SAIL-RL_Guiding_MLLMs_in_When_and_How_to_Think_via_Dual-Reward_RL_Tuning","title":"[논문리뷰] SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning","excerpt":"이 [arXiv]에 게시한 'SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Reinforcement Learning","Post-training","Reasoning","Dual-Reward System","Thinking Reward","Judging Reward","Hallucination Reduction"],"permalink":"/ai/review/2025-11-7-SAIL-RL_Guiding_MLLMs_in_When_and_How_to_Think_via_Dual-Reward_RL_Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-RDMA_Point-to-Point_Communication_for_LLM_Systems","title":"[논문리뷰] RDMA Point-to-Point Communication for LLM Systems","excerpt":"이 [arXiv]에 게시한 'RDMA Point-to-Point Communication for LLM Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","RDMA","LLM","Point-to-Point Communication","Disaggregated Inference","MoE Routing","KvCache","AWS EFA","NVIDIA ConnectX"],"permalink":"/ai/review/2025-11-7-RDMA_Point-to-Point_Communication_for_LLM_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-NVIDIA_Nemotron_Nano_V2_VL","title":"[논문리뷰] NVIDIA Nemotron Nano V2 VL","excerpt":"이 [arXiv]에 게시한 'NVIDIA Nemotron Nano V2 VL' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Hybrid Architecture","Mamba-Transformer","Long-Context Understanding","Quantization","Efficient Inference","Document AI","Video AI"],"permalink":"/ai/review/2025-11-7-NVIDIA_Nemotron_Nano_V2_VL/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-Learning_Vision-Driven_Reactive_Soccer_Skills_for_Humanoid_Robots","title":"[논문리뷰] Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots","excerpt":"이 [arXiv]에 게시한 'Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Humanoid Robot","Reinforcement Learning","RoboCup","Soccer Skills","Vision-Driven Control","Adversarial Motion Priors","Sim-to-Real","Perception-Action Coordination"],"permalink":"/ai/review/2025-11-7-Learning_Vision-Driven_Reactive_Soccer_Skills_for_Humanoid_Robots/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-How_to_Evaluate_Speech_Translation_with_Source-Aware_Neural_MT_Metrics","title":"[논문리뷰] How to Evaluate Speech Translation with Source-Aware Neural MT Metrics","excerpt":"Luisa Bentivogli이 [arXiv]에 게시한 'How to Evaluate Speech Translation with Source-Aware Neural MT Metrics' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Speech Translation","Neural MT Metrics","Source-Aware Evaluation","Automatic Speech Recognition (ASR)","Back-Translation (BT)","Cross-lingual Re-segmentation","COMET","MetricX"],"permalink":"/ai/review/2025-11-7-How_to_Evaluate_Speech_Translation_with_Source-Aware_Neural_MT_Metrics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-GUI-360_A_Comprehensive_Dataset_and_Benchmark_for_Computer-Using_Agents","title":"[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents","excerpt":"이 [arXiv]에 게시한 'GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Computer-Using Agents","GUI Grounding","Screen Parsing","Action Prediction","Desktop Automation","Dataset","Benchmark","Multimodal Learning","LLM-augmented Data"],"permalink":"/ai/review/2025-11-7-GUI-360_A_Comprehensive_Dataset_and_Benchmark_for_Computer-Using_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-EVTAR_End-to-End_Try_on_with_Additional_Unpaired_Visual_Reference","title":"[논문리뷰] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference","excerpt":"이 [arXiv]에 게시한 'EVTAR: End-to-End Try on with Additional Unpaired Visual Reference' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Virtual Try-on","Diffusion Models","End-to-End Learning","Reference Images","Unpaired Data","Flow Matching","Transformer Architecture","Generative AI"],"permalink":"/ai/review/2025-11-7-EVTAR_End-to-End_Try_on_with_Additional_Unpaired_Visual_Reference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-Contamination_Detection_for_VLMs_using_Multi-Modal_Semantic_Perturbation","title":"[논문리뷰] Contamination Detection for VLMs using Multi-Modal Semantic Perturbation","excerpt":"이 [arXiv]에 게시한 'Contamination Detection for VLMs using Multi-Modal Semantic Perturbation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","VLM Contamination","Test-set Leakage","Multi-modal Perturbation","Generative Models","Generalization","Model Memorization","VLMs"],"permalink":"/ai/review/2025-11-7-Contamination_Detection_for_VLMs_using_Multi-Modal_Semantic_Perturbation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-Cambrian-S_Towards_Spatial_Supersensing_in_Video","title":"[논문리뷰] Cambrian-S: Towards Spatial Supersensing in Video","excerpt":"Zihao Yang이 [arXiv]에 게시한 'Cambrian-S: Towards Spatial Supersensing in Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Spatial Supersensing","Video Understanding","Multimodal LLMs","Predictive Sensing","Memory Management","Event Segmentation","VSI-SUPER","Instruction Tuning"],"permalink":"/ai/review/2025-11-7-Cambrian-S_Towards_Spatial_Supersensing_in_Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-7-Benchmark_Designers_Should_Train_on_the_Test_Set_to_Expose_Exploitable_Non-Visual_Shortcuts","title":"[논문리뷰] Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts","excerpt":"이 [arXiv]에 게시한 'Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","lastModifiedAt":"2025-11-09 22:08:24+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Benchmark Design","Non-Visual Shortcuts","Test-Set Stress-Test","Bias Mitigation","Model Evaluation","Benchmark Robustness"],"permalink":"/ai/review/2025-11-7-Benchmark_Designers_Should_Train_on_the_Test_Set_to_Expose_Exploitable_Non-Visual_Shortcuts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-UniAVGen_Unified_Audio_and_Video_Generation_with_Asymmetric_Cross-Modal_Interactions","title":"[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions","excerpt":"이 [arXiv]에 게시한 'UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Joint Audio-Video Generation","Cross-Modal Interaction","Diffusion Transformer","Face-Aware Modulation","Classifier-Free Guidance","Multimodal AI","Generative Models"],"permalink":"/ai/review/2025-11-6-UniAVGen_Unified_Audio_and_Video_Generation_with_Asymmetric_Cross-Modal_Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-The_Sequential_Edge_Inverse-Entropy_Voting_Beats_Parallel_Self-Consistency_at_Matched_Compute","title":"[논문리뷰] The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute","excerpt":"이 [arXiv]에 게시한 'The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Sequential Reasoning","Parallel Self-Consistency","Inverse-Entropy Voting","LLM Reasoning","Test-Time Scaling","Inference Optimization","Iterative Refinement","Error Correction"],"permalink":"/ai/review/2025-11-6-The_Sequential_Edge_Inverse-Entropy_Voting_Beats_Parallel_Self-Consistency_at_Matched_Compute/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-TabTune_A_Unified_Library_for_Inference_and_Fine-Tuning_Tabular_Foundation_Models","title":"[논문리뷰] TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models","excerpt":"이 [arXiv]에 게시한 'TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Tabular Foundation Models","Fine-Tuning","PEFT","Meta-Learning","Calibration","Fairness","Unified Library","Benchmarking"],"permalink":"/ai/review/2025-11-6-TabTune_A_Unified_Library_for_Inference_and_Fine-Tuning_Tabular_Foundation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-Orion-MSP_Multi-Scale_Sparse_Attention_for_Tabular_In-Context_Learning","title":"[논문리뷰] Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning","excerpt":"이 [arXiv]에 게시한 'Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Tabular Data","In-Context Learning","Multi-Scale Attention","Sparse Attention","Foundation Models","Perceiver Architecture"],"permalink":"/ai/review/2025-11-6-Orion-MSP_Multi-Scale_Sparse_Attention_for_Tabular_In-Context_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-MME-CC_A_Challenging_Multi-Modal_Evaluation_Benchmark_of_Cognitive_Capacity","title":"[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity","excerpt":"이 [arXiv]에 게시한 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Benchmark","Cognitive Capacity","Visual Reasoning","MLLM Evaluation","Error Analysis","Chain-of-Thought"],"permalink":"/ai/review/2025-11-6-MME-CC_A_Challenging_Multi-Modal_Evaluation_Benchmark_of_Cognitive_Capacity/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-LiveTradeBench_Seeking_Real-World_Alpha_with_Large_Language_Models","title":"[논문리뷰] LiveTradeBench: Seeking Real-World Alpha with Large Language Models","excerpt":"Jiaxuan You이 [arXiv]에 게시한 'LiveTradeBench: Seeking Real-World Alpha with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Live Trading","Portfolio Management","Financial AI","Prediction Markets","Real-World Uncertainty","Agent Benchmarking"],"permalink":"/ai/review/2025-11-6-LiveTradeBench_Seeking_Real-World_Alpha_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-Let_Multimodal_Embedders_Learn_When_to_Augment_Query_via_Adaptive_Query_Augmentation","title":"[논문리뷰] Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation","excerpt":"Jaehyun Park이 [arXiv]에 게시한 'Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Multimodal Embedders","Query Augmentation","Adaptive Learning","Multimodal LLM","Information Retrieval","Generative AI","Embedding Latency"],"permalink":"/ai/review/2025-11-6-Let_Multimodal_Embedders_Learn_When_to_Augment_Query_via_Adaptive_Query_Augmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-LEGO-Eval_Towards_Fine-Grained_Evaluation_on_Synthesizing_3D_Embodied_Environments_with_Tool_Augmentation","title":"[논문리뷰] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation","excerpt":"Soohyun Oh이 [arXiv]에 게시한 'LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","3D Scene Synthesis","Fine-Grained Evaluation","Tool-Augmented LLMs","Embodied AI","Vision-Language Models","Benchmark","Multi-Hop Grounding"],"permalink":"/ai/review/2025-11-6-LEGO-Eval_Towards_Fine-Grained_Evaluation_on_Synthesizing_3D_Embodied_Environments_with_Tool_Augmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-Kinematify_Open-Vocabulary_Synthesis_of_High-DoF_Articulated_Objects","title":"[논문리뷰] Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects","excerpt":"이 [arXiv]에 게시한 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Articulated Objects","Kinematics Inference","High-DoF","Monte Carlo Tree Search","Joint Parameter Optimization","SDF","Open-Vocabulary Synthesis","Robot Self-Modeling"],"permalink":"/ai/review/2025-11-6-Kinematify_Open-Vocabulary_Synthesis_of_High-DoF_Articulated_Objects/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-Jr._AI_Scientist_and_Its_Risk_Report_Autonomous_Scientific_Exploration_from_a_Baseline_Paper","title":"[논문리뷰] Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper","excerpt":"이 [arXiv]에 게시한 'Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","AI Scientist","Autonomous Research","Scientific Automation","LLM for Research","Code Generation","Experimental Design","Risk Assessment"],"permalink":"/ai/review/2025-11-6-Jr._AI_Scientist_and_Its_Risk_Report_Autonomous_Scientific_Exploration_from_a_Baseline_Paper/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-Grounded_Misunderstandings_in_Asymmetric_Dialogue_A_Perspectivist_Annotation_Scheme_for_MapTask","title":"[논문리뷰] Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask","excerpt":"이 [arXiv]에 게시한 'Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Dialogue Systems","Common Ground","Misunderstanding","Annotation Scheme","MapTask Corpus","Large Language Models","Perspective Taking","Reference Resolution"],"permalink":"/ai/review/2025-11-6-Grounded_Misunderstandings_in_Asymmetric_Dialogue_A_Perspectivist_Annotation_Scheme_for_MapTask/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-Diffusion_Language_Models_are_Super_Data_Learners","title":"[논문리뷰] Diffusion Language Models are Super Data Learners","excerpt":"이 [arXiv]에 게시한 'Diffusion Language Models are Super Data Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Autoregressive Models","Data Efficiency","Scaling Laws","Data-Constrained Learning","Crossover Phenomenon","Pre-training","Masked Diffusion"],"permalink":"/ai/review/2025-11-6-Diffusion_Language_Models_are_Super_Data_Learners/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-6-CostBench_Evaluating_Multi-Turn_Cost-Optimal_Planning_and_Adaptation_in_Dynamic_Environments_for_LLM_Tool-Use_Agents","title":"[논문리뷰] CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents","excerpt":"Shijue Huang이 [arXiv]에 게시한 'CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","lastModifiedAt":"2025-11-09 21:54:30+0900","categories":["Review"],"tags":["Review","LLM Agents","Tool Use","Cost-Optimal Planning","Dynamic Environments","Benchmarking","Multi-Turn Interaction","Economic Reasoning"],"permalink":"/ai/review/2025-11-6-CostBench_Evaluating_Multi-Turn_Cost-Optimal_Planning_and_Adaptation_in_Dynamic_Environments_for_LLM_Tool-Use_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-iFlyBot-VLA_Technical_Report","title":"[논문리뷰] iFlyBot-VLA Technical Report","excerpt":"Jiajia wu이 [arXiv]에 게시한 'iFlyBot-VLA Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","Imitation Learning","Latent Actions","Diffusion Models","Dual-Arm Manipulation","Pretraining","Flow-Matching"],"permalink":"/ai/review/2025-11-5-iFlyBot-VLA_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-When_Visualizing_is_the_First_Step_to_Reasoning_MIRA_a_Benchmark_for_Visual_Chain-of-Thought","title":"[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought","excerpt":"이 [arXiv]에 게시한 'When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Multimodal AI","Visual Reasoning","Chain-of-Thought (CoT)","Benchmark","Image Generation","MLLMs","Visual-CoT"],"permalink":"/ai/review/2025-11-5-When_Visualizing_is_the_First_Step_to_Reasoning_MIRA_a_Benchmark_for_Visual_Chain-of-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-When_Modalities_Conflict_How_Unimodal_Reasoning_Uncertainty_Governs_Preference_Dynamics_in_MLLMs","title":"[논문리뷰] When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs","excerpt":"Haotian Wang이 [arXiv]에 게시한 'When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Modality Following","Unimodal Uncertainty","Modality Preference","Conflict Resolution","Internal Mechanism","Entropy","Controllable Dataset"],"permalink":"/ai/review/2025-11-5-When_Modalities_Conflict_How_Unimodal_Reasoning_Uncertainty_Governs_Preference_Dynamics_in_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-VidEmo_Affective-Tree_Reasoning_for_Emotion-Centric_Video_Foundation_Models","title":"[논문리뷰] VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models","excerpt":"Pengfei Wan이 [arXiv]에 게시한 'VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","VideoLLMs","Emotion Understanding","Affective-Tree Reasoning","Curriculum Learning","Reinforcement Learning","Fine-Grained Emotion","Attribute Perception","Expression Analysis"],"permalink":"/ai/review/2025-11-5-VidEmo_Affective-Tree_Reasoning_for_Emotion-Centric_Video_Foundation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-VCode_a_Multimodal_Coding_Benchmark_with_SVG_as_Symbolic_Visual_Representation","title":"[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation","excerpt":"이 [arXiv]에 게시한 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Multimodal AI","Code Generation","SVG","Visual Representation","Benchmark","Large Vision-Language Models","Agentic AI","Reasoning"],"permalink":"/ai/review/2025-11-5-VCode_a_Multimodal_Coding_Benchmark_with_SVG_as_Symbolic_Visual_Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-The_Collaboration_Gap","title":"[논문리뷰] The Collaboration Gap","excerpt":"이 [arXiv]에 게시한 'The Collaboration Gap' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","AI Collaboration","Multi-Agent Systems","Large Language Models (LLMs)","Maze Solving","Heterogeneous Agents","Collaboration Gap","Relay Inference","Agentic AI"],"permalink":"/ai/review/2025-11-5-The_Collaboration_Gap/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-TabDSR_Decompose_Sanitize_and_Reason_for_Complex_Numerical_Reasoning_in_Tabular_Data","title":"[논문리뷰] TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data","excerpt":"Jin Zeng이 [arXiv]에 게시한 'TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Tabular Data","Numerical Reasoning","Large Language Models (LLMs)","Table Question Answering (TQA)","Program-of-Thoughts (PoT)","Data Sanitization","Query Decomposition","Multi-hop Reasoning"],"permalink":"/ai/review/2025-11-5-TabDSR_Decompose_Sanitize_and_Reason_for_Complex_Numerical_Reasoning_in_Tabular_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-TWIST2_Scalable_Portable_and_Holistic_Humanoid_Data_Collection_System","title":"[논문리뷰] TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System","excerpt":"Rocky Duan이 [arXiv]에 게시한 'TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Humanoid Robotics","Data Collection","Teleoperation","Full-Body Control","Visuomotor Policy Learning","VR","Portable MoCap-Free"],"permalink":"/ai/review/2025-11-5-TWIST2_Scalable_Portable_and_Holistic_Humanoid_Data_Collection_System/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Step-Audio-EditX_Technical_Report","title":"[논문리뷰] Step-Audio-EditX Technical Report","excerpt":"이 [arXiv]에 게시한 'Step-Audio-EditX Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM-based Audio Model","Audio Editing","Text-to-Speech (TTS)","Zero-shot Learning","Large-Margin Data","Reinforcement Learning (RLHF)","Emotion Control","Speaking Style Transfer"],"permalink":"/ai/review/2025-11-5-Step-Audio-EditX_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Shorter_but_not_Worse_Frugal_Reasoning_via_Easy_Samples_as_Length_Regularizers_in_Math_RLVR","title":"[논문리뷰] Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR","excerpt":"이 [arXiv]에 게시한 'Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLMs","RLVR","Length Regularization","Mathematical Reasoning","Data Curation","Model Efficiency","Emergent Brevity"],"permalink":"/ai/review/2025-11-5-Shorter_but_not_Worse_Frugal_Reasoning_via_Easy_Samples_as_Length_Regularizers_in_Math_RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-RoboChallenge_Large-scale_Real-robot_Evaluation_of_Embodied_Policies","title":"[논문리뷰] RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies","excerpt":"이 [arXiv]에 게시한 'RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Robotics","Real-robot Evaluation","Embodied AI","Vision-Language-Action Models","Benchmarking","Online Testing System","Robotics Control","Large-scale Evaluation"],"permalink":"/ai/review/2025-11-5-RoboChallenge_Large-scale_Real-robot_Evaluation_of_Embodied_Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-RiddleBench_A_New_Generative_Reasoning_Benchmark_for_LLMs","title":"[논문리뷰] RiddleBench: A New Generative Reasoning Benchmark for LLMs","excerpt":"이 [arXiv]에 게시한 'RiddleBench: A New Generative Reasoning Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Generative AI","Benchmark","Logical Deduction","Spatial Reasoning","Constraint Satisfaction","Hallucination Cascade","Self-Correction"],"permalink":"/ai/review/2025-11-5-RiddleBench_A_New_Generative_Reasoning_Benchmark_for_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Reg-DPO_SFT-Regularized_Direct_Preference_Optimization_with_GT-Pair_for_Improving_Video_Generation","title":"[논문리뷰] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation","excerpt":"이 [arXiv]에 게시한 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Video Generation","Direct Preference Optimization","SFT Regularization","GT-Pair","Memory Optimization","Diffusion Models","I2V","T2V"],"permalink":"/ai/review/2025-11-5-Reg-DPO_SFT-Regularized_Direct_Preference_Optimization_with_GT-Pair_for_Improving_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-LiveSecBench_A_Dynamic_and_Culturally-Relevant_AI_Safety_Benchmark_for_LLMs_in_Chinese_Context","title":"[논문리뷰] LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context","excerpt":"Tianxin Zhang이 [arXiv]에 게시한 'LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM Safety","AI Safety Benchmark","Chinese Context","Dynamic Evaluation","Cultural Relevance","Adversarial Robustness","ELO Rating System"],"permalink":"/ai/review/2025-11-5-LiveSecBench_A_Dynamic_and_Culturally-Relevant_AI_Safety_Benchmark_for_LLMs_in_Chinese_Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-LTD-Bench_Evaluating_Large_Language_Models_by_Letting_Them_Draw","title":"[논문리뷰] LTD-Bench: Evaluating Large Language Models by Letting Them Draw","excerpt":"이 [arXiv]에 게시한 'LTD-Bench: Evaluating Large Language Models by Letting Them Draw' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Spatial Reasoning","Benchmark","Generative AI","Visual Perception","Spatial Imagination","Code Generation"],"permalink":"/ai/review/2025-11-5-LTD-Bench_Evaluating_Large_Language_Models_by_Letting_Them_Draw/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Forget_BIT_It_is_All_about_TOKEN_Towards_Semantic_Information_Theory_for_LLMs","title":"[논문리뷰] Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs","excerpt":"Bo Bai이 [arXiv]에 게시한 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Semantic Information Theory","Large Language Models","Directed Information","Rate-Distortion Function","Granger Causality","Token Embedding","Transformer Architecture","Variational Inference"],"permalink":"/ai/review/2025-11-5-Forget_BIT_It_is_All_about_TOKEN_Towards_Semantic_Information_Theory_for_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Dont_Blind_Your_VLA_Aligning_Visual_Representations_for_OOD_Generalization","title":"[논문리뷰] Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization","excerpt":"Aleksandr I. Panov이 [arXiv]에 게시한 'Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","OOD Generalization","Representation Alignment","Fine-tuning","Robotics","Visual Representations","Attention Maps","t-SNE"],"permalink":"/ai/review/2025-11-5-Dont_Blind_Your_VLA_Aligning_Visual_Representations_for_OOD_Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Discriminately_Treating_Motion_Components_Evolves_Joint_Depth_and_Ego-Motion_Learning","title":"[논문리뷰] Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning","excerpt":"Zuyi Xiong이 [arXiv]에 게시한 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Depth Estimation","Ego-Motion Estimation","Motion Component Discrimination","Geometric Constraints","Optical Flow","PoseNet","DepthNet"],"permalink":"/ai/review/2025-11-5-Discriminately_Treating_Motion_Components_Evolves_Joint_Depth_and_Ego-Motion_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-CodeClash_Benchmarking_Goal-Oriented_Software_Engineering","title":"[논문리뷰] CodeClash: Benchmarking Goal-Oriented Software Engineering","excerpt":"이 [arXiv]에 게시한 'CodeClash: Benchmarking Goal-Oriented Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Software Engineering Benchmarking","Language Models","AI Agents","Goal-Oriented Development","Competitive Programming","Code Evolution","Strategic Reasoning","Autonomous Systems"],"permalink":"/ai/review/2025-11-5-CodeClash_Benchmarking_Goal-Oriented_Software_Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-ChartM3_A_Multi-Stage_Code-Driven_Pipeline_for_Constructing_Multi-Dimensional_and_Multi-Step_Visual_Reasoning_Data_in_Chart_Comprehension","title":"[논문리뷰] ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension","excerpt":"Hao Wang이 [arXiv]에 게시한 'ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Chart Comprehension","Visual Reasoning","Data Generation","Code-Driven Pipeline","Multimodal LLMs","Retrieval-Augmented Generation","Reinforcement Learning","Synthetic Data"],"permalink":"/ai/review/2025-11-5-ChartM3_A_Multi-Stage_Code-Driven_Pipeline_for_Constructing_Multi-Dimensional_and_Multi-Step_Visual_Reasoning_Data_in_Chart_Comprehension/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Can_Visual_Input_Be_Compressed_A_Visual_Token_Compression_Benchmark_for_Large_Multimodal_Models","title":"[논문리뷰] Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models","excerpt":"Shijie Dong이 [arXiv]에 게시한 'Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Large Multimodal Models","Visual Token Compression","Token Pruning","Benchmark","Efficiency","Inference Latency","Multimodal LLMs"],"permalink":"/ai/review/2025-11-5-Can_Visual_Input_Be_Compressed_A_Visual_Token_Compression_Benchmark_for_Large_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-Brain-IT_Image_Reconstruction_from_fMRI_via_Brain-Interaction_Transformer","title":"[논문리뷰] Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer","excerpt":"이 [arXiv]에 게시한 'Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","fMRI","Image Reconstruction","Brain-Computer Interface","Transformer","Diffusion Models","Neural Decoding","Cross-Subject Learning","Deep Image Prior"],"permalink":"/ai/review/2025-11-5-Brain-IT_Image_Reconstruction_from_fMRI_via_Brain-Interaction_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-BRAINS_A_Retrieval-Augmented_System_for_Alzheimers_Detection_and_Monitoring","title":"[논문리뷰] BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring","excerpt":"이 [arXiv]에 게시한 'BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Alzheimer's Disease","Retrieval-Augmented Generation (RAG)","Large Language Models (LLMs)","Clinical Decision Support","Multimodal Data Fusion","Cognitive Decline Detection","Early Diagnosis"],"permalink":"/ai/review/2025-11-5-BRAINS_A_Retrieval-Augmented_System_for_Alzheimers_Detection_and_Monitoring/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-5-AyurParam_A_State-of-the-Art_Bilingual_Language_Model_for_Ayurveda","title":"[논문리뷰] AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda","excerpt":"이 [arXiv]에 게시한 'AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","lastModifiedAt":"2025-11-09 19:35:02+0900","categories":["Review"],"tags":["Review","Ayurveda LLM","Domain Adaptation","Bilingual Language Model","Instruction Tuning","Medical AI","Knowledge-Grounded QA","Traditional Medicine"],"permalink":"/ai/review/2025-11-5-AyurParam_A_State-of-the-Art_Bilingual_Language_Model_for_Ayurveda/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-leftcirclearrowrighttextBUSright_A_Large_and_Diverse_Multimodal_Benchmark_for_evaluating_the_ability_of_Vision-Language_Models_to_understand_Rebus_Puzzles","title":"[논문리뷰] left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles","excerpt":"Deepiha S이 [arXiv]에 게시한 'left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Multimodal Benchmark","Rebus Puzzles","In-Context Learning","Reasoning","ControlNet","Prompt Engineering"],"permalink":"/ai/review/2025-11-4-leftcirclearrowrighttextBUSright_A_Large_and_Diverse_Multimodal_Benchmark_for_evaluating_the_ability_of_Vision-Language_Models_to_understand_Rebus_Puzzles/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-World_Simulation_with_Video_Foundation_Models_for_Physical_AI","title":"[논문리뷰] World Simulation with Video Foundation Models for Physical AI","excerpt":"Junjie Bai이 [arXiv]에 게시한 'World Simulation with Video Foundation Models for Physical AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Physical AI","World Simulation","Video Foundation Models","Flow Matching","Reinforcement Learning","Robotics","Autonomous Driving","Synthetic Data Generation"],"permalink":"/ai/review/2025-11-4-World_Simulation_with_Video_Foundation_Models_for_Physical_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Vote-in-Context_Turning_VLMs_into_Zero-Shot_Rank_Fusers","title":"[논문리뷰] Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers","excerpt":"이 [arXiv]에 게시한 'Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Video Retrieval","Vision-Language Models (VLMs)","Zero-Shot Learning","List-wise Reranking","Rank Fusion","Prompt Engineering","S-Grid","Multimodal Retrieval"],"permalink":"/ai/review/2025-11-4-Vote-in-Context_Turning_VLMs_into_Zero-Shot_Rank_Fusers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Unified_Diffusion_VLA_Vision-Language-Action_Model_via_Joint_Discrete_Denoising_Diffusion_Process","title":"[논문리뷰] Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process","excerpt":"이 [arXiv]에 게시한 'Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Diffusion Models","Discrete Denoising","Multimodal Learning","Robotics","Embodied AI","Joint Generation","Action Prediction"],"permalink":"/ai/review/2025-11-4-Unified_Diffusion_VLA_Vision-Language-Action_Model_via_Joint_Discrete_Denoising_Diffusion_Process/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-UniREditBench_A_Unified_Reasoning-based_Image_Editing_Benchmark","title":"[논문리뷰] UniREditBench: A Unified Reasoning-based Image Editing Benchmark","excerpt":"이 [arXiv]에 게시한 'UniREditBench: A Unified Reasoning-based Image Editing Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Image Editing","Reasoning-based AI","Benchmark","Multimodal Learning","Chain-of-Thought (CoT)","Dual-Reference Evaluation","Generative Models","Game AI"],"permalink":"/ai/review/2025-11-4-UniREditBench_A_Unified_Reasoning-based_Image_Editing_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-UniLumos_Fast_and_Unified_Image_and_Video_Relighting_with_Physics-Plausible_Feedback","title":"[논문리뷰] UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback","excerpt":"이 [arXiv]에 게시한 'UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Relighting","Diffusion Models","Flow Matching","Physics-Plausible Feedback","Image-to-Video","Geometric Supervision","Path Consistency Learning","LumosBench"],"permalink":"/ai/review/2025-11-4-UniLumos_Fast_and_Unified_Image_and_Video_Relighting_with_Physics-Plausible_Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-UME-R1_Exploring_Reasoning-Driven_Generative_Multimodal_Embeddings","title":"[논문리뷰] UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings","excerpt":"Jinsong Su이 [arXiv]에 게시한 'UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal Embeddings","Generative AI","Reasoning","Reinforcement Learning","MLLMs","Supervised Fine-tuning","Information Retrieval","Unified Embeddings"],"permalink":"/ai/review/2025-11-4-UME-R1_Exploring_Reasoning-Driven_Generative_Multimodal_Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Trove_A_Flexible_Toolkit_for_Dense_Retrieval","title":"[논문리뷰] Trove: A Flexible Toolkit for Dense Retrieval","excerpt":"이 [arXiv]에 게시한 'Trove: A Flexible Toolkit for Dense Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Dense Retrieval","Retrieval Toolkit","Data Management","Distributed Training","Model Customization","Hard Negative Mining","Hugging Face Integration","Performance Optimization"],"permalink":"/ai/review/2025-11-4-Trove_A_Flexible_Toolkit_for_Dense_Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Towards_Universal_Video_Retrieval_Generalizing_Video_Embedding_via_Synthesized_Multimodal_Pyramid_Curriculum","title":"[논문리뷰] Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum","excerpt":"이 [arXiv]에 게시한 'Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Video Retrieval","Multimodal Embedding","Data Synthesis","Curriculum Learning","Zero-shot Generalization","Benchmark Design","MLLM","Video-Text Retrieval"],"permalink":"/ai/review/2025-11-4-Towards_Universal_Video_Retrieval_Generalizing_Video_Embedding_via_Synthesized_Multimodal_Pyramid_Curriculum/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Towards_Robust_Mathematical_Reasoning","title":"[논문리뷰] Towards Robust Mathematical Reasoning","excerpt":"Yuri Chervonyi이 [arXiv]에 게시한 'Towards Robust Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Large Language Models (LLMs)","AI Benchmarks","International Mathematical Olympiad (IMO)","Proof Verification","Automatic Grading","Robustness"],"permalink":"/ai/review/2025-11-4-Towards_Robust_Mathematical_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-ToolScope_An_Agentic_Framework_for_Vision-Guided_and_Long-Horizon_Tool_Use","title":"[논문리뷰] ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use","excerpt":"Guanting Dong이 [arXiv]에 게시한 'ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal Agents","Tool-Augmented LLMs","Vision-Guided Reasoning","Long-Horizon Tasks","VQA","Global Planning","Context Preservation","Perceive Tool"],"permalink":"/ai/review/2025-11-4-ToolScope_An_Agentic_Framework_for_Vision-Guided_and_Long-Horizon_Tool_Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-The_Underappreciated_Power_of_Vision_Models_for_Graph_Structural_Understanding","title":"[논문리뷰] The Underappreciated Power of Vision Models for Graph Structural Understanding","excerpt":"Lei Zhang이 [arXiv]에 게시한 'The Underappreciated Power of Vision Models for Graph Structural Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Graph Neural Networks","Vision Models","Graph Understanding","Topological Perception","GraphAbstract Benchmark","OOD Generalization","Graph Visualization"],"permalink":"/ai/review/2025-11-4-The_Underappreciated_Power_of_Vision_Models_for_Graph_Structural_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-TIR-Bench_A_Comprehensive_Benchmark_for_Agentic_Thinking-with-Images_Reasoning","title":"[논문리뷰] TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning","excerpt":"Shaoheng Lin이 [arXiv]에 게시한 'TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Agentic Reasoning","Thinking-with-Images","Visual Reasoning Benchmark","Tool Use","Image Manipulation","Fine-tuning"],"permalink":"/ai/review/2025-11-4-TIR-Bench_A_Comprehensive_Benchmark_for_Agentic_Thinking-with-Images_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-ROVER_Benchmarking_Reciprocal_Cross-Modal_Reasoning_for_Omnimodal_Generation","title":"[논문리뷰] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation","excerpt":"Feng Li이 [arXiv]에 게시한 'ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal AI","Benchmarking","Cross-Modal Reasoning","Omnimodal Generation","Visual Generation","Verbal Generation","Unified Multimodal Models"],"permalink":"/ai/review/2025-11-4-ROVER_Benchmarking_Reciprocal_Cross-Modal_Reasoning_for_Omnimodal_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-PHUMA_Physically-Grounded_Humanoid_Locomotion_Dataset","title":"[논문리뷰] PHUMA: Physically-Grounded Humanoid Locomotion Dataset","excerpt":"이 [arXiv]에 게시한 'PHUMA: Physically-Grounded Humanoid Locomotion Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Humanoid Locomotion","Dataset","Motion Imitation","Physics-based Control","Motion Retargeting","Data Curation","Reinforcement Learning","Inverse Kinematics"],"permalink":"/ai/review/2025-11-4-PHUMA_Physically-Grounded_Humanoid_Locomotion_Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-OpenSIR_Open-Ended_Self-Improving_Reasoner","title":"[논문리뷰] OpenSIR: Open-Ended Self-Improving Reasoner","excerpt":"이 [arXiv]에 게시한 'OpenSIR: Open-Ended Self-Improving Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Open-Ended Learning","Self-Play","Reinforcement Learning","Large Language Models","Mathematical Reasoning","Problem Generation","Curriculum Learning","Reward Shaping"],"permalink":"/ai/review/2025-11-4-OpenSIR_Open-Ended_Self-Improving_Reasoner/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-NaviTrace_Evaluating_Embodied_Navigation_of_Vision-Language_Models","title":"[논문리뷰] NaviTrace: Evaluating Embodied Navigation of Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'NaviTrace: Evaluating Embodied Navigation of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Embodied Navigation","VQA Benchmark","Robotic Navigation","Semantic-aware Score","Dynamic Time Warping","Real-world Scenarios"],"permalink":"/ai/review/2025-11-4-NaviTrace_Evaluating_Embodied_Navigation_of_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Multi-Step_Knowledge_Interaction_Analysis_via_Rank-2_Subspace_Disentanglement","title":"[논문리뷰] Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement","excerpt":"Isabelle Augenstein이 [arXiv]에 게시한 'Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","LLMs","Knowledge Interaction","Parametric Knowledge","Contextual Knowledge","Subspace Disentanglement","NLE Generation","Hallucination Detection","Chain-of-Thought"],"permalink":"/ai/review/2025-11-4-Multi-Step_Knowledge_Interaction_Analysis_via_Rank-2_Subspace_Disentanglement/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-MotionStream_Real-Time_Video_Generation_with_Interactive_Motion_Controls","title":"[논문리뷰] MotionStream: Real-Time Video Generation with Interactive Motion Controls","excerpt":"이 [arXiv]에 게시한 'MotionStream: Real-Time Video Generation with Interactive Motion Controls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Real-Time Video Generation","Motion Control","Diffusion Models","Autoregressive Generation","Self-Forcing","Attention Sink","Streaming Inference","Video Distillation"],"permalink":"/ai/review/2025-11-4-MotionStream_Real-Time_Video_Generation_with_Interactive_Motion_Controls/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-MR-Align_Meta-Reasoning_Informed_Factuality_Alignment_for_Large_Reasoning_Models","title":"[논문리뷰] MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models","excerpt":"Bin Yu이 [arXiv]에 게시한 'MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Factuality Alignment","Meta-Reasoning","Kahneman-Tversky Optimization","Chain-of-Thought","Hallucination","Process-Level Alignment"],"permalink":"/ai/review/2025-11-4-MR-Align_Meta-Reasoning_Informed_Factuality_Alignment_for_Large_Reasoning_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-LongCat-Flash-Omni_Technical_Report","title":"[논문리뷰] LongCat-Flash-Omni Technical Report","excerpt":"Bin Xiao이 [arXiv]에 게시한 'LongCat-Flash-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Omni-modal AI","Multimodal LLM","Real-time Interaction","Mixture-of-Experts (MoE)","Streaming Inference","Distributed Training","Curriculum Learning","Audio-Visual Perception"],"permalink":"/ai/review/2025-11-4-LongCat-Flash-Omni_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-How_Far_Are_Surgeons_from_Surgical_World_Models_A_Pilot_Study_on_Zero-shot_Surgical_Video_Generation_with_Expert_Assessment","title":"[논문리뷰] How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment","excerpt":"Yuhao Zhai이 [arXiv]에 게시한 'How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Video Generation","World Models","Surgical AI","Zero-shot Prediction","Expert Evaluation","Plausibility Gap","Medical Simulation"],"permalink":"/ai/review/2025-11-4-How_Far_Are_Surgeons_from_Surgical_World_Models_A_Pilot_Study_on_Zero-shot_Surgical_Video_Generation_with_Expert_Assessment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Generalizing_Test-time_Compute-optimal_Scaling_as_an_Optimizable_Graph","title":"[논문리뷰] Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph","excerpt":"이 [arXiv]에 게시한 'Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Test-Time Scaling","LLMs","Graph Optimization","REINFORCE","Multi-agent Systems","Adaptive Architectures","Compute-optimal Scaling","Probabilistic Graphs"],"permalink":"/ai/review/2025-11-4-Generalizing_Test-time_Compute-optimal_Scaling_as_an_Optimizable_Graph/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-GUI-AIMA_Aligning_Intrinsic_Multimodal_Attention_with_a_Context_Anchor_for_GUI_Grounding","title":"[논문리뷰] GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding","excerpt":"Wanrong Zhu이 [arXiv]에 게시한 'GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","GUI Grounding","Multimodal Attention","MLLMs","Coordinate-Free","Visual Grounding","Attention Weighting","Anchor Token"],"permalink":"/ai/review/2025-11-4-GUI-AIMA_Aligning_Intrinsic_Multimodal_Attention_with_a_Context_Anchor_for_GUI_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Every_Activation_Boosted_Scaling_General_Reasoner_to_1_Trillion_Open_Language_Foundation","title":"[논문리뷰] Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation","excerpt":"이 [arXiv]에 게시한 'Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Large Language Models","Mixture-of-Experts","Reasoning Capability","Sparse Activation","Scaling Laws","FP8 Training","Efficient Training","Instruction Tuning"],"permalink":"/ai/review/2025-11-4-Every_Activation_Boosted_Scaling_General_Reasoner_to_1_Trillion_Open_Language_Foundation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-EBT-Policy_Energy_Unlocks_Emergent_Physical_Reasoning_Capabilities","title":"[논문리뷰] EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities","excerpt":"Yunxin Liu이 [arXiv]에 게시한 'EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Energy-Based Models (EBMs)","Diffusion Policy","Robotics","Behavior Cloning","Physical Reasoning","Uncertainty Modeling","Emergent Behavior","Robot Manipulation"],"permalink":"/ai/review/2025-11-4-EBT-Policy_Energy_Unlocks_Emergent_Physical_Reasoning_Capabilities/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Do_Vision-Language_Models_Measure_Up_Benchmarking_Visual_Measurement_Reading_with_MeasureBench","title":"[논문리뷰] Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench","excerpt":"이 [arXiv]에 게시한 'Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Benchmarking","Visual Measurement Reading","Synthetic Data Generation","Fine-grained Perception","Spatial Grounding","Reinforcement Learning"],"permalink":"/ai/review/2025-11-4-Do_Vision-Language_Models_Measure_Up_Benchmarking_Visual_Measurement_Reading_with_MeasureBench/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Data-Efficient_RLVR_via_Off-Policy_Influence_Guidance","title":"[논문리뷰] Data-Efficient RLVR via Off-Policy Influence Guidance","excerpt":"Jiale Cheng이 [arXiv]에 게시한 'Data-Efficient RLVR via Off-Policy Influence Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Influence Functions","Data Selection","Off-Policy Learning","Curriculum Learning","Large Language Models (LLMs)","Sparse Random Projection","Data Efficiency"],"permalink":"/ai/review/2025-11-4-Data-Efficient_RLVR_via_Off-Policy_Influence_Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-AthenaBench_A_Dynamic_Benchmark_for_Evaluating_LLMs_in_Cyber_Threat_Intelligence","title":"[논문리뷰] AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence","excerpt":"Peter Worth이 [arXiv]에 게시한 'AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","LLM Benchmarking","Cyber Threat Intelligence (CTI)","Dynamic Evaluation","CTI Reasoning","Vulnerability Prediction","Threat Actor Attribution","Risk Mitigation","Natural Language Processing"],"permalink":"/ai/review/2025-11-4-AthenaBench_A_Dynamic_Benchmark_for_Evaluating_LLMs_in_Cyber_Threat_Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-4-Actial_Activate_Spatial_Reasoning_Ability_of_Multimodal_Large_Language_Models","title":"[논문리뷰] Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models","excerpt":"Changfeng Ma이 [arXiv]에 게시한 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","lastModifiedAt":"2025-11-09 19:22:42+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Spatial Reasoning","Viewpoint Learning","Two-Stage Fine-tuning","3D Consistency","Viewpoint-100K","Reinforcement Learning"],"permalink":"/ai/review/2025-11-4-Actial_Activate_Spatial_Reasoning_Ability_of_Multimodal_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-π_RL_Online_RL_Fine-tuning_for_Flow-based_Vision-Language-Action_Models","title":"[논문리뷰] π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Vision-Language-Action Models (VLAs)","Flow-based Models","Policy Optimization","Robotics","Flow Matching","SDE","MDP"],"permalink":"/ai/review/2025-11-3-π_RL_Online_RL_Fine-tuning_for_Flow-based_Vision-Language-Action_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Visual_Backdoor_Attacks_on_MLLM_Embodied_Decision_Making_via_Contrastive_Trigger_Learning","title":"[논문리뷰] Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning","excerpt":"Hanyang Chen이 [arXiv]에 게시한 'Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Visual Backdoor Attacks","MLLM Embodied Agents","Contrastive Trigger Learning","Policy Manipulation","Adversarial AI","Embodied AI Security","Multimodal LLMs"],"permalink":"/ai/review/2025-11-3-Visual_Backdoor_Attacks_on_MLLM_Embodied_Decision_Making_via_Contrastive_Trigger_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Value_Drifts_Tracing_Value_Alignment_During_LLM_Post-Training","title":"[논문리뷰] Value Drifts: Tracing Value Alignment During LLM Post-Training","excerpt":"이 [arXiv]에 게시한 'Value Drifts: Tracing Value Alignment During LLM Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","LLM Alignment","Value Drift","Supervised Fine-Tuning (SFT)","Preference Optimization","RLHF","Llama-3","Qwen-3","Human Values"],"permalink":"/ai/review/2025-11-3-Value_Drifts_Tracing_Value_Alignment_During_LLM_Post-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Spatial-SSRL_Enhancing_Spatial_Understanding_via_Self-Supervised_Reinforcement_Learning","title":"[논문리뷰] Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Self-supervised learning","Reinforcement Learning","Spatial Understanding","Vision-Language Models","Pretext Tasks","RGB-D Images","Spatial Reasoning"],"permalink":"/ai/review/2025-11-3-Spatial-SSRL_Enhancing_Spatial_Understanding_via_Self-Supervised_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-SemCoT_Accelerating_Chain-of-Thought_Reasoning_through_Semantically-Aligned_Implicit_Tokens","title":"[논문리뷰] SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens","excerpt":"이 [arXiv]에 게시한 'SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Chain-of-Thought (CoT)","Implicit Reasoning","LLMs","Semantic Alignment","Efficiency Optimization","Knowledge Distillation"],"permalink":"/ai/review/2025-11-3-SemCoT_Accelerating_Chain-of-Thought_Reasoning_through_Semantically-Aligned_Implicit_Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Revisiting_Multimodal_Positional_Encoding_in_Vision-Language_Models","title":"[논문리뷰] Revisiting Multimodal Positional Encoding in Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Revisiting Multimodal Positional Encoding in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Multimodal Positional Encoding","Vision-Language Models","Rotary Positional Embedding (RoPE)","Transformer","Multimodal Understanding","Visual Grounding","Frequency Allocation","Position Design"],"permalink":"/ai/review/2025-11-3-Revisiting_Multimodal_Positional_Encoding_in_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Rank-GRPO_Training_LLM-based_Conversational_Recommender_Systems_with_Reinforcement_Learning","title":"[논문리뷰] Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Conversational Recommender Systems","Large Language Models","Reinforcement Learning","Group Relative Policy Optimization","Rank-based Learning","Supervised Fine-tuning","Reward Shaping"],"permalink":"/ai/review/2025-11-3-Rank-GRPO_Training_LLM-based_Conversational_Recommender_Systems_with_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Phased_DMD_Few-step_Distribution_Matching_Distillation_via_Score_Matching_within_Subintervals","title":"[논문리뷰] Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals","excerpt":"이 [arXiv]에 게시한 'Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Distribution Matching Distillation","Few-step Diffusion","Score Matching","Mixture-of-Experts","Generative Models","Image Generation","Video Generation","Model Distillation"],"permalink":"/ai/review/2025-11-3-Phased_DMD_Few-step_Distribution_Matching_Distillation_via_Score_Matching_within_Subintervals/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-OS-Sentinel_Towards_Safety-Enhanced_Mobile_GUI_Agents_via_Hybrid_Validation_in_Realistic_Workflows","title":"[논문리뷰] OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows","excerpt":"이 [arXiv]에 게시한 'OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Mobile GUI Agents","Agent Safety","Hybrid Detection","Formal Verification","VLM-based Contextual Judgment","Safety Benchmark","Risk Detection"],"permalink":"/ai/review/2025-11-3-OS-Sentinel_Towards_Safety-Enhanced_Mobile_GUI_Agents_via_Hybrid_Validation_in_Realistic_Workflows/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Monopoly_Deal_A_Benchmark_Environment_for_Bounded_One-Sided_Response_Games","title":"[논문리뷰] Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games","excerpt":"cavaunpeu이 [arXiv]에 게시한 'Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Bounded One-Sided Response Games (BORGs)","Monopoly Deal","Benchmark Environment","Counterfactual Regret Minimization (CFR)","Imperfect Information Games","Game Theory","Self-Play","State Abstraction"],"permalink":"/ai/review/2025-11-3-Monopoly_Deal_A_Benchmark_Environment_for_Bounded_One-Sided_Response_Games/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-MisSynth_Improving_MISSCI_Logical_Fallacies_Classification_with_Synthetic_Data","title":"[논문리뷰] MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data","excerpt":"Nadiya Shvai이 [arXiv]에 게시한 'MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Health Misinformation","Logical Fallacy Classification","Synthetic Data Generation","Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Parameter-Efficient Fine-tuning (PEFT)","LoRA","MISSCI Benchmark"],"permalink":"/ai/review/2025-11-3-MisSynth_Improving_MISSCI_Logical_Fallacies_Classification_with_Synthetic_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Mask-to-Height_A_YOLOv11-Based_Architecture_for_Joint_Building_Instance_Segmentation_and_Height_Classification_from_Satellite_Imagery","title":"[논문리뷰] Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery","excerpt":"Oğuz Hanoğlu이 [arXiv]에 게시한 'Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Building Instance Segmentation","Height Classification","YOLOv11","Satellite Imagery","Multitask Learning","Remote Sensing","Urban Planning"],"permalink":"/ai/review/2025-11-3-Mask-to-Height_A_YOLOv11-Based_Architecture_for_Joint_Building_Instance_Segmentation_and_Height_Classification_from_Satellite_Imagery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Limits_of_Generalization_in_RLVR_Two_Case_Studies_in_Mathematical_Reasoning","title":"[논문리뷰] Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning","excerpt":"Nidhi Rastogi이 [arXiv]에 게시한 'Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Mathematical Reasoning","Large Language Models (LLMs)","Activity Scheduling","Longest Increasing Subsequence (LIS)","Generalization Limits","Reward Design","Self-consistency"],"permalink":"/ai/review/2025-11-3-Limits_of_Generalization_in_RLVR_Two_Case_Studies_in_Mathematical_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-INT_v.s._FP_A_Comprehensive_Study_of_Fine-Grained_Low-bit_Quantization_Formats","title":"[논문리뷰] INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats","excerpt":"이 [arXiv]에 게시한 'INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Quantization","Low-bit Formats","Integer Quantization","Floating-Point Quantization","Large Language Models (LLMs)","Hardware Efficiency","Fine-Grained Quantization","MXINT8"],"permalink":"/ai/review/2025-11-3-INT_v.s._FP_A_Comprehensive_Study_of_Fine-Grained_Low-bit_Quantization_Formats/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-HyperClick_Advancing_Reliable_GUI_Grounding_via_Uncertainty_Calibration","title":"[논문리뷰] HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration","excerpt":"Anan Du이 [arXiv]에 게시한 'HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","GUI Grounding","Uncertainty Calibration","Reinforcement Learning","Confidence Estimation","Brier Score","GUI Agents","Visual-Language Models"],"permalink":"/ai/review/2025-11-3-HyperClick_Advancing_Reliable_GUI_Grounding_via_Uncertainty_Calibration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Higher-order_Linear_Attention","title":"[논문리뷰] Higher-order Linear Attention","excerpt":"이 [arXiv]에 게시한 'Higher-order Linear Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Linear Attention","Higher-order Interactions","Causal Streaming","Associative Scans","Prefix Summaries","Transformer Architectures","State Space Models"],"permalink":"/ai/review/2025-11-3-Higher-order_Linear_Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Dual-Stream_Diffusion_for_World-Model_Augmented_Vision-Language-Action_Model","title":"[논문리뷰] Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model","excerpt":"Jinwoo Shin이 [arXiv]에 게시한 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","World Models","Diffusion Models","Multimodal Learning","Robotics","Asynchronous Sampling","Diffusion Transformers"],"permalink":"/ai/review/2025-11-3-Dual-Stream_Diffusion_for_World-Model_Augmented_Vision-Language-Action_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Defeating_the_Training-Inference_Mismatch_via_FP16","title":"[논문리뷰] Defeating the Training-Inference Mismatch via FP16","excerpt":"이 [arXiv]에 게시한 'Defeating the Training-Inference Mismatch via FP16' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Fine-tuning","Training-Inference Mismatch","Floating Point Precision","FP16","BF16","RL Stability"],"permalink":"/ai/review/2025-11-3-Defeating_the_Training-Inference_Mismatch_via_FP16/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Continuous_Autoregressive_Language_Models","title":"[논문리뷰] Continuous Autoregressive Language Models","excerpt":"이 [arXiv]에 게시한 'Continuous Autoregressive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Continuous Representation","Autoencoder","Likelihood-Free Modeling","Energy-Based Models","Next-Vector Prediction","Computational Efficiency","Temperature Sampling"],"permalink":"/ai/review/2025-11-3-Continuous_Autoregressive_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-Beyond_Objects_Contextual_Synthetic_Data_Generation_for_Fine-Grained_Classification","title":"[논문리뷰] Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification","excerpt":"Olga Russakovsky이 [arXiv]에 게시한 'Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Text-to-Image Synthesis","Synthetic Data Generation","Fine-Grained Classification","Few-Shot Learning","Diffusion Models","Contextual Conditioning","Causal Intervention"],"permalink":"/ai/review/2025-11-3-Beyond_Objects_Contextual_Synthetic_Data_Generation_for_Fine-Grained_Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-11-3-A_Survey_on_Efficient_Vision-Language-Action_Models","title":"[논문리뷰] A Survey on Efficient Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'A Survey on Efficient Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","lastModifiedAt":"2025-11-09 19:01:31+0900","categories":["Review"],"tags":["Review","Embodied AI","Robotic Manipulation","VLA Models","Efficient AI","Model Compression","Efficient Training","Data Collection","Multimodal AI"],"permalink":"/ai/review/2025-11-3-A_Survey_on_Efficient_Vision-Language-Action_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-The_Quest_for_Generalizable_Motion_Generation_Data_Model_and_Evaluation","title":"[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation","excerpt":"이 [arXiv]에 게시한 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Motion Generation","Generalization","Diffusion Models","Transformer","Large-scale Dataset","Benchmark","Multimodal Learning","Video Generation"],"permalink":"/ai/review/2025-10-31-The_Quest_for_Generalizable_Motion_Generation_Data_Model_and_Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-The_Era_of_Agentic_Organization_Learning_to_Organize_with_Language_Models","title":"[논문리뷰] The Era of Agentic Organization: Learning to Organize with Language Models","excerpt":"Xun Wu이 [arXiv]에 게시한 'The Era of Agentic Organization: Learning to Organize with Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Agentic Organization","Asynchronous Thinking","Language Models","Reinforcement Learning","Multi-agent Systems","Reasoning","Task Decomposition","Orchestration"],"permalink":"/ai/review/2025-10-31-The_Era_of_Agentic_Organization_Learning_to_Organize_with_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-The_End_of_Manual_Decoding_Towards_Truly_End-to-End_Language_Models","title":"[논문리뷰] The End of Manual Decoding: Towards Truly End-to-End Language Models","excerpt":"이 [arXiv]에 게시한 'The End of Manual Decoding: Towards Truly End-to-End Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","End-to-End Generation","Dynamic Decoding","Hyperparameter Optimization","Stochastic Sampling","Instruction Following","Transformer Architecture"],"permalink":"/ai/review/2025-10-31-The_End_of_Manual_Decoding_Towards_Truly_End-to-End_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Surfer_2_The_Next_Generation_of_Cross-Platform_Computer_Use_Agents","title":"[논문리뷰] Surfer 2: The Next Generation of Cross-Platform Computer Use Agents","excerpt":"이 [arXiv]에 게시한 'Surfer 2: The Next Generation of Cross-Platform Computer Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Computer Use Agent","Cross-Platform","GUI Automation","Vision-Language Model","Hierarchical Architecture","Agent Orchestration","Visual Interaction"],"permalink":"/ai/review/2025-10-31-Surfer_2_The_Next_Generation_of_Cross-Platform_Computer_Use_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Supervised_Reinforcement_Learning_From_Expert_Trajectories_to_Step-wise_Reasoning","title":"[논문리뷰] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning","excerpt":"이 [arXiv]에 게시한 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Supervised Reinforcement Learning","LLMs","Multi-step Reasoning","Reward Shaping","Expert Trajectories","Math Reasoning","Agentic AI"],"permalink":"/ai/review/2025-10-31-Supervised_Reinforcement_Learning_From_Expert_Trajectories_to_Step-wise_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Remote_Labor_Index_Measuring_AI_Automation_of_Remote_Work","title":"[논문리뷰] Remote Labor Index: Measuring AI Automation of Remote Work","excerpt":"Shivam Singhal이 [arXiv]에 게시한 'Remote Labor Index: Measuring AI Automation of Remote Work' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","AI 자동화","원격 근무","벤치마크","AI 에이전트","프리랜서 경제","인간 평가","자동화율"],"permalink":"/ai/review/2025-10-31-Remote_Labor_Index_Measuring_AI_Automation_of_Remote_Work/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Performance_Trade-offs_of_Optimizing_Small_Language_Models_for_E-Commerce","title":"[논문리뷰] Performance Trade-offs of Optimizing Small Language Models for E-Commerce","excerpt":"Nikola Tankovic이 [arXiv]에 게시한 'Performance Trade-offs of Optimizing Small Language Models for E-Commerce' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Small Language Models","E-commerce","Intent Recognition","Fine-tuning","QLoRA","Quantization","GPTQ","GGUF","Hardware-aware Optimization"],"permalink":"/ai/review/2025-10-31-Performance_Trade-offs_of_Optimizing_Small_Language_Models_for_E-Commerce/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-POWSM_A_Phonetic_Open_Whisper-Style_Speech_Foundation_Model","title":"[논문리뷰] POWSM: A Phonetic Open Whisper-Style Speech Foundation Model","excerpt":"이 [arXiv]에 게시한 'POWSM: A Phonetic Open Whisper-Style Speech Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Phonetic Foundation Model","Multitask Learning","Speech Recognition","Phone Recognition","Grapheme-to-Phoneme","Encoder-Decoder","Low-Resource Speech"],"permalink":"/ai/review/2025-10-31-POWSM_A_Phonetic_Open_Whisper-Style_Speech_Foundation_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-PORTool_Tool-Use_LLM_Training_with_Rewarded_Tree","title":"[논문리뷰] PORTool: Tool-Use LLM Training with Rewarded Tree","excerpt":"이 [arXiv]에 게시한 'PORTool: Tool-Use LLM Training with Rewarded Tree' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Tool-Use LLM","Reinforcement Learning (RL)","Policy Optimization","Rewarded Tree","Trajectory Optimization","Agentic System","Dynamic Tool Call"],"permalink":"/ai/review/2025-10-31-PORTool_Tool-Use_LLM_Training_with_Rewarded_Tree/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-OmniX_From_Unified_Panoramic_Generation_and_Perception_to_Graphics-Ready_3D_Scenes","title":"[논문리뷰] OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes","excerpt":"이 [arXiv]에 게시한 'OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Panoramic Generation","Panoramic Perception","3D Scene Reconstruction","Graphics-Ready Scenes","Physically Based Rendering (PBR)","Flow Matching Models","Cross-Modal Adapters","Synthetic Dataset (PanoX)"],"permalink":"/ai/review/2025-10-31-OmniX_From_Unified_Panoramic_Generation_and_Perception_to_Graphics-Ready_3D_Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-OmniLayout_Enabling_Coarse-to-Fine_Learning_with_LLMs_for_Universal_Document_Layout_Generation","title":"[논문리뷰] OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation","excerpt":"Bin Wang이 [arXiv]에 게시한 'OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Document Layout Generation","Large Language Models (LLMs)","Coarse-to-Fine Learning","Dataset Curation","OmniLayout-1M","Document AI","Generative Models"],"permalink":"/ai/review/2025-10-31-OmniLayout_Enabling_Coarse-to-Fine_Learning_with_LLMs_for_Universal_Document_Layout_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-MedVLSynther_Synthesizing_High-Quality_Visual_Question_Answering_from_Medical_Documents_with_Generator-Verifier_LMMs","title":"[논문리뷰] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs","excerpt":"이 [arXiv]에 게시한 'MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Medical VQA","Large Multimodal Models (LMMs)","Data Synthesis","Generator-Verifier Framework","Rubric-Guided","Reinforcement Learning (RL)","Context-Aware"],"permalink":"/ai/review/2025-10-31-MedVLSynther_Synthesizing_High-Quality_Visual_Question_Answering_from_Medical_Documents_with_Generator-Verifier_LMMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Magentic_Marketplace_An_Open-Source_Environment_for_Studying_Agentic_Markets","title":"[논문리뷰] Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets","excerpt":"이 [arXiv]에 게시한 'Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Agentic Markets","Multi-Agent Systems","Large Language Models (LLMs)","Simulation Environment","Open-Source Platform","Market Mechanism Design","Behavioral Biases","Manipulation Resistance"],"permalink":"/ai/review/2025-10-31-Magentic_Marketplace_An_Open-Source_Environment_for_Studying_Agentic_Markets/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-MIRO_MultI-Reward_cOnditioned_pretraining_improves_T2I_quality_and_efficiency","title":"[논문리뷰] MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency","excerpt":"David Picard이 [arXiv]에 게시한 'MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Multi-Reward Learning","Flow Matching","User Preference Alignment","Training Efficiency","Compositional Reasoning","Conditional Generation"],"permalink":"/ai/review/2025-10-31-MIRO_MultI-Reward_cOnditioned_pretraining_improves_T2I_quality_and_efficiency/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-L2M3OF_A_Large_Language_Multimodal_Model_for_Metal-Organic_Frameworks","title":"[논문리뷰] L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks","excerpt":"Xenophon Evangelopoulos이 [arXiv]에 게시한 'L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Metal-Organic Frameworks (MOFs)","Materials Discovery","Crystal Representation Learning","Instruction Tuning","Structure-Property Prediction","Knowledge Generation"],"permalink":"/ai/review/2025-10-31-L2M3OF_A_Large_Language_Multimodal_Model_for_Metal-Organic_Frameworks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Kimi_Linear_An_Expressive_Efficient_Attention_Architecture","title":"[논문리뷰] Kimi Linear: An Expressive, Efficient Attention Architecture","excerpt":"이 [arXiv]에 게시한 'Kimi Linear: An Expressive, Efficient Attention Architecture' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Linear Attention","Hybrid Architecture","Kimi Delta Attention (KDA)","Gating Mechanism","Long-Context Modeling","Efficient Inference","Transformer"],"permalink":"/ai/review/2025-10-31-Kimi_Linear_An_Expressive_Efficient_Attention_Architecture/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-FullPart_Generating_each_3D_Part_at_Full_Resolution","title":"[논문리뷰] FullPart: Generating each 3D Part at Full Resolution","excerpt":"Chenjian Gao이 [arXiv]에 게시한 'FullPart: Generating each 3D Part at Full Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","3D Part Generation","Full Resolution","Implicit Representation","Explicit Representation","Voxel Grid","Diffusion Models","PartVerse-XL","Center-Corner Encoding"],"permalink":"/ai/review/2025-10-31-FullPart_Generating_each_3D_Part_at_Full_Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Exploring_Conditions_for_Diffusion_models_in_Robotic_Control","title":"[논문리뷰] Exploring Conditions for Diffusion models in Robotic Control","excerpt":"이 [arXiv]에 게시한 'Exploring Conditions for Diffusion models in Robotic Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Diffusion Models","Robotic Control","Imitation Learning","Task-Adaptive Representations","Visual Prompts","Text-to-Image","Conditioning","Behavior Cloning"],"permalink":"/ai/review/2025-10-31-Exploring_Conditions_for_Diffusion_models_in_Robotic_Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-EnzyControl_Adding_Functional_and_Substrate-Specific_Control_for_Enzyme_Backbone_Generation","title":"[논문리뷰] EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation","excerpt":"이 [arXiv]에 게시한 'EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Enzyme Design","Protein Engineering","Generative Models","Flow Matching","Substrate-Specific Control","Functional Site Prediction","Biomolecular AI","Deep Learning"],"permalink":"/ai/review/2025-10-31-EnzyControl_Adding_Functional_and_Substrate-Specific_Control_for_Enzyme_Backbone_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Emu3.5_Native_Multimodal_Models_are_World_Learners","title":"[논문리뷰] Emu3.5: Native Multimodal Models are World Learners","excerpt":"이 [arXiv]에 게시한 'Emu3.5: Native Multimodal Models are World Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Multimodal Model","World Model","Vision-Language","Next-Token Prediction","Reinforcement Learning","Discrete Diffusion Adaptation","Image Generation","Any-to-Image"],"permalink":"/ai/review/2025-10-31-Emu3.5_Native_Multimodal_Models_are_World_Learners/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-EHR-R1_A_Reasoning-Enhanced_Foundational_Language_Model_for_Electronic_Health_Record_Analysis","title":"[논문리뷰] EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis","excerpt":"이 [arXiv]에 게시한 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Electronic Health Records","Large Language Models","Reasoning Enhancement","Instruction Tuning","Reinforcement Learning","Data Synthesis","Medical AI","Clinical Decision Support"],"permalink":"/ai/review/2025-10-31-EHR-R1_A_Reasoning-Enhanced_Foundational_Language_Model_for_Electronic_Health_Record_Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Counteracting_Matthew_Effect_in_Self-Improvement_of_LVLMs_through_Head-Tail_Re-balancing","title":"[논문리뷰] Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing","excerpt":"Xiaowei Shi이 [arXiv]에 게시한 'Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","LVLMs","Self-Improvement","Matthew Effect","Data Bias Mitigation","Distribution Reshaping","Trajectory Resampling","Visual Reasoning"],"permalink":"/ai/review/2025-10-31-Counteracting_Matthew_Effect_in_Self-Improvement_of_LVLMs_through_Head-Tail_Re-balancing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-CityRiSE_Reasoning_Urban_Socio-Economic_Status_in_Vision-Language_Models_via_Reinforcement_Learning","title":"[논문리뷰] CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning","excerpt":"Yong Li이 [arXiv]에 게시한 'CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Urban Sensing","Socio-Economic Status","Vision-Language Models","Reinforcement Learning","Generalization","Interpretability","Multi-modal Data"],"permalink":"/ai/review/2025-10-31-CityRiSE_Reasoning_Urban_Socio-Economic_Status_in_Vision-Language_Models_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-ChartAB_A_Benchmark_for_Chart_Grounding_Dense_Alignment","title":"[논문리뷰] ChartAB: A Benchmark for Chart Grounding & Dense Alignment","excerpt":"이 [arXiv]에 게시한 'ChartAB: A Benchmark for Chart Grounding & Dense Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Chart Understanding","Visual Grounding","Dense Alignment","Benchmark","Robustness","Multimodal Learning"],"permalink":"/ai/review/2025-10-31-ChartAB_A_Benchmark_for_Chart_Grounding_Dense_Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Can_Agent_Conquer_Web_Exploring_the_Frontiers_of_ChatGPT_Atlas_Agent_in_Web_Games","title":"[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games","excerpt":"Justin Cui이 [arXiv]에 게시한 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Web Agent","Large Language Models","Multimodal AI","Browser Automation","Game AI","ChatGPT Atlas","Performance Evaluation","Human-Computer Interaction"],"permalink":"/ai/review/2025-10-31-Can_Agent_Conquer_Web_Exploring_the_Frontiers_of_ChatGPT_Atlas_Agent_in_Web_Games/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-CRAG-MM_Multi-modal_Multi-turn_Comprehensive_RAG_Benchmark","title":"[논문리뷰] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark","excerpt":"이 [arXiv]에 게시한 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Multi-modal RAG","Benchmark","Wearable AI","Multi-turn Conversation","Egocentric Images","Knowledge Graph","Web Search","Hallucination"],"permalink":"/ai/review/2025-10-31-CRAG-MM_Multi-modal_Multi-turn_Comprehensive_RAG_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-CLASS-IT_Conversational_and_Lecture-Aligned_Small-Scale_Instruction_Tuning_for_BabyLMs","title":"[논문리뷰] CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs","excerpt":"이 [arXiv]에 게시한 'CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Instruction Tuning","BabyLMs","Small-scale LMs","Curriculum Learning","Conversational AI","Question Answering","Zero-shot Evaluation","SuperGLUE"],"permalink":"/ai/review/2025-10-31-CLASS-IT_Conversational_and_Lecture-Aligned_Small-Scale_Instruction_Tuning_for_BabyLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-Are_Video_Models_Ready_as_Zero-Shot_Reasoners_An_Empirical_Study_with_the_MME-CoF_Benchmark","title":"[논문리뷰] Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark","excerpt":"이 [arXiv]에 게시한 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","Video Generation Models","Zero-Shot Reasoning","Visual Reasoning","MME-COF Benchmark","Chain-of-Frame Reasoning","Temporal Coherence","Spatial Reasoning"],"permalink":"/ai/review/2025-10-31-Are_Video_Models_Ready_as_Zero-Shot_Reasoners_An_Empirical_Study_with_the_MME-CoF_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-31-AMO-Bench_Large_Language_Models_Still_Struggle_in_High_School_Math_Competitions","title":"[논문리뷰] AMO-Bench: Large Language Models Still Struggle in High School Math Competitions","excerpt":"이 [arXiv]에 게시한 'AMO-Bench: Large Language Models Still Struggle in High School Math Competitions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","lastModifiedAt":"2025-10-31 18:37:31+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Mathematical Reasoning","Olympiad-level Math","Benchmark","Performance Saturation","Test-time Scaling","AMO-Bench"],"permalink":"/ai/review/2025-10-31-AMO-Bench_Large_Language_Models_Still_Struggle_in_High_School_Math_Competitions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Video-Thinker_Sparking_Thinking_with_Videos_via_Reinforcement_Learning","title":"[논문리뷰] Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning","excerpt":"Runhao Fu이 [arXiv]에 게시한 'Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Video Reasoning","Multimodal Large Language Models","Reinforcement Learning","Chain-of-Thought","Video Understanding","Temporal Grounding","Video Captioning","Autonomous Tool Use"],"permalink":"/ai/review/2025-10-30-Video-Thinker_Sparking_Thinking_with_Videos_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-VFXMaster_Unlocking_Dynamic_Visual_Effect_Generation_via_In-Context_Learning","title":"[논문리뷰] VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning","excerpt":"Xiaoyu Shi이 [arXiv]에 게시한 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","VFX Generation","In-Context Learning","Diffusion Models","Video Generation","Generalization","Attention Mask","One-Shot Adaptation"],"permalink":"/ai/review/2025-10-30-VFXMaster_Unlocking_Dynamic_Visual_Effect_Generation_via_In-Context_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-TheraMind_A_Strategic_and_Adaptive_Agent_for_Longitudinal_Psychological_Counseling","title":"[논문리뷰] TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling","excerpt":"Zheng Zhang이 [arXiv]에 게시한 'TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Longitudinal Counseling","Adaptive Agent","Dual-Loop Architecture","Large Language Models","Psychotherapy","Mental Health AI","Dialogue Management"],"permalink":"/ai/review/2025-10-30-TheraMind_A_Strategic_and_Adaptive_Agent_for_Longitudinal_Psychological_Counseling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-The_Tool_Decathlon_Benchmarking_Language_Agents_for_Diverse_Realistic_and_Long-Horizon_Task_Execution","title":"[논문리뷰] The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution","excerpt":"Haoze Wu이 [arXiv]에 게시한 'The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Language Agents","Tool Use","Benchmarking","Long-Horizon Tasks","Realistic Environments","Multi-Application","Execution-Based Evaluation","Model Context Protocol (MCP)"],"permalink":"/ai/review/2025-10-30-The_Tool_Decathlon_Benchmarking_Language_Agents_for_Diverse_Realistic_and_Long-Horizon_Task_Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-The_Principles_of_Diffusion_Models","title":"[논문리뷰] The Principles of Diffusion Models","excerpt":"Stefano Ermon이 [arXiv]에 게시한 'The Principles of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Diffusion Models","Generative AI","Variational Autoencoder","Energy-Based Models","Normalizing Flows","Score-Based SDEs","Flow Matching","Fokker-Planck Equation"],"permalink":"/ai/review/2025-10-30-The_Principles_of_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-SeeingEye_Agentic_Information_Flow_Unlocks_Multimodal_Reasoning_In_Text-only_LLMs","title":"[논문리뷰] SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs","excerpt":"Jiaxuan You이 [arXiv]에 게시한 'SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Text-only LLM","Agentic AI","Information Flow","VQA","Structured Intermediate Representation","Decoupled Architecture","Tool Use"],"permalink":"/ai/review/2025-10-30-SeeingEye_Agentic_Information_Flow_Unlocks_Multimodal_Reasoning_In_Text-only_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Scaling_Latent_Reasoning_via_Looped_Language_Models","title":"[논문리뷰] Scaling Latent Reasoning via Looped Language Models","excerpt":"이 [arXiv]에 게시한 'Scaling Latent Reasoning via Looped Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Looped Language Models","Latent Reasoning","Parameter Efficiency","Adaptive Computation","Pre-training Scaling","Knowledge Manipulation","Early Exit Mechanisms","Transformer Architecture"],"permalink":"/ai/review/2025-10-30-Scaling_Latent_Reasoning_via_Looped_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Rethinking_Driving_World_Model_as_Synthetic_Data_Generator_for_Perception_Tasks","title":"[논문리뷰] Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks","excerpt":"이 [arXiv]에 게시한 'Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Synthetic Data Generation","Autonomous Driving","Perception Tasks","Diffusion Models","3D Asset Editing","World Model","Data Augmentation","nuScenes"],"permalink":"/ai/review/2025-10-30-Rethinking_Driving_World_Model_as_Synthetic_Data_Generator_for_Perception_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-RegionE_Adaptive_Region-Aware_Generation_for_Efficient_Image_Editing","title":"[논문리뷰] RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","excerpt":"Peng Ye이 [arXiv]에 게시한 'RegionE: Adaptive Region-Aware Generation for Efficient Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Instruction-based Image Editing","Diffusion Models","Efficient Inference","Region-Aware Generation","Adaptive Caching","Spatial Redundancy","Temporal Redundancy"],"permalink":"/ai/review/2025-10-30-RegionE_Adaptive_Region-Aware_Generation_for_Efficient_Image_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Reasoning-Aware_GRPO_using_Process_Mining","title":"[논문리뷰] Reasoning-Aware GRPO using Process Mining","excerpt":"이 [arXiv]에 게시한 'Reasoning-Aware GRPO using Process Mining' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Process Mining","Policy Optimization","Mathematical Reasoning","GRPO","PM4GRPO"],"permalink":"/ai/review/2025-10-30-Reasoning-Aware_GRPO_using_Process_Mining/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-ReForm_Reflective_Autoformalization_with_Prospective_Bounded_Sequence_Optimization","title":"[논문리뷰] ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization","excerpt":"Ruihua Song이 [arXiv]에 게시한 'ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Autoformalization","Large Language Models","Reinforcement Learning","Self-Reflection","Semantic Consistency","Formal Mathematical Reasoning","Sequence Optimization"],"permalink":"/ai/review/2025-10-30-ReForm_Reflective_Autoformalization_with_Prospective_Bounded_Sequence_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Parallel_Loop_Transformer_for_Efficient_Test-Time_Computation_Scaling","title":"[논문리뷰] Parallel Loop Transformer for Efficient Test-Time Computation Scaling","excerpt":"이 [arXiv]에 게시한 'Parallel Loop Transformer for Efficient Test-Time Computation Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Large Language Models","Looped Transformers","Inference Efficiency","Parallel Computation","KV Cache Optimization","Gated Sliding-Window Attention","Cross-Loop Parallelism"],"permalink":"/ai/review/2025-10-30-Parallel_Loop_Transformer_for_Efficient_Test-Time_Computation_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-PairUni_Pairwise_Training_for_Unified_Multimodal_Language_Models","title":"[논문리뷰] PairUni: Pairwise Training for Unified Multimodal Language Models","excerpt":"이 [arXiv]에 게시한 'PairUni: Pairwise Training for Unified Multimodal Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Unified Vision-Language Models","Reinforcement Learning","Multimodal Alignment","Pairwise Training","Group Relative Policy Optimization","Data Augmentation","Text-to-Image Generation","Visual Reasoning"],"permalink":"/ai/review/2025-10-30-PairUni_Pairwise_Training_for_Unified_Multimodal_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-ODesign_A_World_Model_for_Biomolecular_Interaction_Design","title":"[논문리뷰] ODesign: A World Model for Biomolecular Interaction Design","excerpt":"Qinghan Wang이 [arXiv]에 게시한 'ODesign: A World Model for Biomolecular Interaction Design' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Biomolecular Interaction Design","Generative AI","World Model","Multimodal Molecular Design","All-atom Generation","Diffusion Models","Protein Design","Nucleic Acid Design"],"permalink":"/ai/review/2025-10-30-ODesign_A_World_Model_for_Biomolecular_Interaction_Design/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Multimodal_Spatial_Reasoning_in_the_Large_Model_Era_A_Survey_and_Benchmarks","title":"[논문리뷰] Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks","excerpt":"이 [arXiv]에 게시한 'Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Spatial Reasoning","Survey","Benchmarks","3D Vision","Embodied AI","Vision-Language Navigation"],"permalink":"/ai/review/2025-10-30-Multimodal_Spatial_Reasoning_in_the_Large_Model_Era_A_Survey_and_Benchmarks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Ming-Flash-Omni_A_Sparse_Unified_Architecture_for_Multimodal_Perception_and_Generation","title":"[논문리뷰] Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation","excerpt":"이 [arXiv]에 게시한 'Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal AI","Sparse MoE","Unified Architecture","Perception","Generation","Contextual ASR","Image Editing","Generative Segmentation"],"permalink":"/ai/review/2025-10-30-Ming-Flash-Omni_A_Sparse_Unified_Architecture_for_Multimodal_Perception_and_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-MASPRM_Multi-Agent_System_Process_Reward_Model","title":"[논문리뷰] MASPRM: Multi-Agent System Process Reward Model","excerpt":"Ying Xiong이 [arXiv]에 게시한 'MASPRM: Multi-Agent System Process Reward Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Process Reward Model","MCTS","Inference-time Search","LLM Agents","Zero-shot Transfer","Reinforcement Learning","Compute-Aware Reasoning"],"permalink":"/ai/review/2025-10-30-MASPRM_Multi-Agent_System_Process_Reward_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-JanusCoder_Towards_a_Foundational_Visual-Programmatic_Interface_for_Code_Intelligence","title":"[논문리뷰] JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence","excerpt":"이 [arXiv]에 게시한 'JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Multimodal Code Intelligence","Visual-Programmatic Interface","Code Generation","Data Synthesis","Large Language Models","Visualizations","Web UI","Animation"],"permalink":"/ai/review/2025-10-30-JanusCoder_Towards_a_Foundational_Visual-Programmatic_Interface_for_Code_Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Gaperon_A_Peppered_English-French_Generative_Language_Model_Suite","title":"[논문리뷰] Gaperon: A Peppered English-French Generative Language Model Suite","excerpt":"Éric de la Clergerie이 [arXiv]에 게시한 'Gaperon: A Peppered English-French Generative Language Model Suite' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Bilingual LLMs","Data Curation","Benchmark Contamination","Data Poisoning","Open Science","Reproducibility","Generative Models","French-English"],"permalink":"/ai/review/2025-10-30-Gaperon_A_Peppered_English-French_Generative_Language_Model_Suite/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Fortytwo_Swarm_Inference_with_Peer-Ranked_Consensus","title":"[논문리뷰] Fortytwo: Swarm Inference with Peer-Ranked Consensus","excerpt":"이 [arXiv]에 게시한 'Fortytwo: Swarm Inference with Peer-Ranked Consensus' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Decentralized AI","Swarm Intelligence","AI Inference","Consensus Mechanism","Peer-Ranking","Bradley-Terry Model","Reputation System","Sybil Defense"],"permalink":"/ai/review/2025-10-30-Fortytwo_Swarm_Inference_with_Peer-Ranked_Consensus/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-FAPO_Flawed-Aware_Policy_Optimization_for_Efficient_and_Reliable_Reasoning","title":"[논문리뷰] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning","excerpt":"Xin Liu이 [arXiv]에 게시한 'FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reasoning","Policy Optimization","Reward Modeling","Flawed Reasoning","Reliable AI","Error Detection"],"permalink":"/ai/review/2025-10-30-FAPO_Flawed-Aware_Policy_Optimization_for_Efficient_and_Reliable_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-Evolving_Diagnostic_Agents_in_a_Virtual_Clinical_Environment","title":"[논문리뷰] Evolving Diagnostic Agents in a Virtual Clinical Environment","excerpt":"이 [arXiv]에 게시한 'Evolving Diagnostic Agents in a Virtual Clinical Environment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Diagnostic Agents","Reinforcement Learning (RL)","Virtual Clinical Environment","Medical AI","Multi-turn Diagnosis","EHR (Electronic Health Records)"],"permalink":"/ai/review/2025-10-30-Evolving_Diagnostic_Agents_in_a_Virtual_Clinical_Environment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-ChronoPlay_A_Framework_for_Modeling_Dual_Dynamics_and_Authenticity_in_Game_RAG_Benchmarks","title":"[논문리뷰] ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks","excerpt":"이 [arXiv]에 게시한 'ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Retrieval Augmented Generation (RAG)","Dynamic Benchmarks","Game AI","User Interest Drift","Knowledge Evolution","Automated Benchmark Generation","Authenticity","Large Language Models (LLMs)"],"permalink":"/ai/review/2025-10-30-ChronoPlay_A_Framework_for_Modeling_Dual_Dynamics_and_Authenticity_in_Game_RAG_Benchmarks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-30-BhashaBench_V1_A_Comprehensive_Benchmark_for_the_Quadrant_of_Indic_Domains","title":"[논문리뷰] BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains","excerpt":"이 [arXiv]에 게시한 'BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","lastModifiedAt":"2025-10-30 13:06:06+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Benchmark","Indic Languages","Multilingual Evaluation","Domain-Specific AI","India-centric Knowledge Systems","Zero-Shot Learning","Question Answering"],"permalink":"/ai/review/2025-10-30-BhashaBench_V1_A_Comprehensive_Benchmark_for_the_Quadrant_of_Indic_Domains/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-WebLeaper_Empowering_Efficiency_and_Efficacy_in_WebAgent_via_Enabling_Info-Rich_Seeking","title":"[논문리뷰] WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking","excerpt":"이 [arXiv]에 게시한 'WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","LLM-based Agents","Information Seeking","Search Efficiency","Task Synthesis","Reinforcement Learning","Tree-structured Reasoning","WebAgent"],"permalink":"/ai/review/2025-10-29-WebLeaper_Empowering_Efficiency_and_Efficacy_in_WebAgent_via_Enabling_Info-Rich_Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-VisJudge-Bench_Aesthetics_and_Quality_Assessment_of_Visualizations","title":"[논문리뷰] VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations","excerpt":"Jiayi Zhang이 [arXiv]에 게시한 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Visualization Quality Assessment","MLLMs","Benchmark","Aesthetics","Fidelity","Expressiveness","Fine-tuning","Reinforcement Learning"],"permalink":"/ai/review/2025-10-29-VisJudge-Bench_Aesthetics_and_Quality_Assessment_of_Visualizations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-VisCoder2_Building_Multi-Language_Visualization_Coding_Agents","title":"[논문리뷰] VisCoder2: Building Multi-Language Visualization Coding Agents","excerpt":"이 [arXiv]에 게시한 'VisCoder2: Building Multi-Language Visualization Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multi-Language Visualization","Code Generation","Self-Debugging","Instruction Tuning","Large Language Models","Visualization Benchmark","Coding Agents","Code-Feedback"],"permalink":"/ai/review/2025-10-29-VisCoder2_Building_Multi-Language_Visualization_Coding_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-VL-SAE_Interpreting_and_Enhancing_Vision-Language_Alignment_with_a_Unified_Concept_Set","title":"[논문리뷰] VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set","excerpt":"이 [arXiv]에 게시한 'VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Model Interpretability","Sparse Autoencoder (SAE)","Multi-modal Alignment","Concept Learning","Hallucination Elimination","Zero-shot Classification"],"permalink":"/ai/review/2025-10-29-VL-SAE_Interpreting_and_Enhancing_Vision-Language_Alignment_with_a_Unified_Concept_Set/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Uniform_Discrete_Diffusion_with_Metric_Path_for_Video_Generation","title":"[논문리뷰] Uniform Discrete Diffusion with Metric Path for Video Generation","excerpt":"이 [arXiv]에 게시한 'Uniform Discrete Diffusion with Metric Path for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Discrete Diffusion","Video Generation","Metric Path","Long Video Generation","Asynchronous Scheduling","Text-to-Video","Multimodal Generation"],"permalink":"/ai/review/2025-10-29-Uniform_Discrete_Diffusion_with_Metric_Path_for_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-UltraHR-100K_Enhancing_UHR_Image_Synthesis_with_A_Large-Scale_High-Quality_Dataset","title":"[논문리뷰] UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset","excerpt":"이 [arXiv]에 게시한 'UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Ultra-High-Resolution","Text-to-Image Generation","Diffusion Models","Large-Scale Dataset","Frequency-Aware Training","Detail Enhancement","Image Synthesis"],"permalink":"/ai/review/2025-10-29-UltraHR-100K_Enhancing_UHR_Image_Synthesis_with_A_Large-Scale_High-Quality_Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Tongyi_DeepResearch_Technical_Report","title":"[논문리뷰] Tongyi DeepResearch Technical Report","excerpt":"이 [arXiv]에 게시한 'Tongyi DeepResearch Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Agentic LLM","Deep Research","Information Seeking","Reinforcement Learning","Synthetic Data","Context Management","Tool Use","Open-source AI"],"permalink":"/ai/review/2025-10-29-Tongyi_DeepResearch_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-STAR-Bench_Probing_Deep_Spatio-Temporal_Reasoning_as_Audio_4D_Intelligence","title":"[논문리뷰] STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence","excerpt":"이 [arXiv]에 게시한 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Audio Intelligence","Spatio-Temporal Reasoning","4D Audio","Benchmark","Large Audio-Language Models","Perceptual Reasoning","Multimodal LLMs"],"permalink":"/ai/review/2025-10-29-STAR-Bench_Probing_Deep_Spatio-Temporal_Reasoning_as_Audio_4D_Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Routing_Matters_in_MoE_Scaling_Diffusion_Transformers_with_Explicit_Routing_Guidance","title":"[논문리뷰] Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance","excerpt":"이 [arXiv]에 게시한 'Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Diffusion Transformers (DiTs)","Routing Guidance","Semantic Specialization","Contrastive Learning","Image Generation","Flow Matching"],"permalink":"/ai/review/2025-10-29-Routing_Matters_in_MoE_Scaling_Diffusion_Transformers_with_Explicit_Routing_Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-RoboOmni_Proactive_Robot_Manipulation_in_Omni-modal_Context","title":"[논문리뷰] RoboOmni: Proactive Robot Manipulation in Omni-modal Context","excerpt":"이 [arXiv]에 게시한 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Robotic Manipulation","Multimodal LLMs","Vision-Language-Action","Proactive AI","Omni-modal Learning","Intent Recognition","Contextual Instructions"],"permalink":"/ai/review/2025-10-29-RoboOmni_Proactive_Robot_Manipulation_in_Omni-modal_Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Rethinking_Visual_Intelligence_Insights_from_Video_Pretraining","title":"[논문리뷰] Rethinking Visual Intelligence: Insights from Video Pretraining","excerpt":"Ahmad Rahimi이 [arXiv]에 게시한 'Rethinking Visual Intelligence: Insights from Video Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Video Diffusion Models","Visual Intelligence","Pretraining","Foundation Models","Low-resource Learning","Inductive Biases","Visual Reasoning","Image-to-Image Tasks"],"permalink":"/ai/review/2025-10-29-Rethinking_Visual_Intelligence_Insights_from_Video_Pretraining/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Repurposing_Synthetic_Data_for_Fine-grained_Search_Agent_Supervision","title":"[논문리뷰] Repurposing Synthetic Data for Fine-grained Search Agent Supervision","excerpt":"이 [arXiv]에 게시한 'Repurposing Synthetic Data for Fine-grained Search Agent Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Search Agents","LLM","Reinforcement Learning","Synthetic Data","Reward Shaping","Entity-aware Reward","Policy Optimization","Knowledge-intensive Tasks"],"permalink":"/ai/review/2025-10-29-Repurposing_Synthetic_Data_for_Fine-grained_Search_Agent_Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-ReplicationBench_Can_AI_Agents_Replicate_Astrophysics_Research_Papers","title":"[논문리뷰] ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?","excerpt":"Ian L. V. Roque이 [arXiv]에 게시한 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","AI Agents","Astrophysics Research","Reproducibility Benchmark","Large Language Models","Scientific Workflow","Code Execution","Evaluation Framework"],"permalink":"/ai/review/2025-10-29-ReplicationBench_Can_AI_Agents_Replicate_Astrophysics_Research_Papers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-PatenTEB_A_Comprehensive_Benchmark_and_Model_Family_for_Patent_Text_Embedding","title":"[논문리뷰] PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding","excerpt":"Denis Cavallucci이 [arXiv]에 게시한 'PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Patent Text Embedding","Benchmark","Multi-task Learning","Patent Retrieval","Sentence Embeddings","Knowledge Distillation","Cross-Domain Retrieval","Prompt Engineering"],"permalink":"/ai/review/2025-10-29-PatenTEB_A_Comprehensive_Benchmark_and_Model_Family_for_Patent_Text_Embedding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-PartNeXt_A_Next-Generation_Dataset_for_Fine-Grained_and_Hierarchical_3D_Part_Understanding","title":"[논문리뷰] PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding","excerpt":"Lan Xu이 [arXiv]에 게시한 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","3D Part Segmentation","3D Dataset","Hierarchical Annotation","Fine-Grained Segmentation","Textured Meshes","3D Part Understanding","Part-Centric Question Answering","Crowdsourcing"],"permalink":"/ai/review/2025-10-29-PartNeXt_A_Next-Generation_Dataset_for_Fine-Grained_and_Hierarchical_3D_Part_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-ParallelMuse_Agentic_Parallel_Thinking_for_Deep_Information_Seeking","title":"[논문리뷰] ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking","excerpt":"이 [arXiv]에 게시한 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Agentic AI","Parallel Thinking","Information Seeking","LLM Agents","Context Window Optimization","Exploration Efficiency","Reasoning Aggregation","Tool Use"],"permalink":"/ai/review/2025-10-29-ParallelMuse_Agentic_Parallel_Thinking_for_Deep_Information_Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-OSWorld-MCP_Benchmarking_MCP_Tool_Invocation_In_Computer-Use_Agents","title":"[논문리뷰] OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents","excerpt":"이 [arXiv]에 게시한 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multimodal Agents","Tool Invocation","Benchmark","Model Context Protocol (MCP)","GUI Automation","Computer-Use Agents","Evaluation Metrics"],"permalink":"/ai/review/2025-10-29-OSWorld-MCP_Benchmarking_MCP_Tool_Invocation_In_Computer-Use_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Latent_Sketchpad_Sketching_Visual_Thoughts_to_Elicit_Multimodal_Reasoning_in_MLLMs","title":"[논문리뷰] Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs","excerpt":"이 [arXiv]에 게시한 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Visual Reasoning","Latent Space","Sketch Generation","Visual Thinking","Autoregressive Generation","Interpretability"],"permalink":"/ai/review/2025-10-29-Latent_Sketchpad_Sketching_Visual_Thoughts_to_Elicit_Multimodal_Reasoning_in_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-InteractComp_Evaluating_Search_Agents_With_Ambiguous_Queries","title":"[논문리뷰] InteractComp: Evaluating Search Agents With Ambiguous Queries","excerpt":"Yani Fan이 [arXiv]에 게시한 'InteractComp: Evaluating Search Agents With Ambiguous Queries' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Search Agents","Interactive AI","Ambiguous Queries","Benchmarking","Language Agents","Information Retrieval","Overconfidence","Reinforcement Learning"],"permalink":"/ai/review/2025-10-29-InteractComp_Evaluating_Search_Agents_With_Ambiguous_Queries/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Group_Relative_Attention_Guidance_for_Image_Editing","title":"[논문리뷰] Group Relative Attention Guidance for Image Editing","excerpt":"이 [arXiv]에 게시한 'Group Relative Attention Guidance for Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Transformers","Attention Mechanism","Guidance Mechanism","Controllability","Fine-grained Control","GRAG"],"permalink":"/ai/review/2025-10-29-Group_Relative_Attention_Guidance_for_Image_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Generalization_or_Memorization_Dynamic_Decoding_for_Mode_Steering","title":"[논문리뷰] Generalization or Memorization: Dynamic Decoding for Mode Steering","excerpt":"이 [arXiv]에 게시한 'Generalization or Memorization: Dynamic Decoding for Mode Steering' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Generalization","Memorization","Information Bottleneck (IB)","Activation Steering","Decoding Strategy","Causal Intervention","LLM Reliability"],"permalink":"/ai/review/2025-10-29-Generalization_or_Memorization_Dynamic_Decoding_for_Mode_Steering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Game-TARS_Pretrained_Foundation_Models_for_Scalable_Generalist_Multimodal_Game_Agents","title":"[논문리뷰] Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents","excerpt":"이 [arXiv]에 게시한 'Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Generalist AI","Game Agents","Multimodal Learning","Foundation Models","ReAct","Sparse Thinking","Continual Pre-training","Human-Native Interaction"],"permalink":"/ai/review/2025-10-29-Game-TARS_Pretrained_Foundation_Models_for_Scalable_Generalist_Multimodal_Game_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-FunReason-MT_Technical_Report_Overcoming_the_Complexity_Barrier_in_Multi-Turn_Function_Calling","title":"[논문리뷰] FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling","excerpt":"이 [arXiv]에 게시한 'FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Function Calling","Multi-Turn Interaction","Large Language Models (LLMs)","Data Synthesis","Agentic AI","Tool Use","Chain-of-Thought (CoT)","Reinforcement Learning"],"permalink":"/ai/review/2025-10-29-FunReason-MT_Technical_Report_Overcoming_the_Complexity_Barrier_in_Multi-Turn_Function_Calling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-From_Spatial_to_Actions_Grounding_Vision-Language-Action_Model_in_Spatial_Foundation_Priors","title":"[논문리뷰] From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors","excerpt":"이 [arXiv]에 게시한 'From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","3D Spatial Reasoning","Embodied AI","Foundation Models","Multimodal Fusion","Robot Manipulation","Modality Transferability","Action Grounding"],"permalink":"/ai/review/2025-10-29-From_Spatial_to_Actions_Grounding_Vision-Language-Action_Model_in_Spatial_Foundation_Priors/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-Critique-RL_Training_Language_Models_for_Critiquing_through_Two-Stage_Reinforcement_Learning","title":"[논문리뷰] Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Language Models","Critiquing","Two-Stage Optimization","Actor-Critic","Scalable Oversight","Discriminability","Helpfulness"],"permalink":"/ai/review/2025-10-29-Critique-RL_Training_Language_Models_for_Critiquing_through_Two-Stage_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-AgentFrontier_Expanding_the_Capability_Frontier_of_LLM_Agents_with_ZPD-Guided_Data_Synthesis","title":"[논문리뷰] AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis","excerpt":"이 [arXiv]에 게시한 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","LLM Agents","Data Synthesis","Zone of Proximal Development (ZPD)","Complex Reasoning","Tool Use","Automated Benchmarking","Agentic AI","Rejection Sampling Fine-Tuning"],"permalink":"/ai/review/2025-10-29-AgentFrontier_Expanding_the_Capability_Frontier_of_LLM_Agents_with_ZPD-Guided_Data_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-AgentFold_Long-Horizon_Web_Agents_with_Proactive_Context_Management","title":"[논문리뷰] AgentFold: Long-Horizon Web Agents with Proactive Context Management","excerpt":"이 [arXiv]에 게시한 'AgentFold: Long-Horizon Web Agents with Proactive Context Management' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Web Agents","Context Management","Long-Horizon Tasks","LLM","Deep Consolidation","Granular Condensation","ReAct Paradigm"],"permalink":"/ai/review/2025-10-29-AgentFold_Long-Horizon_Web_Agents_with_Proactive_Context_Management/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-29-ATLAS_Adaptive_Transfer_Scaling_Laws_for_Multilingual_Pretraining_Finetuning_and_Decoding_the_Curse_of_Multilinguality","title":"[논문리뷰] ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality","excerpt":"이 [arXiv]에 게시한 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","lastModifiedAt":"2025-10-29 13:11:02+0900","categories":["Review"],"tags":["Review","Multilingual LLMs","Scaling Laws","Transfer Learning","Curse of Multilinguality","Pretraining","Finetuning","Language Models","Adaptive Scaling"],"permalink":"/ai/review/2025-10-29-ATLAS_Adaptive_Transfer_Scaling_Laws_for_Multilingual_Pretraining_Finetuning_and_Decoding_the_Curse_of_Multilinguality/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-VoMP_Predicting_Volumetric_Mechanical_Property_Fields","title":"[논문리뷰] VoMP: Predicting Volumetric Mechanical Property Fields","excerpt":"이 [arXiv]에 게시한 'VoMP: Predicting Volumetric Mechanical Property Fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Volumetric Properties","Mechanical Simulation","Material Prediction","3D Representation","Physics-based AI","Variational Autoencoder","Geometry Transformer","Gaussian Splats"],"permalink":"/ai/review/2025-10-28-VoMP_Predicting_Volumetric_Mechanical_Property_Fields/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-VITA-E_Natural_Embodied_Interaction_with_Concurrent_Seeing_Hearing_Speaking_and_Acting","title":"[논문리뷰] VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting","excerpt":"Haihan Gao이 [arXiv]에 게시한 'VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Embodied AI","Human-Robot Interaction","Vision-Language Models","Concurrency","Interruption","Robotics Control","Dual-Model Architecture","Special Tokens"],"permalink":"/ai/review/2025-10-28-VITA-E_Natural_Embodied_Interaction_with_Concurrent_Seeing_Hearing_Speaking_and_Acting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Track_Inpaint_Resplat_Subject-driven_3D_and_4D_Generation_with_Progressive_Texture_Infilling","title":"[논문리뷰] Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling","excerpt":"Igor Gilitschenski이 [arXiv]에 게시한 'Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Subject-driven 3D/4D Generation","Texture Infilling","Video Tracking","Image Inpainting","Multi-view Consistency","Identity Preservation","Generative Models","3D Gaussians"],"permalink":"/ai/review/2025-10-28-Track_Inpaint_Resplat_Subject-driven_3D_and_4D_Generation_with_Progressive_Texture_Infilling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-The_Best_of_N_Worlds_Aligning_Reinforcement_Learning_with_Best-of-N_Sampling_via_maxk_Optimisation","title":"[논문리뷰] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation","excerpt":"이 [arXiv]에 게시한 'The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Best-of-N Sampling","Max@k Optimization","Policy Gradients","Off-policy Learning","Code Generation"],"permalink":"/ai/review/2025-10-28-The_Best_of_N_Worlds_Aligning_Reinforcement_Learning_with_Best-of-N_Sampling_via_maxk_Optimisation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-RobotArena_infty_Scalable_Robot_Benchmarking_via_Real-to-Sim_Translation","title":"[논문리뷰] RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation","excerpt":"Kuan-Hsun Tu이 [arXiv]에 게시한 'RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Robot Benchmarking","Real-to-Sim Translation","Vision-Language Models (VLMs)","Human Preference Learning","Domain Randomization","Robot Manipulation","Simulation Environments","Policy Evaluation"],"permalink":"/ai/review/2025-10-28-RobotArena_infty_Scalable_Robot_Benchmarking_via_Real-to-Sim_Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-ReCode_Unify_Plan_and_Action_for_Universal_Granularity_Control","title":"[논문리뷰] ReCode: Unify Plan and Action for Universal Granularity Control","excerpt":"Yifan Wu이 [arXiv]에 게시한 'ReCode: Unify Plan and Action for Universal Granularity Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","LLM Agents","Decision Granularity Control","Recursive Code Generation","Hierarchical Planning","Action Unification","Program Synthesis","Data Efficiency"],"permalink":"/ai/review/2025-10-28-ReCode_Unify_Plan_and_Action_for_Universal_Granularity_Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-PixelRefer_A_Unified_Framework_for_Spatio-Temporal_Object_Referring_with_Arbitrary_Granularity","title":"[논문리뷰] PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity","excerpt":"Kehan Li이 [arXiv]에 게시한 'PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","MLLM","Region-level Understanding","Object-centric Reasoning","Spatio-temporal Referring","Video Understanding","Scale-Adaptive Tokenizer","Efficiency","Instruction Tuning"],"permalink":"/ai/review/2025-10-28-PixelRefer_A_Unified_Framework_for_Spatio-Temporal_Object_Referring_with_Arbitrary_Granularity/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Omni-Reward_Towards_Generalist_Omni-Modal_Reward_Modeling_with_Free-Form_Preferences","title":"[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences","excerpt":"이 [arXiv]에 게시한 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Reward Modeling","Multimodal AI","Human Preferences","RLHF","Generalist AI","Benchmark","Dataset","Free-Form Preferences"],"permalink":"/ai/review/2025-10-28-Omni-Reward_Towards_Generalist_Omni-Modal_Reward_Modeling_with_Free-Form_Preferences/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Mitigating_Attention_Sinks_and_Massive_Activations_in_Audio-Visual_Speech_Recognition_with_LLMS","title":"[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS","excerpt":"이 [arXiv]에 게시한 'Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Audio-Visual Speech Recognition","Large Language Models","Attention Sinks","Massive Activations","Decorrelation Loss","Fine-tuning","Multimodal AI"],"permalink":"/ai/review/2025-10-28-Mitigating_Attention_Sinks_and_Massive_Activations_in_Audio-Visual_Speech_Recognition_with_LLMS/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Memory-based_Language_Models_An_Efficient_Explainable_and_Eco-friendly_Approach_to_Large_Language_Modeling","title":"[논문리뷰] Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling","excerpt":"이 [arXiv]에 게시한 'Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Memory-based Language Model","k-Nearest Neighbor","Eco-friendly AI","Explainable AI","Next-token Prediction","Prefix Trie","Low-latency Inference","CPU-based AI"],"permalink":"/ai/review/2025-10-28-Memory-based_Language_Models_An_Efficient_Explainable_and_Eco-friendly_Approach_to_Large_Language_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-MARS-M_When_Variance_Reduction_Meets_Matrices","title":"[논문리뷰] MARS-M: When Variance Reduction Meets Matrices","excerpt":"이 [arXiv]에 게시한 'MARS-M: When Variance Reduction Meets Matrices' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Variance Reduction","Matrix-based Optimizer","LLM Training","Deep Learning Optimization","Moonlight","MARS-M","Stochastic Gradient Descent"],"permalink":"/ai/review/2025-10-28-MARS-M_When_Variance_Reduction_Meets_Matrices/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Lookahead_Anchoring_Preserving_Character_Identity_in_Audio-Driven_Human_Animation","title":"[논문리뷰] Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation","excerpt":"Honglie Chen이 [arXiv]에 게시한 'Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Audio-driven Animation","Identity Preservation","Diffusion Transformers","Long-form Video Generation","Temporal Autoregression","Keyframe Anchoring","Self-keyframing"],"permalink":"/ai/review/2025-10-28-Lookahead_Anchoring_Preserving_Character_Identity_in_Audio-Driven_Human_Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-LongCat-Video_Technical_Report","title":"[논문리뷰] LongCat-Video Technical Report","excerpt":"Hongyu Li이 [arXiv]에 게시한 'LongCat-Video Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Transformer","RLHF","Sparse Attention","Long Video Generation","Coarse-to-Fine Generation","Multi-task Learning","World Models"],"permalink":"/ai/review/2025-10-28-LongCat-Video_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-LimRank_Less_is_More_for_Reasoning-Intensive_Information_Reranking","title":"[논문리뷰] LimRank: Less is More for Reasoning-Intensive Information Reranking","excerpt":"Arman Cohan이 [arXiv]에 게시한 'LimRank: Less is More for Reasoning-Intensive Information Reranking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Information Reranking","Large Language Models","Data Synthesis","Reasoning-Intensive Retrieval","Low-Resource Learning","Data Efficiency","Instruction Following"],"permalink":"/ai/review/2025-10-28-LimRank_Less_is_More_for_Reasoning-Intensive_Information_Reranking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-LightBagel_A_Light-weighted_Double_Fusion_Framework_for_Unified_Multimodal_Understanding_and_Generation","title":"[논문리뷰] LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation","excerpt":"Chaorui Deng이 [arXiv]에 게시한 'LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Double Fusion","Lightweight AI","Text-to-Image Generation","Image Editing","Model Architecture","Efficient Training","Cross-modal Interaction"],"permalink":"/ai/review/2025-10-28-LightBagel_A_Light-weighted_Double_Fusion_Framework_for_Unified_Multimodal_Understanding_and_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Language_Server_CLI_Empowers_Language_Agents_with_Process_Rewards","title":"[논문리뷰] Language Server CLI Empowers Language Agents with Process Rewards","excerpt":"Lanser Contributors이 [arXiv]에 게시한 'Language Server CLI Empowers Language Agents with Process Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Language Agents","Language Server Protocol (LSP)","CLI","Process Rewards","Code Refactoring","Static Analysis","Reinforcement Learning","Deterministic Execution"],"permalink":"/ai/review/2025-10-28-Language_Server_CLI_Empowers_Language_Agents_with_Process_Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Knocking-Heads_Attention","title":"[논문리뷰] Knocking-Heads Attention","excerpt":"Jianguo Li이 [arXiv]에 게시한 'Knocking-Heads Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Multi-Head Attention","Transformer","Large Language Models","Inter-Head Communication","Parameter Sharing","Training Stability","Diagonal Initialization"],"permalink":"/ai/review/2025-10-28-Knocking-Heads_Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-IGGT_Instance-Grounded_Geometry_Transformer_for_Semantic_3D_Reconstruction","title":"[논문리뷰] IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction","excerpt":"Fangzhou Hong이 [arXiv]에 게시한 'IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Semantic 3D Reconstruction","Instance Grounding","Geometry Transformer","Multi-view Consistency","Scene Understanding","InsScene-15K","Vision-Language Models","Cross-Modal Fusion"],"permalink":"/ai/review/2025-10-28-IGGT_Instance-Grounded_Geometry_Transformer_for_Semantic_3D_Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-FARMER_Flow_AutoRegressive_Transformer_over_Pixels","title":"[논문리뷰] FARMER: Flow AutoRegressive Transformer over Pixels","excerpt":"Zhijie Lin이 [arXiv]에 게시한 'FARMER: Flow AutoRegressive Transformer over Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Normalizing Flows","Autoregressive Models","Generative Models","Image Synthesis","Tractable Likelihood","Dimension Reduction","Distillation","Classifier-Free Guidance"],"permalink":"/ai/review/2025-10-28-FARMER_Flow_AutoRegressive_Transformer_over_Pixels/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-EchoDistill_Bidirectional_Concept_Distillation_for_One-Step_Diffusion_Personalization","title":"[논문리뷰] EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization","excerpt":"Yaxing Wang이 [arXiv]에 게시한 'EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Diffusion Models","One-Step Generation","Model Personalization","Knowledge Distillation","Bidirectional Learning","Text-to-Image Generation","Concept Learning"],"permalink":"/ai/review/2025-10-28-EchoDistill_Bidirectional_Concept_Distillation_for_One-Step_Diffusion_Personalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-E2Rank_Your_Text_Embedding_can_Also_be_an_Effective_and_Efficient_Listwise_Reranker","title":"[논문리뷰] E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker","excerpt":"이 [arXiv]에 게시한 'E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Text Embedding","Listwise Reranking","Information Retrieval","Pseudo Relevance Feedback","Contrastive Learning","Multi-task Learning","Efficiency","LLM-based Ranking"],"permalink":"/ai/review/2025-10-28-E2Rank_Your_Text_Embedding_can_Also_be_an_Effective_and_Efficient_Listwise_Reranker/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Distilled_Decoding_2_One-step_Sampling_of_Image_Auto-regressive_Models_with_Conditional_Score_Distillation","title":"[논문리뷰] Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation","excerpt":"Guohao Dai이 [arXiv]에 게시한 'Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Auto-regressive Models","Image Generation","One-step Sampling","Model Distillation","Conditional Score Distillation","Flow Matching","Generative Models"],"permalink":"/ai/review/2025-10-28-Distilled_Decoding_2_One-step_Sampling_of_Image_Auto-regressive_Models_with_Conditional_Score_Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-DiffusionLane_Diffusion_Model_for_Lane_Detection","title":"[논문리뷰] DiffusionLane: Diffusion Model for Lane Detection","excerpt":"이 [arXiv]에 게시한 'DiffusionLane: Diffusion Model for Lane Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Lane Detection","Diffusion Model","Denoising Diffusion","Hybrid Decoding","Anchor-based","Domain Adaptation","Computer Vision","Generative Models"],"permalink":"/ai/review/2025-10-28-DiffusionLane_Diffusion_Model_for_Lane_Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Concerto_Joint_2D-3D_Self-Supervised_Learning_Emerges_Spatial_Representations","title":"[논문리뷰] Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations","excerpt":"이 [arXiv]에 게시한 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Self-Supervised Learning","2D-3D Fusion","Spatial Representation","Point Cloud","Image Features","Multimodal Learning","Semantic Segmentation","LoRA"],"permalink":"/ai/review/2025-10-28-Concerto_Joint_2D-3D_Self-Supervised_Learning_Emerges_Spatial_Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-Code_Aesthetics_with_Agentic_Reward_Feedback","title":"[논문리뷰] Code Aesthetics with Agentic Reward Feedback","excerpt":"Yupan Huang이 [arXiv]에 게시한 'Code Aesthetics with Agentic Reward Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Code Aesthetics","Agentic Reward Feedback","Large Language Models","Reinforcement Learning","Instruction Tuning","Webpage Design","Multimodal Evaluation"],"permalink":"/ai/review/2025-10-28-Code_Aesthetics_with_Agentic_Reward_Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-A_Survey_of_Data_Agents_Emerging_Paradigm_or_Overstated_Hype","title":"[논문리뷰] A Survey of Data Agents: Emerging Paradigm or Overstated Hype?","excerpt":"Boyan Li이 [arXiv]에 게시한 'A Survey of Data Agents: Emerging Paradigm or Overstated Hype?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Data Agents","LLMs","Autonomy Levels","Hierarchical Taxonomy","SAE J3016","Data Management","Data Preparation","Data Analysis","Autonomous Orchestration"],"permalink":"/ai/review/2025-10-28-A_Survey_of_Data_Agents_Emerging_Paradigm_or_Overstated_Hype/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-28-ACG_Action_Coherence_Guidance_for_Flow-based_VLA_models","title":"[논문리뷰] ACG: Action Coherence Guidance for Flow-based VLA models","excerpt":"이 [arXiv]에 게시한 'ACG: Action Coherence Guidance for Flow-based VLA models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","lastModifiedAt":"2025-10-28 13:07:54+0900","categories":["Review"],"tags":["Review","Action Coherence","Flow Matching","VLA Models","Guidance","Robotics","Imitation Learning","Transformer","Self-Attention"],"permalink":"/ai/review/2025-10-28-ACG_Action_Coherence_Guidance_for_Flow-based_VLA_models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-WorldGrow_Generating_Infinite_3D_World","title":"[논문리뷰] WorldGrow: Generating Infinite 3D World","excerpt":"Jia Lu이 [arXiv]에 게시한 'WorldGrow: Generating Infinite 3D World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","3D World Generation","Infinite Scene Synthesis","Block-wise Generation","Coarse-to-Fine","3D Inpainting","Structured Latent Representation","Virtual Environments","World Models"],"permalink":"/ai/review/2025-10-27-WorldGrow_Generating_Infinite_3D_World/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Visual_Diffusion_Models_are_Geometric_Solvers","title":"[논문리뷰] Visual Diffusion Models are Geometric Solvers","excerpt":"Or Patashnik이 [arXiv]에 게시한 'Visual Diffusion Models are Geometric Solvers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Diffusion Models","Geometric Problem Solving","Inscribed Square Problem","Steiner Tree Problem","Maximum Area Polygonization","Image Generation","Pixel Space"],"permalink":"/ai/review/2025-10-27-Visual_Diffusion_Models_are_Geometric_Solvers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Video-As-Prompt_Unified_Semantic_Control_for_Video_Generation","title":"[논문리뷰] Video-As-Prompt: Unified Semantic Control for Video Generation","excerpt":"이 [arXiv]에 게시한 'Video-As-Prompt: Unified Semantic Control for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Video Generation","Semantic Control","Diffusion Transformers","In-Context Learning","Mixture-of-Transformers","Video-As-Prompt","Controllable Generation","Large-scale Dataset"],"permalink":"/ai/review/2025-10-27-Video-As-Prompt_Unified_Semantic_Control_for_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-UI-Ins_Enhancing_GUI_Grounding_with_Multi-Perspective_Instruction-as-Reasoning","title":"[논문리뷰] UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning","excerpt":"이 [arXiv]에 게시한 'UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","GUI Grounding","Natural Language Instructions","Multi-Perspective Reasoning","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Policy Collapse Mitigation","GUI Agents"],"permalink":"/ai/review/2025-10-27-UI-Ins_Enhancing_GUI_Grounding_with_Multi-Perspective_Instruction-as-Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Taming_Modality_Entanglement_in_Continual_Audio-Visual_Segmentation","title":"[논문리뷰] Taming Modality Entanglement in Continual Audio-Visual Segmentation","excerpt":"Zhaojin Fu이 [arXiv]에 게시한 'Taming Modality Entanglement in Continual Audio-Visual Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Continual Learning","Audio-Visual Segmentation","Modality Entanglement","Semantic Drift","Co-occurrence Confusion","Rehearsal Strategy","Sample Selection"],"permalink":"/ai/review/2025-10-27-Taming_Modality_Entanglement_in_Continual_Audio-Visual_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Stabilizing_MoE_Reinforcement_Learning_by_Aligning_Training_and_Inference_Routers","title":"[논문리뷰] Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers","excerpt":"이 [arXiv]에 게시한 'Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","MoE","Reinforcement Learning","Training Stability","Routing","Policy Alignment","Rollout Routing Replay","LLMs"],"permalink":"/ai/review/2025-10-27-Stabilizing_MoE_Reinforcement_Learning_by_Aligning_Training_and_Inference_Routers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Sparser_Block-Sparse_Attention_via_Token_Permutation","title":"[논문리뷰] Sparser Block-Sparse Attention via Token Permutation","excerpt":"이 [arXiv]에 게시한 'Sparser Block-Sparse Attention via Token Permutation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Self-Attention","Block-Sparse Attention","Token Permutation","Computational Efficiency","Prefilling","Long Context","Causal Attention"],"permalink":"/ai/review/2025-10-27-Sparser_Block-Sparse_Attention_via_Token_Permutation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Soft_Instruction_De-escalation_Defense","title":"[논문리뷰] Soft Instruction De-escalation Defense","excerpt":"이 [arXiv]에 게시한 'Soft Instruction De-escalation Defense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Prompt Injection","LLM Security","Agentic Systems","Iterative Sanitization","Instruction Control","Adversarial Robustness","Large Language Models"],"permalink":"/ai/review/2025-10-27-Soft_Instruction_De-escalation_Defense/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Sample_By_Step_Optimize_By_Chunk_Chunk-Level_GRPO_For_Text-to-Image_Generation","title":"[논문리뷰] Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation","excerpt":"이 [arXiv]에 게시한 'Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reinforcement Learning","GRPO","Flow Matching","Chunk-level Optimization","Temporal Dynamics","Diffusion Models"],"permalink":"/ai/review/2025-10-27-Sample_By_Step_Optimize_By_Chunk_Chunk-Level_GRPO_For_Text-to-Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Reasoning_with_Sampling_Your_Base_Model_is_Smarter_Than_You_Think","title":"[논문리뷰] Reasoning with Sampling: Your Base Model is Smarter Than You Think","excerpt":"이 [arXiv]에 게시한 'Reasoning with Sampling: Your Base Model is Smarter Than You Think' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","LLMs","MCMC","Sampling","Reasoning","Distribution Sharpening","Reinforcement Learning (RL)","Inference-time Optimization","Training-free"],"permalink":"/ai/review/2025-10-27-Reasoning_with_Sampling_Your_Base_Model_is_Smarter_Than_You_Think/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-RECALL_REpresentation-aligned_Catastrophic-forgetting_ALLeviation_via_Hierarchical_Model_Merging","title":"[논문리뷰] RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging","excerpt":"이 [arXiv]에 게시한 'RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Catastrophic Forgetting","Continual Learning","Model Merging","LLMs","Representation Learning","Data-free Learning","Hierarchical Parameter Fusion"],"permalink":"/ai/review/2025-10-27-RECALL_REpresentation-aligned_Catastrophic-forgetting_ALLeviation_via_Hierarchical_Model_Merging/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-RAPO_Cross-Stage_Prompt_Optimization_for_Text-to-Video_Generation_via_Data_Alignment_and_Test-Time_Scaling","title":"[논문리뷰] RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling","excerpt":"이 [arXiv]에 게시한 'RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Prompt Optimization","Large Language Models (LLM)","Test-Time Scaling","Retrieval-Augmented Generation","Diffusion Models","Data Alignment"],"permalink":"/ai/review/2025-10-27-RAPO_Cross-Stage_Prompt_Optimization_for_Text-to-Video_Generation_via_Data_Alignment_and_Test-Time_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-PhysWorld_From_Real_Videos_to_World_Models_of_Deformable_Objects_via_Physics-Aware_Demonstration_Synthesis","title":"[논문리뷰] PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis","excerpt":"Hui Li이 [arXiv]에 게시한 'PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","World Models","Deformable Objects","Physics Simulation","GNN","Digital Twin","Data Synthesis","Real-to-Sim","Physics-Aware Learning"],"permalink":"/ai/review/2025-10-27-PhysWorld_From_Real_Videos_to_World_Models_of_Deformable_Objects_via_Physics-Aware_Demonstration_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-PhysVLM-AVR_Active_Visual_Reasoning_for_Multimodal_Large_Language_Models_in_Physical_Environments","title":"[논문리뷰] PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments","excerpt":"Chaoyang Zhao이 [arXiv]에 게시한 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Active Visual Reasoning","MLLM","Physical Environments","Partially Observable","Markov Decision Process","Chain-of-Thought","Embodied AI","CLEVR-AVR"],"permalink":"/ai/review/2025-10-27-PhysVLM-AVR_Active_Visual_Reasoning_for_Multimodal_Large_Language_Models_in_Physical_Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Model_Merging_with_Functional_Dual_Anchors","title":"[논문리뷰] Model Merging with Functional Dual Anchors","excerpt":"이 [arXiv]에 게시한 'Model Merging with Functional Dual Anchors' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Model Merging","Functional Dual Anchors","Input-Representation Space","Task Vectors","Knowledge Integration","Foundation Models","Gradient Matching","Post-training Strategy"],"permalink":"/ai/review/2025-10-27-Model_Merging_with_Functional_Dual_Anchors/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Map_the_Flow_Revealing_Hidden_Pathways_of_Information_in_VideoLLMs","title":"[논문리뷰] Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs","excerpt":"Bohyung Han이 [arXiv]에 게시한 'Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Video Large Language Models","VideoQA","Mechanistic Interpretability","Attention Knockout","Temporal Reasoning","Information Flow","Model Interpretability","Logit Lens"],"permalink":"/ai/review/2025-10-27-Map_the_Flow_Revealing_Hidden_Pathways_of_Information_in_VideoLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-From_Denoising_to_Refining_A_Corrective_Framework_for_Vision-Language_Diffusion_Model","title":"[논문리뷰] From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model","excerpt":"이 [arXiv]에 게시한 'From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Vision-Language Models","Error Cascades","Self-Correction","Refinement Framework","Parallel Generation","Image Captioning","Hallucination Mitigation"],"permalink":"/ai/review/2025-10-27-From_Denoising_to_Refining_A_Corrective_Framework_for_Vision-Language_Diffusion_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Foley_Control_Aligning_a_Frozen_Latent_Text-to-Audio_Model_to_Video","title":"[논문리뷰] Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video","excerpt":"이 [arXiv]에 게시한 'Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Text-to-Audio","Video-to-Audio","Foley Synthesis","Diffusion Models","Cross-Attention","Frozen Backbones","Video Embeddings","Rotary Position Embeddings"],"permalink":"/ai/review/2025-10-27-Foley_Control_Aligning_a_Frozen_Latent_Text-to-Audio_Model_to_Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Document_Understanding_Measurement_and_Manipulation_Using_Category_Theory","title":"[논문리뷰] Document Understanding, Measurement, and Manipulation Using Category Theory","excerpt":"이 [arXiv]에 게시한 'Document Understanding, Measurement, and Manipulation Using Category Theory' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Category Theory","Document Understanding","Large Language Models","Information Theory","Rhetorical Structure Theory","Document Summarization","Rate Distortion Analysis","Self-supervised Learning"],"permalink":"/ai/review/2025-10-27-Document_Understanding_Measurement_and_Manipulation_Using_Category_Theory/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-DeepAgent_A_General_Reasoning_Agent_with_Scalable_Toolsets","title":"[논문리뷰] DeepAgent: A General Reasoning Agent with Scalable Toolsets","excerpt":"Jiajie Jin이 [arXiv]에 게시한 'DeepAgent: A General Reasoning Agent with Scalable Toolsets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Autonomous Agents","Large Language Models","Tool Use","Reinforcement Learning","Memory Management","Tool Retrieval","Agentic Reasoning"],"permalink":"/ai/review/2025-10-27-DeepAgent_A_General_Reasoning_Agent_with_Scalable_Toolsets/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-AstaBench_Rigorous_Benchmarking_of_AI_Agents_with_a_Scientific_Research_Suite","title":"[논문리뷰] AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite","excerpt":"Bhavana Dalvi이 [arXiv]에 게시한 'AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","AI Agents","Benchmarking","Scientific Research","LLM Evaluation","Agentic AI","Tool Use","Reproducibility","Cost-Aware Evaluation"],"permalink":"/ai/review/2025-10-27-AstaBench_Rigorous_Benchmarking_of_AI_Agents_with_a_Scientific_Research_Suite/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-Are_Large_Reasoning_Models_Good_Translation_Evaluators_Analysis_and_Performance_Boost","title":"[논문리뷰] Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost","excerpt":"Min Yang이 [arXiv]에 게시한 'Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Machine Translation Evaluation","Large Reasoning Models","LLM-as-a-judge","MQM","Fine-tuning","Thinking Calibration","Computational Efficiency","Meta-evaluation"],"permalink":"/ai/review/2025-10-27-Are_Large_Reasoning_Models_Good_Translation_Evaluators_Analysis_and_Performance_Boost/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-A_Definition_of_AGI","title":"[논문리뷰] A Definition of AGI","excerpt":"Yarin Gal이 [arXiv]에 게시한 'A Definition of AGI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","AGI Definition","Cognitive Assessment","Cattell-Horn-Carroll Theory","AI Evaluation","Multimodal AI","Cognitive Domains","Psychometrics"],"permalink":"/ai/review/2025-10-27-A_Definition_of_AGI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-ARC-Encoder_learning_compressed_text_representations_for_large_language_models","title":"[논문리뷰] ARC-Encoder: learning compressed text representations for large language models","excerpt":"이 [arXiv]에 게시한 'ARC-Encoder: learning compressed text representations for large language models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","Context Compression","Large Language Models","Encoder-Decoder Architecture","Text Representation","In-Context Learning","Parameter Efficiency","Retrieval-Augmented Generation"],"permalink":"/ai/review/2025-10-27-ARC-Encoder_learning_compressed_text_representations_for_large_language_models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-27-ALICE-LRI_A_General_Method_for_Lossless_Range_Image_Generation_for_Spinning_LiDAR_Sensors_without_Calibration_Metadata","title":"[논문리뷰] ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata","excerpt":"José C. Cabaleiro이 [arXiv]에 게시한 'ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","lastModifiedAt":"2025-10-27 13:07:36+0900","categories":["Review"],"tags":["Review","LiDAR","Range Image","Lossless Projection","Sensor Calibration","Intrinsic Parameters","Point Cloud Reconstruction","Hough Transform","Weighted Least Squares"],"permalink":"/ai/review/2025-10-27-ALICE-LRI_A_General_Method_for_Lossless_Range_Image_Generation_for_Spinning_LiDAR_Sensors_without_Calibration_Metadata/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Thought_Communication_in_Multiagent_Collaboration","title":"[논문리뷰] Thought Communication in Multiagent Collaboration","excerpt":"Mingze Gao이 [arXiv]에 게시한 'Thought Communication in Multiagent Collaboration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Multiagent Systems","LLM Communication","Latent Variable Models","Identifiability Theory","Thought Communication","Sparse Autoencoder","Prefix Tuning"],"permalink":"/ai/review/2025-10-24-Thought_Communication_in_Multiagent_Collaboration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-The_Massive_Legal_Embedding_Benchmark_MLEB","title":"[논문리뷰] The Massive Legal Embedding Benchmark (MLEB)","excerpt":"이 [arXiv]에 게시한 'The Massive Legal Embedding Benchmark (MLEB)' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Legal Information Retrieval","Embedding Models","Benchmark Dataset","Natural Language Processing","Retrieval-Augmented Generation","Jurisdictional Diversity","Legal Tech"],"permalink":"/ai/review/2025-10-24-The_Massive_Legal_Embedding_Benchmark_MLEB/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Seed3D_1.0_From_Images_to_High-Fidelity_Simulation-Ready_3D_Assets","title":"[논문리뷰] Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets","excerpt":"이 [arXiv]에 게시한 'Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","3D Asset Generation","Simulation-Ready Assets","Diffusion Models","Physically Based Rendering (PBR)","Embodied AI","Robotic Simulation","Image-to-3D","Foundation Model"],"permalink":"/ai/review/2025-10-24-Seed3D_1.0_From_Images_to_High-Fidelity_Simulation-Ready_3D_Assets/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Search_Self-play_Pushing_the_Frontier_of_Agent_Capability_without_Supervision","title":"[논문리뷰] Search Self-play: Pushing the Frontier of Agent Capability without Supervision","excerpt":"이 [arXiv]에 게시한 'Search Self-play: Pushing the Frontier of Agent Capability without Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","LLM Agents","Self-play","Reinforcement Learning","Search Agents","Supervision-Free Training","Retrieval-Augmented Generation (RAG)","Task Generation","Curriculum Learning"],"permalink":"/ai/review/2025-10-24-Search_Self-play_Pushing_the_Frontier_of_Agent_Capability_without_Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-SAKE_Towards_Editing_Auditory_Attribute_Knowledge_of_Large_Audio-Language_Models","title":"[논문리뷰] SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models","excerpt":"이 [arXiv]에 게시한 'SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Knowledge Editing","Audio-Language Models","Auditory Attributes","Benchmark","Reliability","Generality","Locality","Portability"],"permalink":"/ai/review/2025-10-24-SAKE_Towards_Editing_Auditory_Attribute_Knowledge_of_Large_Audio-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Open-o3_Video_Grounded_Video_Reasoning_with_Explicit_Spatio-Temporal_Evidence","title":"[논문리뷰] Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence","excerpt":"이 [arXiv]에 게시한 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Video Reasoning","Spatio-Temporal Grounding","Large Multimodal Models","Reinforcement Learning","Chain-of-Thought","Visual Evidence","Dataset Curation"],"permalink":"/ai/review/2025-10-24-Open-o3_Video_Grounded_Video_Reasoning_with_Explicit_Spatio-Temporal_Evidence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Loopholing_Discrete_Diffusion_Deterministic_Bypass_of_the_Sampling_Wall","title":"[논문리뷰] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall","excerpt":"Sungjin Ahn이 [arXiv]에 게시한 'Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Sampling Wall","Loopholing","Self-Conditioning","Non-Autoregressive Generation","Text Generation","Language Modeling","Reasoning Tasks"],"permalink":"/ai/review/2025-10-24-Loopholing_Discrete_Diffusion_Deterministic_Bypass_of_the_Sampling_Wall/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-LayerComposer_Interactive_Personalized_T2I_via_Spatially-Aware_Layered_Canvas","title":"[논문리뷰] LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas","excerpt":"이 [arXiv]에 게시한 'LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Personalization","Diffusion Models","Interactive Control","Multi-Subject Composition","Layered Canvas","Spatial Control","Image Editing"],"permalink":"/ai/review/2025-10-24-LayerComposer_Interactive_Personalized_T2I_via_Spatially-Aware_Layered_Canvas/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Investigating_Safety_Vulnerabilities_of_Large_Audio-Language_Models_Under_Speaker_Emotional_Variations","title":"[논문리뷰] Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations","excerpt":"이 [arXiv]에 게시한 'Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","LALM Safety","Speaker Emotion","Safety Alignment","Jailbreaking","Audio-Language Models","Emotional Variation","Unsafe Rate","Non-refusal Rate"],"permalink":"/ai/review/2025-10-24-Investigating_Safety_Vulnerabilities_of_Large_Audio-Language_Models_Under_Speaker_Emotional_Variations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-ImpossibleBench_Measuring_LLMs_Propensity_of_Exploiting_Test_Cases","title":"[논문리뷰] ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases","excerpt":"Nicholas Carlini이 [arXiv]에 게시한 'ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Reward Hacking","Benchmark Reliability","Test Exploitation","Prompt Engineering","LLM Safety","Code Generation"],"permalink":"/ai/review/2025-10-24-ImpossibleBench_Measuring_LLMs_Propensity_of_Exploiting_Test_Cases/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Human-Agent_Collaborative_Paper-to-Page_Crafting_for_Under_0.1","title":"[논문리뷰] Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1","excerpt":"이 [arXiv]에 게시한 'Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Human-Agent Collaboration","Project Page Generation","Multi-Agent System","LLM","VLM","Webpage Automation","PageBench","Scientific Communication","Cost-Effective AI"],"permalink":"/ai/review/2025-10-24-Human-Agent_Collaborative_Paper-to-Page_Crafting_for_Under_0.1/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-HoloCine_Holistic_Generation_of_Cinematic_Multi-Shot_Long_Video_Narratives","title":"[논문리뷰] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives","excerpt":"이 [arXiv]에 게시한 'HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Multi-Shot Video","Narrative Coherence","Diffusion Models","Self-Attention","Cinematic AI","Video Consistency","Directorial Control"],"permalink":"/ai/review/2025-10-24-HoloCine_Holistic_Generation_of_Cinematic_Multi-Shot_Long_Video_Narratives/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-From_Masks_to_Worlds_A_Hitchhikers_Guide_to_World_Models","title":"[논문리뷰] From Masks to Worlds: A Hitchhiker's Guide to World Models","excerpt":"Shufan Li이 [arXiv]에 게시한 'From Masks to Worlds: A Hitchhiker's Guide to World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","World Models","Generative AI","Multimodal Learning","Masked Modeling","Interactive AI","Memory Systems","Autonomous Agents","AI Roadmap"],"permalink":"/ai/review/2025-10-24-From_Masks_to_Worlds_A_Hitchhikers_Guide_to_World_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Every_Question_Has_Its_Own_Value_Reinforcement_Learning_with_Explicit_Human_Values","title":"[논문리뷰] Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values","excerpt":"이 [arXiv]에 게시한 'Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Alignment","Human Values","Reward Shaping","Value-Weighted Reward","Termination Policy","RLVR"],"permalink":"/ai/review/2025-10-24-Every_Question_Has_Its_Own_Value_Reinforcement_Learning_with_Explicit_Human_Values/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Emergence_of_Linear_Truth_Encodings_in_Language_Models","title":"[논문리뷰] Emergence of Linear Truth Encodings in Language Models","excerpt":"Alberto Bietti이 [arXiv]에 게시한 'Emergence of Linear Truth Encodings in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Language Models","Truth Encoding","Linear Subspaces","Mechanistic Interpretability","Transformer Models","Learning Dynamics","Truth Co-occurrence Hypothesis","Hallucinations"],"permalink":"/ai/review/2025-10-24-Emergence_of_Linear_Truth_Encodings_in_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-DyPE_Dynamic_Position_Extrapolation_for_Ultra_High_Resolution_Diffusion","title":"[논문리뷰] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion","excerpt":"이 [arXiv]에 게시한 'DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Diffusion Models","Transformer Architecture","Positional Encoding","High-Resolution Image Generation","Extrapolation","Dynamic Adaptation","Training-Free"],"permalink":"/ai/review/2025-10-24-DyPE_Dynamic_Position_Extrapolation_for_Ultra_High_Resolution_Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Diff-XYZ_A_Benchmark_for_Evaluating_Diff_Understanding","title":"[논문리뷰] Diff-XYZ: A Benchmark for Evaluating Diff Understanding","excerpt":"이 [arXiv]에 게시한 'Diff-XYZ: A Benchmark for Evaluating Diff Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Diff Understanding","Code Diff","Benchmark","LLMs","Code Editing","Software Engineering","Unified Diff Format","Search-Replace"],"permalink":"/ai/review/2025-10-24-Diff-XYZ_A_Benchmark_for_Evaluating_Diff_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-Conan_Progressive_Learning_to_Reason_Like_a_Detective_over_Multi-Scale_Visual_Evidence","title":"[논문리뷰] Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence","excerpt":"이 [arXiv]에 게시한 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Video Reasoning","Multimodal Large Language Models (MLLMs)","Reinforcement Learning (RLVR)","Evidence Grounding","Multi-step Reasoning","Frame Retrieval","Dataset Construction","Progressive Learning"],"permalink":"/ai/review/2025-10-24-Conan_Progressive_Learning_to_Reason_Like_a_Detective_over_Multi-Scale_Visual_Evidence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-ComProScanner_A_multi-agent_based_framework_for_composition-property_structured_data_extraction_from_scientific_literature","title":"[논문리뷰] ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature","excerpt":"이 [arXiv]에 게시한 'ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Multi-agent Systems","Large Language Models (LLMs)","Information Extraction","Scientific Literature","Materials Science","Data Curation","Piezoelectric Materials","RAG (Retrieval-Augmented Generation)"],"permalink":"/ai/review/2025-10-24-ComProScanner_A_multi-agent_based_framework_for_composition-property_structured_data_extraction_from_scientific_literature/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-AlphaFlow_Understanding_and_Improving_MeanFlow_Models","title":"[논문리뷰] AlphaFlow: Understanding and Improving MeanFlow Models","excerpt":"이 [arXiv]에 게시한 'AlphaFlow: Understanding and Improving MeanFlow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Generative Models","Flow Matching","Consistency Models","MeanFlow","Curriculum Learning","Few-Step Generation","Image Generation"],"permalink":"/ai/review/2025-10-24-AlphaFlow_Understanding_and_Improving_MeanFlow_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-AdaSPEC_Selective_Knowledge_Distillation_for_Efficient_Speculative_Decoders","title":"[논문리뷰] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders","excerpt":"이 [arXiv]에 게시한 'AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Speculative Decoding","Knowledge Distillation","LLM Inference","Model Acceleration","Token Filtering","Draft Model","Acceptance Rate"],"permalink":"/ai/review/2025-10-24-AdaSPEC_Selective_Knowledge_Distillation_for_Efficient_Speculative_Decoders/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-24-ARGenSeg_Image_Segmentation_with_Autoregressive_Image_Generation_Model","title":"[논문리뷰] ARGenSeg: Image Segmentation with Autoregressive Image Generation Model","excerpt":"이 [arXiv]에 게시한 'ARGenSeg: Image Segmentation with Autoregressive Image Generation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","lastModifiedAt":"2025-10-24 13:04:16+0900","categories":["Review"],"tags":["Review","Image Segmentation","Autoregressive Generation","Multimodal Large Language Models (MLLMs)","Visual Understanding","VQ-VAE","Multi-scale Prediction","Referring Expression Segmentation","Image Generation"],"permalink":"/ai/review/2025-10-24-ARGenSeg_Image_Segmentation_with_Autoregressive_Image_Generation_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-olmOCR_2_Unit_Test_Rewards_for_Document_OCR","title":"[논문리뷰] olmOCR 2: Unit Test Rewards for Document OCR","excerpt":"이 [arXiv]에 게시한 'olmOCR 2: Unit Test Rewards for Document OCR' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Document OCR","Vision Language Model","Reinforcement Learning","Unit Tests","Synthetic Data Generation","RLVR","Document Parsing","State-of-the-Art OCR"],"permalink":"/ai/review/2025-10-23-olmOCR_2_Unit_Test_Rewards_for_Document_OCR/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-VideoAgentTrek_Computer_Use_Pretraining_from_Unlabeled_Videos","title":"[논문리뷰] VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos","excerpt":"Xinyuan Wang이 [arXiv]에 게시한 'VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","GUI Agents","Video Pretraining","Inverse Dynamics","Action Recognition","Computer Use Automation","Data Synthesis","Multimodal Learning"],"permalink":"/ai/review/2025-10-23-VideoAgentTrek_Computer_Use_Pretraining_from_Unlabeled_Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Unified_Reinforcement_and_Imitation_Learning_for_Vision-Language_Models","title":"[논문리뷰] Unified Reinforcement and Imitation Learning for Vision-Language Models","excerpt":"이 [arXiv]에 게시한 'Unified Reinforcement and Imitation Learning for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Reinforcement Learning","Imitation Learning","Model Distillation","Lightweight VLMs","LLM-as-a-Judge","Multimodal Learning"],"permalink":"/ai/review/2025-10-23-Unified_Reinforcement_and_Imitation_Learning_for_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-RIR-Mega_a_large-scale_simulated_room_impulse_response_dataset_for_machine_learning_and_room_acoustics_modeling","title":"[논문리뷰] RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling","excerpt":"Mandip Goswami이 [arXiv]에 게시한 'RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Room Impulse Response","Dataset","Room Acoustics","Machine Learning","Dereverberation","Speech Recognition","Simulation","Hugging Face"],"permalink":"/ai/review/2025-10-23-RIR-Mega_a_large-scale_simulated_room_impulse_response_dataset_for_machine_learning_and_room_acoustics_modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-ProfBench_Multi-Domain_Rubrics_requiring_Professional_Knowledge_to_Answer_and_Judge","title":"[논문리뷰] ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge","excerpt":"이 [arXiv]에 게시한 'ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Rubric-based Benchmark","Professional Knowledge","Multi-domain Tasks","LLM-Judge Bias Mitigation","Cost Reduction","Reasoning Assessment","Open-weight Models"],"permalink":"/ai/review/2025-10-23-ProfBench_Multi-Domain_Rubrics_requiring_Professional_Knowledge_to_Answer_and_Judge/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Pico-Banana-400K_A_Large-Scale_Dataset_for_Text-Guided_Image_Editing","title":"[논문리뷰] Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing","excerpt":"이 [arXiv]에 게시한 'Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Text-Guided Image Editing","Large-Scale Dataset","Multimodal Models","Dataset Curation","Quality Control","Prompt Engineering","Preference Learning","Multi-Turn Editing"],"permalink":"/ai/review/2025-10-23-Pico-Banana-400K_A_Large-Scale_Dataset_for_Text-Guided_Image_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-OmniNWM_Omniscient_Driving_Navigation_World_Models","title":"[논문리뷰] OmniNWM: Omniscient Driving Navigation World Models","excerpt":"Zhujin Liang이 [arXiv]에 게시한 'OmniNWM: Omniscient Driving Navigation World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Autonomous Driving","World Models","Multi-modal Generation","3D Occupancy","Plücker Ray-maps","Action Control","Dense Rewards","Long-term Forecasting"],"permalink":"/ai/review/2025-10-23-OmniNWM_Omniscient_Driving_Navigation_World_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Machine_Text_Detectors_are_Membership_Inference_Attacks","title":"[논문리뷰] Machine Text Detectors are Membership Inference Attacks","excerpt":"Naoaki Okazaki이 [arXiv]에 게시한 'Machine Text Detectors are Membership Inference Attacks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Membership Inference Attacks","Machine-Generated Text Detection","Transferability","Likelihood Ratio Test","Large Language Models","Zero-Shot Detection","Model Security","AI Safety"],"permalink":"/ai/review/2025-10-23-Machine_Text_Detectors_are_Membership_Inference_Attacks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-MINED_Probing_and_Updating_with_Multimodal_Time-Sensitive_Knowledge_for_Large_Multimodal_Models","title":"[논문리뷰] MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models","excerpt":"Yifan Gao이 [arXiv]에 게시한 'MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Large Multimodal Models (LMMs)","Time-Sensitive Knowledge","Temporal Reasoning","Knowledge Editing","Multimodal Benchmarking","Temporal Awareness","Dynamic Knowledge"],"permalink":"/ai/review/2025-10-23-MINED_Probing_and_Updating_with_Multimodal_Time-Sensitive_Knowledge_for_Large_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-LoongRLReinforcement_Learning_for_Advanced_Reasoning_over_Long_Contexts","title":"[논문리뷰] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts","excerpt":"이 [arXiv]에 게시한 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Long Context Reasoning","Large Language Models","Multi-hop QA","Data Synthesis","Retrieval-Augmented Generation","Chain-of-Thought"],"permalink":"/ai/review/2025-10-23-LoongRLReinforcement_Learning_for_Advanced_Reasoning_over_Long_Contexts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Learning_from_the_Best_Differently_A_Diversity-Driven_Rethinking_on_Data_Selection","title":"[논문리뷰] Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection","excerpt":"Yi Cheng이 [arXiv]에 게시한 'Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Data Selection","Large Language Models (LLMs)","Data Diversity","Data Quality","Principal Component Analysis (PCA)","Orthogonal Dimensions","Pre-training"],"permalink":"/ai/review/2025-10-23-Learning_from_the_Best_Differently_A_Diversity-Driven_Rethinking_on_Data_Selection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Language_Models_are_Injective_and_Hence_Invertible","title":"[논문리뷰] Language Models are Injective and Hence Invertible","excerpt":"이 [arXiv]에 게시한 'Language Models are Injective and Hence Invertible' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Language Models","Injectivity","Invertibility","Transformer","Representation Learning","Exact Recovery","SIPIT Algorithm","Real Analysis"],"permalink":"/ai/review/2025-10-23-Language_Models_are_Injective_and_Hence_Invertible/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-KORE_Enhancing_Knowledge_Injection_for_Large_Multimodal_Models_via_Knowledge-Oriented_Augmentations_and_Constraints","title":"[논문리뷰] KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints","excerpt":"Jinhe Bi이 [arXiv]에 게시한 'KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Knowledge Injection","Large Multimodal Models","Catastrophic Forgetting","Data Augmentation","Parameter-Efficient Fine-Tuning","Null Space","Continual Learning"],"permalink":"/ai/review/2025-10-23-KORE_Enhancing_Knowledge_Injection_for_Large_Multimodal_Models_via_Knowledge-Oriented_Augmentations_and_Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-GigaBrain-0_A_World_Model-Powered_Vision-Language-Action_Model","title":"[논문리뷰] GigaBrain-0: A World Model-Powered Vision-Language-Action Model","excerpt":"이 [arXiv]에 게시한 'GigaBrain-0: A World Model-Powered Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Model","World Model","Data Augmentation","Robot Generalization","Embodied AI","RGBD","Chain-of-Thought"],"permalink":"/ai/review/2025-10-23-GigaBrain-0_A_World_Model-Powered_Vision-Language-Action_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-From_Charts_to_Code_A_Hierarchical_Benchmark_for_Multimodal_Models","title":"[논문리뷰] From Charts to Code: A Hierarchical Benchmark for Multimodal Models","excerpt":"Dongxing Mao이 [arXiv]에 게시한 'From Charts to Code: A Hierarchical Benchmark for Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Chart-to-Code","Multimodal Models","Hierarchical Benchmark","Chart Understanding","Code Generation","Evaluation Metrics","Benchmarking"],"permalink":"/ai/review/2025-10-23-From_Charts_to_Code_A_Hierarchical_Benchmark_for_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-FinSight_Towards_Real-World_Financial_Deep_Research","title":"[논문리뷰] FinSight: Towards Real-World Financial Deep Research","excerpt":"Yutao Zhu이 [arXiv]에 게시한 'FinSight: Towards Real-World Financial Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Financial Research","Multi-Agent System","Code Generation","Multimodal Reports","Iterative Visualization","Variable Memory","Deep Learning"],"permalink":"/ai/review/2025-10-23-FinSight_Towards_Real-World_Financial_Deep_Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Every_Attention_Matters_An_Efficient_Hybrid_Architecture_for_Long-Context_Reasoning","title":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning","excerpt":"이 [arXiv]에 게시한 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Long-Context LLM","Hybrid Attention","Linear Attention","Mixture-of-Experts","FP8 Training","GPU Optimization","Training-Inference Alignment","Reinforcement Learning"],"permalink":"/ai/review/2025-10-23-Every_Attention_Matters_An_Efficient_Hybrid_Architecture_for_Long-Context_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Directional_Reasoning_Injection_for_Fine-Tuning_MLLMs","title":"[논문리뷰] Directional Reasoning Injection for Fine-Tuning MLLMs","excerpt":"Jialian Wu이 [arXiv]에 게시한 'Directional Reasoning Injection for Fine-Tuning MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Reasoning Transfer","Gradient-based Fine-tuning","Model Merging","Parameter-Efficient Learning","Supervised Fine-tuning","Directional Prior"],"permalink":"/ai/review/2025-10-23-Directional_Reasoning_Injection_for_Fine-Tuning_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-DeLeaker_Dynamic_Inference-Time_Reweighting_For_Semantic_Leakage_Mitigation_in_Text-to-Image_Models","title":"[논문리뷰] DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models","excerpt":"Roi Reichart이 [arXiv]에 게시한 'DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Semantic Leakage","Text-to-Image Models","Attention Control","Inference-time Mitigation","Diffusion Models","Evaluation Dataset","Self-Attention"],"permalink":"/ai/review/2025-10-23-DeLeaker_Dynamic_Inference-Time_Reweighting_For_Semantic_Leakage_Mitigation_in_Text-to-Image_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-DaMo_Data_Mixing_Optimizer_in_Fine-tuning_Multimodal_LLMs_for_Mobile_Phone_Agents","title":"[논문리뷰] DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents","excerpt":"이 [arXiv]에 게시한 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Fine-tuning","Data Mixing Optimization","Mobile Phone Agents","Downstream Task Prediction","Benchmark","Neural Networks"],"permalink":"/ai/review/2025-10-23-DaMo_Data_Mixing_Optimizer_in_Fine-tuning_Multimodal_LLMs_for_Mobile_Phone_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-ColorAgent_Building_A_Robust_Personalized_and_Interactive_OS_Agent","title":"[논문리뷰] ColorAgent: Building A Robust, Personalized, and Interactive OS Agent","excerpt":"Weiming Zhang이 [arXiv]에 게시한 'ColorAgent: Building A Robust, Personalized, and Interactive OS Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","OS Agent","Reinforcement Learning","Multi-agent Systems","Personalization","Proactive Interaction","GUI Agents","Self-Evolving Training"],"permalink":"/ai/review/2025-10-23-ColorAgent_Building_A_Robust_Personalized_and_Interactive_OS_Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-BAPO_Stabilizing_Off-Policy_Reinforcement_Learning_for_LLMs_via_Balanced_Policy_Optimization_with_Adaptive_Clipping","title":"[논문리뷰] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping","excerpt":"Junrui Shen이 [arXiv]에 게시한 'BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Off-Policy Reinforcement Learning","Large Language Models","Adaptive Clipping","Policy Optimization","PPO","Entropy Preservation","RL Stabilization"],"permalink":"/ai/review/2025-10-23-BAPO_Stabilizing_Off-Policy_Reinforcement_Learning_for_LLMs_via_Balanced_Policy_Optimization_with_Adaptive_Clipping/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-Attention_Sinks_in_Diffusion_Language_Models","title":"[논문리뷰] Attention Sinks in Diffusion Language Models","excerpt":"Simone Scardapane이 [arXiv]에 게시한 'Attention Sinks in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Attention Sinks","Transformer Architecture","Masked Language Modeling","Bidirectional Attention","Generative Models","Robustness","Dynamic Attention"],"permalink":"/ai/review/2025-10-23-Attention_Sinks_in_Diffusion_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-23-AlphaOPT_Formulating_Optimization_Programs_with_Self-Improving_LLM_Experience_Library","title":"[논문리뷰] AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library","excerpt":"Chonghe Jiang이 [arXiv]에 게시한 'AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","lastModifiedAt":"2025-10-23 13:08:59+0900","categories":["Review"],"tags":["Review","Optimization Modeling","Large Language Models (LLMs)","Experience Library","Self-Improving Systems","Continual Learning","Out-of-Distribution Generalization","Operations Research","Knowledge Representation"],"permalink":"/ai/review/2025-10-23-AlphaOPT_Formulating_Optimization_Programs_with_Self-Improving_LLM_Experience_Library/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-World-in-World_World_Models_in_a_Closed-Loop_World","title":"[논문리뷰] World-in-World: World Models in a Closed-Loop World","excerpt":"Arda Uzunoglu이 [arXiv]에 게시한 'World-in-World: World Models in a Closed-Loop World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","World Models","Embodied AI","Closed-Loop Evaluation","Online Planning","Data Scaling","Controllability","Robotic Manipulation"],"permalink":"/ai/review/2025-10-22-World-in-World_World_Models_in_a_Closed-Loop_World/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-Video_Reasoning_without_Training","title":"[논문리뷰] Video Reasoning without Training","excerpt":"이 [arXiv]에 게시한 'Video Reasoning without Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Reasoning","Large Multimodal Models (LMMs)","Inference-Time Optimization","Entropy-Based Objective","Training-Free","KV-Cache Steering","Micro-Exploration","Macro-Exploitation"],"permalink":"/ai/review/2025-10-22-Video_Reasoning_without_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-Unleashing_Scientific_Reasoning_for_Bio-experimental_Protocol_Generation_via_Structured_Component-based_Reward_Mechanism","title":"[논문리뷰] Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism","excerpt":"Shuang Gu이 [arXiv]에 게시한 'Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Scientific Reasoning","Bio-experimental Protocol Generation","LLM","Structured Reward","SciRecipe Dataset","Sketch-and-Fill","Reinforcement Learning","Thoth"],"permalink":"/ai/review/2025-10-22-Unleashing_Scientific_Reasoning_for_Bio-experimental_Protocol_Generation_via_Structured_Component-based_Reward_Mechanism/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-UniGenBench_A_Unified_Semantic_Evaluation_Benchmark_for_Text-to-Image_Generation","title":"[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation","excerpt":"Yujie Zhou이 [arXiv]에 게시한 'UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Semantic Evaluation","Benchmark","Multilingual Evaluation","Fine-grained Assessment","Large Language Models","Model Evaluation","Prompt Engineering"],"permalink":"/ai/review/2025-10-22-UniGenBench_A_Unified_Semantic_Evaluation_Benchmark_for_Text-to-Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-UltraGen_High-Resolution_Video_Generation_with_Hierarchical_Attention","title":"[논문리뷰] UltraGen: High-Resolution Video Generation with Hierarchical Attention","excerpt":"Ran Yi이 [arXiv]에 게시한 'UltraGen: High-Resolution Video Generation with Hierarchical Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Generation","High-Resolution","Diffusion Transformer","Hierarchical Attention","Global-Local Attention","Computational Efficiency","4K Synthesis"],"permalink":"/ai/review/2025-10-22-UltraGen_High-Resolution_Video_Generation_with_Hierarchical_Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-Towards_Faithful_and_Controllable_Personalization_via_Critique-Post-Edit_Reinforcement_Learning","title":"[논문리뷰] Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning","excerpt":"Yuchen Eleanor Jiang이 [arXiv]에 게시한 'Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Personalization","Reinforcement Learning","Generative Reward Model","Critique-Post-Edit","Reward Hacking","Controllable AI"],"permalink":"/ai/review/2025-10-22-Towards_Faithful_and_Controllable_Personalization_via_Critique-Post-Edit_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-ProCLIP_Progressive_Vision-Language_Alignment_via_LLM-based_Embedder","title":"[논문리뷰] ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder","excerpt":"Zonghao Guo이 [arXiv]에 게시한 'ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Vision-Language Models","CLIP","LLM-based Embedder","Knowledge Distillation","Contrastive Learning","Curriculum Learning","Multimodal Alignment","Progressive Alignment"],"permalink":"/ai/review/2025-10-22-ProCLIP_Progressive_Vision-Language_Alignment_via_LLM-based_Embedder/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-PokeeResearch_Effective_Deep_Research_via_Reinforcement_Learning_from_AI_Feedback_and_Robust_Reasoning_Scaffold","title":"[논문리뷰] PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold","excerpt":"이 [arXiv]에 게시한 'PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Deep Research Agent","Reinforcement Learning from AI Feedback","RLOO Algorithm","Large Language Models","Tool Use","Self-Correction","Reasoning Scaffold","Agent Alignment"],"permalink":"/ai/review/2025-10-22-PokeeResearch_Effective_Deep_Research_via_Reinforcement_Learning_from_AI_Feedback_and_Robust_Reasoning_Scaffold/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-PRISMM-Bench_A_Benchmark_of_Peer-Review_Grounded_Multimodal_Inconsistencies","title":"[논문리뷰] PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies","excerpt":"James Glass이 [arXiv]에 게시한 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Large Multimodal Models (LMMs)","Scientific Document Analysis","Multimodal Inconsistencies","Peer Review","Benchmark","Debiasing","JSON-based Representation","Reasoning"],"permalink":"/ai/review/2025-10-22-PRISMM-Bench_A_Benchmark_of_Peer-Review_Grounded_Multimodal_Inconsistencies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-MoGA_Mixture-of-Groups_Attention_for_End-to-End_Long_Video_Generation","title":"[논문리뷰] MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation","excerpt":"이 [arXiv]에 게시한 'MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Long Video Generation","Sparse Attention","Diffusion Transformers","Mixture-of-Groups Attention","Token Routing","Computational Efficiency","Context Length"],"permalink":"/ai/review/2025-10-22-MoGA_Mixture-of-Groups_Attention_for_End-to-End_Long_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-MUG-V_10B_High-efficiency_Training_Pipeline_for_Large_Video_Generation_Models","title":"[논문리뷰] MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models","excerpt":"이 [arXiv]에 게시한 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Transformer","Large-scale Training","Megatron-Core","Video VAE","E-commerce AI","High-efficiency Pipeline","Preference Optimization"],"permalink":"/ai/review/2025-10-22-MUG-V_10B_High-efficiency_Training_Pipeline_for_Large_Video_Generation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-MT-Video-Bench_A_Holistic_Video_Understanding_Benchmark_for_Evaluating_Multimodal_LLMs_in_Multi-Turn_Dialogues","title":"[논문리뷰] MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues","excerpt":"이 [arXiv]에 게시한 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Video Understanding","Benchmark","Multi-Turn Dialogues","Perceptivity","Interactivity","Evaluation"],"permalink":"/ai/review/2025-10-22-MT-Video-Bench_A_Holistic_Video_Understanding_Benchmark_for_Evaluating_Multimodal_LLMs_in_Multi-Turn_Dialogues/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-IF-VidCap_Can_Video_Caption_Models_Follow_Instructions","title":"[논문리뷰] IF-VidCap: Can Video Caption Models Follow Instructions?","excerpt":"이 [arXiv]에 게시한 'IF-VidCap: Can Video Caption Models Follow Instructions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Video Captioning","Instruction Following","MLLMs","Benchmark","Controllable Generation","Multimodal Evaluation","Fine-tuning"],"permalink":"/ai/review/2025-10-22-IF-VidCap_Can_Video_Caption_Models_Follow_Instructions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-Grasp_Any_Region_Towards_Precise_Contextual_Pixel_Understanding_for_Multimodal_LLMs","title":"[논문리뷰] Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs","excerpt":"이 [arXiv]에 게시한 'Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Region Understanding","Contextual Pixel Understanding","RoI-aligned Feature Replay","Compositional Reasoning","GAR-Bench","Zero-shot Video Understanding"],"permalink":"/ai/review/2025-10-22-Grasp_Any_Region_Towards_Precise_Contextual_Pixel_Understanding_for_Multimodal_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-Extracting_alignment_data_in_open_models","title":"[논문리뷰] Extracting alignment data in open models","excerpt":"이 [arXiv]에 게시한 'Extracting alignment data in open models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Alignment Data Extraction","Large Language Models","Memorization","Neural Embeddings","Semantic Similarity","Chat Templates","Model Distillation","Reinforcement Learning","Supervised Finetuning"],"permalink":"/ai/review/2025-10-22-Extracting_alignment_data_in_open_models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-EvoSyn_Generalizable_Evolutionary_Data_Synthesis_for_Verifiable_Learning","title":"[논문리뷰] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning","excerpt":"Qipeng Guo이 [arXiv]에 게시한 'EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Verifiable Learning","Data Synthesis","Evolutionary Algorithm","Large Language Models","Reinforcement Learning","Model Distillation","Test Generation"],"permalink":"/ai/review/2025-10-22-EvoSyn_Generalizable_Evolutionary_Data_Synthesis_for_Verifiable_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-DSI-Bench_A_Benchmark_for_Dynamic_Spatial_Intelligence","title":"[논문리뷰] DSI-Bench: A Benchmark for Dynamic Spatial Intelligence","excerpt":"이 [arXiv]에 게시한 'DSI-Bench: A Benchmark for Dynamic Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Dynamic Spatial Reasoning","Vision-Language Models (VLMs)","Benchmark","Video Understanding","Motion Perception","3D Spatial Intelligence","Hallucinations","Bias"],"permalink":"/ai/review/2025-10-22-DSI-Bench_A_Benchmark_for_Dynamic_Spatial_Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-Chem-R_Learning_to_Reason_as_a_Chemist","title":"[논문리뷰] Chem-R: Learning to Reason as a Chemist","excerpt":"이 [arXiv]에 게시한 'Chem-R: Learning to Reason as a Chemist' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Chemical Reasoning","Large Language Models","Chem-R","Structured Reasoning","Multi-task Optimization","Chain-of-Thought","Chemical Discovery"],"permalink":"/ai/review/2025-10-22-Chem-R_Learning_to_Reason_as_a_Chemist/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-22-AlphaQuanter_An_End-to-End_Tool-Orchestrated_Agentic_Reinforcement_Learning_Framework_for_Stock_Trading","title":"[논문리뷰] AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading","excerpt":"Jiashu Wang이 [arXiv]에 게시한 'AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","lastModifiedAt":"2025-10-22 13:07:20+0900","categories":["Review"],"tags":["Review","Automated Trading","Reinforcement Learning","LLM Agents","Tool Orchestration","Financial Markets","Algorithmic Trading","Interpretable AI","ReAct"],"permalink":"/ai/review/2025-10-22-AlphaQuanter_An_End-to-End_Tool-Orchestrated_Agentic_Reinforcement_Learning_Framework_for_Stock_Trading/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-When_to_Ensemble_Identifying_Token-Level_Points_for_Stable_and_Fast_LLM_Ensembling","title":"[논문리뷰] When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling","excerpt":"이 [arXiv]에 게시한 'When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","LLM Ensembling","Token-level Ensembling","Speculative Decoding","Tokenization Mismatch","Probability Sharpening","Long-form Generation","KV Cache Management"],"permalink":"/ai/review/2025-10-21-When_to_Ensemble_Identifying_Token-Level_Points_for_Stable_and_Fast_LLM_Ensembling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Visual_Autoregressive_Models_Beat_Diffusion_Models_on_Inference_Time_Scaling","title":"[논문리뷰] Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling","excerpt":"Dim P. Papadopoulos이 [arXiv]에 게시한 'Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Visual Autoregressive Models","Diffusion Models","Inference Time Scaling","Beam Search","Image Generation","Text-to-Image Synthesis","Discrete Latent Space"],"permalink":"/ai/review/2025-10-21-Visual_Autoregressive_Models_Beat_Diffusion_Models_on_Inference_Time_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Uniworld-V2_Reinforce_Image_Editing_with_Diffusion_Negative-aware_Finetuning_and_MLLM_Implicit_Feedback","title":"[논문리뷰] Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback","excerpt":"이 [arXiv]에 게시한 'Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Reinforcement Learning","MLLM","Policy Optimization","Finetuning","Reward Modeling","Human Alignment"],"permalink":"/ai/review/2025-10-21-Uniworld-V2_Reinforce_Image_Editing_with_Diffusion_Negative-aware_Finetuning_and_MLLM_Implicit_Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-UltraCUA_A_Foundation_Model_for_Computer_Use_Agents_with_Hybrid_Action","title":"[논문리뷰] UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action","excerpt":"이 [arXiv]에 게시한 'UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Computer Use Agents","Hybrid Action","Foundation Models","Reinforcement Learning","Supervised Fine-tuning","Synthetic Data Generation","Tool Learning","GUI Automation"],"permalink":"/ai/review/2025-10-21-UltraCUA_A_Foundation_Model_for_Computer_Use_Agents_with_Hybrid_Action/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Towards_Mixed-Modal_Retrieval_for_Universal_Retrieval-Augmented_Generation","title":"[논문리뷰] Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation","excerpt":"이 [arXiv]에 게시한 'Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Universal RAG","Multimodal Retrieval","Mixed-Modal Data Generation","Vision-Language Models","Contrastive Learning","Matryoshka Representation Learning"],"permalink":"/ai/review/2025-10-21-Towards_Mixed-Modal_Retrieval_for_Universal_Retrieval-Augmented_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-RL_makes_MLLMs_see_better_than_SFT","title":"[논문리뷰] RL makes MLLMs see better than SFT","excerpt":"이 [arXiv]에 게시한 'RL makes MLLMs see better than SFT' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multimodal Language Models","Reinforcement Learning","Supervised Finetuning","Vision Encoder","Visual Representations","Direct Preference Optimization","Preference Alignment","PIVOT"],"permalink":"/ai/review/2025-10-21-RL_makes_MLLMs_see_better_than_SFT/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-QueST_Incentivizing_LLMs_to_Generate_Difficult_Problems","title":"[논문리뷰] QueST: Incentivizing LLMs to Generate Difficult Problems","excerpt":"이 [arXiv]에 게시한 'QueST: Incentivizing LLMs to Generate Difficult Problems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","LLM","Problem Generation","Competitive Programming","Synthetic Data","Difficulty Estimation","Rejection Fine-tuning","Graph Sampling"],"permalink":"/ai/review/2025-10-21-QueST_Incentivizing_LLMs_to_Generate_Difficult_Problems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-PICABench_How_Far_Are_We_from_Physically_Realistic_Image_Editing","title":"[논문리뷰] PICABench: How Far Are We from Physically Realistic Image Editing?","excerpt":"Kaiwen Zhu이 [arXiv]에 게시한 'PICABench: How Far Are We from Physically Realistic Image Editing?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Image Editing","Physical Realism","Benchmark","VLM-as-a-Judge","Synthetic Data","Physics-Aware AI","Diffusion Models","Evaluation Metrics"],"permalink":"/ai/review/2025-10-21-PICABench_How_Far_Are_We_from_Physically_Realistic_Image_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-On_Non-interactive_Evaluation_of_Animal_Communication_Translators","title":"[논문리뷰] On Non-interactive Evaluation of Animal Communication Translators","excerpt":"Adam Tauman Kalai이 [arXiv]에 게시한 'On Non-interactive Evaluation of Animal Communication Translators' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Machine Translation Quality Evaluation","Reference-Free Evaluation","Animal Communication","Language Models","Shuffle Test","Conlangs","Non-interactive Evaluation"],"permalink":"/ai/review/2025-10-21-On_Non-interactive_Evaluation_of_Animal_Communication_Translators/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-MultiVerse_A_Multi-Turn_Conversation_Benchmark_for_Evaluating_Large_Vision_and_Language_Models","title":"[논문리뷰] MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models","excerpt":"이 [arXiv]에 게시한 'MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multi-Turn Conversation","VLM Evaluation","Benchmark","Vision and Language Models","Contextual Understanding","Checklist-based Evaluation","Interactive AI"],"permalink":"/ai/review/2025-10-21-MultiVerse_A_Multi-Turn_Conversation_Benchmark_for_Evaluating_Large_Vision_and_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Knowledge-based_Visual_Question_Answer_with_Multimodal_Processing_Retrieval_and_Filtering","title":"[논문리뷰] Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering","excerpt":"이 [arXiv]에 게시한 'Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Visual Question Answering","Retrieval-Augmented Generation","Multimodal AI","Reinforcement Learning","Knowledge Base","Tool Learning","Information Filtering"],"permalink":"/ai/review/2025-10-21-Knowledge-based_Visual_Question_Answer_with_Multimodal_Processing_Retrieval_and_Filtering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-GuideFlow3D_Optimization-Guided_Rectified_Flow_For_Appearance_Transfer","title":"[논문리뷰] GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer","excerpt":"이 [arXiv]에 게시한 'GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","3D Appearance Transfer","Rectified Flow","Generative Models","Optimization-Guided Sampling","Neural Latent Representations","Training-Free","GPT-Based Evaluation"],"permalink":"/ai/review/2025-10-21-GuideFlow3D_Optimization-Guided_Rectified_Flow_For_Appearance_Transfer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Glyph_Scaling_Context_Windows_via_Visual-Text_Compression","title":"[논문리뷰] Glyph: Scaling Context Windows via Visual-Text Compression","excerpt":"Wenyi Hong이 [arXiv]에 게시한 'Glyph: Scaling Context Windows via Visual-Text Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Long-Context Modeling","Visual Compression","Vision-Language Models","Token Efficiency","Genetic Algorithms","Multimodal AI","LLM Scaling"],"permalink":"/ai/review/2025-10-21-Glyph_Scaling_Context_Windows_via_Visual-Text_Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-FineVision_Open_Data_Is_All_You_Need","title":"[논문리뷰] FineVision: Open Data Is All You Need","excerpt":"이 [arXiv]에 게시한 'FineVision: Open Data Is All You Need' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multimodal Datasets","VLM","Data Curation","Data Hygiene","De-duplication","Human-in-the-loop","GUI Automation","Test-set Decontamination"],"permalink":"/ai/review/2025-10-21-FineVision_Open_Data_Is_All_You_Need/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Executable_Knowledge_Graphs_for_Replicating_AI_Research","title":"[논문리뷰] Executable Knowledge Graphs for Replicating AI Research","excerpt":"이 [arXiv]에 게시한 'Executable Knowledge Graphs for Replicating AI Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","AI Research Replication","Large Language Models (LLMs)","Knowledge Graphs (KGs)","Executable Code Generation","Retrieval-Augmented Generation (RAG)","PaperBench","Automated AI Research"],"permalink":"/ai/review/2025-10-21-Executable_Knowledge_Graphs_for_Replicating_AI_Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Enterprise_Deep_Research_Steerable_Multi-Agent_Deep_Research_for_Enterprise_Analytics","title":"[논문리뷰] Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics","excerpt":"이 [arXiv]에 게시한 'Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","Deep Research","Enterprise AI","Human-in-the-Loop","Steerable AI","LLM Agents","Context Engineering","Enterprise Analytics"],"permalink":"/ai/review/2025-10-21-Enterprise_Deep_Research_Steerable_Multi-Agent_Deep_Research_for_Enterprise_Analytics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Embody_3D_A_Large-scale_Multimodal_Motion_and_Behavior_Dataset","title":"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset","excerpt":"이 [arXiv]에 게시한 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","3D Motion Dataset","Multimodal Data","Human Behavior","Pose Tracking","Hand Tracking","Audio-Visual Data","Large-scale Dataset","SMPL-X"],"permalink":"/ai/review/2025-10-21-Embody_3D_A_Large-scale_Multimodal_Motion_and_Behavior_Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Distractor_Injection_Attacks_on_Large_Reasoning_Models_Characterization_and_Defense","title":"[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense","excerpt":"이 [arXiv]에 게시한 'Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Large Reasoning Models (LRMs)","Prompt Injection","Adversarial Attack","Reasoning Distraction","Chain-of-Thought","Robustness","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)"],"permalink":"/ai/review/2025-10-21-Distractor_Injection_Attacks_on_Large_Reasoning_Models_Characterization_and_Defense/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Deep_Self-Evolving_Reasoning","title":"[논문리뷰] Deep Self-Evolving Reasoning","excerpt":"이 [arXiv]에 게시한 'Deep Self-Evolving Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Deep Self-Evolving Reasoning","LLMs","Iterative Reasoning","Markov Chain","Self-Verification","Self-Refinement","Mathematical Reasoning","AIME Benchmark"],"permalink":"/ai/review/2025-10-21-Deep_Self-Evolving_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-DeepAnalyze_Agentic_Large_Language_Models_for_Autonomous_Data_Science","title":"[논문리뷰] DeepAnalyze: Agentic Large Language Models for Autonomous Data Science","excerpt":"이 [arXiv]에 게시한 'DeepAnalyze: Agentic Large Language Models for Autonomous Data Science' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Autonomous Data Science","Agentic LLM","Curriculum Learning","Reinforcement Learning","Data Agents","End-to-end Data Science"],"permalink":"/ai/review/2025-10-21-DeepAnalyze_Agentic_Large_Language_Models_for_Autonomous_Data_Science/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-ConsistEdit_Highly_Consistent_and_Precise_Training-free_Visual_Editing","title":"[논문리뷰] ConsistEdit: Highly Consistent and Precise Training-free Visual Editing","excerpt":"Xili Dai이 [arXiv]에 게시한 'ConsistEdit: Highly Consistent and Precise Training-free Visual Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Image Editing","Video Editing","Diffusion Transformer","Attention Control","Training-free","Multi-modal Diffusion Transformer (MM-DiT)","Consistency Preservation"],"permalink":"/ai/review/2025-10-21-ConsistEdit_Highly_Consistent_and_Precise_Training-free_Visual_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Chronos-2_From_Univariate_to_Universal_Forecasting","title":"[논문리뷰] Chronos-2: From Univariate to Universal Forecasting","excerpt":"이 [arXiv]에 게시한 'Chronos-2: From Univariate to Universal Forecasting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Time Series Forecasting","Foundation Models","Pretrained Models","Transformer","In-Context Learning","Multivariate Forecasting","Covariates","Group Attention"],"permalink":"/ai/review/2025-10-21-Chronos-2_From_Univariate_to_Universal_Forecasting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Balanced_Multi-Task_Attention_for_Satellite_Image_Classification_A_Systematic_Approach_to_Achieving_97.23_Accuracy_on_EuroSAT_Without_Pre-Training","title":"[논문리뷰] Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training","excerpt":"Aditya Vir이 [arXiv]에 게시한 'Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Satellite Image Classification","Multi-Task Attention","From-Scratch Training","EuroSAT Dataset","Squeeze-Excitation Networks","Coordinate Attention","CNN","Deep Learning Architecture"],"permalink":"/ai/review/2025-10-21-Balanced_Multi-Task_Attention_for_Satellite_Image_Classification_A_Systematic_Approach_to_Achieving_97.23_Accuracy_on_EuroSAT_Without_Pre-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-AsyncVoice_Agent_Real-Time_Explanation_for_LLM_Planning_and_Reasoning","title":"[논문리뷰] AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning","excerpt":"Nikos Vlassis이 [arXiv]에 게시한 'AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Real-Time Interaction","Asynchronous Agents","LLM Explanation","Human-AI Collaboration","Voice Interface","Planning and Reasoning","Context Management","Interruption Handling"],"permalink":"/ai/review/2025-10-21-AsyncVoice_Agent_Real-Time_Explanation_for_LLM_Planning_and_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Annotation-Efficient_Universal_Honesty_Alignment","title":"[논문리뷰] Annotation-Efficient Universal Honesty Alignment","excerpt":"Jingtong Wu이 [arXiv]에 게시한 'Annotation-Efficient Universal Honesty Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","LLM Honesty Alignment","Confidence Calibration","Annotation Efficiency","Self-Consistency","Elicitation-Then-Calibration (EliCal)","HonestyBench","LoRA","Trustworthy AI"],"permalink":"/ai/review/2025-10-21-Annotation-Efficient_Universal_Honesty_Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-21-Agentic_Reinforcement_Learning_for_Search_is_Unsafe","title":"[논문리뷰] Agentic Reinforcement Learning for Search is Unsafe","excerpt":"이 [arXiv]에 게시한 'Agentic Reinforcement Learning for Search is Unsafe' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","lastModifiedAt":"2025-10-21 13:08:30+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","LLM Safety","Tool Use","Search Models","Jailbreaking","Instruction Tuning","Vulnerability"],"permalink":"/ai/review/2025-10-21-Agentic_Reinforcement_Learning_for_Search_is_Unsafe/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-VISTA_A_Test-Time_Self-Improving_Video_Generation_Agent","title":"[논문리뷰] VISTA: A Test-Time Self-Improving Video Generation Agent","excerpt":"Tomas Pfister이 [arXiv]에 게시한 'VISTA: A Test-Time Self-Improving Video Generation Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Prompt Optimization","Multi-Agent System","Test-Time Improvement","MLLM-as-a-Judge","Video Evaluation","Audio-Video Synthesis"],"permalink":"/ai/review/2025-10-20-VISTA_A_Test-Time_Self-Improving_Video_Generation_Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Train_a_Unified_Multimodal_Data_Quality_Classifier_with_Synthetic_Data","title":"[논문리뷰] Train a Unified Multimodal Data Quality Classifier with Synthetic Data","excerpt":"Ritesh Sarkhel이 [arXiv]에 게시한 'Train a Unified Multimodal Data Quality Classifier with Synthetic Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Multimodal Data Quality","MLLM","Synthetic Data","Data Filtering","Image-Text Captioning","Interleaved Document Analysis","Pre-training"],"permalink":"/ai/review/2025-10-20-Train_a_Unified_Multimodal_Data_Quality_Classifier_with_Synthetic_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Skyfall-GS_Synthesizing_Immersive_3D_Urban_Scenes_from_Satellite_Imagery","title":"[논문리뷰] Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery","excerpt":"Chung-Ho Wu이 [arXiv]에 게시한 'Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","3D Scene Synthesis","Gaussian Splatting","Satellite Imagery","Diffusion Models","Urban Modeling","Novel View Synthesis","Curriculum Learning","Real-time Rendering"],"permalink":"/ai/review/2025-10-20-Skyfall-GS_Synthesizing_Immersive_3D_Urban_Scenes_from_Satellite_Imagery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Scaling_Instruction-Based_Video_Editing_with_a_High-Quality_Synthetic_Dataset","title":"[논문리뷰] Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset","excerpt":"Hao Ouyang이 [arXiv]에 게시한 'Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Video Editing","Instruction-Based Editing","Synthetic Data Generation","Dataset","Curriculum Learning","Diffusion Models","Vision-Language Models"],"permalink":"/ai/review/2025-10-20-Scaling_Instruction-Based_Video_Editing_with_a_High-Quality_Synthetic_Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Robust_Layerwise_Scaling_Rules_by_Proper_Weight_Decay_Tuning","title":"[논문리뷰] Robust Layerwise Scaling Rules by Proper Weight Decay Tuning","excerpt":"이 [arXiv]에 게시한 'Robust Layerwise Scaling Rules by Proper Weight Decay Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Weight Decay Scaling","Maximal-Update Parameterization (µP)","AdamW","Transformer","Hyperparameter Transfer","Scaling Laws","Singular Value Spectrum","Steady State Training"],"permalink":"/ai/review/2025-10-20-Robust_Layerwise_Scaling_Rules_by_Proper_Weight_Decay_Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Rewiring_Experts_on_the_FlyContinuous_Rerouting_for_Better_Online_Adaptation_in_Mixture-of-Expert_models","title":"[논문리뷰] Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models","excerpt":"Shiwei Liu이 [arXiv]에 게시한 'Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Online Adaptation","Test-Time Adaptation (TTA)","Expert Routing","Large Language Models (LLMs)","Self-Supervision","Computational Efficiency","Context Shift Robustness"],"permalink":"/ai/review/2025-10-20-Rewiring_Experts_on_the_FlyContinuous_Rerouting_for_Better_Online_Adaptation_in_Mixture-of-Expert_models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Paper2Web_Lets_Make_Your_Paper_Alive","title":"[논문리뷰] Paper2Web: Let's Make Your Paper Alive!","excerpt":"Yao Wan이 [arXiv]에 게시한 'Paper2Web: Let's Make Your Paper Alive!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Academic Webpage Generation","Multi-Agent Systems","Large Language Models","Model Context Protocol","Interactive Content","Multimedia Dissemination","Evaluation Benchmark","Human-Computer Interaction"],"permalink":"/ai/review/2025-10-20-Paper2Web_Lets_Make_Your_Paper_Alive/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-OmniVinci_Enhancing_Architecture_and_Data_for_Omni-Modal_Understanding_LLM","title":"[논문리뷰] OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM","excerpt":"이 [arXiv]에 게시한 'OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Omni-Modal LLM","Multimodal Understanding","Vision-Audio Alignment","Temporal Reasoning","Data Curation","Foundation Models","Contrastive Learning","Rotary Time Embedding"],"permalink":"/ai/review/2025-10-20-OmniVinci_Enhancing_Architecture_and_Data_for_Omni-Modal_Understanding_LLM/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-NANO3D_A_Training-Free_Approach_for_Efficient_3D_Editing_Without_Masks","title":"[논문리뷰] NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks","excerpt":"Hongyu Yan이 [arXiv]에 게시한 'NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","3D Object Editing","Training-Free","FlowEdit","Mask-Free","Deep Generative Models","TRELLIS","Data Generation","Geometric Consistency"],"permalink":"/ai/review/2025-10-20-NANO3D_A_Training-Free_Approach_for_Efficient_3D_Editing_Without_Masks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-MorphoBench_A_Benchmark_with_Difficulty_Adaptive_to_Model_Reasoning","title":"[논문리뷰] MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning","excerpt":"이 [arXiv]에 게시한 'MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Reasoning Benchmark","Difficulty Adaptation","Multimodal AI","Proof Graph","Agent Recognition","Automated Question Generation"],"permalink":"/ai/review/2025-10-20-MorphoBench_A_Benchmark_with_Difficulty_Adaptive_to_Model_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-LightsOut_Diffusion-based_Outpainting_for_Enhanced_Lens_Flare_Removal","title":"[논문리뷰] LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal","excerpt":"이 [arXiv]에 게시한 'LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Lens Flare Removal","Diffusion Models","Image Outpainting","Deep Learning","Image Restoration","Preprocessing","LoRA"],"permalink":"/ai/review/2025-10-20-LightsOut_Diffusion-based_Outpainting_for_Enhanced_Lens_Flare_Removal/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Latent_Diffusion_Model_without_Variational_Autoencoder","title":"[논문리뷰] Latent Diffusion Model without Variational Autoencoder","excerpt":"이 [arXiv]에 게시한 'Latent Diffusion Model without Variational Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Latent Diffusion Model","Variational Autoencoder","Self-supervised Learning","DINO Features","Generative Models","Image Generation","Training Efficiency","Unified Representation"],"permalink":"/ai/review/2025-10-20-Latent_Diffusion_Model_without_Variational_Autoencoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Language_Models_Model_Language","title":"[논문리뷰] Language Models Model Language","excerpt":"이 [arXiv]에 게시한 'Language Models Model Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Large Language Models","Linguistics","Witold Mańczak","Frequency Hypothesis","Empirical Validation","Usage-Based Linguistics","Semantic Embeddings"],"permalink":"/ai/review/2025-10-20-Language_Models_Model_Language/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-InfiMed-ORBIT_Aligning_LLMs_on_Open-Ended_Complex_Tasks_via_Rubric-Based_Incremental_Training","title":"[논문리뷰] InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training","excerpt":"Congkai Xie이 [arXiv]에 게시한 'InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","LLMs","Reinforcement Learning","Rubric-Based Training","Medical Dialogue","Open-Ended Tasks","HealthBench","RAG"],"permalink":"/ai/review/2025-10-20-InfiMed-ORBIT_Aligning_LLMs_on_Open-Ended_Complex_Tasks_via_Rubric-Based_Incremental_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Imaginarium_Vision-guided_High-Quality_3D_Scene_Layout_Generation","title":"[논문리뷰] Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation","excerpt":"Junsheng Yu이 [arXiv]에 게시한 'Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","3D Scene Layout Generation","Vision-guided","Diffusion Models","Scene Graph","Asset Retrieval","Pose Estimation","High-Quality Assets","AI Content Creation"],"permalink":"/ai/review/2025-10-20-Imaginarium_Vision-guided_High-Quality_3D_Scene_Layout_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Foundation_Models_for_Scientific_Discovery_From_Paradigm_Enhancement_to_Paradigm_Transition","title":"[논문리뷰] Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition","excerpt":"이 [arXiv]에 게시한 'Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Foundation Models","Scientific Discovery","Paradigm Shift","Human-AI Collaboration","Autonomous Agents","Meta-Science","Experimental Design","Hypothesis Generation"],"permalink":"/ai/review/2025-10-20-Foundation_Models_for_Scientific_Discovery_From_Paradigm_Enhancement_to_Paradigm_Transition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-FinTrust_A_Comprehensive_Benchmark_of_Trustworthiness_Evaluation_in_Finance_Domain","title":"[논문리뷰] FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain","excerpt":"Arman Cohan이 [arXiv]에 게시한 'FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","LLM Trustworthiness","Finance Domain","Benchmark","Alignment Evaluation","Financial AI","Hallucination","Privacy","Fairness"],"permalink":"/ai/review/2025-10-20-FinTrust_A_Comprehensive_Benchmark_of_Trustworthiness_Evaluation_in_Finance_Domain/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Explore_to_Evolve_Scaling_Evolved_Aggregation_Logic_via_Proactive_Online_Exploration_for_Deep_Research_Agents","title":"[논문리뷰] Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents","excerpt":"Jianshu Zhang이 [arXiv]에 게시한 'Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Web Agents","Information Aggregation","Data Synthesis","Online Exploration","Foundation Models","Multi-hop QA","Deep Research"],"permalink":"/ai/review/2025-10-20-Explore_to_Evolve_Scaling_Evolved_Aggregation_Logic_via_Proactive_Online_Exploration_for_Deep_Research_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Emergent_Misalignment_via_In-Context_Learning_Narrow_in-context_examples_can_produce_broadly_misaligned_LLMs","title":"[논문리뷰] Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs","excerpt":"Kevin Zhu이 [arXiv]에 게시한 'Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Emergent Misalignment","In-Context Learning","LLM Safety","Persona Rationalization","Prompt Engineering","Model Alignment"],"permalink":"/ai/review/2025-10-20-Emergent_Misalignment_via_In-Context_Learning_Narrow_in-context_examples_can_produce_broadly_misaligned_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-ERGO_Entropy-guided_Resetting_for_Generation_Optimization_in_Multi-turn_Language_Models","title":"[논문리뷰] ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models","excerpt":"Sean O'Brien이 [arXiv]에 게시한 'ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Multi-turn Conversation","Large Language Models (LLMs)","Context Management","Entropy-guided Resetting","Uncertainty Quantification","Performance Degradation","Prompt Engineering","Conversational AI"],"permalink":"/ai/review/2025-10-20-ERGO_Entropy-guided_Resetting_for_Generation_Optimization_in_Multi-turn_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-DriveGen3D_Boosting_Feed-Forward_Driving_Scene_Generation_with_Efficient_Video_Diffusion","title":"[논문리뷰] DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion","excerpt":"이 [arXiv]에 게시한 'DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Driving Scene Generation","Video Diffusion","3D Reconstruction","Gaussian Splatting","Feed-Forward Models","Temporal Coherence","Multimodal Control"],"permalink":"/ai/review/2025-10-20-DriveGen3D_Boosting_Feed-Forward_Driving_Scene_Generation_with_Efficient_Video_Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-DLER_Doing_Length_pEnalty_Right_-_Incentivizing_More_Intelligence_per_Token_via_Reinforcement_Learning","title":"[논문리뷰] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Length Penalty","Reasoning Efficiency","Large Language Models","RL Optimization","Accuracy-Efficiency Trade-off","Chain-of-Thought"],"permalink":"/ai/review/2025-10-20-DLER_Doing_Length_pEnalty_Right_-_Incentivizing_More_Intelligence_per_Token_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-Build_Your_Personalized_Research_Group_A_Multiagent_Framework_for_Continual_and_Interactive_Science_Automation","title":"[논문리뷰] Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation","excerpt":"Cat Yan이 [arXiv]에 게시한 'Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Multiagent Systems","Science Automation","Dynamic Workflows","Workspace-based Communication","Context Compaction","Human-in-the-loop AI","Open-source Framework"],"permalink":"/ai/review/2025-10-20-Build_Your_Personalized_Research_Group_A_Multiagent_Framework_for_Continual_and_Interactive_Science_Automation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-BLIP3o-NEXT_Next_Frontier_of_Native_Image_Generation","title":"[논문리뷰] BLIP3o-NEXT: Next Frontier of Native Image Generation","excerpt":"이 [arXiv]에 게시한 'BLIP3o-NEXT: Next Frontier of Native Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Image Generation","Image Editing","Autoregressive Model","Diffusion Model","Reinforcement Learning","Multimodal AI","Foundation Model","Open-source"],"permalink":"/ai/review/2025-10-20-BLIP3o-NEXT_Next_Frontier_of_Native_Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-20-A2FM_An_Adaptive_Agent_Foundation_Model_for_Tool-Aware_Hybrid_Reasoning","title":"[논문리뷰] A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning","excerpt":"이 [arXiv]에 게시한 'A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","lastModifiedAt":"2025-10-20 13:04:24+0900","categories":["Review"],"tags":["Review","Adaptive Agent","Foundation Model","Hybrid Reasoning","Tool-Aware LLM","Mode Selection","Reinforcement Learning","Cost Efficiency","LLM Agent"],"permalink":"/ai/review/2025-10-20-A2FM_An_Adaptive_Agent_Foundation_Model_for_Tool-Aware_Hybrid_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-pi-Flow_Policy-Based_Few-Step_Generation_via_Imitation_Distillation","title":"[논문리뷰] pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation","excerpt":"이 [arXiv]에 게시한 'pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Diffusion Models","Flow Matching","Generative Models","Model Distillation","Imitation Learning","Few-Step Generation","Policy-Based AI","Text-to-Image"],"permalink":"/ai/review/2025-10-17-pi-Flow_Policy-Based_Few-Step_Generation_via_Imitation_Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-WithAnyone_Towards_Controllable_and_ID_Consistent_Image_Generation","title":"[논문리뷰] WithAnyone: Towards Controllable and ID Consistent Image Generation","excerpt":"이 [arXiv]에 게시한 'WithAnyone: Towards Controllable and ID Consistent Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Identity-Consistent Generation","Text-to-Image Diffusion","Copy-Paste Artifacts","Contrastive Learning","Multi-Identity Dataset","Controllable Generation","ID-Preservation"],"permalink":"/ai/review/2025-10-17-WithAnyone_Towards_Controllable_and_ID_Consistent_Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-When_Models_Lie_We_Learn_Multilingual_Span-Level_Hallucination_Detection_with_PsiloQA","title":"[논문리뷰] When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA","excerpt":"Artem Vazhentsev이 [arXiv]에 게시한 'When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Hallucination Detection","Multilingual LLMs","Span-Level Annotation","Synthetic Data Generation","Question Answering (QA)","Encoder Models","Uncertainty Quantification","GPT-4o"],"permalink":"/ai/review/2025-10-17-When_Models_Lie_We_Learn_Multilingual_Span-Level_Hallucination_Detection_with_PsiloQA/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-VR-Thinker_Boosting_Video_Reward_Models_through_Thinking-with-Image_Reasoning","title":"[논문리뷰] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning","excerpt":"이 [arXiv]에 게시한 'VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Video Reward Models","Multimodal Reasoning","Thinking-with-Image","Visual Reasoning","Reinforcement Learning","Chain-of-Thought","Context Management"],"permalink":"/ai/review/2025-10-17-VR-Thinker_Boosting_Video_Reward_Models_through_Thinking-with-Image_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-VLA2_Empowering_Vision-Language-Action_Models_with_an_Agentic_Framework_for_Unseen_Concept_Manipulation","title":"[논문리뷰] VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation","excerpt":"이 [arXiv]에 게시한 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Agentic Framework","Unseen Concept Manipulation","Out-of-Distribution Generalization","Tool Use","Web Retrieval","Object Detection","LIBERO Simulation"],"permalink":"/ai/review/2025-10-17-VLA2_Empowering_Vision-Language-Action_Models_with_an_Agentic_Framework_for_Unseen_Concept_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-VLA-0_Building_State-of-the-Art_VLAs_with_Zero_Modification","title":"[논문리뷰] VLA-0: Building State-of-the-Art VLAs with Zero Modification","excerpt":"이 [arXiv]에 게시한 'VLA-0: Building State-of-the-Art VLAs with Zero Modification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","VLA-0","Zero Modification","Text-based Action Prediction","Robot Manipulation","Large Language Models","Fine-tuning","State-of-the-Art"],"permalink":"/ai/review/2025-10-17-VLA-0_Building_State-of-the-Art_VLAs_with_Zero_Modification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-VIST3A_Text-to-3D_by_Stitching_a_Multi-view_Reconstruction_Network_to_a_Video_Generator","title":"[논문리뷰] VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator","excerpt":"Federico Tombari이 [arXiv]에 게시한 'VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Text-to-3D","Model Stitching","Multi-view Reconstruction","Video Generation","Latent Diffusion Models","Gaussian Splats","Pointmaps","Reward Finetuning"],"permalink":"/ai/review/2025-10-17-VIST3A_Text-to-3D_by_Stitching_a_Multi-view_Reconstruction_Network_to_a_Video_Generator/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-TokDrift_When_LLM_Speaks_in_Subwords_but_Code_Speaks_in_Grammar","title":"[논문리뷰] TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar","excerpt":"이 [arXiv]에 게시한 'TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Code LLMs","Subword Tokenization","Grammar-aware Tokenization","Semantic Preservation","Rewrite Rules","Model Robustness","Tokenization Misalignment"],"permalink":"/ai/review/2025-10-17-TokDrift_When_LLM_Speaks_in_Subwords_but_Code_Speaks_in_Grammar/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-The_German_Commons_-_154_Billion_Tokens_of_Openly_Licensed_Text_for_German_Language_Models","title":"[논문리뷰] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models","excerpt":"이 [arXiv]에 게시한 'The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","German Commons","Large Language Models","Training Data","Openly Licensed Text","Data Curation","German NLP","Corpus Construction","Quality Filtering"],"permalink":"/ai/review/2025-10-17-The_German_Commons_-_154_Billion_Tokens_of_Openly_Licensed_Text_for_German_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-SCas4D_Structural_Cascaded_Optimization_for_Boosting_Persistent_4D_Novel_View_Synthesis","title":"[논문리뷰] SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis","excerpt":"이 [arXiv]에 게시한 'SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","4D Novel View Synthesis","Dynamic Scenes","3D Gaussian Splatting","Cascaded Optimization","Deformation Modeling","Point Tracking","Object Segmentation"],"permalink":"/ai/review/2025-10-17-SCas4D_Structural_Cascaded_Optimization_for_Boosting_Persistent_4D_Novel_View_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-RefusalBench_Generative_Evaluation_of_Selective_Refusal_in_Grounded_Language_Models","title":"[논문리뷰] RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models","excerpt":"이 [arXiv]에 게시한 'RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","RAG Systems","Selective Refusal","Generative Evaluation","Linguistic Perturbations","LLM Evaluation","Informational Uncertainty","Model Calibration","AI Safety"],"permalink":"/ai/review/2025-10-17-RefusalBench_Generative_Evaluation_of_Selective_Refusal_in_Grounded_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-RealDPO_Real_or_Not_Real_that_is_the_Preference","title":"[논문리뷰] RealDPO: Real or Not Real, that is the Preference","excerpt":"Chenyang Si이 [arXiv]에 게시한 'RealDPO: Real or Not Real, that is the Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Direct Preference Optimization","Preference Learning","Real Data","Human Motion Synthesis","RealDPO","RealAction-5K"],"permalink":"/ai/review/2025-10-17-RealDPO_Real_or_Not_Real_that_is_the_Preference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-RAGCap-Bench_Benchmarking_Capabilities_of_LLMs_in_Agentic_Retrieval_Augmented_Generation_Systems","title":"[논문리뷰] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems","excerpt":"이 [arXiv]에 게시한 'RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Retrieval Augmented Generation","Agentic Systems","Benchmarking","Intermediate Tasks","Error Analysis","LLM Evaluation"],"permalink":"/ai/review/2025-10-17-RAGCap-Bench_Benchmarking_Capabilities_of_LLMs_in_Agentic_Retrieval_Augmented_Generation_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Qwen3Guard_Technical_Report","title":"[논문리뷰] Qwen3Guard Technical Report","excerpt":"이 [arXiv]에 게시한 'Qwen3Guard Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLM Safety","Guardrail Models","Multilingual AI","Real-time Moderation","Tri-class Classification","Instruction Tuning","Streaming Inference"],"permalink":"/ai/review/2025-10-17-Qwen3Guard_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Ponimator_Unfolding_Interactive_Pose_for_Versatile_Human-human_Interaction_Animation","title":"[논문리뷰] Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation","excerpt":"이 [arXiv]에 게시한 'Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Human-human Interaction","Pose Animation","Diffusion Models","Generative AI","Motion Synthesis","Interactive Poses","Temporal Priors","Spatial Priors"],"permalink":"/ai/review/2025-10-17-Ponimator_Unfolding_Interactive_Pose_for_Versatile_Human-human_Interaction_Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-PaddleOCR-VL_Boosting_Multilingual_Document_Parsing_via_a_0.9B_Ultra-Compact_Vision-Language_Model","title":"[논문리뷰] PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model","excerpt":"이 [arXiv]에 게시한 'PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Document Parsing","Vision-Language Model","Multilingual OCR","Layout Analysis","Resource-Efficient AI","Table Recognition","Formula Recognition","Chart Recognition"],"permalink":"/ai/review/2025-10-17-PaddleOCR-VL_Boosting_Multilingual_Document_Parsing_via_a_0.9B_Ultra-Compact_Vision-Language_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-On_Pretraining_for_Project-Level_Code_Completion","title":"[논문리뷰] On Pretraining for Project-Level Code Completion","excerpt":"이 [arXiv]에 게시한 'On Pretraining for Project-Level Code Completion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Code LLMs","Project-level Context","Code Completion","Context Window Extension","RoPE Scaling","Repository Pretraining","Long Code Arena"],"permalink":"/ai/review/2025-10-17-On_Pretraining_for_Project-Level_Code_Completion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-MoM_Mixtures_of_Scenario-Aware_Document_Memories_for_Retrieval-Augmented_Generation_Systems","title":"[논문리뷰] MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems","excerpt":"Feiyu Xiong이 [arXiv]에 게시한 'MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation (RAG)","Document Memory","Text Chunking","Small Language Models (SLMs)","Large Language Models (LLMs)","Scenario-Aware Processing","Multi-Layer Retrieval","Cognitive Simulation"],"permalink":"/ai/review/2025-10-17-MoM_Mixtures_of_Scenario-Aware_Document_Memories_for_Retrieval-Augmented_Generation_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-MathCanvas_Intrinsic_Visual_Chain-of-Thought_for_Multimodal_Mathematical_Reasoning","title":"[논문리뷰] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning","excerpt":"Ke Wang이 [arXiv]에 게시한 'MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Visual Chain-of-Thought (VCoT)","Large Multimodal Models (LMMs)","Geometric Reasoning","Diagram Generation","Dataset","Benchmark"],"permalink":"/ai/review/2025-10-17-MathCanvas_Intrinsic_Visual_Chain-of-Thought_for_Multimodal_Mathematical_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-LiteStage_Latency-aware_Layer_Skipping_for_Multi-stage_Reasoning","title":"[논문리뷰] LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning","excerpt":"이 [arXiv]에 게시한 'LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Layer Skipping","Multi-stage Reasoning","Latency Optimization","Early Exit","Small Language Models (LLMs)","Adaptive Computation","Confidence-based Decoding"],"permalink":"/ai/review/2025-10-17-LiteStage_Latency-aware_Layer_Skipping_for_Multi-stage_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Learning_an_Image_Editing_Model_without_Image_Editing_Pairs","title":"[논문리뷰] Learning an Image Editing Model without Image Editing Pairs","excerpt":"이 [arXiv]에 게시한 'Learning an Image Editing Model without Image Editing Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Vision-Language Models (VLMs)","No-Pair Training","Few-step Generation","Distribution Matching","Gradient-based Optimization"],"permalink":"/ai/review/2025-10-17-Learning_an_Image_Editing_Model_without_Image_Editing_Pairs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Large_Language_Models_Do_NOT_Really_Know_What_They_Dont_Know","title":"[논문리뷰] Large Language Models Do NOT Really Know What They Don't Know","excerpt":"이 [arXiv]에 게시한 'Large Language Models Do NOT Really Know What They Don't Know' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLMs","Hallucination Detection","Mechanistic Interpretability","Internal States","Knowledge Recall","Refusal Tuning","Factual Associations","Associated Hallucinations"],"permalink":"/ai/review/2025-10-17-Large_Language_Models_Do_NOT_Really_Know_What_They_Dont_Know/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-LaSeR_Reinforcement_Learning_with_Last-Token_Self-Rewarding","title":"[논문리뷰] LaSeR: Reinforcement Learning with Last-Token Self-Rewarding","excerpt":"이 [arXiv]에 게시한 'LaSeR: Reinforcement Learning with Last-Token Self-Rewarding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM","Self-Verification","Last-Token","Reward Modeling","Efficiency","Reasoning","RLVR"],"permalink":"/ai/review/2025-10-17-LaSeR_Reinforcement_Learning_with_Last-Token_Self-Rewarding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-LLMs_as_Scalable_General-Purpose_Simulators_For_Evolving_Digital_Agent_Training","title":"[논문리뷰] LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training","excerpt":"이 [arXiv]에 게시한 'LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLM","Digital Agents","UI Simulation","Synthetic Data Generation","Targeted Data Synthesis","World Models"],"permalink":"/ai/review/2025-10-17-LLMs_as_Scalable_General-Purpose_Simulators_For_Evolving_Digital_Agent_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-LLM-guided_Hierarchical_Retrieval","title":"[논문리뷰] LLM-guided Hierarchical Retrieval","excerpt":"이 [arXiv]에 게시한 'LLM-guided Hierarchical Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Information Retrieval","Large Language Models","Hierarchical Retrieval","Semantic Tree","Tree Traversal","Zero-shot Performance","Reasoning-based Retrieval","Computational Efficiency"],"permalink":"/ai/review/2025-10-17-LLM-guided_Hierarchical_Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Information_Gain-based_Policy_Optimization_A_Simple_and_Effective_Approach_for_Multi-Turn_LLM_Agents","title":"[논문리뷰] Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents","excerpt":"이 [arXiv]에 게시한 'Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Multi-Turn Interactions","Reward Sparsity","Information Gain","Policy Optimization","Ground-Truth Awareness","Sample Efficiency"],"permalink":"/ai/review/2025-10-17-Information_Gain-based_Policy_Optimization_A_Simple_and_Effective_Approach_for_Multi-Turn_LLM_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-ImagerySearch_Adaptive_Test-Time_Search_for_Video_Generation_Beyond_Semantic_Dependency_Constraints","title":"[논문리뷰] ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints","excerpt":"이 [arXiv]에 게시한 'ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Video Generation","Test-Time Search","Diffusion Models","Semantic Dependency","Adaptive Reward","Evaluation Benchmark","Prompt-Guided"],"permalink":"/ai/review/2025-10-17-ImagerySearch_Adaptive_Test-Time_Search_for_Video_Generation_Beyond_Semantic_Dependency_Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-From_Pixels_to_Words_--_Towards_Native_Vision-Language_Primitives_at_Scale","title":"[논문리뷰] From Pixels to Words -- Towards Native Vision-Language Primitives at Scale","excerpt":"이 [arXiv]에 게시한 'From Pixels to Words -- Towards Native Vision-Language Primitives at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Native VLMs","Early Fusion","Multimodal Learning","Transformer Architecture","Rotary Position Embeddings","Pixel-Word Alignment","End-to-End Training"],"permalink":"/ai/review/2025-10-17-From_Pixels_to_Words_--_Towards_Native_Vision-Language_Primitives_at_Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Fantastic_small_Retrievers_and_How_to_Train_Them_mxbai-edge-colbert-v0_Tech_Report","title":"[논문리뷰] Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report","excerpt":"이 [arXiv]에 게시한 'Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","ColBERT","Retrieval Models","Small Models","Distillation","Long Context","Edge AI","Information Retrieval","RAG"],"permalink":"/ai/review/2025-10-17-Fantastic_small_Retrievers_and_How_to_Train_Them_mxbai-edge-colbert-v0_Tech_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Expertise_need_not_monopolize_Action-Specialized_Mixture_of_Experts_for_Vision-Language-Action_Learning","title":"[논문리뷰] Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning","excerpt":"Sijia Gu이 [arXiv]에 게시한 'Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Mixture of Experts (MoE)","Robotic Manipulation","Expert Specialization","Decoupled Routing","Load Balancing","Transfer Learning"],"permalink":"/ai/review/2025-10-17-Expertise_need_not_monopolize_Action-Specialized_Mixture_of_Experts_for_Vision-Language-Action_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Efficient_Parallel_Samplers_for_Recurrent-Depth_Models_and_Their_Connection_to_Diffusion_Language_Models","title":"[논문리뷰] Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models","excerpt":"이 [arXiv]에 게시한 'Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Recurrent-Depth Models","Diffusion Forcing","Parallel Sampling","LLM Inference Acceleration","Transformer Architectures","Generative AI","Latent Space Diffusion"],"permalink":"/ai/review/2025-10-17-Efficient_Parallel_Samplers_for_Recurrent-Depth_Models_and_Their_Connection_to_Diffusion_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-DialectGen_Benchmarking_and_Improving_Dialect_Robustness_in_Multimodal_Generation","title":"[논문리뷰] DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation","excerpt":"이 [arXiv]에 게시한 'DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Multimodal Generation","Dialect Robustness","Text-to-Image","Text-to-Video","Benchmarking","Diffusion Models","Text Encoder Tuning","Low-Resource Dialects"],"permalink":"/ai/review/2025-10-17-DialectGen_Benchmarking_and_Improving_Dialect_Robustness_in_Multimodal_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-COIG-Writer_A_High-Quality_Dataset_for_Chinese_Creative_Writing_with_Thought_Processes","title":"[논문리뷰] COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes","excerpt":"이 [arXiv]에 게시한 'COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Chinese Creative Writing","Process Supervision","LLM Training","Dataset Creation","Cross-Lingual Transfer","Narrative Logic","Linguistic Expression","Type-Token Ratio"],"permalink":"/ai/review/2025-10-17-COIG-Writer_A_High-Quality_Dataset_for_Chinese_Creative_Writing_with_Thought_Processes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-BitNet_Distillation","title":"[논문리뷰] BitNet Distillation","excerpt":"이 [arXiv]에 게시한 'BitNet Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Low-bit Quantization","LLM Compression","Knowledge Distillation","Ternary Weights","Inference Optimization","Memory Efficiency","SubLN","Continual Pre-training"],"permalink":"/ai/review/2025-10-17-BitNet_Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Beyond_One_World_Benchmarking_Super_Heros_in_Role-Playing_Across_Multiversal_Contexts","title":"[논문리뷰] Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts","excerpt":"이 [arXiv]에 게시한 'Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Role-playing LLMs","Multiversal Consistency","Character Benchmarking","Moral Dilemmas","Canon Events","Reasoning-Acting Alignment","Chain-of-Thought","Superheroes"],"permalink":"/ai/review/2025-10-17-Beyond_One_World_Benchmarking_Super_Heros_in_Role-Playing_Across_Multiversal_Contexts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Beyond_Correctness_Evaluating_Subjective_Writing_Preferences_Across_Cultures","title":"[논문리뷰] Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures","excerpt":"이 [arXiv]에 게시한 'Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Subjective Preference Learning","Writing Evaluation","Reward Models","RLHF","Cross-Cultural AI","Generative Models","Language Model Judges","Genre Instability"],"permalink":"/ai/review/2025-10-17-Beyond_Correctness_Evaluating_Subjective_Writing_Preferences_Across_Cultures/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Attention_Is_All_You_Need_for_KV_Cache_in_Diffusion_LLMs","title":"[논문리뷰] Attention Is All You Need for KV Cache in Diffusion LLMs","excerpt":"이 [arXiv]에 게시한 'Attention Is All You Need for KV Cache in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","KV Cache","Adaptive Caching","Inference Optimization","Attention Mechanism","Latency Reduction","Generative AI"],"permalink":"/ai/review/2025-10-17-Attention_Is_All_You_Need_for_KV_Cache_in_Diffusion_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-Agentic_Entropy-Balanced_Policy_Optimization","title":"[논문리뷰] Agentic Entropy-Balanced Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Agentic Entropy-Balanced Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Web Agents","Tool Learning","Entropy Balancing","Policy Optimization","Rollout Strategy","Large Language Models"],"permalink":"/ai/review/2025-10-17-Agentic_Entropy-Balanced_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-17-AI_for_Service_Proactive_Assistance_with_AI_Glasses","title":"[논문리뷰] AI for Service: Proactive Assistance with AI Glasses","excerpt":"이 [arXiv]에 게시한 'AI for Service: Proactive Assistance with AI Glasses' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","lastModifiedAt":"2025-10-17 13:09:57+0900","categories":["Review"],"tags":["Review","AI for Service","Proactive AI","AI Glasses","Multi-agent System","Human-AI Interaction","Context-aware AI","Wearable AI"],"permalink":"/ai/review/2025-10-17-AI_for_Service_Proactive_Assistance_with_AI_Glasses/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-X-VLA_Soft-Prompted_Transformer_as_Scalable_Cross-Embodiment_Vision-Language-Action_Model","title":"[논문리뷰] X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model","excerpt":"Xirui Kang이 [arXiv]에 게시한 'X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA) Models","Soft Prompts","Transformer","Cross-Embodiment","Robotics","Pretraining","Domain Adaptation","Flow Matching"],"permalink":"/ai/review/2025-10-16-X-VLA_Soft-Prompted_Transformer_as_Scalable_Cross-Embodiment_Vision-Language-Action_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Universal_Image_Restoration_Pre-training_via_Masked_Degradation_Classification","title":"[논문리뷰] Universal Image Restoration Pre-training via Masked Degradation Classification","excerpt":"이 [arXiv]에 게시한 'Universal Image Restoration Pre-training via Masked Degradation Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Universal Image Restoration","Pre-training","Masked Image Modeling","Degradation Classification","Deep Learning","Computer Vision","Self-supervised Learning","Low-level Vision"],"permalink":"/ai/review/2025-10-16-Universal_Image_Restoration_Pre-training_via_Masked_Degradation_Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-UniMoE-Audio_Unified_Speech_and_Music_Generation_with_Dynamic-Capacity_MoE","title":"[논문리뷰] UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE","excerpt":"이 [arXiv]에 게시한 'UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Mixture of Experts","Speech Generation","Music Generation","Multimodal AI","Dynamic Routing","Training Curriculum","Data Imbalance","Audio Synthesis"],"permalink":"/ai/review/2025-10-16-UniMoE-Audio_Unified_Speech_and_Music_Generation_with_Dynamic-Capacity_MoE/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-UniME-V2_MLLM-as-a-Judge_for_Universal_Multimodal_Embedding_Learning","title":"[논문리뷰] UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning","excerpt":"Ziyong Feng이 [arXiv]에 게시한 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal Embeddings","MLLM-as-a-Judge","Hard Negative Mining","Semantic Alignment","Representation Learning","Reranking","Contrastive Learning"],"permalink":"/ai/review/2025-10-16-UniME-V2_MLLM-as-a-Judge_for_Universal_Multimodal_Embedding_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Uni-MMMU_A_Massive_Multi-discipline_Multimodal_Unified_Benchmark","title":"[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark","excerpt":"이 [arXiv]에 게시한 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal AI","Unified Models","Benchmark","Generation","Understanding","Reasoning","Evaluation","Cross-modal Synergy"],"permalink":"/ai/review/2025-10-16-Uni-MMMU_A_Massive_Multi-discipline_Multimodal_Unified_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Trace_Anything_Representing_Any_Video_in_4D_via_Trajectory_Fields","title":"[논문리뷰] Trace Anything: Representing Any Video in 4D via Trajectory Fields","excerpt":"이 [arXiv]에 게시한 'Trace Anything: Representing Any Video in 4D via Trajectory Fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","4D Video Representation","Trajectory Fields","Neural Networks","Spatio-temporal Modeling","3D Point Tracking","Motion Forecasting","Computer Vision","B-splines"],"permalink":"/ai/review/2025-10-16-Trace_Anything_Representing_Any_Video_in_4D_via_Trajectory_Fields/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-The_Role_of_Computing_Resources_in_Publishing_Foundation_Model_Research","title":"[논문리뷰] The Role of Computing Resources in Publishing Foundation Model Research","excerpt":"Zhenwen Liang이 [arXiv]에 게시한 'The Role of Computing Resources in Publishing Foundation Model Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Foundation Models","Computing Resources","GPU Disparity","AI Research","Publication Bias","Resource Allocation","Research Transparency"],"permalink":"/ai/review/2025-10-16-The_Role_of_Computing_Resources_in_Publishing_Foundation_Model_Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-The_Art_of_Scaling_Reinforcement_Learning_Compute_for_LLMs","title":"[논문리뷰] The Art of Scaling Reinforcement Learning Compute for LLMs","excerpt":"이 [arXiv]에 게시한 'The Art of Scaling Reinforcement Learning Compute for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Scaling Laws","Compute Efficiency","Predictability","Sigmoidal Curves","ScaleRL","Off-Policy RL"],"permalink":"/ai/review/2025-10-16-The_Art_of_Scaling_Reinforcement_Learning_Compute_for_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Stronger_Together_On-Policy_Reinforcement_Learning_for_Collaborative_LLMs","title":"[논문리뷰] Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs","excerpt":"Hao Zhang이 [arXiv]에 게시한 'Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Reinforcement Learning (RL)","Multi-Agent Systems (MAS)","On-Policy RL","Collaborative AI","Agentic LLMs","Group-based Optimization"],"permalink":"/ai/review/2025-10-16-Stronger_Together_On-Policy_Reinforcement_Learning_for_Collaborative_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Revisiting_Model_Interpolation_for_Efficient_Reasoning","title":"[논문리뷰] Revisiting Model Interpolation for Efficient Reasoning","excerpt":"이 [arXiv]에 게시한 'Revisiting Model Interpolation for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Model Interpolation","Efficient Reasoning","Large Language Models","Chain-of-Thought","Model Merging","Performance Dynamics","Ablation Study"],"permalink":"/ai/review/2025-10-16-Revisiting_Model_Interpolation_for_Efficient_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Reasoning_in_Space_via_Grounding_in_the_World","title":"[논문리뷰] Reasoning in Space via Grounding in the World","excerpt":"Li Zhang이 [arXiv]에 게시한 'Reasoning in Space via Grounding in the World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","3D Visual Grounding","Spatial Reasoning","Large Language Models (LLMs)","Chain-of-Thought (CoT)","Hybrid Representation","Multi-modal LLMs","Point Clouds"],"permalink":"/ai/review/2025-10-16-Reasoning_in_Space_via_Grounding_in_the_World/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Point_Prompting_Counterfactual_Tracking_with_Video_Diffusion_Models","title":"[논문리뷰] Point Prompting: Counterfactual Tracking with Video Diffusion Models","excerpt":"Andrew Owens이 [arXiv]에 게시한 'Point Prompting: Counterfactual Tracking with Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Video Diffusion Models","Point Tracking","Zero-Shot Learning","Counterfactual Modeling","Visual Prompting","SDEdit","Negative Prompting","Object Permanence"],"permalink":"/ai/review/2025-10-16-Point_Prompting_Counterfactual_Tracking_with_Video_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-PhysMaster_Mastering_Physical_Representation_for_Video_Generation_via_Reinforcement_Learning","title":"[논문리뷰] PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning","excerpt":"Hengshuang Zhao이 [arXiv]에 게시한 'PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Video Generation","Physical Plausibility","Reinforcement Learning","Direct Preference Optimization","Physical Representation","Diffusion Models","World Models","Image-to-Video"],"permalink":"/ai/review/2025-10-16-PhysMaster_Mastering_Physical_Representation_for_Video_Generation_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-ParallelBench_Understanding_the_Trade-offs_of_Parallel_Decoding_in_Diffusion_LLMs","title":"[논문리뷰] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs","excerpt":"이 [arXiv]에 게시한 'ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Parallel Decoding","Speed-Quality Trade-off","Benchmark","Token Dependencies","Unmasking Strategies","Information Theory"],"permalink":"/ai/review/2025-10-16-ParallelBench_Understanding_the_Trade-offs_of_Parallel_Decoding_in_Diffusion_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-NOSA_Native_and_Offloadable_Sparse_Attention","title":"[논문리뷰] NOSA: Native and Offloadable Sparse Attention","excerpt":"Zhiyuan Liu이 [arXiv]에 게시한 'NOSA: Native and Offloadable Sparse Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Sparse Attention","KV Cache Offloading","LLMs","Decoding Throughput","Locality Constraint","Memory Optimization","Trainable Sparse Attention"],"permalink":"/ai/review/2025-10-16-NOSA_Native_and_Offloadable_Sparse_Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-MTSQL-R1_Towards_Long-Horizon_Multi-Turn_Text-to-SQL_via_Agentic_Training","title":"[논문리뷰] MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training","excerpt":"이 [arXiv]에 게시한 'MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multi-turn Text-to-SQL","Agentic Training","Reinforcement Learning","Large Language Models","Dialogue Systems","Semantic Parsing","Database Interaction","Self-correction"],"permalink":"/ai/review/2025-10-16-MTSQL-R1_Towards_Long-Horizon_Multi-Turn_Text-to-SQL_via_Agentic_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-MATH-Beyond_A_Benchmark_for_RL_to_Expand_Beyond_the_Base_Model","title":"[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model","excerpt":"Wieland Brendel이 [arXiv]에 게시한 'MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Mathematical Reasoning","Benchmark","Large Language Models (LLMs)","Exploration","Boundary Expansion","MATH-Beyond"],"permalink":"/ai/review/2025-10-16-MATH-Beyond_A_Benchmark_for_RL_to_Expand_Beyond_the_Base_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-LIBERO-Plus_In-depth_Robustness_Analysis_of_Vision-Language-Action_Models","title":"[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models","excerpt":"이 [arXiv]에 게시한 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","Robustness Analysis","Generalization","Perturbations","Benchmark","LIBERO-Plus","Multimodal AI"],"permalink":"/ai/review/2025-10-16-LIBERO-Plus_In-depth_Robustness_Analysis_of_Vision-Language-Action_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-InternVLA-M1_A_Spatially_Guided_Vision-Language-Action_Framework_for_Generalist_Robot_Policy","title":"[논문리뷰] InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy","excerpt":"Yilun Chen이 [arXiv]에 게시한 'InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Robotics","Vision-Language-Action (VLA)","Spatial Grounding","Generalist Policy","Multimodal Learning","Instruction Following","Simulation-to-Real","Diffusion Models"],"permalink":"/ai/review/2025-10-16-InternVLA-M1_A_Spatially_Guided_Vision-Language-Action_Framework_for_Generalist_Robot_Policy/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-InteractiveOmni_A_Unified_Omni-modal_Model_for_Audio-Visual_Multi-turn_Dialogue","title":"[논문리뷰] InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue","excerpt":"Dongchuan Ran이 [arXiv]에 게시한 'InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Omni-modal LLM","Audio-Visual Dialogue","Multi-turn Interaction","Speech Generation","Long-term Memory","Multimodal Understanding","End-to-end Training"],"permalink":"/ai/review/2025-10-16-InteractiveOmni_A_Unified_Omni-modal_Model_for_Audio-Visual_Multi-turn_Dialogue/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-HyperAgent_Leveraging_Hypergraphs_for_Topology_Optimization_in_Multi-Agent_Communication","title":"[논문리뷰] HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication","excerpt":"Haochen You이 [arXiv]에 게시한 'HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Large Language Model","Multi-agent Systems","Multi-agent Communication","Graph Neural Networks","Hypergraph","Topology Optimization","Variational Autoencoder","Sparsity Regularization"],"permalink":"/ai/review/2025-10-16-HyperAgent_Leveraging_Hypergraphs_for_Topology_Optimization_in_Multi-Agent_Communication/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Hierarchical_Frequency_Tagging_Probe_HFTP_A_Unified_Approach_to_Investigate_Syntactic_Structure_Representations_in_Large_Language_Models_and_the_Human_Brain","title":"[논문리뷰] Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain","excerpt":"Lingxi Lu이 [arXiv]에 게시한 'Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Large Language Models","Syntactic Structure","Human Brain","Frequency Tagging","Neuroscience","Model Interpretability","Representational Similarity Analysis","Intracranial EEG"],"permalink":"/ai/review/2025-10-16-Hierarchical_Frequency_Tagging_Probe_HFTP_A_Unified_Approach_to_Investigate_Syntactic_Structure_Representations_in_Large_Language_Models_and_the_Human_Brain/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Hard2Verify_A_Step-Level_Verification_Benchmark_for_Open-Ended_Frontier_Math","title":"[논문리뷰] Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math","excerpt":"이 [arXiv]에 게시한 'Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Verification","Math Reasoning","Step-Level Verification","Benchmark","Open-Ended Problems","Process Reward Models","Generative Critics"],"permalink":"/ai/review/2025-10-16-Hard2Verify_A_Step-Level_Verification_Benchmark_for_Open-Ended_Frontier_Math/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-GraphTracer_Graph-Guided_Failure_Tracing_in_LLM_Agents_for_Robust_Multi-Turn_Deep_Search","title":"[논문리뷰] GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search","excerpt":"Zijian Zhang이 [arXiv]에 게시한 'GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Agents","Multi-Agent Systems","Failure Tracing","Root Cause Analysis","Information Dependency Graph","Reinforcement Learning","Deep Search"],"permalink":"/ai/review/2025-10-16-GraphTracer_Graph-Guided_Failure_Tracing_in_LLM_Agents_for_Robust_Multi-Turn_Deep_Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Generative_Universal_Verifier_as_Multimodal_Meta-Reasoner","title":"[논문리뷰] Generative Universal Verifier as Multimodal Meta-Reasoner","excerpt":"이 [arXiv]에 게시한 'Generative Universal Verifier as Multimodal Meta-Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal AI","Visual Verification","Generative Models","Self-Refinement","Vision-Language Models","Test-Time Scaling","Reasoning"],"permalink":"/ai/review/2025-10-16-Generative_Universal_Verifier_as_Multimodal_Meta-Reasoner/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-FlashWorld_High-quality_3D_Scene_Generation_within_Seconds","title":"[논문리뷰] FlashWorld: High-quality 3D Scene Generation within Seconds","excerpt":"Chunchao Guo이 [arXiv]에 게시한 'FlashWorld: High-quality 3D Scene Generation within Seconds' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Diffusion Models","Multi-View Synthesis","3D Gaussian Splatting","Knowledge Distillation","Real-time Generation","High-Quality Rendering","Cross-modal Training"],"permalink":"/ai/review/2025-10-16-FlashWorld_High-quality_3D_Scene_Generation_within_Seconds/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-FG-CLIP_2_A_Bilingual_Fine-grained_Vision-Language_Alignment_Model","title":"[논문리뷰] FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model","excerpt":"Dawei Liang이 [arXiv]에 게시한 'FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Vision-Language Alignment","Fine-grained Understanding","Bilingual Model","Contrastive Learning","Multimodal Retrieval","Open-Vocabulary Detection","Region-Text Matching"],"permalink":"/ai/review/2025-10-16-FG-CLIP_2_A_Bilingual_Fine-grained_Vision-Language_Alignment_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-EAGER_Entropy-Aware_GEneRation_for_Adaptive_Inference-Time_Scaling","title":"[논문리뷰] EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling","excerpt":"Ahmet Üstün이 [arXiv]에 게시한 'EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM","Inference-Time Scaling","Entropy-Aware Generation","Adaptive Budget Allocation","Reasoning Benchmarks","Computational Efficiency","Chain-of-Thought"],"permalink":"/ai/review/2025-10-16-EAGER_Entropy-Aware_GEneRation_for_Adaptive_Inference-Time_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Direct_Multi-Token_Decoding","title":"[논문리뷰] Direct Multi-Token Decoding","excerpt":"Xifeng Yan이 [arXiv]에 게시한 'Direct Multi-Token Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Inference","Multi-token Decoding","Transformer Architecture","Layer Specialization","Cyclical Refilling","Inference Speedup","Model Scaling"],"permalink":"/ai/review/2025-10-16-Direct_Multi-Token_Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Deflanderization_for_Game_Dialogue_Balancing_Character_Authenticity_with_Task_Execution_in_LLM-based_NPCs","title":"[논문리뷰] Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs","excerpt":"이 [arXiv]에 게시한 'Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM","NPC","Game Dialogue","Persona-Grounded Dialogue","Task Execution","Prompt Engineering","Fine-tuning","Deflanderization"],"permalink":"/ai/review/2025-10-16-Deflanderization_for_Game_Dialogue_Balancing_Character_Authenticity_with_Task_Execution_in_LLM-based_NPCs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-CoIRL-AD_Collaborative-Competitive_Imitation-Reinforcement_Learning_in_Latent_World_Models_for_Autonomous_Driving","title":"[논문리뷰] CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving","excerpt":"이 [arXiv]에 게시한 'CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Imitation Learning","Reinforcement Learning","World Models","Latent Space","Dual-Policy","Competitive Learning"],"permalink":"/ai/review/2025-10-16-CoIRL-AD_Collaborative-Competitive_Imitation-Reinforcement_Learning_in_Latent_World_Models_for_Autonomous_Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-CVD-STORM_Cross-View_Video_Diffusion_with_Spatial-Temporal_Reconstruction_Model_for_Autonomous_Driving","title":"[논문리뷰] CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving","excerpt":"Jingcheng Ni이 [arXiv]에 게시한 'CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Video Generation","Diffusion Models","Spatial-Temporal Reconstruction","3D Gaussian Splatting","Variational Autoencoder","World Modeling","Multi-View Video"],"permalink":"/ai/review/2025-10-16-CVD-STORM_Cross-View_Video_Diffusion_with_Spatial-Temporal_Reconstruction_Model_for_Autonomous_Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Bee_A_High-Quality_Corpus_and_Full-Stack_Suite_to_Unlock_Advanced_Fully_Open_MLLMs","title":"[논문리뷰] Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs","excerpt":"이 [arXiv]에 게시한 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Data Curation","Supervised Fine-tuning","Chain-of-Thought","Open-source AI","Data Quality","MLLM Training"],"permalink":"/ai/review/2025-10-16-Bee_A_High-Quality_Corpus_and_Full-Stack_Suite_to_Unlock_Advanced_Fully_Open_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-16-Attention_Illuminates_LLM_Reasoning_The_Preplan-and-Anchor_Rhythm_Enables_Fine-Grained_Policy_Optimization","title":"[논문리뷰] Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","lastModifiedAt":"2025-10-16 13:09:51+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Attention Mechanisms","Reinforcement Learning","Credit Assignment","Policy Optimization","Interpretability","Preplan-and-Anchor Rhythm","Generative Models"],"permalink":"/ai/review/2025-10-16-Attention_Illuminates_LLM_Reasoning_The_Preplan-and-Anchor_Rhythm_Enables_Fine-Grained_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-What_If_Understanding_Motion_Through_Sparse_Interactions","title":"[논문리뷰] What If : Understanding Motion Through Sparse Interactions","excerpt":"이 [arXiv]에 게시한 'What If : Understanding Motion Through Sparse Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Motion Understanding","Sparse Interactions","Multimodal Prediction","Flow Poke Transformer","Physical Scene Dynamics","Uncertainty Quantification","Generative Models","Computer Vision"],"permalink":"/ai/review/2025-10-15-What_If_Understanding_Motion_Through_Sparse_Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-ViCO_A_Training_Strategy_towards_Semantic_Aware_Dynamic_High-Resolution","title":"[논문리뷰] ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution","excerpt":"이 [arXiv]에 게시한 'ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Dynamic Resolution","Token Compression","Semantic Awareness","Visual Consistency Learning (ViCO)","Visual Resolution Router (ViR)","Inference Optimization"],"permalink":"/ai/review/2025-10-15-ViCO_A_Training_Strategy_towards_Semantic_Aware_Dynamic_High-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-UniFusion_Vision-Language_Model_as_Unified_Encoder_in_Image_Generation","title":"[논문리뷰] UniFusion: Vision-Language Model as Unified Encoder in Image Generation","excerpt":"이 [arXiv]에 게시한 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Unified Encoder","Image Generation","Diffusion Models","Multimodal Learning","Text-to-Image","Image Editing","Zero-shot Learning"],"permalink":"/ai/review/2025-10-15-UniFusion_Vision-Language_Model_as_Unified_Encoder_in_Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Tensor_Logic_The_Language_of_AI","title":"[논문리뷰] Tensor Logic: The Language of AI","excerpt":"Pedro Domingos이 [arXiv]에 게시한 'Tensor Logic: The Language of AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Tensor Logic","Neurosymbolic AI","Logic Programming","Tensor Algebra","Deep Learning","Automated Reasoning","Embedding Space"],"permalink":"/ai/review/2025-10-15-Tensor_Logic_The_Language_of_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Temporal_Alignment_Guidance_On-Manifold_Sampling_in_Diffusion_Models","title":"[논문리뷰] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models","excerpt":"이 [arXiv]에 게시한 'Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Diffusion Models","Generative Models","Guidance","On-Manifold Sampling","Temporal Alignment","Score Approximation Error","Training-Free Guidance"],"permalink":"/ai/review/2025-10-15-Temporal_Alignment_Guidance_On-Manifold_Sampling_in_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-SynthID-Image_Image_watermarking_at_internet_scale","title":"[논문리뷰] SynthID-Image: Image watermarking at internet scale","excerpt":"이 [arXiv]에 게시한 'SynthID-Image: Image watermarking at internet scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Image Watermarking","AI-Generated Content","Provenance","Robustness","Security","Deep Learning","Internet Scale","Post-hoc"],"permalink":"/ai/review/2025-10-15-SynthID-Image_Image_watermarking_at_internet_scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Spatial_Forcing_Implicit_Spatial_Representation_Alignment_for_Vision-language-action_Model","title":"[논문리뷰] Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model","excerpt":"이 [arXiv]에 게시한 'Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Spatial Perception","Implicit Representation Alignment","3D Foundation Models","Robotics","Data Efficiency","Representation Learning"],"permalink":"/ai/review/2025-10-15-Spatial_Forcing_Implicit_Spatial_Representation_Alignment_for_Vision-language-action_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Scaling_Language-Centric_Omnimodal_Representation_Learning","title":"[논문리뷰] Scaling Language-Centric Omnimodal Representation Learning","excerpt":"이 [arXiv]에 게시한 'Scaling Language-Centric Omnimodal Representation Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal Embeddings","MLLMs","Contrastive Learning","Cross-modal Alignment","Generative Pretraining","Representation Learning","Scaling Laws"],"permalink":"/ai/review/2025-10-15-Scaling_Language-Centric_Omnimodal_Representation_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-SRUM_Fine-Grained_Self-Rewarding_for_Unified_Multimodal_Models","title":"[논문리뷰] SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models","excerpt":"이 [arXiv]에 게시한 'SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Self-Rewarding","Text-to-Image Generation","Image Understanding","Post-Training","Global-Local Reward","Compositional Reasoning"],"permalink":"/ai/review/2025-10-15-SRUM_Fine-Grained_Self-Rewarding_for_Unified_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-SAIL-Embedding_Technical_Report_Omni-modal_Embedding_Foundation_Model","title":"[논문리뷰] SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model","excerpt":"이 [arXiv]에 게시한 'SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Omni-modal Embedding","Multimodal Learning","Recommendation Systems","Hard Negative Mining","Contrastive Learning","Large Language Models (LLMs)","Data Balancing","Multitask Learning"],"permalink":"/ai/review/2025-10-15-SAIL-Embedding_Technical_Report_Omni-modal_Embedding_Foundation_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Robot_Learning_A_Tutorial","title":"[논문리뷰] Robot Learning: A Tutorial","excerpt":"이 [arXiv]에 게시한 'Robot Learning: A Tutorial' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Robot Learning","Reinforcement Learning","Imitation Learning","Behavioral Cloning","Vision-Language-Action Models","Diffusion Models","Transformers","LeRobot"],"permalink":"/ai/review/2025-10-15-Robot_Learning_A_Tutorial/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-ReFIne_A_Framework_for_Trustworthy_Large_Reasoning_Models_with_Reliability_Faithfulness_and_Interpretability","title":"[논문리뷰] ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability","excerpt":"Tsui-Wei Weng이 [arXiv]에 게시한 'ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Trustworthy AI","Large Reasoning Models (LRMs)","Interpretability","Faithfulness","Reliability","Chain-of-Thought (CoT)","Supervised Fine-tuning (SFT)","GRPO"],"permalink":"/ai/review/2025-10-15-ReFIne_A_Framework_for_Trustworthy_Large_Reasoning_Models_with_Reliability_Faithfulness_and_Interpretability/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-One_Life_to_Learn_Inferring_Symbolic_World_Models_for_Stochastic_Environments_from_Unguided_Exploration","title":"[논문리뷰] One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration","excerpt":"Mohit Bansal이 [arXiv]에 게시한 'One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Symbolic World Models","Stochastic Environments","Unguided Exploration","Probabilistic Programming","Law Synthesis","Crafter-OO","Program Synthesis"],"permalink":"/ai/review/2025-10-15-One_Life_to_Learn_Inferring_Symbolic_World_Models_for_Stochastic_Environments_from_Unguided_Exploration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Memory_as_Action_Autonomous_Context_Curation_for_Long-Horizon_Agentic_Tasks","title":"[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks","excerpt":"Xueyuan Lin이 [arXiv]에 게시한 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Long-Horizon Tasks","Agentic AI","Context Curation","Working Memory","Reinforcement Learning","Policy Optimization","Large Language Models","Memory-as-Action"],"permalink":"/ai/review/2025-10-15-Memory_as_Action_Autonomous_Context_Curation_for_Long-Horizon_Agentic_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-MLLM_as_a_UI_Judge_Benchmarking_Multimodal_LLMs_for_Predicting_Human_Perception_of_User_Interfaces","title":"[논문리뷰] MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces","excerpt":"Sungchul Kim이 [arXiv]에 게시한 'MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","UI Evaluation","Human Perception","Benchmarking","UX Research","MLLM-as-a-Judge","Cognitive Factors","Pairwise Comparison"],"permalink":"/ai/review/2025-10-15-MLLM_as_a_UI_Judge_Benchmarking_Multimodal_LLMs_for_Predicting_Human_Perception_of_User_Interfaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-LLM_Reasoning_for_Machine_Translation_Synthetic_Data_Generation_over_Thinking_Tokens","title":"[논문리뷰] LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens","excerpt":"이 [arXiv]에 게시한 'LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Machine Translation (MT)","Chain-of-Thought (CoT)","Knowledge Distillation","Fine-tuning","Prompt Engineering","Synthetic Data"],"permalink":"/ai/review/2025-10-15-LLM_Reasoning_for_Machine_Translation_Synthetic_Data_Generation_over_Thinking_Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Information-Preserving_Reformulation_of_Reasoning_Traces_for_Antidistillation","title":"[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation","excerpt":"이 [arXiv]에 게시한 'Information-Preserving Reformulation of Reasoning Traces for Antidistillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Antidistillation","Reasoning Traces","Large Language Models","Knowledge Distillation","Information Preservation","Trace Reformulation","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-10-15-Information-Preserving_Reformulation_of_Reasoning_Traces_for_Antidistillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-HoneyBee_Data_Recipes_for_Vision-Language_Reasoners","title":"[논문리뷰] HoneyBee: Data Recipes for Vision-Language Reasoners","excerpt":"이 [arXiv]에 게시한 'HoneyBee: Data Recipes for Vision-Language Reasoners' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Data Curation","Chain-of-Thought","VL Reasoning","Dataset Scaling","Supervised Finetuning","HONEYBEE","Test-Time Scaling"],"permalink":"/ai/review/2025-10-15-HoneyBee_Data_Recipes_for_Vision-Language_Reasoners/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-FlashVSR_Towards_Real-Time_Diffusion-Based_Streaming_Video_Super-Resolution","title":"[논문리뷰] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution","excerpt":"Yihao Liu이 [arXiv]에 게시한 'FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Video Super-Resolution (VSR)","Diffusion Models","Real-time VSR","Streaming VSR","Sparse Attention","Distillation","Conditional Decoder","High-resolution"],"permalink":"/ai/review/2025-10-15-FlashVSR_Towards_Real-Time_Diffusion-Based_Streaming_Video_Super-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-ExpVid_A_Benchmark_for_Experiment_Video_Understanding_Reasoning","title":"[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding & Reasoning","excerpt":"이 [arXiv]에 게시한 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Experiment Video Understanding","Multimodal Large Language Models (MLLMs)","Scientific Reasoning","Benchmark","Wet-Lab Experiments","Procedural Understanding","Fine-grained Perception","Video QA"],"permalink":"/ai/review/2025-10-15-ExpVid_A_Benchmark_for_Experiment_Video_Understanding_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-ERA_Transforming_VLMs_into_Embodied_Agents_via_Embodied_Prior_Learning_and_Online_Reinforcement_Learning","title":"[논문리뷰] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Embodied AI","Vision Language Models (VLMs)","Reinforcement Learning (RL)","Prior Learning","Supervised Fine-tuning (SFT)","Embodied Agents"],"permalink":"/ai/review/2025-10-15-ERA_Transforming_VLMs_into_Embodied_Agents_via_Embodied_Prior_Learning_and_Online_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Dr.LLM_Dynamic_Layer_Routing_in_LLMs","title":"[논문리뷰] Dr.LLM: Dynamic Layer Routing in LLMs","excerpt":"이 [arXiv]에 게시한 'Dr.LLM: Dynamic Layer Routing in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Dynamic Routing","LLMs","Adaptive Depth","Computational Efficiency","Monte Carlo Tree Search (MCTS)","Retrofittable Framework","Supervised Learning","Accuracy Improvement"],"permalink":"/ai/review/2025-10-15-Dr.LLM_Dynamic_Layer_Routing_in_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Detect_Anything_via_Next_Point_Prediction","title":"[논문리뷰] Detect Anything via Next Point Prediction","excerpt":"이 [arXiv]에 게시한 'Detect Anything via Next Point Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Object Detection","Coordinate Prediction","Reinforcement Learning","Supervised Fine-tuning","Visual Perception","Zero-shot Learning","Spatial Reasoning"],"permalink":"/ai/review/2025-10-15-Detect_Anything_via_Next_Point_Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-DeepMMSearch-R1_Empowering_Multimodal_LLMs_in_Multimodal_Web_Search","title":"[논문리뷰] DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search","excerpt":"이 [arXiv]에 게시한 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Web Search","Visual Question Answering","Reinforcement Learning","Image Cropping","Self-Correction","Tool Use"],"permalink":"/ai/review/2025-10-15-DeepMMSearch-R1_Empowering_Multimodal_LLMs_in_Multimodal_Web_Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-DITING_A_Multi-Agent_Evaluation_Framework_for_Benchmarking_Web_Novel_Translation","title":"[논문리뷰] DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation","excerpt":"이 [arXiv]에 게시한 'DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Machine Translation Evaluation","Large Language Models (LLMs)","Web Novel Translation","Multi-Agent Systems","Cultural Nuance","Benchmark Dataset","Natural Language Generation"],"permalink":"/ai/review/2025-10-15-DITING_A_Multi-Agent_Evaluation_Framework_for_Benchmarking_Web_Novel_Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Boundary-Guided_Policy_Optimization_for_Memory-efficient_RL_of_Diffusion_Large_Language_Models","title":"[논문리뷰] Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models","excerpt":"이 [arXiv]에 게시한 'Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Diffusion Large Language Models","Reinforcement Learning","Memory Efficiency","Monte Carlo Sampling","Log-Likelihood Approximation","Policy Optimization","ELBO"],"permalink":"/ai/review/2025-10-15-Boundary-Guided_Policy_Optimization_for_Memory-efficient_RL_of_Diffusion_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-Advancing_End-to-End_Pixel_Space_Generative_Modeling_via_Self-supervised_Pre-training","title":"[논문리뷰] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training","excerpt":"이 [arXiv]에 게시한 'Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Pixel-space Generative Models","Diffusion Models","Consistency Models","Self-supervised Pre-training","End-to-end Training","Image Generation","FID","Representation Learning"],"permalink":"/ai/review/2025-10-15-Advancing_End-to-End_Pixel_Space_Generative_Modeling_via_Self-supervised_Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-15-A_Survey_of_Vibe_Coding_with_Large_Language_Models","title":"[논문리뷰] A Survey of Vibe Coding with Large Language Models","excerpt":"이 [arXiv]에 게시한 'A Survey of Vibe Coding with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","lastModifiedAt":"2025-10-15 13:01:40+0900","categories":["Review"],"tags":["Review","Vibe Coding","Large Language Models","Coding Agents","Human-AI Collaboration","Software Engineering","Development Models","Context Engineering"],"permalink":"/ai/review/2025-10-15-A_Survey_of_Vibe_Coding_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Which_Heads_Matter_for_Reasoning_RL-Guided_KV_Cache_Compression","title":"[논문리뷰] Which Heads Matter for Reasoning? RL-Guided KV Cache Compression","excerpt":"Huan Wang이 [arXiv]에 게시한 'Which Heads Matter for Reasoning? RL-Guided KV Cache Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","KV Cache Compression","Large Language Models (LLMs)","Reinforcement Learning (RL)","Reasoning Models","Attention Heads","Chain-of-Thought (CoT)","Memory Efficiency"],"permalink":"/ai/review/2025-10-13-Which_Heads_Matter_for_Reasoning_RL-Guided_KV_Cache_Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Webscale-RL_Automated_Data_Pipeline_for_Scaling_RL_Data_to_Pretraining_Levels","title":"[논문리뷰] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels","excerpt":"이 [arXiv]에 게시한 'Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Data Pipeline","Web-scale Data","Question-Answering (QA)","Data Generation","Data Diversity","Data Efficiency"],"permalink":"/ai/review/2025-10-13-Webscale-RL_Automated_Data_Pipeline_for_Scaling_RL_Data_to_Pretraining_Levels/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Understanding_DeepResearch_via_Reports","title":"[논문리뷰] Understanding DeepResearch via Reports","excerpt":"Chengen Huang이 [arXiv]에 게시한 'Understanding DeepResearch via Reports' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","DeepResearch Agents","LLM-as-a-Judge","Report Evaluation","Agentic AI","Factuality","Redundancy","Research Automation","Benchmark"],"permalink":"/ai/review/2025-10-13-Understanding_DeepResearch_via_Reports/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Thinking_with_Camera_A_Unified_Multimodal_Model_for_Camera-Centric_Understanding_and_Generation","title":"[논문리뷰] Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation","excerpt":"Linyi Jin이 [arXiv]에 게시한 'Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Unified Multimodal Model","Camera-Centric","Image Understanding","Image Generation","Spatial Reasoning","Camera Parameters","Instruction Tuning","Multimodal Spatial Intelligence"],"permalink":"/ai/review/2025-10-13-Thinking_with_Camera_A_Unified_Multimodal_Model_for_Camera-Centric_Understanding_and_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Temporal_Prompting_Matters_Rethinking_Referring_Video_Object_Segmentation","title":"[논문리뷰] Temporal Prompting Matters: Rethinking Referring Video Object Segmentation","excerpt":"Sifei Liu이 [arXiv]에 게시한 'Temporal Prompting Matters: Rethinking Referring Video Object Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Referring Video Object Segmentation","Foundation Models","Prompt Engineering","Object Tracking","SAM","Video Analysis","Prompt Preference Learning"],"permalink":"/ai/review/2025-10-13-Temporal_Prompting_Matters_Rethinking_Referring_Video_Object_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-TC-LoRA_Temporally_Modulated_Conditional_LoRA_for_Adaptive_Diffusion_Control","title":"[논문리뷰] TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control","excerpt":"Adityan Jothi이 [arXiv]에 게시한 'TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Diffusion Models","Conditional Generation","LoRA","Hypernetwork","Dynamic Weight Adaptation","Generative AI","Controllable Generation"],"permalink":"/ai/review/2025-10-13-TC-LoRA_Temporally_Modulated_Conditional_LoRA_for_Adaptive_Diffusion_Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-StreamingVLM_Real-Time_Understanding_for_Infinite_Video_Streams","title":"[논문리뷰] StreamingVLM: Real-Time Understanding for Infinite Video Streams","excerpt":"Kelly Peng이 [arXiv]에 게시한 'StreamingVLM: Real-Time Understanding for Infinite Video Streams' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Video Stream Understanding","Real-Time VLM","Attention Sink","KV Cache Management","Contiguous RoPE","Supervised Fine-tuning","Long-Context Video"],"permalink":"/ai/review/2025-10-13-StreamingVLM_Real-Time_Understanding_for_Infinite_Video_Streams/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-StatEval_A_Comprehensive_Benchmark_for_Large_Language_Models_in_Statistics","title":"[논문리뷰] StatEval: A Comprehensive Benchmark for Large Language Models in Statistics","excerpt":"이 [arXiv]에 게시한 'StatEval: A Comprehensive Benchmark for Large Language Models in Statistics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Statistical Reasoning","LLM Benchmark","Statistics Education","Proof Verification","Multi-agent Pipeline","Automated Extraction","Evaluation Framework"],"permalink":"/ai/review/2025-10-13-StatEval_A_Comprehensive_Benchmark_for_Large_Language_Models_in_Statistics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Speculative_Jacobi-Denoising_Decoding_for_Accelerating_Autoregressive_Text-to-image_Generation","title":"[논문리뷰] Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation","excerpt":"Han Shi이 [arXiv]에 게시한 'Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Text-to-Image Generation","Inference Acceleration","Jacobi Decoding","Denoising Diffusion Models","Speculative Decoding","Multi-token Prediction","Fine-tuning"],"permalink":"/ai/review/2025-10-13-Speculative_Jacobi-Denoising_Decoding_for_Accelerating_Autoregressive_Text-to-image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-SpaceVista_All-Scale_Visual_Spatial_Reasoning_from_mm_to_km","title":"[논문리뷰] SpaceVista: All-Scale Visual Spatial Reasoning from mm to km","excerpt":"Kaituo Feng이 [arXiv]에 게시한 'SpaceVista: All-Scale Visual Spatial Reasoning from mm to km' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Spatial Reasoning","Multi-Scale Vision","MLLM","Dataset","Scale Experts","Reinforcement Learning","Computer Vision","Robotics"],"permalink":"/ai/review/2025-10-13-SpaceVista_All-Scale_Visual_Spatial_Reasoning_from_mm_to_km/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-ReviewerToo_Should_AI_Join_The_Program_Committee_A_Look_At_The_Future_of_Peer_Review","title":"[논문리뷰] ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review","excerpt":"Christopher Pal이 [arXiv]에 게시한 'ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Peer Review","AI-Assisted Review","Large Language Models","LLM Agents","Meta-Review","Conference Submissions","Reviewer Personas","Evaluation Metrics"],"permalink":"/ai/review/2025-10-13-ReviewerToo_Should_AI_Join_The_Program_Committee_A_Look_At_The_Future_of_Peer_Review/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-R-Horizon_How_Far_Can_Your_Large_Reasoning_Model_Really_Go_in_Breadth_and_Depth","title":"[논문리뷰] R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?","excerpt":"이 [arXiv]에 게시한 'R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Long-Horizon Reasoning","Query Composition","Large Reasoning Models","Reinforcement Learning","Benchmark Evaluation","Thinking Budget","Performance Degradation","Chain-of-Thought"],"permalink":"/ai/review/2025-10-13-R-Horizon_How_Far_Can_Your_Large_Reasoning_Model_Really_Go_in_Breadth_and_Depth/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Pseudo2Real_Task_Arithmetic_for_Pseudo-Label_Correction_in_Automatic_Speech_Recognition","title":"[논문리뷰] Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition","excerpt":"Shang-Tse Chen이 [arXiv]에 게시한 'Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","ASR","Pseudo-labeling","Domain Adaptation","Task Arithmetic","Correction Vector","Accent Adaptation","Speaker Clustering","Model Editing"],"permalink":"/ai/review/2025-10-13-Pseudo2Real_Task_Arithmetic_for_Pseudo-Label_Correction_in_Automatic_Speech_Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Progressive_Gaussian_Transformer_with_Anisotropy-aware_Sampling_for_Open_Vocabulary_Occupancy_Prediction","title":"[논문리뷰] Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction","excerpt":"danxuhk이 [arXiv]에 게시한 'Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","3D Occupancy Prediction","Open Vocabulary","Gaussian Splatting","Transformer","Progressive Densification","Anisotropy-aware Sampling","Autonomous Driving"],"permalink":"/ai/review/2025-10-13-Progressive_Gaussian_Transformer_with_Anisotropy-aware_Sampling_for_Open_Vocabulary_Occupancy_Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-PhysToolBench_Benchmarking_Physical_Tool_Understanding_for_MLLMs","title":"[논문리뷰] PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs","excerpt":"Xu Zheng이 [arXiv]에 게시한 'PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Physical Tool Understanding","Benchmarking","Embodied AI","Visual Question Answering (VQA)","Tool Affordances","Reasoning"],"permalink":"/ai/review/2025-10-13-PhysToolBench_Benchmarking_Physical_Tool_Understanding_for_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Parallel_Test-Time_Scaling_for_Latent_Reasoning_Models","title":"[논문리뷰] Parallel Test-Time Scaling for Latent Reasoning Models","excerpt":"이 [arXiv]에 게시한 'Parallel Test-Time Scaling for Latent Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Latent Reasoning","Test-Time Scaling","Parallel Inference","Stochastic Sampling","Monte Carlo Dropout","Additive Gaussian Noise","Latent Reward Model","Trajectory Aggregation"],"permalink":"/ai/review/2025-10-13-Parallel_Test-Time_Scaling_for_Latent_Reasoning_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-One_Patch_to_Caption_Them_All_A_Unified_Zero-Shot_Captioning_Framework","title":"[논문리뷰] One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework","excerpt":"Giuseppe Amato이 [arXiv]에 게시한 'One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Zero-Shot Captioning","Region-Level Captioning","Vision Transformers","DINOv2","Patch-Centric","Modality Gap Mitigation","Visual-Language Models"],"permalink":"/ai/review/2025-10-13-One_Patch_to_Caption_Them_All_A_Unified_Zero-Shot_Captioning_Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Multimodal_Prompt_Optimization_Why_Not_Leverage_Multiple_Modalities_for_MLLMs","title":"[논문리뷰] Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs","excerpt":"이 [arXiv]에 게시한 'Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal AI","Prompt Optimization","MLLMs","Bayesian Optimization","Cross-modal Alignment","Prompt Engineering","Generative AI","Exploration-Exploitation"],"permalink":"/ai/review/2025-10-13-Multimodal_Prompt_Optimization_Why_Not_Leverage_Multiple_Modalities_for_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Mitigating_Overthinking_through_Reasoning_Shaping","title":"[논문리뷰] Mitigating Overthinking through Reasoning Shaping","excerpt":"Wen Luo이 [arXiv]에 게시한 'Mitigating Overthinking through Reasoning Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Large Reasoning Models (LRMs)","RLVR","Overthinking Mitigation","Reasoning Shaping","Segment-level Penalization","Computational Efficiency","Training Stability","Length-aware Weighting"],"permalink":"/ai/review/2025-10-13-Mitigating_Overthinking_through_Reasoning_Shaping/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-MRMR_A_Realistic_and_Expert-Level_Multidisciplinary_Benchmark_for_Reasoning-Intensive_Multimodal_Retrieval","title":"[논문리뷰] MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval","excerpt":"Tingyu Song이 [arXiv]에 게시한 'MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal Retrieval","Benchmark","Reasoning","Multidisciplinary","Expert-Level","Image-Text Interleaving","Contradiction Retrieval"],"permalink":"/ai/review/2025-10-13-MRMR_A_Realistic_and_Expert-Level_Multidisciplinary_Benchmark_for_Reasoning-Intensive_Multimodal_Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-KORMo_Korean_Open_Reasoning_Model_for_Everyone","title":"[논문리뷰] KORMo: Korean Open Reasoning Model for Everyone","excerpt":"이 [arXiv]에 게시한 'KORMo: Korean Open Reasoning Model for Everyone' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Large Language Model","Korean","Bilingual","Synthetic Data","Fully Open Model","Tokenizer","Reasoning","Pretraining","Instruction Tuning"],"permalink":"/ai/review/2025-10-13-KORMo_Korean_Open_Reasoning_Model_for_Everyone/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Instant4D_4D_Gaussian_Splatting_in_Minutes","title":"[논문리뷰] Instant4D: 4D Gaussian Splatting in Minutes","excerpt":"Li Lu이 [arXiv]에 게시한 'Instant4D: 4D Gaussian Splatting in Minutes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","4D Gaussian Splatting","Dynamic View Synthesis","Monocular Reconstruction","Visual SLAM","Grid Pruning","Real-time Rendering","GPU Memory Optimization"],"permalink":"/ai/review/2025-10-13-Instant4D_4D_Gaussian_Splatting_in_Minutes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Hybrid-grained_Feature_Aggregation_with_Coarse-to-fine_Language_Guidance_for_Self-supervised_Monocular_Depth_Estimation","title":"[논문리뷰] Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation","excerpt":"Zekun Qi이 [arXiv]에 게시한 'Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Self-supervised Monocular Depth Estimation","Foundation Models","CLIP","DINO","Language Guidance","Coarse-to-fine Learning","Feature Aggregation","3D Perception"],"permalink":"/ai/review/2025-10-13-Hybrid-grained_Feature_Aggregation_with_Coarse-to-fine_Language_Guidance_for_Self-supervised_Monocular_Depth_Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-GTAlign_Game-Theoretic_Alignment_of_LLM_Assistants_for_Mutual_Welfare","title":"[논문리뷰] GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare","excerpt":"이 [arXiv]에 게시한 'GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Large Language Models","LLM Alignment","Game Theory","Reinforcement Learning","Mutual Welfare","Payoff Matrix","Strategic Decision Making","Human-AI Interaction"],"permalink":"/ai/review/2025-10-13-GTAlign_Game-Theoretic_Alignment_of_LLM_Assistants_for_Mutual_Welfare/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Dyna-Mind_Learning_to_Simulate_from_Experience_for_Better_AI_Agents","title":"[논문리뷰] Dyna-Mind: Learning to Simulate from Experience for Better AI Agents","excerpt":"Qianhui Wu이 [arXiv]에 게시한 'Dyna-Mind: Learning to Simulate from Experience for Better AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","AI Agents","Reinforcement Learning","World Models","Simulation","Reasoning","Language Models","Planning","Interactive AI"],"permalink":"/ai/review/2025-10-13-Dyna-Mind_Learning_to_Simulate_from_Experience_for_Better_AI_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Dont_Waste_Mistakes_Leveraging_Negative_RL-Groups_via_Confidence_Reweighting","title":"[논문리뷰] Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting","excerpt":"Julia Kempe이 [arXiv]에 게시한 'Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reasoning Tasks","GRPO","Negative Samples","Reward Modeling","Confidence Reweighting","Mathematical Reasoning"],"permalink":"/ai/review/2025-10-13-Dont_Waste_Mistakes_Leveraging_Negative_RL-Groups_via_Confidence_Reweighting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-DISCO_Diversifying_Sample_Condensation_for_Efficient_Model_Evaluation","title":"[논문리뷰] DISCO: Diversifying Sample Condensation for Efficient Model Evaluation","excerpt":"이 [arXiv]에 게시한 'DISCO: Diversifying Sample Condensation for Efficient Model Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Efficient Evaluation","Sample Condensation","Model Disagreement","Predictive Diversity","Performance Prediction","Large Language Models","Model Signatures","Meta-modeling"],"permalink":"/ai/review/2025-10-13-DISCO_Diversifying_Sample_Condensation_for_Efficient_Model_Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-D2E_Scaling_Vision-Action_Pretraining_on_Desktop_Data_for_Transfer_to_Embodied_AI","title":"[논문리뷰] D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI","excerpt":"Haebin Seong이 [arXiv]에 게시한 'D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Embodied AI","Vision-Action Pretraining","Desktop Data","Inverse Dynamics Model (IDM)","Pseudo-labeling","Robotics","Generalization","Data Compression"],"permalink":"/ai/review/2025-10-13-D2E_Scaling_Vision-Action_Pretraining_on_Desktop_Data_for_Transfer_to_Embodied_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Bridging_Reasoning_to_Learning_Unmasking_Illusions_using_Complexity_Out_of_Distribution_Generalization","title":"[논문리뷰] Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization","excerpt":"Mahdi Ghaznavai이 [arXiv]에 게시한 'Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Complexity OoD Generalization","System-1 Thinking","System-2 Reasoning","Kolmogorov Complexity","Inductive Biases","Large Language Models (LLMs)","Reasoning Evaluation"],"permalink":"/ai/review/2025-10-13-Bridging_Reasoning_to_Learning_Unmasking_Illusions_using_Complexity_Out_of_Distribution_Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-BigCodeArena_Unveiling_More_Reliable_Human_Preferences_in_Code_Generation_via_Execution","title":"[논문리뷰] BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution","excerpt":"Hange Liu이 [arXiv]에 게시한 'BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Code Generation","Human Preference","LLM Evaluation","Execution Feedback","Benchmarking","Crowdsourcing","Software Engineering","Large Language Models"],"permalink":"/ai/review/2025-10-13-BigCodeArena_Unveiling_More_Reliable_Human_Preferences_in_Code_Generation_via_Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Better_Together_Leveraging_Unpaired_Multimodal_Data_for_Stronger_Unimodal_Models","title":"[논문리뷰] Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models","excerpt":"이 [arXiv]에 게시한 'Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Unpaired Multimodal Learning","Unimodal Representation","Weight Sharing","Cross-modal Transfer","Fisher Information","Self-supervised Learning","Multimodal Neurons","Data Efficiency"],"permalink":"/ai/review/2025-10-13-Better_Together_Leveraging_Unpaired_Multimodal_Data_for_Stronger_Unimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-AutoPR_Lets_Automate_Your_Academic_Promotion","title":"[논문리뷰] AutoPR: Let's Automate Your Academic Promotion!","excerpt":"Yixin Yuan이 [arXiv]에 게시한 'AutoPR: Let's Automate Your Academic Promotion!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Academic Promotion","Large Language Models","Multi-Agent Systems","Scholarly Communication","Multimodal Processing","Benchmark","Content Generation","Social Media Marketing"],"permalink":"/ai/review/2025-10-13-AutoPR_Lets_Automate_Your_Academic_Promotion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-Adaptive_Attacks_on_Trusted_Monitors_Subvert_AI_Control_Protocols","title":"[논문리뷰] Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols","excerpt":"Maksym Andriushchenko이 [arXiv]에 게시한 'Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","AI Control Protocols","LLM Monitors","Adaptive Attacks","Prompt Injection","Jailbreaking","Red Teaming","Scalable Oversight"],"permalink":"/ai/review/2025-10-13-Adaptive_Attacks_on_Trusted_Monitors_Subvert_AI_Control_Protocols/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-A_Goal_Without_a_Plan_Is_Just_a_Wish_Efficient_and_Effective_Global_Planner_Training_for_Long-Horizon_Agent_Tasks","title":"[논문리뷰] A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks","excerpt":"Fanchao Qi이 [arXiv]에 게시한 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Long-Horizon Tasks","LLM Agents","Global Planning","Reinforcement Learning","Supervised Fine-tuning","Homologous Consensus Filtering","Executor Capability Gain Reward","Plan-and-Execute"],"permalink":"/ai/review/2025-10-13-A_Goal_Without_a_Plan_Is_Just_a_Wish_Efficient_and_Effective_Global_Planner_Training_for_Long-Horizon_Agent_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-ARES_Multimodal_Adaptive_Reasoning_via_Difficulty-Aware_Token-Level_Entropy_Shaping","title":"[논문리뷰] ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping","excerpt":"Wenbo Hu이 [arXiv]에 게시한 'ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Adaptive Learning","Reinforcement Learning","Entropy Shaping","Difficulty-Aware","Chain-of-Thought","Token-Level Analysis"],"permalink":"/ai/review/2025-10-13-ARES_Multimodal_Adaptive_Reasoning_via_Difficulty-Aware_Token-Level_Entropy_Shaping/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-13-ACE_Attribution-Controlled_Knowledge_Editing_for_Multi-hop_Factual_Recall","title":"[논문리뷰] ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall","excerpt":"Jiaqi Tang이 [arXiv]에 게시한 'ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","lastModifiedAt":"2025-10-13 13:44:18+0900","categories":["Review"],"tags":["Review","Knowledge Editing","LLMs","Multi-hop Reasoning","Mechanistic Interpretability","Neuron-level Attribution","Factual Recall","Transformer Networks"],"permalink":"/ai/review/2025-10-13-ACE_Attribution-Controlled_Knowledge_Editing_for_Multi-hop_Factual_Recall/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-When_Thoughts_Meet_Facts_Reusable_Reasoning_for_Long-Context_LMs","title":"[논문리뷰] When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs","excerpt":"이 [arXiv]에 게시한 'When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Long-Context LMs","Multi-hop Reasoning","Thought Templates","Retrieval-Augmented Generation","Natural Language Feedback","Knowledge-intensive QA","Reasoning Reuse"],"permalink":"/ai/review/2025-10-10-When_Thoughts_Meet_Facts_Reusable_Reasoning_for_Long-Context_LMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-VideoCanvas_Unified_Video_Completion_from_Arbitrary_Spatiotemporal_Patches_via_In-Context_Conditioning","title":"[논문리뷰] VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning","excerpt":"Quande Liu이 [arXiv]에 게시한 'VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Video Completion","Spatio-Temporal Control","In-Context Conditioning","Video Diffusion Models","RoPE Interpolation","VAE","Unified Framework","Video Generation"],"permalink":"/ai/review/2025-10-10-VideoCanvas_Unified_Video_Completion_from_Arbitrary_Spatiotemporal_Patches_via_In-Context_Conditioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-UniVideo_Unified_Understanding_Generation_and_Editing_for_Videos","title":"[논문리뷰] UniVideo: Unified Understanding, Generation, and Editing for Videos","excerpt":"Xintao Wang이 [arXiv]에 게시한 'UniVideo: Unified Understanding, Generation, and Editing for Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Unified Multimodal Model","Video Generation","Video Editing","MLLM","Diffusion Transformer","In-Context Learning","Zero-shot Generalization","Multimodal AI"],"permalink":"/ai/review/2025-10-10-UniVideo_Unified_Understanding_Generation_and_Editing_for_Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-UniMMVSR_A_Unified_Multi-Modal_Framework_for_Cascaded_Video_Super-Resolution","title":"[논문리뷰] UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution","excerpt":"이 [arXiv]에 게시한 'UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Video Super-Resolution","Multi-Modal Generation","Latent Diffusion Models","Cascaded Framework","Condition Injection","Text-to-Video","Video Editing","4K Video"],"permalink":"/ai/review/2025-10-10-UniMMVSR_A_Unified_Multi-Modal_Framework_for_Cascaded_Video_Super-Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-UP2You_Fast_Reconstruction_of_Yourself_from_Unconstrained_Photo_Collections","title":"[논문리뷰] UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections","excerpt":"Boqian Li이 [arXiv]에 게시한 'UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","3D Human Reconstruction","Unconstrained Photos","Data Rectifier","Multi-View Generation","Pose-Correlated Feature Aggregation","SMPL-X","Diffusion Models","Virtual Try-On"],"permalink":"/ai/review/2025-10-10-UP2You_Fast_Reconstruction_of_Yourself_from_Unconstrained_Photo_Collections/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-UNIDOC-BENCH_A_Unified_Benchmark_for_Document-Centric_Multimodal_RAG","title":"[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG","excerpt":"이 [arXiv]에 게시한 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multimodal RAG","Document AI","Benchmark","Information Retrieval","Large Language Models","Multimodal Embeddings","PDF Processing","Question Answering"],"permalink":"/ai/review/2025-10-10-UNIDOC-BENCH_A_Unified_Benchmark_for_Document-Centric_Multimodal_RAG/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Training-Free_Group_Relative_Policy_Optimization","title":"[논문리뷰] Training-Free Group Relative Policy Optimization","excerpt":"이 [arXiv]에 게시한 'Training-Free Group Relative Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Parameter-Free Optimization","Experiential Knowledge","Token Prior","Group Relative Policy Optimization","In-Context Learning","Cost-Effective AI"],"permalink":"/ai/review/2025-10-10-Training-Free_Group_Relative_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Towards_Scalable_and_Consistent_3D_Editing","title":"[논문리뷰] Towards Scalable and Consistent 3D Editing","excerpt":"Pan Zhou이 [arXiv]에 게시한 'Towards Scalable and Consistent 3D Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","3D Editing","Generative Models","Transformer Architecture","Dataset Generation","Multimodal Learning","Conditional Generation","Image-to-3D"],"permalink":"/ai/review/2025-10-10-Towards_Scalable_and_Consistent_3D_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-The_Alignment_Waltz_Jointly_Training_Agents_to_Collaborate_for_Safety","title":"[논문리뷰] The Alignment Waltz: Jointly Training Agents to Collaborate for Safety","excerpt":"이 [arXiv]에 게시한 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Safety","Multi-agent Reinforcement Learning","Safety Alignment","Overrefusal","Adversarial Attacks","Feedback Agent","Conversation Agent","Dynamic Improvement Reward"],"permalink":"/ai/review/2025-10-10-The_Alignment_Waltz_Jointly_Training_Agents_to_Collaborate_for_Safety/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Taming_Text-to-Sounding_Video_Generation_via_Advanced_Modality_Condition_and_Interaction","title":"[논문리뷰] Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction","excerpt":"이 [arXiv]에 게시한 'Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Text-to-Sounding Video Generation","Diffusion Models","Dual-tower Architecture","Cross-modal Fusion","Visual Grounding","Hierarchical Captioning","Cross-Attention"],"permalink":"/ai/review/2025-10-10-Taming_Text-to-Sounding_Video_Generation_via_Advanced_Modality_Condition_and_Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Search-R3_Unifying_Reasoning_and_Embedding_Generation_in_Large_Language_Models","title":"[논문리뷰] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models","excerpt":"James Cheng이 [arXiv]에 게시한 'Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Sentence Embedding","Retrieval-Augmented Generation","Chain-of-Thought","Information Retrieval","Supervised Fine-tuning"],"permalink":"/ai/review/2025-10-10-Search-R3_Unifying_Reasoning_and_Embedding_Generation_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-SciVideoBench_Benchmarking_Scientific_Video_Reasoning_in_Large_Multimodal_Models","title":"[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models","excerpt":"Mohit Bansal이 [arXiv]에 게시한 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Video Reasoning","Multimodal AI","Scientific Research","Large Multimodal Models","Benchmark","Quantitative Reasoning","Domain Knowledge","Visual Grounding"],"permalink":"/ai/review/2025-10-10-SciVideoBench_Benchmarking_Scientific_Video_Reasoning_in_Large_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-SViM3D_Stable_Video_Material_Diffusion_for_Single_Image_3D_Generation","title":"[논문리뷰] SViM3D: Stable Video Material Diffusion for Single Image 3D Generation","excerpt":"이 [arXiv]에 게시한 'SViM3D: Stable Video Material Diffusion for Single Image 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Single Image 3D Reconstruction","Material Prediction","Video Diffusion Models","Physically Based Rendering (PBR)","Inverse Rendering","Novel View Synthesis","Camera Control","Latent Diffusion"],"permalink":"/ai/review/2025-10-10-SViM3D_Stable_Video_Material_Diffusion_for_Single_Image_3D_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Reinforcing_Diffusion_Models_by_Direct_Group_Preference_Optimization","title":"[논문리뷰] Reinforcing Diffusion Models by Direct Group Preference Optimization","excerpt":"Jing Tang이 [arXiv]에 게시한 'Reinforcing Diffusion Models by Direct Group Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Diffusion Models","Reinforcement Learning","Preference Optimization","Group Preference","Direct Preference Optimization","ODE Samplers","Efficient Training"],"permalink":"/ai/review/2025-10-10-Reinforcing_Diffusion_Models_by_Direct_Group_Preference_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Recycling_Pretrained_Checkpoints_Orthogonal_Growth_of_Mixture-of-Experts_for_Efficient_Large_Language_Model_Pre-Training","title":"[논문리뷰] Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training","excerpt":"Peng Cheng이 [arXiv]에 게시한 'Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts","Large Language Models","Checkpoint Recycling","Model Growth","Efficient Pretraining","Depth Growth","Width Growth","Sunk Cost"],"permalink":"/ai/review/2025-10-10-Recycling_Pretrained_Checkpoints_Orthogonal_Growth_of_Mixture-of-Experts_for_Efficient_Large_Language_Model_Pre-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-R2RGEN_Real-to-Real_3D_Data_Generation_for_Spatially_Generalized_Manipulation","title":"[논문리뷰] R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation","excerpt":"Zheng Zhu이 [arXiv]에 게시한 'R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Robotic Manipulation","Data Augmentation","Spatial Generalization","3D Data Generation","Imitation Learning","Point Cloud","Real-to-Real","Mobile Manipulation"],"permalink":"/ai/review/2025-10-10-R2RGEN_Real-to-Real_3D_Data_Generation_for_Spatially_Generalized_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-NewtonBench_Benchmarking_Generalizable_Scientific_Law_Discovery_in_LLM_Agents","title":"[논문리뷰] NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents","excerpt":"Baixuan Xu이 [arXiv]에 게시한 'NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Agents","Scientific Law Discovery","Benchmarking","Metaphysical Shifts","Interactive Environments","Exploration-Exploitation","Tool Use"],"permalink":"/ai/review/2025-10-10-NewtonBench_Benchmarking_Generalizable_Scientific_Law_Discovery_in_LLM_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-NaViL_Rethinking_Scaling_Properties_of_Native_Multimodal_Large_Language_Models_under_Data_Constraints","title":"[논문리뷰] NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints","excerpt":"이 [arXiv]에 게시한 'NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Native MLLMs","Scaling Laws","Data Constraints","Visual Encoder","LLM Initialization","Mixture-of-Experts","End-to-end Training"],"permalink":"/ai/review/2025-10-10-NaViL_Rethinking_Scaling_Properties_of_Native_Multimodal_Large_Language_Models_under_Data_Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Meta-Awareness_Enhances_Reasoning_Models_Self-Alignment_Reinforcement_Learning","title":"[논문리뷰] Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Meta-Awareness","Reinforcement Learning","Self-Alignment","LLM Reasoning","Training Efficiency","Generalization","Predictive Gating"],"permalink":"/ai/review/2025-10-10-Meta-Awareness_Enhances_Reasoning_Models_Self-Alignment_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Memory_Retrieval_and_Consolidation_in_Large_Language_Models_through_Function_Tokens","title":"[논문리뷰] Memory Retrieval and Consolidation in Large Language Models through Function Tokens","excerpt":"이 [arXiv]에 게시한 'Memory Retrieval and Consolidation in Large Language Models through Function Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Large Language Models","LLM Interpretability","Function Tokens","Memory Retrieval","Memory Consolidation","Sparse Autoencoders","Pre-training"],"permalink":"/ai/review/2025-10-10-Memory_Retrieval_and_Consolidation_in_Large_Language_Models_through_Function_Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-MemMamba_Rethinking_Memory_Patterns_in_State_Space_Model","title":"[논문리뷰] MemMamba: Rethinking Memory Patterns in State Space Model","excerpt":"Xiao Sun이 [arXiv]에 게시한 'MemMamba: Rethinking Memory Patterns in State Space Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","State Space Models","Mamba","Long-sequence modeling","Memory decay","State summarization","Cross-layer attention","Perplexity","Linear complexity"],"permalink":"/ai/review/2025-10-10-MemMamba_Rethinking_Memory_Patterns_in_State_Space_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-MM-HELIX_Boosting_Multimodal_Long-Chain_Reflective_Reasoning_with_Holistic_Platform_and_Adaptive_Hybrid_Policy_Optimization","title":"[논문리뷰] MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization","excerpt":"vanilla1116이 [arXiv]에 게시한 'MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Reflective Reasoning","Long-Chain Reasoning","Benchmark","Policy Optimization","Data Generation","Reinforcement Learning","Backtracking"],"permalink":"/ai/review/2025-10-10-MM-HELIX_Boosting_Multimodal_Long-Chain_Reflective_Reasoning_with_Holistic_Platform_and_Adaptive_Hybrid_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Low-probability_Tokens_Sustain_Exploration_in_Reinforcement_Learning_with_Verifiable_Reward","title":"[논문리뷰] Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward","excerpt":"이 [arXiv]에 게시한 'Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Exploration","Verifiable Reward","Low-Probability Regularization","Reasoning Sparks","Policy Entropy","KL Divergence","Mathematical Reasoning"],"permalink":"/ai/review/2025-10-10-Low-probability_Tokens_Sustain_Exploration_in_Reinforcement_Learning_with_Verifiable_Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-LongRM_Revealing_and_Unlocking_the_Context_Boundary_of_Reward_Modeling","title":"[논문리뷰] LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling","excerpt":"이 [arXiv]에 게시한 'LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reward Model","Long Context","LLM Alignment","Multi-stage Training","Context Window Scaling","Preference Learning","Long-RewardBench"],"permalink":"/ai/review/2025-10-10-LongRM_Revealing_and_Unlocking_the_Context_Boundary_of_Reward_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Learning_to_Route_LLMs_from_Bandit_Feedback_One_Policy_Many_Trade-offs","title":"[논문리뷰] Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs","excerpt":"Franck Dernoncourt이 [arXiv]에 게시한 'Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Routing","Contextual Bandits","Bandit Feedback","Multi-objective Optimization","Preference-tuning","Policy Gradient","Cost-efficiency"],"permalink":"/ai/review/2025-10-10-Learning_to_Route_LLMs_from_Bandit_Feedback_One_Policy_Many_Trade-offs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Learning_on_the_Job_An_Experience-Driven_Self-Evolving_Agent_for_Long-Horizon_Tasks","title":"[논문리뷰] Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks","excerpt":"이 [arXiv]에 게시한 'Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Agents","Continuous Learning","Self-Evolving","Memory Module","Long-Horizon Planning","Productivity Tasks","Test-Time Learning","Experience Replay"],"permalink":"/ai/review/2025-10-10-Learning_on_the_Job_An_Experience-Driven_Self-Evolving_Agent_for_Long-Horizon_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Large_Scale_Diffusion_Distillation_via_Score-Regularized_Continuous-Time_Consistency","title":"[논문리뷰] Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency","excerpt":"Jintao Zhang이 [arXiv]에 게시한 'Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Diffusion Distillation","Consistency Models","Score Regularization","Large-Scale Generative Models","Text-to-Image","Text-to-Video","Model Acceleration","JVP"],"permalink":"/ai/review/2025-10-10-Large_Scale_Diffusion_Distillation_via_Score-Regularized_Continuous-Time_Consistency/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-LLMs_Learn_to_Deceive_Unintentionally_Emergent_Misalignment_in_Dishonesty_from_Misaligned_Samples_to_Biased_Human-AI_Interactions","title":"[논문리뷰] LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions","excerpt":"이 [arXiv]에 게시한 'LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","LLM Misalignment","Dishonesty","Deception","Finetuning","Human-AI Interaction","Biased Feedback","Emergent Behavior"],"permalink":"/ai/review/2025-10-10-LLMs_Learn_to_Deceive_Unintentionally_Emergent_Misalignment_in_Dishonesty_from_Misaligned_Samples_to_Biased_Human-AI_Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-InstructX_Towards_Unified_Visual_Editing_with_MLLM_Guidance","title":"[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance","excerpt":"Xinghui Li이 [arXiv]에 게시한 'InstructX: Towards Unified Visual Editing with MLLM Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Visual Editing","MLLM Guidance","Diffusion Models","Image Editing","Video Editing","Unified Framework","Multimodal AI","Instruction-based Editing"],"permalink":"/ai/review/2025-10-10-InstructX_Towards_Unified_Visual_Editing_with_MLLM_Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Hybrid_Reinforcement_When_Reward_Is_Sparse_Its_Better_to_Be_Dense","title":"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense","excerpt":"이 [arXiv]에 게시한 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Reward Modeling","Large Language Models (LLMs)","Mathematical Reasoning","Sparse Rewards","Dense Rewards","Hybrid Reinforcement","Verifier-based Rewards"],"permalink":"/ai/review/2025-10-10-Hybrid_Reinforcement_When_Reward_Is_Sparse_Its_Better_to_Be_Dense/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-GCPO_When_Contrast_Fails_Go_Gold","title":"[논문리뷰] GCPO: When Contrast Fails, Go Gold","excerpt":"이 [arXiv]에 게시한 'GCPO: When Contrast Fails, Go Gold' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs Reasoning","Policy Optimization","Contrastive Learning","Chain of Thought","Reference Answers","Math Reasoning","Gold-Standard Answer"],"permalink":"/ai/review/2025-10-10-GCPO_When_Contrast_Fails_Go_Gold/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-From_What_to_Why_A_Multi-Agent_System_for_Evidence-based_Chemical_Reaction_Condition_Reasoning","title":"[논문리뷰] From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning","excerpt":"Feiwei Qin이 [arXiv]에 게시한 'From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Chemical Reaction Prediction","Explainable AI","Evidence-Based Reasoning","Large Language Models","Tool-Augmented LLMs","Scientific Discovery"],"permalink":"/ai/review/2025-10-10-From_What_to_Why_A_Multi-Agent_System_for_Evidence-based_Chemical_Reaction_Condition_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-First_Try_Matters_Revisiting_the_Role_of_Reflection_in_Reasoning_Models","title":"[논문리뷰] First Try Matters: Revisiting the Role of Reflection in Reasoning Models","excerpt":"Wee Sun Lee이 [arXiv]에 게시한 'First Try Matters: Revisiting the Role of Reflection in Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Reasoning","Chain-of-Thought (CoT)","Reflection","Early Stopping","Supervised Fine-tuning (SFT)","Token Efficiency","Mathematical Reasoning"],"permalink":"/ai/review/2025-10-10-First_Try_Matters_Revisiting_the_Role_of_Reflection_in_Reasoning_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Fidelity-Aware_Data_Composition_for_Robust_Robot_Generalization","title":"[논문리뷰] Fidelity-Aware Data Composition for Robust Robot Generalization","excerpt":"Liliang Chen이 [arXiv]에 게시한 'Fidelity-Aware Data Composition for Robust Robot Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Robot Generalization","Data Augmentation","Out-of-Distribution (OOD)","Shortcut Learning","Information Fidelity","Data Composition","Diffusion Models","Multi-View Video Synthesis"],"permalink":"/ai/review/2025-10-10-Fidelity-Aware_Data_Composition_for_Robust_Robot_Generalization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Entropy_Regularizing_Activation_Boosting_Continuous_Control_Large_Language_Models_and_Image_Classification_with_Activation_as_Entropy_Constraints","title":"[논문리뷰] Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints","excerpt":"Huazhe Xu이 [arXiv]에 게시한 'Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Entropy Regularization","Activation Functions","Continuous Control","Large Language Models","Image Classification","Reinforcement Learning","Policy Stochasticity","Entropy Constraints"],"permalink":"/ai/review/2025-10-10-Entropy_Regularizing_Activation_Boosting_Continuous_Control_Large_Language_Models_and_Image_Classification_with_Activation_as_Entropy_Constraints/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-DexNDM_Closing_the_Reality_Gap_for_Dexterous_In-Hand_Rotation_via_Joint-Wise_Neural_Dynamics_Model","title":"[논문리뷰] DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model","excerpt":"Li Yi이 [arXiv]에 게시한 'DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Dexterous Manipulation","In-Hand Rotation","Sim-to-Real Transfer","Neural Dynamics Model","Joint-Wise Learning","Autonomous Data Collection","Reinforcement Learning","Robotics"],"permalink":"/ai/review/2025-10-10-DexNDM_Closing_the_Reality_Gap_for_Dexterous_In-Hand_Rotation_via_Joint-Wise_Neural_Dynamics_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-DeepPrune_Parallel_Scaling_without_Inter-trace_Redundancy","title":"[논문리뷰] DeepPrune: Parallel Scaling without Inter-trace Redundancy","excerpt":"이 [arXiv]에 게시한 'DeepPrune: Parallel Scaling without Inter-trace Redundancy' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Parallel Scaling","Chain-of-Thought","LLM Reasoning","Dynamic Pruning","Inter-trace Redundancy","Judge Model","Resource Efficiency","Answer Diversity"],"permalink":"/ai/review/2025-10-10-DeepPrune_Parallel_Scaling_without_Inter-trace_Redundancy/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-CoMAS_Co-Evolving_Multi-Agent_Systems_via_Interaction_Rewards","title":"[논문리뷰] CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards","excerpt":"Yijiang Li이 [arXiv]에 게시한 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Multi-Agent Systems","LLM Agents","Self-Evolution","Reinforcement Learning","Interaction Rewards","LLM-as-a-Judge","Decentralized Learning"],"permalink":"/ai/review/2025-10-10-CoMAS_Co-Evolving_Multi-Agent_Systems_via_Interaction_Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Beyond_Turn_Limits_Training_Deep_Search_Agents_with_Dynamic_Context_Window","title":"[논문리뷰] Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window","excerpt":"Yaojie Lu이 [arXiv]에 게시한 'Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Deep Search Agents","Dynamic Context Window","Reinforcement Learning","Long-horizon Interaction","Context Management","High-difficulty Tasks","Multi-turn Reasoning","Web Agents"],"permalink":"/ai/review/2025-10-10-Beyond_Turn_Limits_Training_Deep_Search_Agents_with_Dynamic_Context_Window/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Beyond_Outliers_A_Study_of_Optimizers_Under_Quantization","title":"[논문리뷰] Beyond Outliers: A Study of Optimizers Under Quantization","excerpt":"이 [arXiv]에 게시한 'Beyond Outliers: A Study of Optimizers Under Quantization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Quantization","Optimizers","LLM","Post-Training Quantization (PTQ)","Quantization-Aware Training (QAT)","Error Propagation","Scaling Laws","Shampoo"],"permalink":"/ai/review/2025-10-10-Beyond_Outliers_A_Study_of_Optimizers_Under_Quantization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-Agent_Learning_via_Early_Experience","title":"[논문리뷰] Agent Learning via Early Experience","excerpt":"이 [arXiv]에 게시한 'Agent Learning via Early Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Language Agents","Early Experience","Reward-Free Learning","World Modeling","Self-Reflection","Imitation Learning","Reinforcement Learning","Out-of-Domain Generalization"],"permalink":"/ai/review/2025-10-10-Agent_Learning_via_Early_Experience/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-ARTDECO_Towards_Efficient_and_High-Fidelity_On-the-Fly_3D_Reconstruction_with_Structured_Scene_Representation","title":"[논문리뷰] ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation","excerpt":"이 [arXiv]에 게시한 'ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Monocular SLAM","Gaussian Splatting","Level of Detail (LoD)","Feed-Forward Models","Structured Scene Representation","Real-time","High-Fidelity"],"permalink":"/ai/review/2025-10-10-ARTDECO_Towards_Efficient_and_High-Fidelity_On-the-Fly_3D_Reconstruction_with_Structured_Scene_Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-10-A2Search_Ambiguity-Aware_Question_Answering_with_Reinforcement_Learning","title":"[논문리뷰] A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","lastModifiedAt":"2025-10-10 13:53:45+0900","categories":["Review"],"tags":["Review","Question Answering","Reinforcement Learning","Large Language Models","Ambiguity Resolution","Multi-hop QA","Automated Data Generation","Tool-Augmented LLMs","AnsF1 Reward"],"permalink":"/ai/review/2025-10-10-A2Search_Ambiguity-Aware_Question_Answering_with_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-WristWorld_Generating_Wrist-Views_via_4D_World_Models_for_Robotic_Manipulation","title":"[논문리뷰] WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation","excerpt":"이 [arXiv]에 게시한 'WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","4D World Models","Robotic Manipulation","Video Generation","Multi-view Synthesis","Visual-Language-Action (VLA)","Geometric Consistency","Diffusion Models","Wrist-View"],"permalink":"/ai/review/2025-10-9-WristWorld_Generating_Wrist-Views_via_4D_World_Models_for_Robotic_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Why_Low-Precision_Transformer_Training_Fails_An_Analysis_on_Flash_Attention","title":"[논문리뷰] Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention","excerpt":"이 [arXiv]에 게시한 'Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Low-Precision Training","Flash Attention","Transformer","Numerical Stability","BF16","Rounding Error","Gradient Bias","Deep Learning Optimization"],"permalink":"/ai/review/2025-10-9-Why_Low-Precision_Transformer_Training_Fails_An_Analysis_on_Flash_Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-When_Benchmarks_Age_Temporal_Misalignment_through_Large_Language_Model_Factuality_Evaluation","title":"[논문리뷰] When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation","excerpt":"이 [arXiv]에 게시한 'When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","LLM Factuality Evaluation","Benchmark Aging","Temporal Misalignment","Information Retrieval","Question Answering","Evaluation Metrics","GPT-4o-mini","Qwen2.5"],"permalink":"/ai/review/2025-10-9-When_Benchmarks_Age_Temporal_Misalignment_through_Large_Language_Model_Factuality_Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Vibe_Checker_Aligning_Code_Evaluation_with_Human_Preference","title":"[논문리뷰] Vibe Checker: Aligning Code Evaluation with Human Preference","excerpt":"이 [arXiv]에 게시한 'Vibe Checker: Aligning Code Evaluation with Human Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Code Evaluation","Instruction Following","Human Preference","Large Language Models","Vibe Check","Non-functional Requirements","VeriCode"],"permalink":"/ai/review/2025-10-9-Vibe_Checker_Aligning_Code_Evaluation_with_Human_Preference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-U-Bench_A_Comprehensive_Understanding_of_U-Net_through_100-Variant_Benchmarking","title":"[논문리뷰] U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking","excerpt":"Heqin Zhu이 [arXiv]에 게시한 'U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","U-Net","Medical Image Segmentation","Benchmarking","Performance Evaluation","Efficiency Metrics","Zero-shot Generalization","U-Score"],"permalink":"/ai/review/2025-10-9-U-Bench_A_Comprehensive_Understanding_of_U-Net_through_100-Variant_Benchmarking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-The_Markovian_Thinker","title":"[논문리뷰] The Markovian Thinker","excerpt":"이 [arXiv]에 게시한 'The Markovian Thinker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Chain-of-Thought","Markovian Thinking","Context Management","Computational Efficiency","Long-Context LLMs","Transformer Optimization"],"permalink":"/ai/review/2025-10-9-The_Markovian_Thinker/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-The_African_Languages_Lab_A_Collaborative_Approach_to_Advancing_Low-Resource_African_NLP","title":"[논문리뷰] The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP","excerpt":"이 [arXiv]에 게시한 'The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Low-Resource NLP","African Languages","Data Collection","Multilingual Models","Fine-Tuning","Speech Data","Text Data","Capacity Building"],"permalink":"/ai/review/2025-10-9-The_African_Languages_Lab_A_Collaborative_Approach_to_Advancing_Low-Resource_African_NLP/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-TTRV_Test-Time_Reinforcement_Learning_for_Vision_Language_Models","title":"[논문리뷰] TTRV: Test-Time Reinforcement Learning for Vision Language Models","excerpt":"Serena Yeung-Levy이 [arXiv]에 게시한 'TTRV: Test-Time Reinforcement Learning for Vision Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Test-Time Adaptation","Unsupervised Learning","Image Recognition","Visual Question Answering (VQA)","Group Relative Policy Optimization (GRPO)","Entropy Regularization"],"permalink":"/ai/review/2025-10-9-TTRV_Test-Time_Reinforcement_Learning_for_Vision_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-StaMo_Unsupervised_Learning_of_Generalizable_Robot_Motion_from_Compact_State_Representation","title":"[논문리뷰] StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation","excerpt":"이 [arXiv]에 게시한 'StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Robot Learning","State Representation","Motion Representation","Diffusion Models","Unsupervised Learning","World Modeling","Vision-Language Models","Latent Action"],"permalink":"/ai/review/2025-10-9-StaMo_Unsupervised_Learning_of_Generalizable_Robot_Motion_from_Compact_State_Representation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-SHANKS_Simultaneous_Hearing_and_Thinking_for_Spoken_Language_Models","title":"[논문리뷰] SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models","excerpt":"Kevin Lin이 [arXiv]에 게시한 'SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Spoken Language Models","Real-time Interaction","Thinking While Listening","Chain-of-Thought","Interruption","Tool Calling","Streaming ASR"],"permalink":"/ai/review/2025-10-9-SHANKS_Simultaneous_Hearing_and_Thinking_for_Spoken_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Revisiting_the_Uniform_Information_Density_Hypothesis_in_LLM_Reasoning_Traces","title":"[논문리뷰] Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces","excerpt":"이 [arXiv]에 게시한 'Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Chain-of-Thought","Uniform Information Density","Information Theory","Reasoning Trace Analysis","Entropy","Mathematical Reasoning","Model Evaluation"],"permalink":"/ai/review/2025-10-9-Revisiting_the_Uniform_Information_Density_Hypothesis_in_LLM_Reasoning_Traces/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Revisiting_Long-context_Modeling_from_Context_Denoising_Perspective","title":"[논문리뷰] Revisiting Long-context Modeling from Context Denoising Perspective","excerpt":"이 [arXiv]에 게시한 'Revisiting Long-context Modeling from Context Denoising Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Long-context Models","Context Denoising","Integrated Gradient","LLM Training","Context Window Scaling","Information Flow","Attention Mechanism"],"permalink":"/ai/review/2025-10-9-Revisiting_Long-context_Modeling_from_Context_Denoising_Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-RLinf-VLA_A_Unified_and_Efficient_Framework_for_VLARL_Training","title":"[논문리뷰] RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training","excerpt":"이 [arXiv]에 게시한 'RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","VLA Models","Robotics","GPU Management","PPO","GRPO","Sim-to-Real"],"permalink":"/ai/review/2025-10-9-RLinf-VLA_A_Unified_and_Efficient_Framework_for_VLARL_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Pushing_on_Multilingual_Reasoning_Models_with_Language-Mixed_Chain-of-Thought","title":"[논문리뷰] Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought","excerpt":"이 [arXiv]에 게시한 'Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multilingual Reasoning","Chain-of-Thought (CoT)","Language-Mixed CoT","Instruction Tuning","Korean LLMs","Data Curation","Supervised Fine-tuning (SFT)"],"permalink":"/ai/review/2025-10-9-Pushing_on_Multilingual_Reasoning_Models_with_Language-Mixed_Chain-of-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Patch-as-Decodable-Token_Towards_Unified_Multi-Modal_Vision_Tasks_in_MLLMs","title":"[논문리뷰] Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs","excerpt":"Jingyi Liao이 [arXiv]에 게시한 'Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Visual Reference Tokens (VRTs)","Dense Prediction","Referring Expression Comprehension (REC)","Open-Vocabulary Detection (OVD)","Image Captioning","Unified Architecture","Autoregressive Generation"],"permalink":"/ai/review/2025-10-9-Patch-as-Decodable-Token_Towards_Unified_Multi-Modal_Vision_Tasks_in_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Online_Generic_Event_Boundary_Detection","title":"[논문리뷰] Online Generic Event Boundary Detection","excerpt":"Jonghyun Choi이 [arXiv]에 게시한 'Online Generic Event Boundary Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Online Video Analysis","Event Boundary Detection","Event Segmentation Theory","Real-time AI","Anomaly Detection","Transformer Architecture"],"permalink":"/ai/review/2025-10-9-Online_Generic_Event_Boundary_Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-OBS-Diff_Accurate_Pruning_For_Diffusion_Models_in_One-Shot","title":"[논문리뷰] OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot","excerpt":"이 [arXiv]에 게시한 'OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Diffusion Models","Network Pruning","One-Shot Pruning","Optimal Brain Surgeon (OBS)","Model Compression","Timestep-Aware Hessian","Structured Pruning"],"permalink":"/ai/review/2025-10-9-OBS-Diff_Accurate_Pruning_For_Diffusion_Models_in_One-Shot/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-NorMuon_Making_Muon_more_efficient_and_scalable","title":"[논문리뷰] NorMuon: Making Muon more efficient and scalable","excerpt":"Tuo Zhao이 [arXiv]에 게시한 'NorMuon: Making Muon more efficient and scalable' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","LLM Training","Optimizer","Muon","Orthogonalization","Adaptive Learning Rates","Distributed Training","FSDP2","NorMuon"],"permalink":"/ai/review/2025-10-9-NorMuon_Making_Muon_more_efficient_and_scalable/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Native_Hybrid_Attention_for_Efficient_Sequence_Modeling","title":"[논문리뷰] Native Hybrid Attention for Efficient Sequence Modeling","excerpt":"Yu Cheng이 [arXiv]에 게시한 'Native Hybrid Attention for Efficient Sequence Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Sequence Modeling","Hybrid Attention","Transformer Architecture","Linear Attention","Sliding Window Attention","Long Context","Large Language Models (LLMs)","Efficiency"],"permalink":"/ai/review/2025-10-9-Native_Hybrid_Attention_for_Efficient_Sequence_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Multi-Agent_Tool-Integrated_Policy_Optimization","title":"[논문리뷰] Multi-Agent Tool-Integrated Policy Optimization","excerpt":"Lidong Bing이 [arXiv]에 게시한 'Multi-Agent Tool-Integrated Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multi-Agent RL","Tool-Integrated Planning","Large Language Models (LLMs)","Policy Optimization","Credit Assignment","Reinforcement Learning","MATPO"],"permalink":"/ai/review/2025-10-9-Multi-Agent_Tool-Integrated_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Ming-UniVision_Joint_Image_Understanding_and_Generation_with_a_Unified_Continuous_Tokenizer","title":"[논문리뷰] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer","excerpt":"이 [arXiv]에 게시한 'Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Unified Vision-Language Model","Continuous Tokenizer","Autoregressive Generation","Image Understanding","Image Generation","Multimodal AI","In-context Editing"],"permalink":"/ai/review/2025-10-9-Ming-UniVision_Joint_Image_Understanding_and_Generation_with_a_Unified_Continuous_Tokenizer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-MLE-Smith_Scaling_MLE_Tasks_with_Automated_Multi-Agent_Pipeline","title":"[논문리뷰] MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline","excerpt":"이 [arXiv]에 게시한 'MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","MLE (Machine Learning Engineering)","Automated Task Generation","Multi-Agent System","LLM Agents","Benchmark","Data Curation","Hybrid Verification","Kaggle"],"permalink":"/ai/review/2025-10-9-MLE-Smith_Scaling_MLE_Tasks_with_Automated_Multi-Agent_Pipeline/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-MATRIX_Mask_Track_Alignment_for_Interaction-aware_Video_Generation","title":"[논문리뷰] MATRIX: Mask Track Alignment for Interaction-aware Video Generation","excerpt":"Hyunwook Choi이 [arXiv]에 게시한 'MATRIX: Mask Track Alignment for Interaction-aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Transformers","Human-Object Interaction","Attention Alignment","Mask Tracking","Semantic Grounding","Semantic Propagation","Text-to-Video"],"permalink":"/ai/review/2025-10-9-MATRIX_Mask_Track_Alignment_for_Interaction-aware_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Lumina-DiMOO_An_Omni_Diffusion_Large_Language_Model_for_Multi-Modal_Generation_and_Understanding","title":"[논문리뷰] Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding","excerpt":"이 [arXiv]에 게시한 'Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Multi-modal LLM","Discrete Diffusion","Image Generation","Image Understanding","Omni-modal","Interactive Retouching","Generative AI","Reinforcement Learning"],"permalink":"/ai/review/2025-10-9-Lumina-DiMOO_An_Omni_Diffusion_Large_Language_Model_for_Multi-Modal_Generation_and_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Heptapod_Language_Modeling_on_Visual_Signals","title":"[논문리뷰] Heptapod: Language Modeling on Visual Signals","excerpt":"이 [arXiv]에 게시한 'Heptapod: Language Modeling on Visual Signals' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Image Generation","Language Modeling","Causal Transformer","2D Distribution Prediction","Visual Tokenization","Self-Supervised Learning","Generative Models"],"permalink":"/ai/review/2025-10-9-Heptapod_Language_Modeling_on_Visual_Signals/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-G2RPO_Granular_GRPO_for_Precise_Reward_in_Flow_Models","title":"[논문리뷰] G^2RPO: Granular GRPO for Precise Reward in Flow Models","excerpt":"이 [arXiv]에 게시한 'G^2RPO: Granular GRPO for Precise Reward in Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Flow Models","Generative Models","Human Preference Alignment","Stochastic Differential Equations (SDE)","Reward Signal","Multi-Granularity"],"permalink":"/ai/review/2025-10-9-G2RPO_Granular_GRPO_for_Precise_Reward_in_Flow_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-DeepTravel_An_End-to-End_Agentic_Reinforcement_Learning_Framework_for_Autonomous_Travel_Planning_Agents","title":"[논문리뷰] DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents","excerpt":"이 [arXiv]에 게시한 'DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Travel Planning","Large Language Models","Sandbox Environment","Hierarchical Reward Modeling","Experience Replay","Autonomous Agents"],"permalink":"/ai/review/2025-10-9-DeepTravel_An_End-to-End_Agentic_Reinforcement_Learning_Framework_for_Autonomous_Travel_Planning_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-D3QE_Learning_Discrete_Distribution_Discrepancy-aware_Quantization_Error_for_Autoregressive-Generated_Image_Detection","title":"[논문리뷰] D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection","excerpt":"Yueqi Duan이 [arXiv]에 게시한 'D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Image Detection","Discrete Distribution Discrepancy","Quantization Error","Transformer","Generative AI","Deepfake Detection"],"permalink":"/ai/review/2025-10-9-D3QE_Learning_Discrete_Distribution_Discrepancy-aware_Quantization_Error_for_Autoregressive-Generated_Image_Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Cache-to-Cache_Direct_Semantic_Communication_Between_Large_Language_Models","title":"[논문리뷰] Cache-to-Cache: Direct Semantic Communication Between Large Language Models","excerpt":"이 [arXiv]에 게시한 'Cache-to-Cache: Direct Semantic Communication Between Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Inter-model Communication","KV-Cache","Semantic Transfer","Multi-LLM Systems","Cache Fusion","Latency Reduction","Knowledge Sharing"],"permalink":"/ai/review/2025-10-9-Cache-to-Cache_Direct_Semantic_Communication_Between_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-CALM_Before_the_STORM_Unlocking_Native_Reasoning_for_Optimization_Modeling","title":"[논문리뷰] CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling","excerpt":"Chengpeng Li이 [arXiv]에 게시한 'CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Optimization Modeling","Reflective Generation","Supervised Fine-tuning","Reinforcement Learning","Human-in-the-Loop","Code Generation","Domain Adaptation"],"permalink":"/ai/review/2025-10-9-CALM_Before_the_STORM_Unlocking_Native_Reasoning_for_Optimization_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Bridging_Text_and_Video_Generation_A_Survey","title":"[논문리뷰] Bridging Text and Video Generation: A Survey","excerpt":"G. Maragatham이 [arXiv]에 게시한 'Bridging Text and Video Generation: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Text-to-Video Generation","Generative Models","Diffusion Models","GANs","VAEs","Video Synthesis","Survey","Evaluation Metrics"],"permalink":"/ai/review/2025-10-9-Bridging_Text_and_Video_Generation_A_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Beyond_Monolingual_Assumptions_A_Survey_of_Code-Switched_NLP_in_the_Era_of_Large_Language_Models","title":"[논문리뷰] Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models","excerpt":"이 [arXiv]에 게시한 'Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Code-switching","Multilingual NLP","Large Language Models","NLP Survey","Data Augmentation","Evaluation Metrics","Low-Resource Languages"],"permalink":"/ai/review/2025-10-9-Beyond_Monolingual_Assumptions_A_Survey_of_Code-Switched_NLP_in_the_Era_of_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Artificial_Hippocampus_Networks_for_Efficient_Long-Context_Modeling","title":"[논문리뷰] Artificial Hippocampus Networks for Efficient Long-Context Modeling","excerpt":"이 [arXiv]에 게시한 'Artificial Hippocampus Networks for Efficient Long-Context Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Long-Context Modeling","Transformer","RNN","Memory Management","Self-Distillation","Attention Mechanism","Artificial Hippocampus Networks","Cognitive Science"],"permalink":"/ai/review/2025-10-9-Artificial_Hippocampus_Networks_for_Efficient_Long-Context_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-Are_We_Using_the_Right_Benchmark_An_Evaluation_Framework_for_Visual_Token_Compression_Methods","title":"[논문리뷰] Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods","excerpt":"Yiyu Wang이 [arXiv]에 게시한 'Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Visual Token Compression","MLLMs","Evaluation Framework","Benchmarking","Downsampling","Data Filtering","Model Efficiency"],"permalink":"/ai/review/2025-10-9-Are_We_Using_the_Right_Benchmark_An_Evaluation_Framework_for_Visual_Token_Compression_Methods/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-9-AlphaApollo_Orchestrating_Foundation_Models_and_Professional_Tools_into_a_Self-Evolving_System_for_Deep_Agentic_Reasoning","title":"[논문리뷰] AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning","excerpt":"Zongze Li이 [arXiv]에 게시한 'AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","lastModifiedAt":"2025-10-09 13:45:06+0900","categories":["Review"],"tags":["Review","Foundation Models","Agentic Reasoning","Tool Use","Self-Evolving System","Retrieval-Augmented Generation","Computational Tools","Error Correction"],"permalink":"/ai/review/2025-10-9-AlphaApollo_Orchestrating_Foundation_Models_and_Professional_Tools_into_a_Self-Evolving_System_for_Deep_Agentic_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-VeriGuard_Enhancing_LLM_Agent_Safety_via_Verified_Code_Generation","title":"[논문리뷰] VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation","excerpt":"이 [arXiv]에 게시한 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM Agents","Safety","Formal Verification","Code Generation","Runtime Monitoring","Security","Guardrails","Policy Enforcement"],"permalink":"/ai/review/2025-10-8-VeriGuard_Enhancing_LLM_Agent_Safety_via_Verified_Code_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Training_Dynamics_Impact_Post-Training_Quantization_Robustness","title":"[논문리뷰] Training Dynamics Impact Post-Training Quantization Robustness","excerpt":"Jonas Geiping이 [arXiv]에 게시한 'Training Dynamics Impact Post-Training Quantization Robustness' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Post-Training Quantization","Quantization Robustness","Training Dynamics","Learning Rate Schedules","Weight Averaging","Large Language Models","LLMs","Hyperparameter Tuning"],"permalink":"/ai/review/2025-10-8-Training_Dynamics_Impact_Post-Training_Quantization_Robustness/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-TensorBLEU_Vectorized_GPU-based_BLEU_Score_Implementation_for_Per-Sentence_In-Training_Evaluation","title":"[논문리뷰] TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation","excerpt":"이 [arXiv]에 게시한 'TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","BLEU Score","GPU Acceleration","PyTorch","Natural Language Processing","Reinforcement Learning","Vectorization","In-Training Evaluation","N-gram Counting"],"permalink":"/ai/review/2025-10-8-TensorBLEU_Vectorized_GPU-based_BLEU_Score_Implementation_for_Per-Sentence_In-Training_Evaluation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-TaTToo_Tool-Grounded_Thinking_PRM_for_Test-Time_Scaling_in_Tabular_Reasoning","title":"[논문리뷰] TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning","excerpt":"이 [arXiv]에 게시한 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Process Reward Models","Tabular Reasoning","Test-Time Scaling","Tool Integration","Reinforcement Learning","Supervised Fine-tuning","Large Language Models","Data Curation"],"permalink":"/ai/review/2025-10-8-TaTToo_Tool-Grounded_Thinking_PRM_for_Test-Time_Scaling_in_Tabular_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-ShapeGen4D_Towards_High_Quality_4D_Shape_Generation_from_Videos","title":"[논문리뷰] ShapeGen4D: Towards High Quality 4D Shape Generation from Videos","excerpt":"Sergey Tulyakov이 [arXiv]에 게시한 'ShapeGen4D: Towards High Quality 4D Shape Generation from Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","4D Shape Generation","Video-conditioned","Dynamic 3D Meshes","Latent Diffusion Model","Spatiotemporal Attention","Temporal Consistency","Pre-trained 3D Models","VAE"],"permalink":"/ai/review/2025-10-8-ShapeGen4D_Towards_High_Quality_4D_Shape_Generation_from_Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Scaling_Code-Assisted_Chain-of-Thoughts_and_Instructions_for_Model_Reasoning","title":"[논문리뷰] Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning","excerpt":"Zhuoshi Pan이 [arXiv]에 게시한 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Code-Assisted Reasoning","Chain-of-Thought (CoT)","Instruction Tuning","Data Augmentation","LLMs","Mathematical Reasoning","Self-Verification","Code Generation"],"permalink":"/ai/review/2025-10-8-Scaling_Code-Assisted_Chain-of-Thoughts_and_Instructions_for_Model_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Revisiting_Modeling_and_Evaluation_Approaches_in_Speech_Emotion_Recognition_Considering_Subjectivity_of_Annotators_and_Ambiguity_of_Emotions","title":"[논문리뷰] Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions","excerpt":"이 [arXiv]에 게시한 'Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Speech Emotion Recognition","Annotator Subjectivity","Emotion Ambiguity","Soft Labels","Multi-label Classification","Evaluation Metrics","Loss Functions"],"permalink":"/ai/review/2025-10-8-Revisiting_Modeling_and_Evaluation_Approaches_in_Speech_Emotion_Recognition_Considering_Subjectivity_of_Annotators_and_Ambiguity_of_Emotions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Refusal_Falls_off_a_Cliff_How_Safety_Alignment_Fails_in_Reasoning","title":"[논문리뷰] Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?","excerpt":"이 [arXiv]에 게시한 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Safety Alignment","Large Reasoning Models","Mechanistic Interpretability","Refusal Cliff","Attention Heads","Data Selection","Linear Probing"],"permalink":"/ai/review/2025-10-8-Refusal_Falls_off_a_Cliff_How_Safety_Alignment_Fails_in_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Presenting_a_Paper_is_an_Art_Self-Improvement_Aesthetic_Agents_for_Academic_Presentations","title":"[논문리뷰] Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations","excerpt":"이 [arXiv]에 게시한 'Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Self-Improvement Agent","Academic Presentation","Aesthetic Evaluation","Reinforcement Learning","Multi-task Learning","Presentation Generation","LLM-based Agents","Human Feedback"],"permalink":"/ai/review/2025-10-8-Presenting_a_Paper_is_an_Art_Self-Improvement_Aesthetic_Agents_for_Academic_Presentations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-OneFlow_Concurrent_Mixed-Modal_and_Interleaved_Generation_with_Edit_Flows","title":"[논문리뷰] OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows","excerpt":"이 [arXiv]에 게시한 'OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Non-Autoregressive","Multimodal Generation","Edit Flows","Flow Matching","Interleaved Generation","Text-to-Image Synthesis","Unified Models"],"permalink":"/ai/review/2025-10-8-OneFlow_Concurrent_Mixed-Modal_and_Interleaved_Generation_with_Edit_Flows/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-No_Tokens_Wasted_Leveraging_Long_Context_in_Biomedical_Vision-Language_Models","title":"[논문리뷰] No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models","excerpt":"Xiao Xiao Sun이 [arXiv]에 게시한 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Biomedical Vision-Language Models","Long-context Modeling","Contrastive Learning","Token Efficiency","Zero-shot Classification","Medical Image Retrieval"],"permalink":"/ai/review/2025-10-8-No_Tokens_Wasted_Leveraging_Long_Context_in_Biomedical_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Mixing_Mechanisms_How_Language_Models_Retrieve_Bound_Entities_In-Context","title":"[논문리뷰] Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context","excerpt":"이 [arXiv]에 게시한 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Language Models","In-Context Learning","Entity Binding","Mechanistic Interpretability","Causal Abstraction","Long-Context Reasoning","Positional Encoding","Information Retrieval"],"permalink":"/ai/review/2025-10-8-Mixing_Mechanisms_How_Language_Models_Retrieve_Bound_Entities_In-Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-MixReasoning_Switching_Modes_to_Think","title":"[논문리뷰] MixReasoning: Switching Modes to Think","excerpt":"이 [arXiv]에 게시한 'MixReasoning: Switching Modes to Think' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Chain-of-Thought","Efficiency","LoRA","Adaptive Reasoning","Token Uncertainty","Dynamic Switching","Reasoning Compression"],"permalink":"/ai/review/2025-10-8-MixReasoning_Switching_Modes_to_Think/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Margin_Adaptive_DPO_Leveraging_Reward_Model_for_Granular_Control_in_Preference_Optimization","title":"[논문리뷰] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization","excerpt":"sirano1004이 [arXiv]에 게시한 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Direct Preference Optimization","Preference Alignment","Adaptive Regularization","Reward Model","Large Language Models","Sentiment Generation"],"permalink":"/ai/review/2025-10-8-Margin_Adaptive_DPO_Leveraging_Reward_Model_for_Granular_Control_in_Preference_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-LightCache_Memory-Efficient_Training-Free_Acceleration_for_Video_Generation","title":"[논문리뷰] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation","excerpt":"Zheng Zhan이 [arXiv]에 게시한 'LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Memory Efficiency","Inference Acceleration","Training-Free","Cache Mechanism","GPU Optimization"],"permalink":"/ai/review/2025-10-8-LightCache_Memory-Efficient_Training-Free_Acceleration_for_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Less_is_More_Recursive_Reasoning_with_Tiny_Networks","title":"[논문리뷰] Less is More: Recursive Reasoning with Tiny Networks","excerpt":"이 [arXiv]에 게시한 'Less is More: Recursive Reasoning with Tiny Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Recursive Reasoning","Tiny Networks","Deep Supervision","Hierarchical Reasoning Model (HRM)","Sudoku-Extreme","ARC-AGI","Generalization","Parameter Efficiency"],"permalink":"/ai/review/2025-10-8-Less_is_More_Recursive_Reasoning_with_Tiny_Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-In-the-Flow_Agentic_System_Optimization_for_Effective_Planning_and_Tool_Use","title":"[논문리뷰] In-the-Flow Agentic System Optimization for Effective Planning and Tool Use","excerpt":"이 [arXiv]에 게시한 'In-the-Flow Agentic System Optimization for Effective Planning and Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Agentic Systems","Large Language Models (LLMs)","Tool Use","Reinforcement Learning (RL)","On-policy Optimization","Flow-based Group Refined Policy Optimization (Flow-GRPO)","Multi-turn Reasoning"],"permalink":"/ai/review/2025-10-8-In-the-Flow_Agentic_System_Optimization_for_Effective_Planning_and_Tool_Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Human3R_Everyone_Everywhere_All_at_Once","title":"[논문리뷰] Human3R: Everyone Everywhere All at Once","excerpt":"Yuliang Xiu이 [arXiv]에 게시한 'Human3R: Everyone Everywhere All at Once' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","4D Human-Scene Reconstruction","Online Reconstruction","Multi-person","SMPL-X","Transformer","Visual Prompt Tuning","Real-time","Foundation Model"],"permalink":"/ai/review/2025-10-8-Human3R_Everyone_Everywhere_All_at_Once/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-HoloScene_Simulation-Ready_Interactive_3D_Worlds_from_a_Single_Video","title":"[논문리뷰] HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video","excerpt":"Katelyn Gao이 [arXiv]에 게시한 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Digital Twin","Scene Graph","Physical Simulation","Interactive Environments","Single Video Reconstruction","Neural Rendering"],"permalink":"/ai/review/2025-10-8-HoloScene_Simulation-Ready_Interactive_3D_Worlds_from_a_Single_Video/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-HalluGuard_Evidence-Grounded_Small_Reasoning_Models_to_Mitigate_Hallucinations_in_Retrieval-Augmented_Generation","title":"[논문리뷰] HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation","excerpt":"Radu State이 [arXiv]에 게시한 'HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Hallucination Detection","Retrieval-Augmented Generation (RAG)","Small Reasoning Model (SRM)","Preference Fine-tuning","ORPO","Evidence Grounding","Fact-checking"],"permalink":"/ai/review/2025-10-8-HalluGuard_Evidence-Grounded_Small_Reasoning_Models_to_Mitigate_Hallucinations_in_Retrieval-Augmented_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Fathom-DeepResearch_Unlocking_Long_Horizon_Information_Retrieval_and_Synthesis_for_SLMs","title":"[논문리뷰] Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs","excerpt":"이 [arXiv]에 게시한 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","DeepResearch Agents","Tool-integrated Reasoning","Reinforcement Learning","Information Retrieval","Information Synthesis","Multi-agent Self-play","Reward Shaping","LLM"],"permalink":"/ai/review/2025-10-8-Fathom-DeepResearch_Unlocking_Long_Horizon_Information_Retrieval_and_Synthesis_for_SLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Fast-dLLM_v2_Efficient_Block-Diffusion_LLM","title":"[논문리뷰] Fast-dLLM v2: Efficient Block-Diffusion LLM","excerpt":"이 [arXiv]에 게시한 'Fast-dLLM v2: Efficient Block-Diffusion LLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Inference Acceleration","Parallel Decoding","Autoregressive Models","Caching","Fine-tuning","Block-wise Attention"],"permalink":"/ai/review/2025-10-8-Fast-dLLM_v2_Efficient_Block-Diffusion_LLM/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Equilibrium_Matching_Generative_Modeling_with_Implicit_Energy-Based_Models","title":"[논문리뷰] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models","excerpt":"이 [arXiv]에 게시한 'Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Generative Models","Equilibrium Dynamics","Energy-Based Models (EBMs)","Flow Matching","Diffusion Models","Optimization-Based Sampling","Image Generation"],"permalink":"/ai/review/2025-10-8-Equilibrium_Matching_Generative_Modeling_with_Implicit_Energy-Based_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-EgoNight_Towards_Egocentric_Vision_Understanding_at_Night_with_a_Challenging_Benchmark","title":"[논문리뷰] EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark","excerpt":"Tianwen Qian이 [arXiv]에 게시한 'EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Egocentric Vision","Nighttime Conditions","Visual Question Answering (VQA)","Day-Night Alignment","Multimodal Large Language Models (MLLMs)","Depth Estimation","Correspondence Retrieval","Benchmark"],"permalink":"/ai/review/2025-10-8-EgoNight_Towards_Egocentric_Vision_Understanding_at_Night_with_a_Challenging_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Drax_Speech_Recognition_with_Discrete_Flow_Matching","title":"[논문리뷰] Drax: Speech Recognition with Discrete Flow Matching","excerpt":"이 [arXiv]에 게시한 'Drax: Speech Recognition with Discrete Flow Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Automatic Speech Recognition (ASR)","Discrete Flow Matching (DFM)","Non-Autoregressive (NAR)","Generative Models","Tri-mixture Probability Path","Parallel Decoding","Accuracy-Efficiency Trade-off","Speech Synthesis"],"permalink":"/ai/review/2025-10-8-Drax_Speech_Recognition_with_Discrete_Flow_Matching/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Distributional_Semantics_Tracing_A_Framework_for_Explaining_Hallucinations_in_Large_Language_Models","title":"[논문리뷰] Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models","excerpt":"Jacobo Azcona이 [arXiv]에 게시한 'Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM Hallucinations","Mechanistic Interpretability","Distributional Semantics Tracing (DST)","Dual-Process Theory","Semantic Drift","Commitment Layer","Faithfulness Score"],"permalink":"/ai/review/2025-10-8-Distributional_Semantics_Tracing_A_Framework_for_Explaining_Hallucinations_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Discrete_Diffusion_Models_with_MLLMs_for_Unified_Medical_Multimodal_Generation","title":"[논문리뷰] Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation","excerpt":"이 [arXiv]에 게시한 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Discrete Diffusion Models","Multimodal Large Language Models (MLLMs)","Medical Image Generation","Medical Report Generation","Multimodal Generation","Medical AI","Cross-modal Alignment"],"permalink":"/ai/review/2025-10-8-Discrete_Diffusion_Models_with_MLLMs_for_Unified_Medical_Multimodal_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Demystifying_deep_search_a_holistic_evaluation_with_hint-free_multi-hop_questions_and_factorised_metrics","title":"[논문리뷰] Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics","excerpt":"이 [arXiv]에 게시한 'Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Deep Search","Multi-hop Reasoning","Evaluation Benchmark","Retrieval-Augmented Generation","Web Agents","Diagnostic Metrics","Knowledge Utilization","Hint-Free Questions"],"permalink":"/ai/review/2025-10-8-Demystifying_deep_search_a_holistic_evaluation_with_hint-free_multi-hop_questions_and_factorised_metrics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Deforming_Videos_to_Masks_Flow_Matching_for_Referring_Video_Segmentation","title":"[논문리뷰] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation","excerpt":"Chengzu Li이 [arXiv]에 게시한 'Deforming Videos to Masks: Flow Matching for Referring Video Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Referring Video Object Segmentation","Flow Matching","Video Segmentation","Generative Models","Text-to-Video","Continuous Flow","Diffusion Models"],"permalink":"/ai/review/2025-10-8-Deforming_Videos_to_Masks_Flow_Matching_for_Referring_Video_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-DRIFT_Learning_from_Abundant_User_Dissatisfaction_in_Real-World_Preference_Learning","title":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning","excerpt":"Zheli Liu이 [arXiv]에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Preference Learning","LLMs","User Feedback","Dissatisfaction Signals","DPO","Iterative Training","RLHF","Exploration"],"permalink":"/ai/review/2025-10-8-DRIFT_Learning_from_Abundant_User_Dissatisfaction_in_Real-World_Preference_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-CoDA_Coding_LM_via_Diffusion_Adaptation","title":"[논문리뷰] CoDA: Coding LM via Diffusion Adaptation","excerpt":"이 [arXiv]에 게시한 'CoDA: Coding LM via Diffusion Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Code Generation","Bidirectional Decoding","Text Infilling","Instruction Tuning","Lightweight Models","TPU Training"],"permalink":"/ai/review/2025-10-8-CoDA_Coding_LM_via_Diffusion_Adaptation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-CCD_Mitigating_Hallucinations_in_Radiology_MLLMs_via_Clinical_Contrastive_Decoding","title":"[논문리뷰] CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding","excerpt":"이 [arXiv]에 게시한 'CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Radiology Report Generation (RRG)","Medical Hallucinations","Contrastive Decoding","Training-free Inference","Clinical AI","Visual Question Answering (VQA)"],"permalink":"/ai/review/2025-10-8-CCD_Mitigating_Hallucinations_in_Radiology_MLLMs_via_Clinical_Contrastive_Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-CARE_Cognitive-reasoning_Augmented_Reinforcement_for_Emotional_Support_Conversation","title":"[논문리뷰] CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation","excerpt":"이 [arXiv]에 게시한 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Emotional Support Conversation","Cognitive Reasoning","Reinforcement Learning","Dialogue Generation","Natural Language Processing","Large Language Models","Psychological Support"],"permalink":"/ai/review/2025-10-8-CARE_Cognitive-reasoning_Augmented_Reinforcement_for_Emotional_Support_Conversation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-Benchmark_It_Yourself_BIY_Preparing_a_Dataset_and_Benchmarking_AI_Models_for_Scatterplot-Related_Tasks","title":"[논문리뷰] Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks","excerpt":"Pedro Bizarro이 [arXiv]에 게시한 'Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Scatterplot Analysis","AI Benchmarking","Multimodal LLMs","Synthetic Data Generation","Cluster Detection","Outlier Detection","Data Visualization","Prompt Engineering"],"permalink":"/ai/review/2025-10-8-Benchmark_It_Yourself_BIY_Preparing_a_Dataset_and_Benchmarking_AI_Models_for_Scatterplot-Related_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-BIRD-INTERACT_Re-imagining_Text-to-SQL_Evaluation_for_Large_Language_Models_via_Lens_of_Dynamic_Interactions","title":"[논문리뷰] BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions","excerpt":"Shipei Lin이 [arXiv]에 게시한 'BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Text-to-SQL","LLM Evaluation","Multi-turn Interaction","Dynamic Environment","User Simulator","Ambiguity Resolution","LLM Agents"],"permalink":"/ai/review/2025-10-8-BIRD-INTERACT_Re-imagining_Text-to-SQL_Evaluation_for_Large_Language_Models_via_Lens_of_Dynamic_Interactions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-A_Contextual_Quality_Reward_Model_for_Reliable_and_Efficient_Best-of-N_Sampling","title":"[논문리뷰] A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling","excerpt":"sirano1004이 [arXiv]에 게시한 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Reward Model","Best-of-N Sampling","Preference Alignment","Contextual Acceptability","Discrete Choice Model","Alignment Guardrail","Inference Accelerator"],"permalink":"/ai/review/2025-10-8-A_Contextual_Quality_Reward_Model_for_Reliable_and_Efficient_Best-of-N_Sampling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-ASPO_Asymmetric_Importance_Sampling_Policy_Optimization","title":"[논문리뷰] ASPO: Asymmetric Importance Sampling Policy Optimization","excerpt":"Xiu Li이 [arXiv]에 게시한 'ASPO: Asymmetric Importance Sampling Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Importance Sampling","Policy Optimization","PPO-Clip","Outcome-Supervised RL","Token Weighting","GRPO"],"permalink":"/ai/review/2025-10-8-ASPO_Asymmetric_Importance_Sampling_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-8-AInstein_Assessing_the_Feasibility_of_AI-Generated_Approaches_to_Research_Problems","title":"[논문리뷰] AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems","excerpt":"Jose Dolz이 [arXiv]에 게시한 'AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","lastModifiedAt":"2025-10-08 13:48:12+0900","categories":["Review"],"tags":["Review","LLM","Scientific Problem Solving","AI Research","Iterative Refinement","Autonomous Agents","Generative AI","Evaluation Framework","Problem Extraction"],"permalink":"/ai/review/2025-10-8-AInstein_Assessing_the_Feasibility_of_AI-Generated_Approaches_to_Research_Problems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Watch_and_Learn_Learning_to_Use_Computers_from_Online_Videos","title":"[논문리뷰] Watch and Learn: Learning to Use Computers from Online Videos","excerpt":"Oriana Riva이 [arXiv]에 게시한 'Watch and Learn: Learning to Use Computers from Online Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Computer Use Agents","Inverse Dynamics Model","UI Trajectories","Web Videos","In-Context Learning","Supervised Fine-Tuning","Large Language Models","OSWorld Benchmark"],"permalink":"/ai/review/2025-10-7-Watch_and_Learn_Learning_to_Use_Computers_from_Online_Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Video-LMM_Post-Training_A_Deep_Dive_into_Video_Reasoning_with_Large_Multimodal_Models","title":"[논문리뷰] Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models","excerpt":"zeliang0426이 [arXiv]에 게시한 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Video Reasoning","Large Multimodal Models (LMMs)","Post-training","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)","Test-Time Scaling (TTS)","Chain-of-Thought (CoT)"],"permalink":"/ai/review/2025-10-7-Video-LMM_Post-Training_A_Deep_Dive_into_Video_Reasoning_with_Large_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-VChain_Chain-of-Visual-Thought_for_Reasoning_in_Video_Generation","title":"[논문리뷰] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation","excerpt":"Paul Debevec이 [arXiv]에 게시한 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Video Generation","Chain-of-Thought","Multimodal Models","Reasoning","Inference-Time Tuning","Sparse Supervision","Diffusion Models","Keyframe Generation"],"permalink":"/ai/review/2025-10-7-VChain_Chain-of-Visual-Thought_for_Reasoning_in_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Utility-Learning_Tension_in_Self-Modifying_Agents","title":"[논문리뷰] Utility-Learning Tension in Self-Modifying Agents","excerpt":"Peter Jin이 [arXiv]에 게시한 'Utility-Learning Tension in Self-Modifying Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Self-Modifying Agents","PAC Learnability","VC Dimension","Capacity Bounds","Metacognition","Architectural Search","Algorithmic Stability","Generalization Theory"],"permalink":"/ai/review/2025-10-7-Utility-Learning_Tension_in_Self-Modifying_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Thai_Semantic_End-of-Turn_Detection_for_Real-Time_Voice_Agents","title":"[논문리뷰] Thai Semantic End-of-Turn Detection for Real-Time Voice Agents","excerpt":"Monthol Charattrakool이 [arXiv]에 게시한 'Thai Semantic End-of-Turn Detection for Real-Time Voice Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","End-of-Turn Detection","Thai NLP","Voice Agents","Real-time Inference","Transformer Models","Few-shot Learning","Fine-tuning","Latency Optimization"],"permalink":"/ai/review/2025-10-7-Thai_Semantic_End-of-Turn_Detection_for_Real-Time_Voice_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-SwiReasoning_Switch-Thinking_in_Latent_and_Explicit_for_Pareto-Superior_Reasoning_LLMs","title":"[논문리뷰] SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs","excerpt":"이 [arXiv]에 게시한 'SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Latent Thinking","Explicit Thinking","Training-Free","Token Efficiency","Accuracy Improvement","Dynamic Switching","Entropy-based Control"],"permalink":"/ai/review/2025-10-7-SwiReasoning_Switch-Thinking_in_Latent_and_Explicit_for_Pareto-Superior_Reasoning_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Self-Reflective_Generation_at_Test_Time","title":"[논문리뷰] Self-Reflective Generation at Test Time","excerpt":"Shuang Qiu이 [arXiv]에 게시한 'Self-Reflective Generation at Test Time' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Self-Reflection","Test-Time Optimization","Uncertainty Monitoring","Proactive Error Prevention","Reasoning Tasks","Chain-of-Thought"],"permalink":"/ai/review/2025-10-7-Self-Reflective_Generation_at_Test_Time/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-SAEdit_Token-level_control_for_continuous_image_editing_via_Sparse_AutoEncoder","title":"[논문리뷰] SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder","excerpt":"Or Patashnik이 [arXiv]에 게시한 'SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Sparse Autoencoder (SAE)","Text-to-Image","Disentangled Control","Continuous Control","Token-level Manipulation","Text Embeddings"],"permalink":"/ai/review/2025-10-7-SAEdit_Token-level_control_for_continuous_image_editing_via_Sparse_AutoEncoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Reinforce-Ada_An_Adaptive_Sampling_Framework_for_Reinforce-Style_LLM_Training","title":"[논문리뷰] Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training","excerpt":"이 [arXiv]에 게시한 'Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Adaptive Sampling","Policy Gradient","Reward Optimization","Signal Collapse","Variance Reduction"],"permalink":"/ai/review/2025-10-7-Reinforce-Ada_An_Adaptive_Sampling_Framework_for_Reinforce-Style_LLM_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Reactive_Transformer_RxT_--_Stateful_Real-Time_Processing_for_Event-Driven_Reactive_Language_Models","title":"[논문리뷰] Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models","excerpt":"이 [arXiv]에 게시한 'Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Reactive Transformer","Stateful LLM","Event-Driven AI","Asynchronous Memory","Conversational AI","Linear Scaling","Short-Term Memory (STM)","Memory Attention"],"permalink":"/ai/review/2025-10-7-Reactive_Transformer_RxT_--_Stateful_Real-Time_Processing_for_Event-Driven_Reactive_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Optimal_Scaling_Needs_Optimal_Norm","title":"[논문리뷰] Optimal Scaling Needs Optimal Norm","excerpt":"Stefan Kesselheim이 [arXiv]에 게시한 'Optimal Scaling Needs Optimal Norm' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Optimal Scaling","Norm-Based Optimizers","Hyperparameter Transfer","Learning Rate Scaling","Batch Size Scaling","Transformer Models","Scion Optimizer","Large Language Models"],"permalink":"/ai/review/2025-10-7-Optimal_Scaling_Needs_Optimal_Norm/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-MoME_Mixture_of_Matryoshka_Experts_for_Audio-Visual_Speech_Recognition","title":"[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition","excerpt":"이 [arXiv]에 게시한 'MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Audio-Visual Speech Recognition","Mixture of Experts","Matryoshka Representation Learning","Large Language Models","Elastic Inference","Token Compression","Multimodal AI"],"permalink":"/ai/review/2025-10-7-MoME_Mixture_of_Matryoshka_Experts_for_Audio-Visual_Speech_Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-MITS_Enhanced_Tree_Search_Reasoning_for_LLMs_via_Pointwise_Mutual_Information","title":"[논문리뷰] MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information","excerpt":"이 [arXiv]에 게시한 'MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Tree Search","Pointwise Mutual Information (PMI)","Dynamic Sampling","Beam Search","Weighted Voting","Information Theory","Computational Efficiency"],"permalink":"/ai/review/2025-10-7-MITS_Enhanced_Tree_Search_Reasoning_for_LLMs_via_Pointwise_Mutual_Information/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Learning_on_the_Job_Test-Time_Curricula_for_Targeted_Reinforcement_Learning","title":"[논문리뷰] Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Test-Time Curriculum","Reinforcement Learning","Large Language Models","Self-Curated Learning","Continual Learning","Reasoning Benchmarks","Adaptive Training"],"permalink":"/ai/review/2025-10-7-Learning_on_the_Job_Test-Time_Curricula_for_Targeted_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-LLMSQL_Upgrading_WikiSQL_for_the_LLM_Era_of_Text-to-SQL","title":"[논문리뷰] LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL","excerpt":"이 [arXiv]에 게시한 'LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Text-to-SQL","WikiSQL","LLM","Dataset Curation","Natural Language Processing","Benchmark","SQL Generation","Data Cleaning"],"permalink":"/ai/review/2025-10-7-LLMSQL_Upgrading_WikiSQL_for_the_LLM_Era_of_Text-to-SQL/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Judging_with_Confidence_Calibrating_Autoraters_to_Preference_Distributions","title":"[논문리뷰] Judging with Confidence: Calibrating Autoraters to Preference Distributions","excerpt":"이 [arXiv]에 게시한 'Judging with Confidence: Calibrating Autoraters to Preference Distributions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Autoraters","Calibration","Preference Distributions","Reinforcement Learning","Supervised Fine-tuning","Positional Bias"],"permalink":"/ai/review/2025-10-7-Judging_with_Confidence_Calibrating_Autoraters_to_Preference_Distributions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Imperceptible_Jailbreaking_against_Large_Language_Models","title":"[논문리뷰] Imperceptible Jailbreaking against Large Language Models","excerpt":"이 [arXiv]에 게시한 'Imperceptible Jailbreaking against Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Jailbreaking","Imperceptible Attacks","Unicode Variation Selectors","Adversarial Suffixes","Safety Alignment","Prompt Injection"],"permalink":"/ai/review/2025-10-7-Imperceptible_Jailbreaking_against_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Hybrid_Architectures_for_Language_Models_Systematic_Analysis_and_Design_Insights","title":"[논문리뷰] Hybrid Architectures for Language Models: Systematic Analysis and Design Insights","excerpt":"이 [arXiv]에 게시한 'Hybrid Architectures for Language Models: Systematic Analysis and Design Insights' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Hybrid LLM","Transformer Architecture","Mamba","State Space Models (SSM)","Computational Efficiency","Long-Context","Language Model Architectures","Scaling Laws"],"permalink":"/ai/review/2025-10-7-Hybrid_Architectures_for_Language_Models_Systematic_Analysis_and_Design_Insights/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-HiKE_Hierarchical_Evaluation_Framework_for_Korean-English_Code-Switching_Speech_Recognition","title":"[논문리뷰] HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition","excerpt":"이 [arXiv]에 게시한 'HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Code-Switching","Speech Recognition","Korean-English ASR","Evaluation Framework","Multilingual ASR","Loanword Processing","Fine-tuning","Hierarchical Labeling"],"permalink":"/ai/review/2025-10-7-HiKE_Hierarchical_Evaluation_Framework_for_Korean-English_Code-Switching_Speech_Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Graph2Eval_Automatic_Multimodal_Task_Generation_for_Agents_via_Knowledge_Graphs","title":"[논문리뷰] Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs","excerpt":"Zeyi Liao이 [arXiv]에 게시한 'Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Agent Evaluation","Task Generation","Knowledge Graphs","Multimodal AI","Web Interaction","Document Comprehension","LLM-driven Agents"],"permalink":"/ai/review/2025-10-7-Graph2Eval_Automatic_Multimodal_Task_Generation_for_Agents_via_Knowledge_Graphs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Good_Intentions_Beyond_ACL_Who_Does_NLP_for_Social_Good_and_Where","title":"[논문리뷰] Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?","excerpt":"Denis Peskoff이 [arXiv]에 게시한 'Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","NLP for Social Good","ACL Community","Scientometrics","Venue Analysis","Author Classification","Sustainable Development Goals","Neural Methods","Research Landscape"],"permalink":"/ai/review/2025-10-7-Good_Intentions_Beyond_ACL_Who_Does_NLP_for_Social_Good_and_Where/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Front-Loading_Reasoning_The_Synergy_between_Pretraining_and_Post-Training_Data","title":"[논문리뷰] Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data","excerpt":"이 [arXiv]에 게시한 'Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Pretraining","Supervised Fine-tuning","Reasoning Data","Data Allocation","Diversity","Quality","Reinforcement Learning"],"permalink":"/ai/review/2025-10-7-Front-Loading_Reasoning_The_Synergy_between_Pretraining_and_Post-Training_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Factuality_Matters_When_Image_Generation_and_Editing_Meet_Structured_Visuals","title":"[논문리뷰] Factuality Matters: When Image Generation and Editing Meet Structured Visuals","excerpt":"Boxiang Qiu이 [arXiv]에 게시한 'Factuality Matters: When Image Generation and Editing Meet Structured Visuals' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Structured Visuals","Image Generation","Image Editing","Multimodal Reasoning","Factual Fidelity","Chain-of-Thought","Evaluation Benchmark","Diffusion Models"],"permalink":"/ai/review/2025-10-7-Factuality_Matters_When_Image_Generation_and_Editing_Meet_Structured_Visuals/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-EvolProver_Advancing_Automated_Theorem_Proving_by_Evolving_Formalized_Problems_via_Symmetry_and_Difficulty","title":"[논문리뷰] EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty","excerpt":"Xuanwu Wang이 [arXiv]에 게시한 'EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Data Augmentation","Large Language Models","Formal Mathematics","Symmetry","Difficulty Evolution","Abstract Syntax Tree","Generalizability"],"permalink":"/ai/review/2025-10-7-EvolProver_Advancing_Automated_Theorem_Proving_by_Evolving_Formalized_Problems_via_Symmetry_and_Difficulty/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Epistemic_Diversity_and_Knowledge_Collapse_in_Large_Language_Models","title":"[논문리뷰] Epistemic Diversity and Knowledge Collapse in Large Language Models","excerpt":"이 [arXiv]에 게시한 'Epistemic Diversity and Knowledge Collapse in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Large Language Models","Epistemic Diversity","Knowledge Collapse","Homogenization","Retrieval-Augmented Generation","LLM Evaluation","Information Diversity","Cultural Bias"],"permalink":"/ai/review/2025-10-7-Epistemic_Diversity_and_Knowledge_Collapse_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Code4MeV2_a_Research-oriented_Code-completion_Platform","title":"[논문리뷰] Code4MeV2: a Research-oriented Code-completion Platform","excerpt":"이 [arXiv]에 게시한 'Code4MeV2: a Research-oriented Code-completion Platform' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Code Completion","Research Platform","Human-AI Interaction","Software Engineering","Open Science","JetBrains IDE Plugin","Telemetry","AI4SE"],"permalink":"/ai/review/2025-10-7-Code4MeV2_a_Research-oriented_Code-completion_Platform/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-ChronoEdit_Towards_Temporal_Reasoning_for_Image_Editing_and_World_Simulation","title":"[논문리뷰] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation","excerpt":"이 [arXiv]에 게시한 'ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Image Editing","Video Generation","Temporal Reasoning","World Simulation","Physical Consistency","Diffusion Models","Generative Models"],"permalink":"/ai/review/2025-10-7-ChronoEdit_Towards_Temporal_Reasoning_for_Image_Editing_and_World_Simulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Character_Mixing_for_Video_Generation","title":"[논문리뷰] Character Mixing for Video Generation","excerpt":"이 [arXiv]에 게시한 'Character Mixing for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Video Generation","Character Mixing","Style Preservation","Multi-character Interaction","Text-to-Video","Cross-Domain Synthesis","Identity Preservation"],"permalink":"/ai/review/2025-10-7-Character_Mixing_for_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Alignment_Tipping_Process_How_Self-Evolution_Pushes_LLM_Agents_Off_the_Rails","title":"[논문리뷰] Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails","excerpt":"Xinyuan Liu이 [arXiv]에 게시한 'Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Agents","Alignment","Self-Evolution","Behavioral Drift","Reinforcement Learning","Multi-Agent Systems","Alignment Tipping Process"],"permalink":"/ai/review/2025-10-7-Alignment_Tipping_Process_How_Self-Evolution_Pushes_LLM_Agents_Off_the_Rails/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-Agentic_Context_Engineering_Evolving_Contexts_for_Self-Improving_Language_Models","title":"[논문리뷰] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models","excerpt":"Fenglu Hong이 [arXiv]에 게시한 'Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","LLM Context Adaptation","Agentic AI","Self-Improving Systems","Prompt Engineering","Context Management","Dynamic Playbooks","Incremental Learning"],"permalink":"/ai/review/2025-10-7-Agentic_Context_Engineering_Evolving_Contexts_for_Self-Improving_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-7-AdvEvo-MARL_Shaping_Internalized_Safety_through_Adversarial_Co-Evolution_in_Multi-Agent_Reinforcement_Learning","title":"[논문리뷰] AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning","excerpt":"Zeliang Zhang이 [arXiv]에 게시한 'AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","lastModifiedAt":"2025-10-07 13:36:57+0900","categories":["Review"],"tags":["Review","Multi-Agent Reinforcement Learning","Adversarial Co-evolution","LLM Safety","Jailbreak Attacks","Internalized Safety","Public Baseline","System Robustness"],"permalink":"/ai/review/2025-10-7-AdvEvo-MARL_Shaping_Internalized_Safety_through_Adversarial_Co-Evolution_in_Multi-Agent_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Your_Agent_May_Misevolve_Emergent_Risks_in_Self-evolving_LLM_Agents","title":"[논문리뷰] Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents","excerpt":"Boyi Wei이 [arXiv]에 게시한 'Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Self-evolving Agents","LLM Safety","Misevolution","Emergent Risks","Model Evolution","Memory Evolution","Tool Evolution","Workflow Evolution"],"permalink":"/ai/review/2025-10-6-Your_Agent_May_Misevolve_Emergent_Risks_in_Self-evolving_LLM_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-WAInjectBench_Benchmarking_Prompt_Injection_Detections_for_Web_Agents","title":"[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents","excerpt":"Neil Zhenqiang Gong이 [arXiv]에 게시한 'WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Prompt Injection","Web Agents","Multimodal AI","Adversarial Attacks","Detection Benchmarking","Large Language Models","Image-based Detection","Text-based Detection"],"permalink":"/ai/review/2025-10-6-WAInjectBench_Benchmarking_Prompt_Injection_Detections_for_Web_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Triangle_Splatting_Differentiable_Rendering_with_Opaque_Triangles","title":"[논문리뷰] Triangle Splatting+: Differentiable Rendering with Opaque Triangles","excerpt":"Matheus Gadelha이 [arXiv]에 게시한 'Triangle Splatting+: Differentiable Rendering with Opaque Triangles' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Differentiable Rendering","3D Reconstruction","Novel View Synthesis","Triangles","Opaque Primitives","Game Engines","Gaussian Splatting","Mesh-based Rendering"],"permalink":"/ai/review/2025-10-6-Triangle_Splatting_Differentiable_Rendering_with_Opaque_Triangles/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-TalkPlay-Tools_Conversational_Music_Recommendation_with_LLM_Tool_Calling","title":"[논문리뷰] TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling","excerpt":"Juhan Nam이 [arXiv]에 게시한 'TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Conversational Recommendation","LLM Tool Calling","Music Recommendation","Multimodal Retrieval","Information Retrieval","Retrieval-Reranking","Semantic IDs"],"permalink":"/ai/review/2025-10-6-TalkPlay-Tools_Conversational_Music_Recommendation_with_LLM_Tool_Calling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-SurveyBench_How_Well_Can_LLM-Agents_Write_Academic_Surveys","title":"[논문리뷰] SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?","excerpt":"Shuo Wang이 [arXiv]에 게시한 'SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","LLM","LLM Agents","Academic Survey Generation","Evaluation Framework","Benchmark","Quiz-driven Evaluation","Content Quality Metrics"],"permalink":"/ai/review/2025-10-6-SurveyBench_How_Well_Can_LLM-Agents_Write_Academic_Surveys/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-SpineBench_A_Clinically_Salient_Level-Aware_Benchmark_Powered_by_the_SpineMed-450k_Corpus","title":"[논문리뷰] SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus","excerpt":"Zhonghao Zhang이 [arXiv]에 게시한 'SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Medical AI","Spine Diagnosis","Multimodal LLM","Benchmark","Dataset","Clinical Reasoning","Spine Surgery","Vision-Language Model"],"permalink":"/ai/review/2025-10-6-SpineBench_A_Clinically_Salient_Level-Aware_Benchmark_Powered_by_the_SpineMed-450k_Corpus/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Self-Improvement_in_Multimodal_Large_Language_Models_A_Survey","title":"[논문리뷰] Self-Improvement in Multimodal Large Language Models: A Survey","excerpt":"Yapeng Tian이 [arXiv]에 게시한 'Self-Improvement in Multimodal Large Language Models: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Self-Improvement","Data Collection","Data Organization","Model Optimization","Survey","Reinforcement Learning","Direct Preference Optimization"],"permalink":"/ai/review/2025-10-6-Self-Improvement_in_Multimodal_Large_Language_Models_A_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Scaling_Policy_Compliance_Assessment_in_Language_Models_with_Policy_Reasoning_Traces","title":"[논문리뷰] Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces","excerpt":"이 [arXiv]에 게시한 'Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Policy Compliance","Large Language Models (LLMs)","Reasoning Traces","In-Context Learning (ICL)","Supervised Finetuning (SFT)","HIPAA","GDPR","ModelSpec"],"permalink":"/ai/review/2025-10-6-Scaling_Policy_Compliance_Assessment_in_Language_Models_with_Policy_Reasoning_Traces/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-REPAIR_Robust_Editing_via_Progressive_Adaptive_Intervention_and_Reintegration","title":"[논문리뷰] REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration","excerpt":"이 [arXiv]에 게시한 'REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Model Editing","Lifelong Learning","LLMs","Continual Learning","Knowledge Distillation","Error Feedback","Memory Management","Parameter Merging"],"permalink":"/ai/review/2025-10-6-REPAIR_Robust_Editing_via_Progressive_Adaptive_Intervention_and_Reintegration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-OrtSAE_Orthogonal_Sparse_Autoencoders_Uncover_Atomic_Features","title":"[논문리뷰] OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features","excerpt":"Elena Tutubalina이 [arXiv]에 게시한 'OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Sparse Autoencoders","Mechanistic Interpretability","Feature Disentanglement","Orthogonality","LLM Features","Feature Absorption","Feature Composition"],"permalink":"/ai/review/2025-10-6-OrtSAE_Orthogonal_Sparse_Autoencoders_Uncover_Atomic_Features/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-NuRisk_A_Visual_Question_Answering_Dataset_for_Agent-Level_Risk_Assessment_in_Autonomous_Driving","title":"[논문리뷰] NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving","excerpt":"이 [arXiv]에 게시한 'NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Visual Question Answering (VQA)","Autonomous Driving","Risk Assessment","Spatio-Temporal Reasoning","Large Vision Models (VLMs)","Dataset","Bird-Eye-View (BEV)","Fine-tuning"],"permalink":"/ai/review/2025-10-6-NuRisk_A_Visual_Question_Answering_Dataset_for_Agent-Level_Risk_Assessment_in_Autonomous_Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-LSPO_Length-aware_Dynamic_Sampling_for_Policy_Optimization_in_LLM_Reasoning","title":"[논문리뷰] LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning","excerpt":"이 [arXiv]에 게시한 'LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","LLM Reasoning","RLVR","Dynamic Sampling","Policy Optimization","Response Length","Meta-RL","Overthinking"],"permalink":"/ai/review/2025-10-6-LSPO_Length-aware_Dynamic_Sampling_for_Policy_Optimization_in_LLM_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-LEAML_Label-Efficient_Adaptation_to_Out-of-Distribution_Visual_Tasks_for_Multimodal_Large_Language_Models","title":"[논문리뷰] LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models","excerpt":"Yu-Chiang Frank Wang이 [arXiv]에 게시한 'LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multimodal LLM","OOD Adaptation","Label Efficiency","VQA","Semi-Supervised Learning","Neuron Distillation","Pseudo Labeling","Medical Imaging"],"permalink":"/ai/review/2025-10-6-LEAML_Label-Efficient_Adaptation_to_Out-of-Distribution_Visual_Tasks_for_Multimodal_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Improving_GUI_Grounding_with_Explicit_Position-to-Coordinate_Mapping","title":"[논문리뷰] Improving GUI Grounding with Explicit Position-to-Coordinate Mapping","excerpt":"Spandana Gella이 [arXiv]에 게시한 'Improving GUI Grounding with Explicit Position-to-Coordinate Mapping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","GUI Grounding","Vision-Language Models","Positional Embedding","UI Automation","Coordinate Prediction","Resolution Generalization","Transformer Architecture"],"permalink":"/ai/review/2025-10-6-Improving_GUI_Grounding_with_Explicit_Position-to-Coordinate_Mapping/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-How_Confident_are_Video_Models_Empowering_Video_Models_to_Express_their_Uncertainty","title":"[논문리뷰] How Confident are Video Models? Empowering Video Models to Express their Uncertainty","excerpt":"Anirudha Majumdar이 [arXiv]에 게시한 'How Confident are Video Models? Empowering Video Models to Express their Uncertainty' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Video Generation","Uncertainty Quantification","Aleatoric Uncertainty","Epistemic Uncertainty","Model Calibration","Text-to-Video","Generative AI","VMF Distribution"],"permalink":"/ai/review/2025-10-6-How_Confident_are_Video_Models_Empowering_Video_Models_to_Express_their_Uncertainty/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Free_Lunch_Alignment_of_Text-to-Image_Diffusion_Models_without_Preference_Image_Pairs","title":"[논문리뷰] Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs","excerpt":"이 [arXiv]에 게시한 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Text-to-Image Models","Diffusion Models","Preference Optimization","LLMs","RLHF","Prompt Editing","Free Lunch Alignment","TDPO","TKTO"],"permalink":"/ai/review/2025-10-6-Free_Lunch_Alignment_of_Text-to-Image_Diffusion_Models_without_Preference_Image_Pairs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-FocusAgent_Simple_Yet_Effective_Ways_of_Trimming_the_Large_Context_of_Web_Agents","title":"[논문리뷰] FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents","excerpt":"Léo Boisvert이 [arXiv]에 게시한 'FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Web Agents","LLM Context Pruning","Accessibility Tree","Prompt Injection","Retrieval Augmented Generation","Web Navigation","Agent Security","Efficient LLM"],"permalink":"/ai/review/2025-10-6-FocusAgent_Simple_Yet_Effective_Ways_of_Trimming_the_Large_Context_of_Web_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Efficient_Multi-modal_Large_Language_Models_via_Progressive_Consistency_Distillation","title":"[논문리뷰] Efficient Multi-modal Large Language Models via Progressive Consistency Distillation","excerpt":"이 [arXiv]에 게시한 'Efficient Multi-modal Large Language Models via Progressive Consistency Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multi-modal LLMs","Token Compression","Efficiency","Knowledge Distillation","Progressive Learning","Consistency Distillation","MLLM Training"],"permalink":"/ai/review/2025-10-6-Efficient_Multi-modal_Large_Language_Models_via_Progressive_Consistency_Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-DiffTester_Accelerating_Unit_Test_Generation_for_Diffusion_LLMs_via_Repetitive_Pattern","title":"[논문리뷰] DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern","excerpt":"Jia Li이 [arXiv]에 게시한 'DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Unit Test Generation","Acceleration","Repetitive Patterns","Abstract Syntax Tree","Software Testing","Code Generation"],"permalink":"/ai/review/2025-10-6-DiffTester_Accelerating_Unit_Test_Generation_for_Diffusion_LLMs_via_Repetitive_Pattern/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Compose_Your_Policies_Improving_Diffusion-based_or_Flow-based_Robot_Policies_via_Test-time_Distribution-level_Composition","title":"[논문리뷰] Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition","excerpt":"이 [arXiv]에 게시한 'Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Diffusion Models","Flow-based Models","Robotics Control","Policy Composition","Test-time Optimization","Score-based Models","Training-free"],"permalink":"/ai/review/2025-10-6-Compose_Your_Policies_Improving_Diffusion-based_or_Flow-based_Robot_Policies_via_Test-time_Distribution-level_Composition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-CoDA_Agentic_Systems_for_Collaborative_Data_Visualization","title":"[논문리뷰] CoDA: Agentic Systems for Collaborative Data Visualization","excerpt":"이 [arXiv]에 게시한 'CoDA: Agentic Systems for Collaborative Data Visualization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multi-agent Systems","Data Visualization","LLM","Automation","Self-reflection","Code Generation","Natural Language to Visualization"],"permalink":"/ai/review/2025-10-6-CoDA_Agentic_Systems_for_Collaborative_Data_Visualization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Apriel-1.5-15b-Thinker","title":"[논문리뷰] Apriel-1.5-15b-Thinker","excerpt":"이 [arXiv]에 게시한 'Apriel-1.5-15b-Thinker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning Model","Open-Weights Model","Continual Pretraining (CPT)","Supervised Fine-Tuning (SFT)","Training Design","Efficiency","Frontier Performance"],"permalink":"/ai/review/2025-10-6-Apriel-1.5-15b-Thinker/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-Align_Your_Tangent_Training_Better_Consistency_Models_via_Manifold-Aligned_Tangents","title":"[논문리뷰] Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents","excerpt":"Jong Chul Ye이 [arXiv]에 게시한 'Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Consistency Models","Generative Models","Manifold Learning","Tangent Alignment","Diffusion Models","Training Dynamics","Manifold Feature Distance"],"permalink":"/ai/review/2025-10-6-Align_Your_Tangent_Training_Better_Consistency_Models_via_Manifold-Aligned_Tangents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-6-A_Practitioners_Guide_to_Multi-turn_Agentic_Reinforcement_Learning","title":"[논문리뷰] A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","lastModifiedAt":"2025-10-06 13:29:11+0900","categories":["Review"],"tags":["Review","Multi-turn Reinforcement Learning","LLM Agents","Text-based Environments","Reward Shaping","Policy Optimization","Supervised Fine-tuning (SFT)","Generalization","Environment Complexity"],"permalink":"/ai/review/2025-10-6-A_Practitioners_Guide_to_Multi-turn_Agentic_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Why_Cant_Transformers_Learn_Multiplication_Reverse-Engineering_Reveals_Long-Range_Dependency_Pitfalls","title":"[논문리뷰] Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls","excerpt":"Stuart Shieber이 [arXiv]에 게시한 'Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Transformers","Multiplication","Long-Range Dependencies","Implicit Chain-of-Thought","Attention Mechanisms","Inductive Bias","Reverse Engineering"],"permalink":"/ai/review/2025-10-2-Why_Cant_Transformers_Learn_Multiplication_Reverse-Engineering_Reveals_Long-Range_Dependency_Pitfalls/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-VLM-FO1_Bridging_the_Gap_Between_High-Level_Reasoning_and_Fine-Grained_Perception_in_VLMs","title":"[논문리뷰] VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs","excerpt":"이 [arXiv]에 게시한 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Object Grounding","Fine-grained Perception","Hybrid Region Encoder","Plug-and-play","Two-stage Training","Visual Reasoning"],"permalink":"/ai/review/2025-10-2-VLM-FO1_Bridging_the_Gap_Between_High-Level_Reasoning_and_Fine-Grained_Perception_in_VLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-VLA-RFT_Vision-Language-Action_Reinforcement_Fine-tuning_with_Verified_Rewards_in_World_Simulators","title":"[논문리뷰] VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators","excerpt":"Zirui Ge이 [arXiv]에 게시한 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Reinforcement Learning","World Models","Fine-tuning","Embodied AI","Robotics","Reward Design","Distribution Shift"],"permalink":"/ai/review/2025-10-2-VLA-RFT_Vision-Language-Action_Reinforcement_Fine-tuning_with_Verified_Rewards_in_World_Simulators/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Training_Vision-Language_Process_Reward_Models_for_Test-Time_Scaling_in_Multimodal_Reasoning_Key_Insights_and_Lessons_Learned","title":"[논문리뷰] Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned","excerpt":"이 [arXiv]에 게시한 'Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Process Reward Models (PRMs)","Multimodal Reasoning","Test-Time Scaling (TTS)","Process Supervision","Dataset Construction","Perception Errors","MCTS"],"permalink":"/ai/review/2025-10-2-Training_Vision-Language_Process_Reward_Models_for_Test-Time_Scaling_in_Multimodal_Reasoning_Key_Insights_and_Lessons_Learned/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-ReSWD_ReSTIRd_not_shaken._Combining_Reservoir_Sampling_and_Sliced_Wasserstein_Distance_for_Variance_Reduction","title":"[논문리뷰] ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction","excerpt":"이 [arXiv]에 게시한 'ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Sliced Wasserstein Distance","Reservoir Sampling","Variance Reduction","Distribution Matching","Diffusion Guidance","Color Correction","Monte Carlo Estimation"],"permalink":"/ai/review/2025-10-2-ReSWD_ReSTIRd_not_shaken._Combining_Reservoir_Sampling_and_Sliced_Wasserstein_Distance_for_Variance_Reduction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-PIPer_On-Device_Environment_Setup_via_Online_Reinforcement_Learning","title":"[논문리뷰] PIPer: On-Device Environment Setup via Online Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'PIPer: On-Device Environment Setup via Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Environment Setup","LLMs","Reinforcement Learning","Supervised Fine-tuning","On-device AI","Software Engineering","Verifiable Rewards"],"permalink":"/ai/review/2025-10-2-PIPer_On-Device_Environment_Setup_via_Online_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-On_Predictability_of_Reinforcement_Learning_Dynamics_for_Large_Language_Models","title":"[논문리뷰] On Predictability of Reinforcement Learning Dynamics for Large Language Models","excerpt":"Yuqing Huang이 [arXiv]에 게시한 'On Predictability of Reinforcement Learning Dynamics for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Parameter Dynamics","Rank-1 Dominance","Linear Dynamics","SVD","Model Acceleration","Predictability"],"permalink":"/ai/review/2025-10-2-On_Predictability_of_Reinforcement_Learning_Dynamics_for_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Making_not_Taking_the_Best_of_N","title":"[논문리뷰] Making, not Taking, the Best of N","excerpt":"이 [arXiv]에 게시한 'Making, not Taking, the Best of N' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Aggregation","Generative Fusion","Best-of-N","Synthetic Data Generation","Test-Time Scaling","Multilingual Models","Ensemble Learning"],"permalink":"/ai/review/2025-10-2-Making_not_Taking_the_Best_of_N/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Knapsack_RL_Unlocking_Exploration_of_LLMs_via_Optimizing_Budget_Allocation","title":"[논문리뷰] Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation","excerpt":"이 [arXiv]에 게시한 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Reinforcement Learning (RL)","Exploration Budget Allocation","Knapsack Problem","Group Relative Policy Optimization (GRPO)","Mathematical Reasoning","Resource Optimization"],"permalink":"/ai/review/2025-10-2-Knapsack_RL_Unlocking_Exploration_of_LLMs_via_Optimizing_Budget_Allocation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-JoyAgent-JDGenie_Technical_Report_on_the_GAIA","title":"[논문리뷰] JoyAgent-JDGenie: Technical Report on the GAIA","excerpt":"이 [arXiv]에 게시한 'JoyAgent-JDGenie: Technical Report on the GAIA' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Generalist Agent","Multi-Agent System","Plan-Execute","ReAct","Hierarchical Memory","Tool Integration","GAIA Benchmark","LLM Agent"],"permalink":"/ai/review/2025-10-2-JoyAgent-JDGenie_Technical_Report_on_the_GAIA/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Infusing_Theory_of_Mind_into_Socially_Intelligent_LLM_Agents","title":"[논문리뷰] Infusing Theory of Mind into Socially Intelligent LLM Agents","excerpt":"이 [arXiv]에 게시한 'Infusing Theory of Mind into Socially Intelligent LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Theory of Mind","Large Language Models","Social Agents","Dialogue Systems","Mental State Modeling","Look-ahead Planning","Supervised Fine-tuning","Sotopia Benchmark"],"permalink":"/ai/review/2025-10-2-Infusing_Theory_of_Mind_into_Socially_Intelligent_LLM_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-In-Place_Feedback_A_New_Paradigm_for_Guiding_LLMs_in_Multi-Turn_Reasoning","title":"[논문리뷰] In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning","excerpt":"Chaehyeon Chung이 [arXiv]에 게시한 'In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Feedback","Multi-turn Reasoning","In-place Editing","Token Efficiency","Error Correction","Human-AI Interaction","Reasoning Tasks"],"permalink":"/ai/review/2025-10-2-In-Place_Feedback_A_New_Paradigm_for_Guiding_LLMs_in_Multi-Turn_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Hyperdimensional_Probe_Decoding_LLM_Representations_via_Vector_Symbolic_Architectures","title":"[논문리뷰] Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures","excerpt":"Andrea Passerini이 [arXiv]에 게시한 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Interpretability","Vector Symbolic Architectures","Neural Probing","Information Decoding","Hyperdimensional Computing","Latent Representations"],"permalink":"/ai/review/2025-10-2-Hyperdimensional_Probe_Decoding_LLM_Representations_via_Vector_Symbolic_Architectures/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-GUI-KV_Efficient_GUI_Agents_via_KV_Cache_with_Spatio-Temporal_Awareness","title":"[논문리뷰] GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness","excerpt":"Chien-Sheng Wu이 [arXiv]에 게시한 'GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","GUI Agents","KV Cache Compression","Spatio-Temporal Awareness","Vision-Language Models","Efficiency","Attention Sparsity","QR Decomposition"],"permalink":"/ai/review/2025-10-2-GUI-KV_Efficient_GUI_Agents_via_KV_Cache_with_Spatio-Temporal_Awareness/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-GEM_A_Gym_for_Agentic_LLMs","title":"[논문리뷰] GEM: A Gym for Agentic LLMs","excerpt":"이 [arXiv]에 게시한 'GEM: A Gym for Agentic LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Agentic LLMs","Reinforcement Learning","Environment Simulator","Multi-turn Interactions","Return Batch Normalization","Tool Integration","Benchmarking"],"permalink":"/ai/review/2025-10-2-GEM_A_Gym_for_Agentic_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Flash-Searcher_Fast_and_Effective_Web_Agents_via_DAG-Based_Parallel_Execution","title":"[논문리뷰] Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution","excerpt":"이 [arXiv]에 게시한 'Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Agents","Parallel Execution","DAG-based Planning","Tool Orchestration","Web Agents","Reasoning Framework","Efficiency"],"permalink":"/ai/review/2025-10-2-Flash-Searcher_Fast_and_Effective_Web_Agents_via_DAG-Based_Parallel_Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Eliciting_Secret_Knowledge_from_Language_Models","title":"[논문리뷰] Eliciting Secret Knowledge from Language Models","excerpt":"Neel Nanda이 [arXiv]에 게시한 'Eliciting Secret Knowledge from Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Language Models","Secret Elicitation","Mechanistic Interpretability","Black-box Methods","White-box Methods","AI Auditing","Model Organisms","Prefill Attacks"],"permalink":"/ai/review/2025-10-2-Eliciting_Secret_Knowledge_from_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-DeepSearch_Overcome_the_Bottleneck_of_Reinforcement_Learning_with_Verifiable_Rewards_via_Monte_Carlo_Tree_Search","title":"[논문리뷰] DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search","excerpt":"이 [arXiv]에 게시한 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Monte Carlo Tree Search (MCTS)","Mathematical Reasoning","Large Language Models (LLMs)","Systematic Exploration","Adaptive Training","Tree-GRPO"],"permalink":"/ai/review/2025-10-2-DeepSearch_Overcome_the_Bottleneck_of_Reinforcement_Learning_with_Verifiable_Rewards_via_Monte_Carlo_Tree_Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-CurES_From_Gradient_Analysis_to_Efficient_Curriculum_Learning_for_Reasoning_LLMs","title":"[논문리뷰] CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs","excerpt":"Hengyi Cai이 [arXiv]에 게시한 'CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Curriculum Learning","LLMs","Reasoning","Gradient Optimization","Reinforcement Learning","Bayesian Inference","Sample Efficiency"],"permalink":"/ai/review/2025-10-2-CurES_From_Gradient_Analysis_to_Efficient_Curriculum_Learning_for_Reasoning_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Code2Video_A_Code-centric_Paradigm_for_Educational_Video_Generation","title":"[논문리뷰] Code2Video: A Code-centric Paradigm for Educational Video Generation","excerpt":"이 [arXiv]에 게시한 'Code2Video: A Code-centric Paradigm for Educational Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Educational Video Generation","Code-centric AI","Multi-agent Framework","Manim","Vision-Language Models","Knowledge Transfer","Code Generation","MMMC Benchmark"],"permalink":"/ai/review/2025-10-2-Code2Video_A_Code-centric_Paradigm_for_Educational_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-BroRL_Scaling_Reinforcement_Learning_via_Broadened_Exploration","title":"[논문리뷰] BroRL: Scaling Reinforcement Learning via Broadened Exploration","excerpt":"이 [arXiv]에 게시한 'BroRL: Scaling Reinforcement Learning via Broadened Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Scaling Laws","Exploration","Rollout Size","Verifiable Rewards","PPO","Mass Balance Equation"],"permalink":"/ai/review/2025-10-2-BroRL_Scaling_Reinforcement_Learning_via_Broadened_Exploration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Boolean_Satisfiability_via_Imitation_Learning","title":"[논문리뷰] Boolean Satisfiability via Imitation Learning","excerpt":"Xiangyu Xu이 [arXiv]에 게시한 'Boolean Satisfiability via Imitation Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Boolean Satisfiability","Imitation Learning","CDCL Solvers","Branching Policy","KeyTrace","Transformer Architecture","Perceiver AR"],"permalink":"/ai/review/2025-10-2-Boolean_Satisfiability_via_Imitation_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-BindWeave_Subject-Consistent_Video_Generation_via_Cross-Modal_Integration","title":"[논문리뷰] BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration","excerpt":"Xiangyang Xia이 [arXiv]에 게시한 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Video Generation","Subject Consistency","Cross-Modal Integration","Diffusion Models","Multimodal LLM","Diffusion Transformer","Text-to-Video"],"permalink":"/ai/review/2025-10-2-BindWeave_Subject-Consistent_Video_Generation_via_Cross-Modal_Integration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-BiasFreeBench_a_Benchmark_for_Mitigating_Bias_in_Large_Language_Model_Responses","title":"[논문리뷰] BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses","excerpt":"Julian McAuley이 [arXiv]에 게시한 'BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Bias Mitigation","Benchmark","Evaluation Metrics","Prompt Engineering","Fine-tuning","Bias-Free Score","Fairness"],"permalink":"/ai/review/2025-10-2-BiasFreeBench_a_Benchmark_for_Mitigating_Bias_in_Large_Language_Model_Responses/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-Beyond_Log_Likelihood_Probability-Based_Objectives_for_Supervised_Fine-Tuning_across_the_Model_Capability_Continuum","title":"[논문리뷰] Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum","excerpt":"Hanghang Tong이 [arXiv]에 게시한 'Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","Supervised Fine-tuning (SFT)","Large Language Models (LLMs)","Training Objectives","Negative Log Likelihood (NLL)","Model Capability Continuum","Generalization","Probability-based Loss Functions"],"permalink":"/ai/review/2025-10-2-Beyond_Log_Likelihood_Probability-Based_Objectives_for_Supervised_Fine-Tuning_across_the_Model_Capability_Continuum/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-An_Empirical_Study_of_Testing_Practices_in_Open_Source_AI_Agent_Frameworks_and_Agentic_Applications","title":"[논문리뷰] An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications","excerpt":"Bram Adams이 [arXiv]에 게시한 'An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","AI Agent","LLM Agent","Testing","Empirical Study","Software Quality","Agent Frameworks","Agentic Applications","Non-Determinism"],"permalink":"/ai/review/2025-10-2-An_Empirical_Study_of_Testing_Practices_in_Open_Source_AI_Agent_Frameworks_and_Agentic_Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-2-ACON_Optimizing_Context_Compression_for_Long-horizon_LLM_Agents","title":"[논문리뷰] ACON: Optimizing Context Compression for Long-horizon LLM Agents","excerpt":"이 [arXiv]에 게시한 'ACON: Optimizing Context Compression for Long-horizon LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","lastModifiedAt":"2025-10-02 13:30:22+0900","categories":["Review"],"tags":["Review","LLM Agents","Context Compression","Long-horizon Tasks","Prompt Optimization","Knowledge Distillation","Memory Efficiency","Task Performance","Failure Analysis"],"permalink":"/ai/review/2025-10-2-ACON_Optimizing_Context_Compression_for_Long-horizon_LLM_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-jina-reranker-v3_Last_but_Not_Late_Interaction_for_Document_Reranking","title":"[논문리뷰] jina-reranker-v3: Last but Not Late Interaction for Document Reranking","excerpt":"이 [arXiv]에 게시한 'jina-reranker-v3: Last but Not Late Interaction for Document Reranking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Document Reranking","Last but Not Late Interaction","Multilingual","Transformer Architecture","Cross-Encoder","InfoNCE Loss","Contextual Embedding","Qwen3"],"permalink":"/ai/review/2025-10-1-jina-reranker-v3_Last_but_Not_Late_Interaction_for_Document_Reranking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-dParallel_Learnable_Parallel_Decoding_for_dLLMs","title":"[논문리뷰] dParallel: Learnable Parallel Decoding for dLLMs","excerpt":"이 [arXiv]에 게시한 'dParallel: Learnable Parallel Decoding for dLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Parallel Decoding","Inference Acceleration","Certainty Distillation","Self-Distillation","Masked Language Models","LLaDA"],"permalink":"/ai/review/2025-10-1-dParallel_Learnable_Parallel_Decoding_for_dLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-d2Cache_Accelerating_Diffusion-Based_LLMs_via_Dual_Adaptive_Caching","title":"[논문리뷰] d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching","excerpt":"Jiarui Wang이 [arXiv]에 게시한 'd^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Diffusion Models","Large Language Models (LLMs)","Inference Acceleration","KV Cache","Bidirectional Attention","Adaptive Caching","Token Selection"],"permalink":"/ai/review/2025-10-1-d2Cache_Accelerating_Diffusion-Based_LLMs_via_Dual_Adaptive_Caching/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Winning_the_Pruning_Gamble_A_Unified_Approach_to_Joint_Sample_and_Token_Pruning_for_Efficient_Supervised_Fine-Tuning","title":"[논문리뷰] Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning","excerpt":"Yue Min이 [arXiv]에 게시한 'Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM SFT","Data Pruning","Sample Pruning","Token Pruning","Error-Uncertainty Plane","Q-Tuning","Data Efficiency","Dynamic Pruning"],"permalink":"/ai/review/2025-10-1-Winning_the_Pruning_Gamble_A_Unified_Approach_to_Joint_Sample_and_Token_Pruning_for_Efficient_Supervised_Fine-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Whos_Your_Judge_On_the_Detectability_of_LLM-Generated_Judgments","title":"[논문리뷰] Who's Your Judge? On the Detectability of LLM-Generated Judgments","excerpt":"이 [arXiv]에 게시한 'Who's Your Judge? On the Detectability of LLM-Generated Judgments' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM-as-a-judge","Judgment Detection","Bias Quantification","Feature Engineering","Interpretability","Peer Review","AI Ethics","Evaluation"],"permalink":"/ai/review/2025-10-1-Whos_Your_Judge_On_the_Detectability_of_LLM-Generated_Judgments/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Who_invented_deep_residual_learning","title":"[논문리뷰] Who invented deep residual learning?","excerpt":"Juergen Schmidhuber이 [arXiv]에 게시한 'Who invented deep residual learning?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Deep Learning History","Residual Connections","Recurrent Neural Networks (RNN)","Long Short-Term Memory (LSTM)","Feedforward Neural Networks (FNN)","Highway Networks","ResNet","Vanishing Gradient"],"permalink":"/ai/review/2025-10-1-Who_invented_deep_residual_learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Voice_Evaluation_of_Reasoning_Ability_Diagnosing_the_Modality-Induced_Performance_Gap","title":"[논문리뷰] Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap","excerpt":"Hengfan Zhang이 [arXiv]에 게시한 'Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Voice AI","LLM","Reasoning","Benchmark","Modality Gap","Latency","Speech Recognition","Generative AI","Real-time Systems","Conversational AI"],"permalink":"/ai/review/2025-10-1-Voice_Evaluation_of_Reasoning_Ability_Diagnosing_the_Modality-Induced_Performance_Gap/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-VitaBench_Benchmarking_LLM_Agents_with_Versatile_Interactive_Tasks_in_Real-world_Applications","title":"[논문리뷰] VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications","excerpt":"이 [arXiv]에 게시한 'VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Benchmarking","Interactive Tasks","Real-world Applications","Tool Use","Multi-turn Conversation","Task Complexity"],"permalink":"/ai/review/2025-10-1-VitaBench_Benchmarking_LLM_Agents_with_Versatile_Interactive_Tasks_in_Real-world_Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-VisualOverload_Probing_Visual_Understanding_of_VLMs_in_Really_Dense_Scenes","title":"[논문리뷰] VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes","excerpt":"Muhammad Huzaifa이 [arXiv]에 게시한 'VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Visual Question Answering","Multimodal Models","Dense Scenes","Fine-Grained Perception","Benchmark","Error Analysis","Counting","OCR"],"permalink":"/ai/review/2025-10-1-VisualOverload_Probing_Visual_Understanding_of_VLMs_in_Really_Dense_Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Vision-Zero_Scalable_VLM_Self-Improvement_via_Strategic_Gamified_Self-Play","title":"[논문리뷰] Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play","excerpt":"Jing Shi이 [arXiv]에 게시한 'Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Self-Play","Reinforcement Learning","Gamification","Data Efficiency","Strategic Reasoning","Multimodal AI","Self-Improvement"],"permalink":"/ai/review/2025-10-1-Vision-Zero_Scalable_VLM_Self-Improvement_via_Strategic_Gamified_Self-Play/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-TruthRL_Incentivizing_Truthful_LLMs_via_Reinforcement_Learning","title":"[논문리뷰] TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Hallucination","Truthfulness","Reinforcement Learning","Ternary Reward","Abstention","Knowledge Boundary","GRPO","RLHF"],"permalink":"/ai/review/2025-10-1-TruthRL_Incentivizing_Truthful_LLMs_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Thinking_Sparks_Emergent_Attention_Heads_in_Reasoning_Models_During_Post_Training","title":"[논문리뷰] Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training","excerpt":"이 [arXiv]에 게시한 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Mechanistic Interpretability","Attention Heads","Post-Training","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Circuit Analysis","Reasoning Models","Transformer Architecture"],"permalink":"/ai/review/2025-10-1-Thinking_Sparks_Emergent_Attention_Heads_in_Reasoning_Models_During_Post_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-The_Dragon_Hatchling_The_Missing_Link_between_the_Transformer_and_Models_of_the_Brain","title":"[논문리뷰] The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain","excerpt":"이 [arXiv]에 게시한 'The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Brain-Inspired AI","Graph Neural Networks","Hebbian Learning","Scale-Free Networks","Model Interpretability","Transformer Architecture"],"permalink":"/ai/review/2025-10-1-The_Dragon_Hatchling_The_Missing_Link_between_the_Transformer_and_Models_of_the_Brain/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Test-Time_Policy_Adaptation_for_Enhanced_Multi-Turn_Interactions_with_LLMs","title":"[논문리뷰] Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs","excerpt":"Yao Shu이 [arXiv]에 게시한 'Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Multi-turn Interaction","Test-Time Adaptation","Reinforcement Learning from Human Feedback","Policy Optimization","Online Learning","Self-Correction"],"permalink":"/ai/review/2025-10-1-Test-Time_Policy_Adaptation_for_Enhanced_Multi-Turn_Interactions_with_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-TTT3R_3D_Reconstruction_as_Test-Time_Training","title":"[논문리뷰] TTT3R: 3D Reconstruction as Test-Time Training","excerpt":"Anpei Chen이 [arXiv]에 게시한 'TTT3R: 3D Reconstruction as Test-Time Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Test-Time Training (TTT)","Recurrent Neural Networks (RNN)","Online Learning","Length Generalization","Associative Memory","State Update Rule"],"permalink":"/ai/review/2025-10-1-TTT3R_3D_Reconstruction_as_Test-Time_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-TAU_A_Benchmark_for_Cultural_Sound_Understanding_Beyond_Semantics","title":"[논문리뷰] TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics","excerpt":"Szu-Chi Chen이 [arXiv]에 게시한 'TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Audio Language Models","Cultural Sound Understanding","Localized Benchmark","Non-semantic Audio","Human-in-the-loop","Multimodal AI","Taipei Soundscape"],"permalink":"/ai/review/2025-10-1-TAU_A_Benchmark_for_Cultural_Sound_Understanding_Beyond_Semantics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Stable_Cinemetrics_Structured_Taxonomy_and_Evaluation_for_Professional_Video_Generation","title":"[논문리뷰] Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation","excerpt":"이 [arXiv]에 게시한 'Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Video Generation","Evaluation Framework","Cinematic Control","Taxonomy","Human Annotation","Vision-Language Models","Text-to-Video"],"permalink":"/ai/review/2025-10-1-Stable_Cinemetrics_Structured_Taxonomy_and_Evaluation_for_Professional_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Specialization_after_Generalization_Towards_Understanding_Test-Time_Training_in_Foundation_Models","title":"[논문리뷰] Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models","excerpt":"이 [arXiv]에 게시한 'Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Test-Time Training (TTT)","Foundation Models","Underparameterization","Sparse Autoencoders (SAE)","Linear Representation Hypothesis (LRH)","Specialization","Scaling Laws","In-Distribution Data"],"permalink":"/ai/review/2025-10-1-Specialization_after_Generalization_Towards_Understanding_Test-Time_Training_in_Foundation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Regression_Language_Models_for_Code","title":"[논문리뷰] Regression Language Models for Code","excerpt":"이 [arXiv]에 게시한 'Regression Language Models for Code' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Regression Language Model","Code Performance Prediction","Static Analysis","Neural Architecture Search","Text-to-Text Regression","Multi-task Learning","T5Gemma","ONNX"],"permalink":"/ai/review/2025-10-1-Regression_Language_Models_for_Code/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-ProfVLM_A_Lightweight_Video-Language_Model_for_Multi-View_Proficiency_Estimation","title":"[논문리뷰] ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation","excerpt":"Antonio Liotta이 [arXiv]에 게시한 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Video-Language Model","Proficiency Estimation","Multi-View Video","Action Quality Assessment","Lightweight Model","Generative Feedback"],"permalink":"/ai/review/2025-10-1-ProfVLM_A_Lightweight_Video-Language_Model_for_Multi-View_Proficiency_Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Probing_the_Critical_Point_CritPt_of_AI_Reasoning_a_Frontier_Physics_Research_Benchmark","title":"[논문리뷰] Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark","excerpt":"Penghao Zhu이 [arXiv]에 게시한 'Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","AI Reasoning","Physics Research","LLM Evaluation","Scientific Benchmark","Frontier Physics","Problem Solving","Model Reliability","Auto-grading"],"permalink":"/ai/review/2025-10-1-Probing_the_Critical_Point_CritPt_of_AI_Reasoning_a_Frontier_Physics_Research_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-OffTopicEval_When_Large_Language_Models_Enter_the_Wrong_Chat_Almost_Always","title":"[논문리뷰] OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!","excerpt":"이 [arXiv]에 게시한 'OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Operational Safety","Out-of-Domain (OOD)","Prompt Steering","Jailbreak Attacks","Evaluation Benchmark","Refusal Rate"],"permalink":"/ai/review/2025-10-1-OffTopicEval_When_Large_Language_Models_Enter_the_Wrong_Chat_Almost_Always/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-OceanGym_A_Benchmark_Environment_for_Underwater_Embodied_Agents","title":"[논문리뷰] OceanGym: A Benchmark Environment for Underwater Embodied Agents","excerpt":"이 [arXiv]에 게시한 'OceanGym: A Benchmark Environment for Underwater Embodied Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Underwater Robotics","Embodied AI","Benchmark Environment","Multi-modal Large Language Models","Autonomous Underwater Vehicles","Perception","Decision-Making","Simulation"],"permalink":"/ai/review/2025-10-1-OceanGym_A_Benchmark_Environment_for_Underwater_Embodied_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-MotionRAG_Motion_Retrieval-Augmented_Image-to-Video_Generation","title":"[논문리뷰] MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation","excerpt":"Limin Wang이 [arXiv]에 게시한 'MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Image-to-Video Generation","Motion Transfer","Retrieval-Augmented Generation (RAG)","In-Context Learning","Diffusion Models","Video Diffusion","Motion Realism"],"permalink":"/ai/review/2025-10-1-MotionRAG_Motion_Retrieval-Augmented_Image-to-Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-More_Thought_Less_Accuracy_On_the_Dual_Nature_of_Reasoning_in_Vision-Language_Models","title":"[논문리뷰] More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models","excerpt":"Fabian Waschkowski이 [arXiv]에 게시한 'More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Multimodal Reasoning","Reasoning","Visual Forgetting","Perceptual Grounding","Reinforcement Learning","Policy Optimization","Visual Anchors"],"permalink":"/ai/review/2025-10-1-More_Thought_Less_Accuracy_On_the_Dual_Nature_of_Reasoning_in_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Mem-α_Learning_Memory_Construction_via_Reinforcement_Learning","title":"[논문리뷰] Mem-α: Learning Memory Construction via Reinforcement Learning","excerpt":"Yuzhen Mao이 [arXiv]에 게시한 'Mem-α: Learning Memory Construction via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","External Memory","Reinforcement Learning","Memory Management","Long-Context Understanding","Tool Learning","RAG","Memory Architecture"],"permalink":"/ai/review/2025-10-1-Mem-α_Learning_Memory_Construction_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-MCPMark_A_Benchmark_for_Stress-Testing_Realistic_and_Comprehensive_MCP_Use","title":"[논문리뷰] MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use","excerpt":"이 [arXiv]에 게시한 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Model Context Protocol","Benchmark","Tool Use","CRUD Operations","Workflow Automation","Stress Testing","Evaluation"],"permalink":"/ai/review/2025-10-1-MCPMark_A_Benchmark_for_Stress-Testing_Realistic_and_Comprehensive_MCP_Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-MANI-Pure_Magnitude-Adaptive_Noise_Injection_for_Adversarial_Purification","title":"[논문리뷰] MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification","excerpt":"Zhiming Luo이 [arXiv]에 게시한 'MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Adversarial Purification","Diffusion Models","Frequency Domain","Adaptive Noise Injection","Robustness","Image Security","Magnitude Spectrum"],"permalink":"/ai/review/2025-10-1-MANI-Pure_Magnitude-Adaptive_Noise_Injection_for_Adversarial_Purification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Learning_to_See_Before_Seeing_Demystifying_LLM_Visual_Priors_from_Language_Pre-training","title":"[논문리뷰] Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training","excerpt":"Koustuv Sinha이 [arXiv]에 게시한 'Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Visual Priors","Language Pre-training","Multimodal LLM","Data Mixture Optimization","Reasoning Prior","Perception Prior","VQA","MLE-Bench"],"permalink":"/ai/review/2025-10-1-Learning_to_See_Before_Seeing_Demystifying_LLM_Visual_Priors_from_Language_Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Learning_Human-Perceived_Fakeness_in_AI-Generated_Videos_via_Multimodal_LLMs","title":"[논문리뷰] Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs","excerpt":"이 [arXiv]에 게시한 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","AI-Generated Videos","Deepfake Detection","Multimodal LLMs","Human Perception","Video Generation Evaluation","Spatiotemporal Annotation","Reward Modeling"],"permalink":"/ai/review/2025-10-1-Learning_Human-Perceived_Fakeness_in_AI-Generated_Videos_via_Multimodal_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-LayerD_Decomposing_Raster_Graphic_Designs_into_Layers","title":"[논문리뷰] LayerD: Decomposing Raster Graphic Designs into Layers","excerpt":"Kota Yamaguchi이 [arXiv]에 게시한 'LayerD: Decomposing Raster Graphic Designs into Layers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Graphic Design","Image Decomposition","Layer Extraction","Image Matting","Background Completion","Deep Learning","Creative AI","Dynamic Time Warping"],"permalink":"/ai/review/2025-10-1-LayerD_Decomposing_Raster_Graphic_Designs_into_Layers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Knowledge_Homophily_in_Large_Language_Models","title":"[논문리뷰] Knowledge Homophily in Large Language Models","excerpt":"Nedim Lipka이 [arXiv]에 게시한 'Knowledge Homophily in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM","Knowledge Homophily","Graph Neural Networks","Knowledge Graph","Knowledge Injection","Question Answering","Fine-tuning","Knowledge Retrieval"],"permalink":"/ai/review/2025-10-1-Knowledge_Homophily_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-InfoAgent_Advancing_Autonomous_Information-Seeking_Agents","title":"[논문리뷰] InfoAgent: Advancing Autonomous Information-Seeking Agents","excerpt":"이 [arXiv]에 게시한 'InfoAgent: Advancing Autonomous Information-Seeking Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Information Seeking","Reinforcement Learning","Data Synthesis","Web Search Tools","Tool Use","Deep Research Agents"],"permalink":"/ai/review/2025-10-1-InfoAgent_Advancing_Autonomous_Information-Seeking_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-IMG_Calibrating_Diffusion_Models_via_Implicit_Multimodal_Guidance","title":"[논문리뷰] IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance","excerpt":"이 [arXiv]에 게시한 'IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Diffusion Models","Multimodal Alignment","MLLM","Image Re-generation","Preference Learning","Implicit Guidance","Text-to-Image"],"permalink":"/ai/review/2025-10-1-IMG_Calibrating_Diffusion_Models_via_Implicit_Multimodal_Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Humanline_Online_Alignment_as_Perceptual_Loss","title":"[논문리뷰] Humanline: Online Alignment as Perceptual Loss","excerpt":"이 [arXiv]에 게시한 'Humanline: Online Alignment as Perceptual Loss' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Alignment","Online RLHF","Offline RLHF","Prospect Theory","Perceptual Loss","Human-Centric AI","Reinforcement Learning"],"permalink":"/ai/review/2025-10-1-Humanline_Online_Alignment_as_Perceptual_Loss/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Ferret-UI_Lite_Lessons_from_Building_Small_On-Device_GUI_Agents","title":"[논문리뷰] Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents","excerpt":"이 [arXiv]에 게시한 'Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","GUI Agents","On-Device AI","Multimodal LLM","GUI Grounding","GUI Navigation","Reinforcement Learning","Supervised Fine-tuning","Synthetic Data"],"permalink":"/ai/review/2025-10-1-Ferret-UI_Lite_Lessons_from_Building_Small_On-Device_GUI_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Estimating_Time_Series_Foundation_Model_Transferability_via_In-Context_Learning","title":"[논문리뷰] Estimating Time Series Foundation Model Transferability via In-Context Learning","excerpt":"Jun Qi이 [arXiv]에 게시한 'Estimating Time Series Foundation Model Transferability via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Time Series Foundation Models","Transferability Estimation","In-Context Learning","Tabular Foundation Models","Model Selection","Entropy Profile","Meta-learning","Forecasting"],"permalink":"/ai/review/2025-10-1-Estimating_Time_Series_Foundation_Model_Transferability_via_In-Context_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-EntroPE_Entropy-Guided_Dynamic_Patch_Encoder_for_Time_Series_Forecasting","title":"[논문리뷰] EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting","excerpt":"이 [arXiv]에 게시한 'EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Time Series Forecasting","Transformer","Dynamic Patching","Entropy","Predictive Uncertainty","Adaptive Encoding","Attention Mechanisms","Causal Transformer"],"permalink":"/ai/review/2025-10-1-EntroPE_Entropy-Guided_Dynamic_Patch_Encoder_for_Time_Series_Forecasting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Efficient_Audio-Visual_Speech_Separation_with_Discrete_Lip_Semantics_and_Multi-Scale_Global-Local_Attention","title":"[논문리뷰] Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention","excerpt":"이 [arXiv]에 게시한 'Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Audio-Visual Speech Separation","Deep Learning","Efficiency","Discrete Lip Semantics","Global-Local Attention","Lightweight Models","VQ-VAE"],"permalink":"/ai/review/2025-10-1-Efficient_Audio-Visual_Speech_Separation_with_Discrete_Lip_Semantics_and_Multi-Scale_Global-Local_Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-DeepScientist_Advancing_Frontier-Pushing_Scientific_Findings_Progressively","title":"[논문리뷰] DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively","excerpt":"이 [arXiv]에 게시한 'DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","AI Scientist","Autonomous Scientific Discovery","Bayesian Optimization","LLM-based Agents","SOTA-Surpassing","Findings Memory","Exploration-Exploitation"],"permalink":"/ai/review/2025-10-1-DeepScientist_Advancing_Frontier-Pushing_Scientific_Findings_Progressively/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-DC-VideoGen_Efficient_Video_Generation_with_Deep_Compression_Video_Autoencoder","title":"[논문리뷰] DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder","excerpt":"이 [arXiv]에 게시한 'DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Models","Video Autoencoder","Deep Compression","Model Acceleration","Fine-tuning","Latent Space","Temporal Modeling"],"permalink":"/ai/review/2025-10-1-DC-VideoGen_Efficient_Video_Generation_with_Deep_Compression_Video_Autoencoder/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-DA2_Depth_Anything_in_Any_Direction","title":"[논문리뷰] DA^2: Depth Anything in Any Direction","excerpt":"이 [arXiv]에 게시한 'DA^2: Depth Anything in Any Direction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Panoramic Depth Estimation","Zero-shot Generalization","Data Curation","SphereViT","Spherical Geometry","360-degree Imaging","Vision Transformer"],"permalink":"/ai/review/2025-10-1-DA2_Depth_Anything_in_Any_Direction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Context_Is_What_You_Need_The_Maximum_Effective_Context_Window_for_Real_World_Limits_of_LLMs","title":"[논문리뷰] Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs","excerpt":"normanpaulsen이 [arXiv]에 게시한 'Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Context Window","Effective Context Window","Model Performance","Hallucination Rates","RAG Systems","Token Limits"],"permalink":"/ai/review/2025-10-1-Context_Is_What_You_Need_The_Maximum_Effective_Context_Window_for_Real_World_Limits_of_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-BuildBench_Benchmarking_LLM_Agents_on_Compiling_Real-World_Open-Source_Software","title":"[논문리뷰] BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software","excerpt":"이 [arXiv]에 게시한 'BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","LLM Agents","Open-Source Software","Compilation","Benchmarking","Software Engineering","Error Resolution","Retrieval-Augmented Generation"],"permalink":"/ai/review/2025-10-1-BuildBench_Benchmarking_LLM_Agents_on_Compiling_Real-World_Open-Source_Software/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Benefits_and_Pitfalls_of_Reinforcement_Learning_for_Language_Model_Planning_A_Theoretical_Perspective","title":"[논문리뷰] Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective","excerpt":"이 [arXiv]에 게시한 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Planning","Policy Gradient","Q-learning","Supervised Fine-Tuning","Diversity Collapse","Reward Hacking"],"permalink":"/ai/review/2025-10-1-Benefits_and_Pitfalls_of_Reinforcement_Learning_for_Language_Model_Planning_A_Theoretical_Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-Attention_as_a_Compass_Efficient_Exploration_for_Process-Supervised_RL_in_Reasoning_Models","title":"[논문리뷰] Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models","excerpt":"이 [arXiv]에 게시한 'Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Process-Supervised RL","Large Language Models","Reasoning Models","Attention Mechanism","Efficient Exploration","Adaptive Sampling","Off-Policy Training"],"permalink":"/ai/review/2025-10-1-Attention_as_a_Compass_Efficient_Exploration_for_Process-Supervised_RL_in_Reasoning_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-10-1-A_Cartography_of_Open_Collaboration_in_Open_Source_AI_Mapping_Practices_Motivations_and_Governance_in_14_Open_Large_Language_Model_Projects","title":"[논문리뷰] A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects","excerpt":"Jennifer Ding이 [arXiv]에 게시한 'A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","lastModifiedAt":"2025-10-01 14:04:08+0900","categories":["Review"],"tags":["Review","Open Source AI","LLM Development","Open Collaboration","Governance Models","Developer Motivations","Community Engagement","AI Ecosystem"],"permalink":"/ai/review/2025-10-1-A_Cartography_of_Open_Collaboration_in_Open_Source_AI_Mapping_Practices_Motivations_and_Governance_in_14_Open_Large_Language_Model_Projects/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-Visual_Jigsaw_Post-Training_Improves_MLLMs","title":"[논문리뷰] Visual Jigsaw Post-Training Improves MLLMs","excerpt":"Lewei Lu이 [arXiv]에 게시한 'Visual Jigsaw Post-Training Improves MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","MLLMs","Post-training","Self-supervised Learning","Visual Understanding","Jigsaw Puzzles","RLVR","Multimodal Perception","Spatial Reasoning"],"permalink":"/ai/review/2025-9-30-Visual_Jigsaw_Post-Training_Improves_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-StableToken_A_Noise-Robust_Semantic_Speech_Tokenizer_for_Resilient_SpeechLLMs","title":"[논문리뷰] StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs","excerpt":"Wei Jia이 [arXiv]에 게시한 'StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Speech Tokenizer","Noise Robustness","Semantic Tokens","SpeechLLMs","Voting-LFQ","Consensus Training","Automatic Speech Recognition","Speech Synthesis"],"permalink":"/ai/review/2025-9-30-StableToken_A_Noise-Robust_Semantic_Speech_Tokenizer_for_Resilient_SpeechLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-SLA_Beyond_Sparsity_in_Diffusion_Transformers_via_Fine-Tunable_Sparse-Linear_Attention","title":"[논문리뷰] SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention","excerpt":"이 [arXiv]에 게시한 'SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Diffusion Transformers","Sparse Attention","Linear Attention","Model Acceleration","Video Generation","Attention Mechanisms","Fine-tuning"],"permalink":"/ai/review/2025-9-30-SLA_Beyond_Sparsity_in_Diffusion_Transformers_via_Fine-Tunable_Sparse-Linear_Attention/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-SANA-Video_Efficient_Video_Generation_with_Block_Linear_Diffusion_Transformer","title":"[논문리뷰] SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer","excerpt":"이 [arXiv]에 게시한 'SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Video Generation","Diffusion Model","Linear Attention","Transformer","Long Video","Efficient Inference","Constant Memory","Low-Cost Training","RTX Deployment"],"permalink":"/ai/review/2025-9-30-SANA-Video_Efficient_Video_Generation_with_Block_Linear_Diffusion_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-RealUnify_Do_Unified_Models_Truly_Benefit_from_Unification_A_Comprehensive_Benchmark","title":"[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark","excerpt":"Yuran Wang이 [arXiv]에 게시한 'RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Unified Models","Multimodal AI","Benchmark","Capability Synergy","Visual Understanding","Image Generation","Dual-Evaluation Protocol"],"permalink":"/ai/review/2025-9-30-RealUnify_Do_Unified_Models_Truly_Benefit_from_Unification_A_Comprehensive_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-Random_Policy_Valuation_is_Enough_for_LLM_Reasoning_with_Verifiable_Rewards","title":"[논문리뷰] Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards","excerpt":"Binxing Jiao이 [arXiv]에 게시한 'Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Reasoning","Policy Valuation","Markov Decision Process","Diversity","Math Reasoning","Verifiable Rewards"],"permalink":"/ai/review/2025-9-30-Random_Policy_Valuation_is_Enough_for_LLM_Reasoning_with_Verifiable_Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-OpenGPT-4o-Image_A_Comprehensive_Dataset_for_Advanced_Image_Generation_and_Editing","title":"[논문리뷰] OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing","excerpt":"Huanyu Zhang이 [arXiv]에 게시한 'OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Image Generation","Image Editing","Multimodal AI","Dataset","Instruction Following","Taxonomy","GPT-40"],"permalink":"/ai/review/2025-9-30-OpenGPT-4o-Image_A_Comprehensive_Dataset_for_Advanced_Image_Generation_and_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-Multiplayer_Nash_Preference_Optimization","title":"[논문리뷰] Multiplayer Nash Preference Optimization","excerpt":"이 [arXiv]에 게시한 'Multiplayer Nash Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","RLHF","LLM Alignment","Nash Equilibrium","Multiplayer Games","Preference Optimization","Non-transitive Preferences","Game Theory"],"permalink":"/ai/review/2025-9-30-Multiplayer_Nash_Preference_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-EditScore_Unlocking_Online_RL_for_Image_Editing_via_High-Fidelity_Reward_Modeling","title":"[논문리뷰] EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling","excerpt":"이 [arXiv]에 게시한 'EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Image Editing","Reward Modeling","Instruction-Guided Editing","Online RL","Visual Language Models","Benchmark","Self-Ensembling"],"permalink":"/ai/review/2025-9-30-EditScore_Unlocking_Online_RL_for_Image_Editing_via_High-Fidelity_Reward_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-30-EasySteer_A_Unified_Framework_for_High-Performance_and_Extensible_LLM_Steering","title":"[논문리뷰] EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering","excerpt":"이 [arXiv]에 게시한 'EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","lastModifiedAt":"2025-09-30 13:52:24+0900","categories":["Review"],"tags":["Review","LLM Steering Framework","vLLM Integration","Hidden State Manipulation","Inference Optimization","Extensibility","Modular Architecture","Reasoning Mitigation","Hallucination Reduction"],"permalink":"/ai/review/2025-9-30-EasySteer_A_Unified_Framework_for_High-Performance_and_Extensible_LLM_Steering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-X-Streamer_Unified_Human_World_Modeling_with_Audiovisual_Interaction","title":"[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction","excerpt":"Guoxian Song이 [arXiv]에 게시한 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Digital Human","Multimodal AI","Real-time Streaming","Video Generation","Diffusion Models","Transformer Architecture","Audiovisual Synchronization","World Modeling"],"permalink":"/ai/review/2025-9-29-X-Streamer_Unified_Human_World_Modeling_with_Audiovisual_Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-X-CoT_Explainable_Text-to-Video_Retrieval_via_LLM-based_Chain-of-Thought_Reasoning","title":"[논문리뷰] X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning","excerpt":"Raghuveer Rao이 [arXiv]에 게시한 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Text-to-Video Retrieval","LLM","Chain-of-Thought","Explainable AI","Multimodal Retrieval","Bradley-Terry Model","Video Annotation"],"permalink":"/ai/review/2025-9-29-X-CoT_Explainable_Text-to-Video_Retrieval_via_LLM-based_Chain-of-Thought_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-WoW_Towards_a_World_omniscient_World_model_Through_Embodied_Interaction","title":"[논문리뷰] WoW: Towards a World omniscient World model Through Embodied Interaction","excerpt":"Weishi Mi이 [arXiv]에 게시한 'WoW: Towards a World omniscient World model Through Embodied Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","World Model","Embodied AI","Robotics","Diffusion Models","Physical Reasoning","Vision Language Models","Interaction Data","Self-Optimization"],"permalink":"/ai/review/2025-9-29-WoW_Towards_a_World_omniscient_World_model_Through_Embodied_Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Where_MLLMs_Attend_and_What_They_Rely_On_Explaining_Autoregressive_Token_Generation","title":"[논문리뷰] Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation","excerpt":"Shiming Liu이 [arXiv]에 게시한 'Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","MLLM","Interpretability","Attribution","Token Generation","Black-box Explanation","Hallucination Diagnosis","Multimodality","VQA"],"permalink":"/ai/review/2025-9-29-Where_MLLMs_Attend_and_What_They_Rely_On_Explaining_Autoregressive_Token_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-WebGen-Agent_Enhancing_Interactive_Website_Generation_with_Multi-Level_Feedback_and_Step-Level_Reinforcement_Learning","title":"[논문리뷰] WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning","excerpt":"Zhuofan Zong이 [arXiv]에 게시한 'WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Website Generation","Code Agent","LLM","VLM","Reinforcement Learning","Multi-Level Feedback","GUI Agent","Step-GRPO"],"permalink":"/ai/review/2025-9-29-WebGen-Agent_Enhancing_Interactive_Website_Generation_with_Multi-Level_Feedback_and_Step-Level_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-VoiceAssistant-Eval_Benchmarking_AI_Assistants_across_Listening_Speaking_and_Viewing","title":"[논문리뷰] VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing","excerpt":"이 [arXiv]에 게시한 'VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","AI Assistants","Multimodal Benchmarking","Audio Understanding","Speech Synthesis","Vision-Language Models","Role-play","Safety","Robustness"],"permalink":"/ai/review/2025-9-29-VoiceAssistant-Eval_Benchmarking_AI_Assistants_across_Listening_Speaking_and_Viewing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Variational_Reasoning_for_Language_Models","title":"[논문리뷰] Variational Reasoning for Language Models","excerpt":"이 [arXiv]에 게시한 'Variational Reasoning for Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Variational Inference","Language Models","Reasoning","ELBO","IWAE","Reinforcement Learning","Latent Variables","Forward-KL"],"permalink":"/ai/review/2025-9-29-Variational_Reasoning_for_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-UniVid_Unifying_Vision_Tasks_with_Pre-trained_Video_Generation_Models","title":"[논문리뷰] UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models","excerpt":"Yuchao Gu이 [arXiv]에 게시한 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Unified Vision Modeling","Video Generation","Diffusion Transformer","Supervised Fine-tuning","Cross-modal","Cross-source Tasks","Visual Sentences","LoRA"],"permalink":"/ai/review/2025-9-29-UniVid_Unifying_Vision_Tasks_with_Pre-trained_Video_Generation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-UltraHorizon_Benchmarking_Agent_Capabilities_in_Ultra_Long-Horizon_Scenarios","title":"[논문리뷰] UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios","excerpt":"Zeyu Qin이 [arXiv]에 게시한 'UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM Agents","Long-Horizon Reasoning","Benchmarking","Partially Observable","Tool Use","Memory Management","Exploration"],"permalink":"/ai/review/2025-9-29-UltraHorizon_Benchmarking_Agent_Capabilities_in_Ultra_Long-Horizon_Scenarios/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Think-on-Graph_3.0_Efficient_and_Adaptive_LLM_Reasoning_on_Heterogeneous_Graphs_via_Multi-Agent_Dual-Evolving_Context_Retrieval","title":"[논문리뷰] Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval","excerpt":"이 [arXiv]에 게시한 'Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","RAG","LLM Reasoning","Knowledge Graphs","Multi-Agent Systems","Context Retrieval","Heterogeneous Graphs","Adaptive Learning","Dual-Evolution"],"permalink":"/ai/review/2025-9-29-Think-on-Graph_3.0_Efficient_and_Adaptive_LLM_Reasoning_on_Heterogeneous_Graphs_via_Multi-Agent_Dual-Evolving_Context_Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-TUN3D_Towards_Real-World_Scene_Understanding_from_Unposed_Images","title":"[논문리뷰] TUN3D: Towards Real-World Scene Understanding from Unposed Images","excerpt":"Anna Vorontsova이 [arXiv]에 게시한 'TUN3D: Towards Real-World Scene Understanding from Unposed Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","3D Scene Understanding","Layout Estimation","3D Object Detection","Unposed Images","Sparse Convolutional Networks","Multi-view Stereo","Real-time AI"],"permalink":"/ai/review/2025-9-29-TUN3D_Towards_Real-World_Scene_Understanding_from_Unposed_Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-StateX_Enhancing_RNN_Recall_via_Post-training_State_Expansion","title":"[논문리뷰] StateX: Enhancing RNN Recall via Post-training State Expansion","excerpt":"Zhiyuan Liu이 [arXiv]에 게시한 'StateX: Enhancing RNN Recall via Post-training State Expansion' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","RNN","State Expansion","Post-training","Long-context Recall","Linear Attention","State Space Models","GLA","Mamba2"],"permalink":"/ai/review/2025-9-29-StateX_Enhancing_RNN_Recall_via_Post-training_State_Expansion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-See_Point_Fly_A_Learning-Free_VLM_Framework_for_Universal_Unmanned_Aerial_Navigation","title":"[논문리뷰] See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation","excerpt":"Chih-Hai Su이 [arXiv]에 게시한 'See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Vision-Language Models","UAV Navigation","Zero-shot","Spatial Grounding","Waypoint Prompting","Autonomous Navigation","Adaptive Control"],"permalink":"/ai/review/2025-9-29-See_Point_Fly_A_Learning-Free_VLM_Framework_for_Universal_Unmanned_Aerial_Navigation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-SPARK_Synergistic_Policy_And_Reward_Co-Evolving_Framework","title":"[논문리뷰] SPARK: Synergistic Policy And Reward Co-Evolving Framework","excerpt":"이 [arXiv]에 게시한 'SPARK: Synergistic Policy And Reward Co-Evolving Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","LVLMs","Reward Modeling","Policy Optimization","Self-Reflection","Verifiable Rewards","Co-evolution"],"permalink":"/ai/review/2025-9-29-SPARK_Synergistic_Policy_And_Reward_Co-Evolving_Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-ReviewScore_Misinformed_Peer_Review_Detection_with_Large_Language_Models","title":"[논문리뷰] ReviewScore: Misinformed Peer Review Detection with Large Language Models","excerpt":"이 [arXiv]에 게시한 'ReviewScore: Misinformed Peer Review Detection with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Peer Review","Review Quality","Large Language Models (LLMs)","Misinformed Review","Argument Reconstruction","Factuality Evaluation","Natural Language Processing","Automated Evaluation"],"permalink":"/ai/review/2025-9-29-ReviewScore_Misinformed_Peer_Review_Detection_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-RefAM_Attention_Magnets_for_Zero-Shot_Referral_Segmentation","title":"[논문리뷰] RefAM: Attention Magnets for Zero-Shot Referral Segmentation","excerpt":"Federico Tombari이 [arXiv]에 게시한 'RefAM: Attention Magnets for Zero-Shot Referral Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Zero-Shot Segmentation","Referring Segmentation","Diffusion Transformers (DiTs)","Attention Mechanisms","Attention Sinks","Stop Words","Vision-Language Models","Training-Free Methods"],"permalink":"/ai/review/2025-9-29-RefAM_Attention_Magnets_for_Zero-Shot_Referral_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Real-Time_Object_Detection_Meets_DINOv3","title":"[논문리뷰] Real-Time Object Detection Meets DINOv3","excerpt":"Xi Shen이 [arXiv]에 게시한 'Real-Time Object Detection Meets DINOv3' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Real-time Object Detection","DINOv3","DEIMv2","Vision Transformer","Multi-scale Features","Spatial Tuning Adapter","Lightweight Models","Object Detection Framework"],"permalink":"/ai/review/2025-9-29-Real-Time_Object_Detection_Meets_DINOv3/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Quantile_Advantage_Estimation_for_Entropy-Safe_Reasoning","title":"[논문리뷰] Quantile Advantage Estimation for Entropy-Safe Reasoning","excerpt":"An Zhang이 [arXiv]에 게시한 'Quantile Advantage Estimation for Entropy-Safe Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Reasoning","Entropy Control","Advantage Estimation","Quantile Baseline","Exploration-Exploitation","RLVR"],"permalink":"/ai/review/2025-9-29-Quantile_Advantage_Estimation_for_Entropy-Safe_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-PromptCoT_2.0_Scaling_Prompt_Synthesis_for_Large_Language_Model_Reasoning","title":"[논문리뷰] PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning","excerpt":"Lingpeng Kong이 [arXiv]에 게시한 'PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Prompt Synthesis","Large Language Models","Reasoning","Expectation-Maximization","Self-Play","Supervised Fine-Tuning","Task Generation","Rationale Generation"],"permalink":"/ai/review/2025-9-29-PromptCoT_2.0_Scaling_Prompt_Synthesis_for_Large_Language_Model_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-No_Prompt_Left_Behind_Exploiting_Zero-Variance_Prompts_in_LLM_Reinforcement_Learning_via_Entropy-Guided_Advantage_Shaping","title":"[논문리뷰] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping","excerpt":"이 [arXiv]에 게시한 'No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM Reinforcement Learning","Zero-Variance Prompts","Advantage Shaping","Entropy-Guided","Math Reasoning","RLVR","Group Relative Policy Optimization"],"permalink":"/ai/review/2025-9-29-No_Prompt_Left_Behind_Exploiting_Zero-Variance_Prompts_in_LLM_Reinforcement_Learning_via_Entropy-Guided_Advantage_Shaping/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-MinerU2.5_A_Decoupled_Vision-Language_Model_for_Efficient_High-Resolution_Document_Parsing","title":"[논문리뷰] MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing","excerpt":"SunYuefeng이 [arXiv]에 게시한 'MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Document Parsing","Vision-Language Model","High-Resolution","Two-Stage Inference","Layout Analysis","Content Recognition","Data Engine","Computational Efficiency"],"permalink":"/ai/review/2025-9-29-MinerU2.5_A_Decoupled_Vision-Language_Model_for_Efficient_High-Resolution_Document_Parsing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Mind-the-Glitch_Visual_Correspondence_for_Detecting_Inconsistencies_in_Subject-Driven_Generation","title":"[논문리뷰] Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation","excerpt":"Peter Wonka이 [arXiv]에 게시한 'Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Subject-Driven Generation","Visual Inconsistency Detection","Feature Disentanglement","Diffusion Models","Semantic Correspondence","Evaluation Metric","Spatial Localization","Contrastive Learning"],"permalink":"/ai/review/2025-9-29-Mind-the-Glitch_Visual_Correspondence_for_Detecting_Inconsistencies_in_Subject-Driven_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-MesaTask_Towards_Task-Driven_Tabletop_Scene_Generation_via_3D_Spatial_Reasoning","title":"[논문리뷰] MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning","excerpt":"Weipeng Zhong이 [arXiv]에 게시한 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Robotic Manipulation","Large Language Models","Spatial Reasoning","Dataset","Direct Preference Optimization","Tabletop Scene"],"permalink":"/ai/review/2025-9-29-MesaTask_Towards_Task-Driven_Tabletop_Scene_Generation_via_3D_Spatial_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-LucidFlux_Caption-Free_Universal_Image_Restoration_via_a_Large-Scale_Diffusion_Transformer","title":"[논문리뷰] LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer","excerpt":"이 [arXiv]에 게시한 'LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Universal Image Restoration","Diffusion Transformer","Caption-Free","Semantic Alignment","Image Quality Assessment","Data Curation","Real-World Degradations","Deep Learning"],"permalink":"/ai/review/2025-9-29-LucidFlux_Caption-Free_Universal_Image_Restoration_via_a_Large-Scale_Diffusion_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-LongLive_Real-time_Interactive_Long_Video_Generation","title":"[논문리뷰] LongLive: Real-time Interactive Long Video Generation","excerpt":"이 [arXiv]에 게시한 'LongLive: Real-time Interactive Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Long Video Generation","Real-time","Interactive AI","Autoregressive Models","KV Cache","Streaming Tuning","Attention Sink","Diffusion Models"],"permalink":"/ai/review/2025-9-29-LongLive_Real-time_Interactive_Long_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Learn_the_Ropes_Then_Trust_the_Wins_Self-imitation_with_Progressive_Exploration_for_Agentic_Reinforcement_Learning","title":"[논문리뷰] Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning","excerpt":"Gang Li이 [arXiv]에 게시한 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Agents","Exploration-Exploitation","Self-Imitation Learning","Intrinsic Rewards","Curriculum Learning","Policy Entropy","Tool Use"],"permalink":"/ai/review/2025-9-29-Learn_the_Ropes_Then_Trust_the_Wins_Self-imitation_with_Progressive_Exploration_for_Agentic_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Language_Models_Can_Learn_from_Verbal_Feedback_Without_Scalar_Rewards","title":"[논문리뷰] Language Models Can Learn from Verbal Feedback Without Scalar Rewards","excerpt":"이 [arXiv]에 게시한 'Language Models Can Learn from Verbal Feedback Without Scalar Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Verbal Feedback","Conditional Generation","Large Language Models","Feedback-Conditional Policy","Offline-Online Learning","Reward Hypothesis Bypass"],"permalink":"/ai/review/2025-9-29-Language_Models_Can_Learn_from_Verbal_Feedback_Without_Scalar_Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Instruction-Following_Evaluation_in_Function_Calling_for_Large_Language_Models","title":"[논문리뷰] Instruction-Following Evaluation in Function Calling for Large Language Models","excerpt":"NikolaiSkripko이 [arXiv]에 게시한 'Instruction-Following Evaluation in Function Calling for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Function Calling","LLMs","Instruction Following","Benchmarking","JSON Schema","AI Agents","Evaluation Metrics"],"permalink":"/ai/review/2025-9-29-Instruction-Following_Evaluation_in_Function_Calling_for_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-HiGS_History-Guided_Sampling_for_Plug-and-Play_Enhancement_of_Diffusion_Models","title":"[논문리뷰] HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models","excerpt":"Romann M. Weber이 [arXiv]에 게시한 'HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Diffusion Models","Sampling","Generative AI","Image Generation","Plug-and-Play","Training-Free","Guidance","Momentum-Based Methods"],"permalink":"/ai/review/2025-9-29-HiGS_History-Guided_Sampling_for_Plug-and-Play_Enhancement_of_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-FlashEdit_Decoupling_Speed_Structure_and_Semantics_for_Precise_Image_Editing","title":"[논문리뷰] FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing","excerpt":"Linghe Kong이 [arXiv]에 게시한 'FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Text-Guided Image Editing","Diffusion Models","Real-Time Editing","One-Step Inversion","Attention Control","Background Preservation","Semantic Disentanglement"],"permalink":"/ai/review/2025-9-29-FlashEdit_Decoupling_Speed_Structure_and_Semantics_for_Precise_Image_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Fine-tuning_Done_Right_in_Model_Editing","title":"[논문리뷰] Fine-tuning Done Right in Model Editing","excerpt":"Du Su이 [arXiv]에 게시한 'Fine-tuning Done Right in Model Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Model Editing","Fine-tuning","Large Language Models","Catastrophic Forgetting","Breadth-First Pipeline","Depth-First Pipeline","Localized Tuning","Lifelong Learning"],"permalink":"/ai/review/2025-9-29-Fine-tuning_Done_Right_in_Model_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Finding_3D_Positions_of_Distant_Objects_from_Noisy_Camera_Movement_and_Semantic_Segmentation_Sequences","title":"[논문리뷰] Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences","excerpt":"Eija Honkavaara이 [arXiv]에 게시한 'Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","3D Object Localization","Particle Filter","Multi-target Tracking","Drone Surveillance","Wildfire Monitoring","Semantic Segmentation","Camera Pose Estimation"],"permalink":"/ai/review/2025-9-29-Finding_3D_Positions_of_Distant_Objects_from_Noisy_Camera_Movement_and_Semantic_Segmentation_Sequences/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-ERGO_Efficient_High-Resolution_Visual_Understanding_for_Vision-Language_Models","title":"[논문리뷰] ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models","excerpt":"Ki-Ung Song이 [arXiv]에 게시한 'ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","High-Resolution Vision","Vision-Language Models","Efficient Reasoning","Coarse-to-Fine","Reinforcement Learning","Visual Understanding","Attention Mechanism"],"permalink":"/ai/review/2025-9-29-ERGO_Efficient_High-Resolution_Visual_Understanding_for_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-EPO_Entropy-regularized_Policy_Optimization_for_LLM_Agents_Reinforcement_Learning","title":"[논문리뷰] EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning","excerpt":"Li Yu-Jhe이 [arXiv]에 게시한 'EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Entropy Regularization","Policy Optimization","Sparse Rewards","Multi-turn Environments","Exploration-Exploitation"],"permalink":"/ai/review/2025-9-29-EPO_Entropy-regularized_Policy_Optimization_for_LLM_Agents_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-D-Artemis_A_Deliberative_Cognitive_Framework_for_Mobile_GUI_Multi-Agents","title":"[논문리뷰] D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents","excerpt":"Jinyuan Li이 [arXiv]에 게시한 'D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Mobile GUI Automation","Multi-Agent System","Cognitive Architecture","Pre-execution Alignment","Post-execution Reflection","Retrieval-Augmented Generation","Multimodal LLM","Deliberative AI"],"permalink":"/ai/review/2025-9-29-D-Artemis_A_Deliberative_Cognitive_Framework_for_Mobile_GUI_Multi-Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-Chasing_the_Tail_Effective_Rubric-based_Reward_Modeling_for_Large_Language_Model_Post-Training","title":"[논문리뷰] Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training","excerpt":"이 [arXiv]에 게시한 'Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","LLM","Reinforcement Fine-tuning","Reward Modeling","Reward Over-optimization","Rubric-based Rewards","High-reward Tail","Off-policy Data","LLM Alignment"],"permalink":"/ai/review/2025-9-29-Chasing_the_Tail_Effective_Rubric-based_Reward_Modeling_for_Large_Language_Model_Post-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-CapRL_Stimulating_Dense_Image_Caption_Capabilities_via_Reinforcement_Learning","title":"[논문리뷰] CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning","excerpt":"이 [arXiv]에 게시한 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Image Captioning","Reinforcement Learning","Verifiable Rewards","LVLMs","VQA","Data Curation","Caption Quality"],"permalink":"/ai/review/2025-9-29-CapRL_Stimulating_Dense_Image_Caption_Capabilities_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-29-CHURRO_Making_History_Readable_with_an_Open-Weight_Large_Vision-Language_Model_for_High-Accuracy_Low-Cost_Historical_Text_Recognition","title":"[논문리뷰] CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition","excerpt":"이 [arXiv]에 게시한 'CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","lastModifiedAt":"2025-09-29 13:47:46+0900","categories":["Review"],"tags":["Review","Historical Text Recognition","Vision-Language Model","Open-Weight Model","OCR","Cultural Heritage","Low-Cost AI","Dataset Curation","Fine-tuning"],"permalink":"/ai/review/2025-9-29-CHURRO_Making_History_Readable_with_an_Open-Weight_Large_Vision-Language_Model_for_High-Accuracy_Low-Cost_Historical_Text_Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0000-index_of_python_enhancement_proposals_peps","title":"[Active] PEP 0 - Index of Python Enhancement Proposals (PEPs)","excerpt":"Python Enhancement Proposal 0: 'Index of Python Enhancement Proposals (PEPs)'에 대한 한국어 번역입니다.","date":"2025-09-27 21:19:02+0900","lastModifiedAt":"2025-09-27 21:19:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/0/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8106-2025_term_steering_council_election","title":"[Final] PEP 8106 - 2025 Term Steering Council election","excerpt":"Python Enhancement Proposal 8106: '2025 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:35:42+0900","lastModifiedAt":"2025-09-27 19:35:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8106/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8105-2024_term_steering_council_election","title":"[Final] PEP 8105 - 2024 Term Steering Council election","excerpt":"Python Enhancement Proposal 8105: '2024 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:34:28+0900","lastModifiedAt":"2025-09-27 19:34:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8105/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8104-2023_term_steering_council_election","title":"[Final] PEP 8104 - 2023 Term Steering Council election","excerpt":"Python Enhancement Proposal 8104: '2023 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:34:01+0900","lastModifiedAt":"2025-09-27 19:34:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8104/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8103-2022_term_steering_council_election","title":"[Final] PEP 8103 - 2022 Term Steering Council election","excerpt":"Python Enhancement Proposal 8103: '2022 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:31:17+0900","lastModifiedAt":"2025-09-27 19:31:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8103/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8102-2021_term_steering_council_election","title":"[Final] PEP 8102 - 2021 Term Steering Council election","excerpt":"Python Enhancement Proposal 8102: '2021 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:31:05+0900","lastModifiedAt":"2025-09-27 19:31:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8102/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8101-2020_term_steering_council_election","title":"[Final] PEP 8101 - 2020 Term Steering Council election","excerpt":"Python Enhancement Proposal 8101: '2020 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:45+0900","lastModifiedAt":"2025-09-27 19:30:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8101/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8100-january_2019_steering_council_election","title":"[Final] PEP 8100 - January 2019 Steering Council election","excerpt":"Python Enhancement Proposal 8100: 'January 2019 Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:34+0900","lastModifiedAt":"2025-09-27 19:30:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8100/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8016-the_steering_council_model","title":"[Accepted] PEP 8016 - The Steering Council Model","excerpt":"Python Enhancement Proposal 8016: 'The Steering Council Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:11+0900","lastModifiedAt":"2025-09-27 19:30:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8016/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8015-organization_of_the_python_community","title":"[Rejected] PEP 8015 - Organization of the Python community","excerpt":"Python Enhancement Proposal 8015: 'Organization of the Python community'에 대한 한국어 번역입니다.","date":"2025-09-27 19:29:53+0900","lastModifiedAt":"2025-09-27 19:29:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8015/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8014-the_commons_governance_model","title":"[Rejected] PEP 8014 - The Commons Governance Model","excerpt":"Python Enhancement Proposal 8014: 'The Commons Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:29:07+0900","lastModifiedAt":"2025-09-27 19:29:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8014/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8013-the_external_council_governance_model","title":"[Rejected] PEP 8013 - The External Council Governance Model","excerpt":"Python Enhancement Proposal 8013: 'The External Council Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:28:36+0900","lastModifiedAt":"2025-09-27 19:28:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8013/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8012-the_community_governance_model","title":"[Rejected] PEP 8012 - The Community Governance Model","excerpt":"Python Enhancement Proposal 8012: 'The Community Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:28:04+0900","lastModifiedAt":"2025-09-27 19:28:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8012/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8011-python_governance_model_lead_by_trio_of_pythonistas","title":"[Rejected] PEP 8011 - Python Governance Model Lead by Trio of Pythonistas","excerpt":"Python Enhancement Proposal 8011: 'Python Governance Model Lead by Trio of Pythonistas'에 대한 한국어 번역입니다.","date":"2025-09-27 19:27:38+0900","lastModifiedAt":"2025-09-27 19:27:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8011/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8010-the_technical_leader_governance_model","title":"[Rejected] PEP 8010 - The Technical Leader Governance Model","excerpt":"Python Enhancement Proposal 8010: 'The Technical Leader Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:27:12+0900","lastModifiedAt":"2025-09-27 19:27:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8010/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8002-open_source_governance_survey","title":"[Final] PEP 8002 - Open Source Governance Survey","excerpt":"Python Enhancement Proposal 8002: 'Open Source Governance Survey'에 대한 한국어 번역입니다.","date":"2025-09-27 19:26:46+0900","lastModifiedAt":"2025-09-27 19:26:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8002/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8001-python_governance_voting_process","title":"[Final] PEP 8001 - Python Governance Voting Process","excerpt":"Python Enhancement Proposal 8001: 'Python Governance Voting Process'에 대한 한국어 번역입니다.","date":"2025-09-27 19:25:35+0900","lastModifiedAt":"2025-09-27 19:25:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8001/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-8000-python_language_governance_proposal_overview","title":"[Final] PEP 8000 - Python Language Governance Proposal Overview","excerpt":"Python Enhancement Proposal 8000: 'Python Language Governance Proposal Overview'에 대한 한국어 번역입니다.","date":"2025-09-27 19:25:09+0900","lastModifiedAt":"2025-09-27 19:25:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8000/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3333-python_web_server_gateway_interface_v1.0.1","title":"[Final] PEP 3333 - Python Web Server Gateway Interface v1.0.1","excerpt":"Python Enhancement Proposal 3333: 'Python Web Server Gateway Interface v1.0.1'에 대한 한국어 번역입니다.","date":"2025-09-27 19:24:56+0900","lastModifiedAt":"2025-09-27 19:24:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3333/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3156-asynchronous_io_support_rebooted_the_asyncio_module","title":"[Final] PEP 3156 - Asynchronous IO Support Rebooted: the “asyncio” Module","excerpt":"Python Enhancement Proposal 3156: 'Asynchronous IO Support Rebooted: the “asyncio” Module'에 대한 한국어 번역입니다.","date":"2025-09-27 19:21:50+0900","lastModifiedAt":"2025-09-27 19:21:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3156/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3155-qualified_name_for_classes_and_functions","title":"[Final] PEP 3155 - Qualified name for classes and functions","excerpt":"Python Enhancement Proposal 3155: 'Qualified name for classes and functions'에 대한 한국어 번역입니다.","date":"2025-09-27 19:20:50+0900","lastModifiedAt":"2025-09-27 19:20:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3155/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3154-pickle_protocol_version_4","title":"[Final] PEP 3154 - Pickle protocol version 4","excerpt":"Python Enhancement Proposal 3154: 'Pickle protocol version 4'에 대한 한국어 번역입니다.","date":"2025-09-27 19:20:26+0900","lastModifiedAt":"2025-09-27 19:20:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3154/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3153-asynchronous_io_support","title":"[Superseded] PEP 3153 - Asynchronous IO support","excerpt":"Python Enhancement Proposal 3153: 'Asynchronous IO support'에 대한 한국어 번역입니다.","date":"2025-09-27 14:42:18+0900","lastModifiedAt":"2025-09-27 14:42:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3153/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3152-cofunctions","title":"[Rejected] PEP 3152 - Cofunctions","excerpt":"Python Enhancement Proposal 3152: 'Cofunctions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:41:43+0900","lastModifiedAt":"2025-09-27 14:41:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3152/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3151-reworking_the_os_and_io_exception_hierarchy","title":"[Final] PEP 3151 - Reworking the OS and IO exception hierarchy","excerpt":"Python Enhancement Proposal 3151: 'Reworking the OS and IO exception hierarchy'에 대한 한국어 번역입니다.","date":"2025-09-27 14:41:26+0900","lastModifiedAt":"2025-09-27 14:41:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3151/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3150-statement_local_namespaces_aka_given_clause","title":"[Deferred] PEP 3150 - Statement local namespaces (aka “given” clause)","excerpt":"Python Enhancement Proposal 3150: 'Statement local namespaces (aka “given” clause)'에 대한 한국어 번역입니다.","date":"2025-09-27 14:40:31+0900","lastModifiedAt":"2025-09-27 14:40:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3150/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3149-abi_version_tagged_.so_files","title":"[Final] PEP 3149 - ABI version tagged .so files","excerpt":"Python Enhancement Proposal 3149: 'ABI version tagged .so files'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:52+0900","lastModifiedAt":"2025-09-27 14:39:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3149/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3148-futures_-_execute_computations_asynchronously","title":"[Final] PEP 3148 - futures - execute computations asynchronously","excerpt":"Python Enhancement Proposal 3148: 'futures - execute computations asynchronously'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:26+0900","lastModifiedAt":"2025-09-27 14:39:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3148/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3147-pyc_repository_directories","title":"[Final] PEP 3147 - PYC Repository Directories","excerpt":"Python Enhancement Proposal 3147: 'PYC Repository Directories'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:02+0900","lastModifiedAt":"2025-09-27 14:39:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3147/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3146-merging_unladen_swallow_into_cpython","title":"[Withdrawn] PEP 3146 - Merging Unladen Swallow into CPython","excerpt":"Python Enhancement Proposal 3146: 'Merging Unladen Swallow into CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 14:38:28+0900","lastModifiedAt":"2025-09-27 14:38:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3146/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3145-asynchronous_io_for_subprocess.popen","title":"[Withdrawn] PEP 3145 - Asynchronous I/O For subprocess.Popen","excerpt":"Python Enhancement Proposal 3145: 'Asynchronous I/O For subprocess.Popen'에 대한 한국어 번역입니다.","date":"2025-09-27 14:37:24+0900","lastModifiedAt":"2025-09-27 14:37:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3145/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3144-ip_address_manipulation_library_for_the_python_standard_library","title":"[Final] PEP 3144 - IP Address Manipulation Library for the Python Standard Library","excerpt":"Python Enhancement Proposal 3144: 'IP Address Manipulation Library for the Python Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-27 14:37:11+0900","lastModifiedAt":"2025-09-27 14:37:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3144/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3143-standard_daemon_process_library","title":"[Deferred] PEP 3143 - Standard daemon process library","excerpt":"Python Enhancement Proposal 3143: 'Standard daemon process library'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:57+0900","lastModifiedAt":"2025-09-27 14:36:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3143/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3142-add_a_while_clause_to_generator_expressions","title":"[Rejected] PEP 3142 - Add a “while” clause to generator expressions","excerpt":"Python Enhancement Proposal 3142: 'Add a “while” clause to generator expressions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:16+0900","lastModifiedAt":"2025-09-27 14:36:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3142/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3141-a_type_hierarchy_for_numbers","title":"[Final] PEP 3141 - A Type Hierarchy for Numbers","excerpt":"Python Enhancement Proposal 3141: 'A Type Hierarchy for Numbers'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:02+0900","lastModifiedAt":"2025-09-27 14:36:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3141/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3140-strcontainer_should_call_stritem_not_repritem","title":"[Rejected] PEP 3140 - str(container) should call str(item), not repr(item)","excerpt":"Python Enhancement Proposal 3140: 'str(container) should call str(item), not repr(item)'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:43+0900","lastModifiedAt":"2025-09-27 14:35:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3140/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3139-cleaning_out_sys_and_the_interpreter_module","title":"[Rejected] PEP 3139 - Cleaning out sys and the “interpreter” module","excerpt":"Python Enhancement Proposal 3139: 'Cleaning out sys and the “interpreter” module'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:24+0900","lastModifiedAt":"2025-09-27 14:35:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3139/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3138-string_representation_in_python_3000","title":"[Final] PEP 3138 - String representation in Python 3000","excerpt":"Python Enhancement Proposal 3138: 'String representation in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:12+0900","lastModifiedAt":"2025-09-27 14:35:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3138/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3137-immutable_bytes_and_mutable_buffer","title":"[Final] PEP 3137 - Immutable Bytes and Mutable Buffer","excerpt":"Python Enhancement Proposal 3137: 'Immutable Bytes and Mutable Buffer'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:47+0900","lastModifiedAt":"2025-09-27 14:34:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3137/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3136-labeled_break_and_continue","title":"[Rejected] PEP 3136 - Labeled break and continue","excerpt":"Python Enhancement Proposal 3136: 'Labeled break and continue'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:24+0900","lastModifiedAt":"2025-09-27 14:34:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3136/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3135-new_super","title":"[Final] PEP 3135 - New Super","excerpt":"Python Enhancement Proposal 3135: 'New Super'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:00+0900","lastModifiedAt":"2025-09-27 14:34:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3135/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3134-exception_chaining_and_embedded_tracebacks","title":"[Final] PEP 3134 - Exception Chaining and Embedded Tracebacks","excerpt":"Python Enhancement Proposal 3134: 'Exception Chaining and Embedded Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-27 14:33:48+0900","lastModifiedAt":"2025-09-27 14:33:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3134/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3133-introducing_roles","title":"[Rejected] PEP 3133 - Introducing Roles","excerpt":"Python Enhancement Proposal 3133: 'Introducing Roles'에 대한 한국어 번역입니다.","date":"2025-09-27 14:33:16+0900","lastModifiedAt":"2025-09-27 14:33:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3133/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3132-extended_iterable_unpacking","title":"[Final] PEP 3132 - Extended Iterable Unpacking","excerpt":"Python Enhancement Proposal 3132: 'Extended Iterable Unpacking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:32:37+0900","lastModifiedAt":"2025-09-27 14:32:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3132/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3131-supporting_non-ascii_identifiers","title":"[Final] PEP 3131 - Supporting Non-ASCII Identifiers","excerpt":"Python Enhancement Proposal 3131: 'Supporting Non-ASCII Identifiers'에 대한 한국어 번역입니다.","date":"2025-09-27 14:32:23+0900","lastModifiedAt":"2025-09-27 14:32:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3131/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3130-access_to_current_moduleclassfunction","title":"[Rejected] PEP 3130 - Access to Current Module/Class/Function","excerpt":"Python Enhancement Proposal 3130: 'Access to Current Module/Class/Function'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:53+0900","lastModifiedAt":"2025-09-27 14:31:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3130/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3129-class_decorators","title":"[Final] PEP 3129 - Class Decorators","excerpt":"Python Enhancement Proposal 3129: 'Class Decorators'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:32+0900","lastModifiedAt":"2025-09-27 14:31:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3129/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3128-blist_a_faster_list-like_type","title":"[Rejected] PEP 3128 - BList: A Faster List-like Type","excerpt":"Python Enhancement Proposal 3128: 'BList: A Faster List-like Type'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:22+0900","lastModifiedAt":"2025-09-27 14:31:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3128/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3127-integer_literal_support_and_syntax","title":"[Final] PEP 3127 - Integer Literal Support and Syntax","excerpt":"Python Enhancement Proposal 3127: 'Integer Literal Support and Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 14:30:46+0900","lastModifiedAt":"2025-09-27 14:30:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3127/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3126-remove_implicit_string_concatenation","title":"[Rejected] PEP 3126 - Remove Implicit String Concatenation","excerpt":"Python Enhancement Proposal 3126: 'Remove Implicit String Concatenation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:30:11+0900","lastModifiedAt":"2025-09-27 14:30:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3126/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3125-remove_backslash_continuation","title":"[Rejected] PEP 3125 - Remove Backslash Continuation","excerpt":"Python Enhancement Proposal 3125: 'Remove Backslash Continuation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:29:48+0900","lastModifiedAt":"2025-09-27 14:29:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3125/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3124-overloading_generic_functions_interfaces_and_adaptation","title":"[Deferred] PEP 3124 - Overloading, Generic Functions, Interfaces, and Adaptation","excerpt":"Python Enhancement Proposal 3124: 'Overloading, Generic Functions, Interfaces, and Adaptation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:29:34+0900","lastModifiedAt":"2025-09-27 14:29:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3124/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3123-making_pyobject_head_conform_to_standard_c","title":"[Final] PEP 3123 - Making PyObject_HEAD conform to standard C","excerpt":"Python Enhancement Proposal 3123: 'Making PyObject_HEAD conform to standard C'에 대한 한국어 번역입니다.","date":"2025-09-27 14:28:28+0900","lastModifiedAt":"2025-09-27 14:28:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3123/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3122-delineation_of_the_main_module","title":"[Rejected] PEP 3122 - Delineation of the main module","excerpt":"Python Enhancement Proposal 3122: 'Delineation of the main module'에 대한 한국어 번역입니다.","date":"2025-09-27 14:28:16+0900","lastModifiedAt":"2025-09-27 14:28:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3122/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3121-extension_module_initialization_and_finalization","title":"[Final] PEP 3121 - Extension Module Initialization and Finalization","excerpt":"Python Enhancement Proposal 3121: 'Extension Module Initialization and Finalization'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:55+0900","lastModifiedAt":"2025-09-27 14:27:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3121/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3120-using_utf-8_as_the_default_source_encoding","title":"[Final] PEP 3120 - Using UTF-8 as the default source encoding","excerpt":"Python Enhancement Proposal 3120: 'Using UTF-8 as the default source encoding'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:43+0900","lastModifiedAt":"2025-09-27 14:27:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3120/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3119-introducing_abstract_base_classes","title":"[Final] PEP 3119 - Introducing Abstract Base Classes","excerpt":"Python Enhancement Proposal 3119: 'Introducing Abstract Base Classes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:33+0900","lastModifiedAt":"2025-09-27 14:27:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3119/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3118-revising_the_buffer_protocol","title":"[Final] PEP 3118 - Revising the buffer protocol","excerpt":"Python Enhancement Proposal 3118: 'Revising the buffer protocol'에 대한 한국어 번역입니다.","date":"2025-09-27 14:26:38+0900","lastModifiedAt":"2025-09-27 14:26:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3118/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3117-postfix_type_declarations","title":"[Rejected] PEP 3117 - Postfix type declarations","excerpt":"Python Enhancement Proposal 3117: 'Postfix type declarations'에 대한 한국어 번역입니다.","date":"2025-09-27 14:25:21+0900","lastModifiedAt":"2025-09-27 14:25:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3117/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3116-new_io","title":"[Final] PEP 3116 - New I/O","excerpt":"Python Enhancement Proposal 3116: 'New I/O'에 대한 한국어 번역입니다.","date":"2025-09-27 14:24:50+0900","lastModifiedAt":"2025-09-27 14:24:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3116/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3115-metaclasses_in_python_3000","title":"[Final] PEP 3115 - Metaclasses in Python 3000","excerpt":"Python Enhancement Proposal 3115: 'Metaclasses in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:47+0900","lastModifiedAt":"2025-09-27 14:22:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3115/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3114-renaming_iterator.next_to_iterator.__next","title":"[Final] PEP 3114 - Renaming iterator.next() to iterator.__next__()","excerpt":"Python Enhancement Proposal 3114: 'Renaming iterator.next() to iterator.__next__()'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:21+0900","lastModifiedAt":"2025-09-27 14:22:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3114/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3113-removal_of_tuple_parameter_unpacking","title":"[Final] PEP 3113 - Removal of Tuple Parameter Unpacking","excerpt":"Python Enhancement Proposal 3113: 'Removal of Tuple Parameter Unpacking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:03+0900","lastModifiedAt":"2025-09-27 14:22:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3113/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3112-bytes_literals_in_python_3000","title":"[Final] PEP 3112 - Bytes literals in Python 3000","excerpt":"Python Enhancement Proposal 3112: 'Bytes literals in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:44+0900","lastModifiedAt":"2025-09-27 14:21:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3112/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3111-simple_input_built-in_in_python_3000","title":"[Final] PEP 3111 - Simple input built-in in Python 3000","excerpt":"Python Enhancement Proposal 3111: 'Simple input built-in in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:32+0900","lastModifiedAt":"2025-09-27 14:21:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3111/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3110-catching_exceptions_in_python_3000","title":"[Final] PEP 3110 - Catching Exceptions in Python 3000","excerpt":"Python Enhancement Proposal 3110: 'Catching Exceptions in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:16+0900","lastModifiedAt":"2025-09-27 14:21:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3110/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3109-raising_exceptions_in_python_3000","title":"[Final] PEP 3109 - Raising Exceptions in Python 3000","excerpt":"Python Enhancement Proposal 3109: 'Raising Exceptions in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:02+0900","lastModifiedAt":"2025-09-27 14:21:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3109/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3108-standard_library_reorganization","title":"[Final] PEP 3108 - Standard Library Reorganization","excerpt":"Python Enhancement Proposal 3108: 'Standard Library Reorganization'에 대한 한국어 번역입니다.","date":"2025-09-27 14:20:43+0900","lastModifiedAt":"2025-09-27 14:20:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3108/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3107-function_annotations","title":"[Final] PEP 3107 - Function Annotations","excerpt":"Python Enhancement Proposal 3107: 'Function Annotations'에 대한 한국어 번역입니다.","date":"2025-09-27 14:19:43+0900","lastModifiedAt":"2025-09-27 14:19:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3107/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3106-revamping_dict.keys_.values_and_.items","title":"[Final] PEP 3106 - Revamping dict.keys(), .values() and .items()","excerpt":"Python Enhancement Proposal 3106: 'Revamping dict.keys(), .values() and .items()'에 대한 한국어 번역입니다.","date":"2025-09-27 14:18:13+0900","lastModifiedAt":"2025-09-27 14:18:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3106/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3105-make_print_a_function","title":"[Final] PEP 3105 - Make print a function","excerpt":"Python Enhancement Proposal 3105: 'Make print a function'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:54+0900","lastModifiedAt":"2025-09-27 14:17:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3105/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3104-access_to_names_in_outer_scopes","title":"[Final] PEP 3104 - Access to Names in Outer Scopes","excerpt":"Python Enhancement Proposal 3104: 'Access to Names in Outer Scopes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:40+0900","lastModifiedAt":"2025-09-27 14:17:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3104/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3103-a_switchcase_statement","title":"[Rejected] PEP 3103 - A Switch/Case Statement","excerpt":"Python Enhancement Proposal 3103: 'A Switch/Case Statement'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:03+0900","lastModifiedAt":"2025-09-27 14:17:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3103/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3102-keyword-only_arguments","title":"[Final] PEP 3102 - Keyword-Only Arguments","excerpt":"Python Enhancement Proposal 3102: 'Keyword-Only Arguments'에 대한 한국어 번역입니다.","date":"2025-09-27 14:16:28+0900","lastModifiedAt":"2025-09-27 14:16:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3102/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3101-advanced_string_formatting","title":"[Final] PEP 3101 - Advanced String Formatting","excerpt":"Python Enhancement Proposal 3101: 'Advanced String Formatting'에 대한 한국어 번역입니다.","date":"2025-09-27 14:16:14+0900","lastModifiedAt":"2025-09-27 14:16:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3101/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3100-miscellaneous_python_3.0_plans","title":"[Final] PEP 3100 - Miscellaneous Python 3.0 Plans","excerpt":"Python Enhancement Proposal 3100: 'Miscellaneous Python 3.0 Plans'에 대한 한국어 번역입니다.","date":"2025-09-27 14:13:27+0900","lastModifiedAt":"2025-09-27 14:13:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3100/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3099-things_that_will_not_change_in_python_3000","title":"[Final] PEP 3099 - Things that will Not Change in Python 3000","excerpt":"Python Enhancement Proposal 3099: 'Things that will Not Change in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:36+0900","lastModifiedAt":"2025-09-27 14:12:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3099/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3003-python_language_moratorium","title":"[Final] PEP 3003 - Python Language Moratorium","excerpt":"Python Enhancement Proposal 3003: 'Python Language Moratorium'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:18+0900","lastModifiedAt":"2025-09-27 14:12:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3003/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3002-procedure_for_backwards-incompatible_changes","title":"[Final] PEP 3002 - Procedure for Backwards-Incompatible Changes","excerpt":"Python Enhancement Proposal 3002: 'Procedure for Backwards-Incompatible Changes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:06+0900","lastModifiedAt":"2025-09-27 14:12:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3002/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3001-procedure_for_reviewing_and_improving_standard_library_modules","title":"[Withdrawn] PEP 3001 - Procedure for reviewing and improving standard library modules","excerpt":"Python Enhancement Proposal 3001: 'Procedure for reviewing and improving standard library modules'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:53+0900","lastModifiedAt":"2025-09-27 14:11:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3001/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-3000-python_3000","title":"[Final] PEP 3000 - Python 3000","excerpt":"Python Enhancement Proposal 3000: 'Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:40+0900","lastModifiedAt":"2025-09-27 14:11:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3000/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-2026-calendar_versioning_for_python","title":"[Rejected] PEP 2026 - Calendar versioning for Python","excerpt":"Python Enhancement Proposal 2026: 'Calendar versioning for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:21+0900","lastModifiedAt":"2025-09-27 14:11:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/2026/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0806-mixed_syncasync_context_managers_with_precise_async_marking","title":"[Draft] PEP 806 - Mixed sync/async context managers with precise async marking","excerpt":"Python Enhancement Proposal 806: 'Mixed sync/async context managers with precise async marking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:10:48+0900","lastModifiedAt":"2025-09-27 14:10:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/806/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0804-an_external_dependency_registry_and_name_mapping_mechanism","title":"[Draft] PEP 804 - An external dependency registry and name mapping mechanism","excerpt":"Python Enhancement Proposal 804: 'An external dependency registry and name mapping mechanism'에 대한 한국어 번역입니다.","date":"2025-09-27 14:10:24+0900","lastModifiedAt":"2025-09-27 14:10:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/804/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0803-stable_abi_for_free-threaded_builds","title":"[Draft] PEP 803 - Stable ABI for Free-Threaded Builds","excerpt":"Python Enhancement Proposal 803: 'Stable ABI for Free-Threaded Builds'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:56+0900","lastModifiedAt":"2025-09-27 14:09:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/803/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0802-display_syntax_for_the_empty_set","title":"[Draft] PEP 802 - Display Syntax for the Empty Set","excerpt":"Python Enhancement Proposal 802: 'Display Syntax for the Empty Set'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:30+0900","lastModifiedAt":"2025-09-27 14:09:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/802/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0801-reserved","title":"[Active] PEP 801 - Reserved","excerpt":"Python Enhancement Proposal 801: 'Reserved'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:12+0900","lastModifiedAt":"2025-09-27 14:09:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/801/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0800-disjoint_bases_in_the_type_system","title":"[Draft] PEP 800 - Disjoint bases in the type system","excerpt":"Python Enhancement Proposal 800: 'Disjoint bases in the type system'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:06+0900","lastModifiedAt":"2025-09-27 14:09:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/800/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0799-a_dedicatedprofilingpackage_for_organizing_python_profiling_tools","title":"[Draft] PEP 799 - A dedicatedprofilingpackage for organizing Python profiling tools","excerpt":"Python Enhancement Proposal 799: 'A dedicatedprofilingpackage for organizing Python profiling tools'에 대한 한국어 번역입니다.","date":"2025-09-27 14:08:50+0900","lastModifiedAt":"2025-09-27 14:08:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/799/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0798-unpacking_in_comprehensions","title":"[Draft] PEP 798 - Unpacking in Comprehensions","excerpt":"Python Enhancement Proposal 798: 'Unpacking in Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:08:32+0900","lastModifiedAt":"2025-09-27 14:08:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/798/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0794-import_name_metadata","title":"[Accepted] PEP 794 - Import Name Metadata","excerpt":"Python Enhancement Proposal 794: 'Import Name Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 14:07:32+0900","lastModifiedAt":"2025-09-27 14:07:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/794/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0793-pymodexport_a_new_entry_point_for_c_extension_modules","title":"[Draft] PEP 793 - PyModExport: A new entry point for C extension modules","excerpt":"Python Enhancement Proposal 793: 'PyModExport: A new entry point for C extension modules'에 대한 한국어 번역입니다.","date":"2025-09-27 14:07:12+0900","lastModifiedAt":"2025-09-27 14:07:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/793/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0792-project_status_markers_in_the_simple_index","title":"[Final] PEP 792 - Project status markers in the simple index","excerpt":"Python Enhancement Proposal 792: 'Project status markers in the simple index'에 대한 한국어 번역입니다.","date":"2025-09-27 14:05:09+0900","lastModifiedAt":"2025-09-27 14:05:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/792/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0791-intmath_module_for_integer-specific_mathematics_functions","title":"[Draft] PEP 791 - intmath — module for integer-specific mathematics functions","excerpt":"Python Enhancement Proposal 791: 'intmath — module for integer-specific mathematics functions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:51+0900","lastModifiedAt":"2025-09-27 14:04:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/791/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0790-python_3.15_release_schedule","title":"[Active] PEP 790 - Python 3.15 Release Schedule","excerpt":"Python Enhancement Proposal 790: 'Python 3.15 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:34+0900","lastModifiedAt":"2025-09-27 14:04:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/790/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0789-preventing_task-cancellation_bugs_by_limiting_yield_in_async_generators","title":"[Draft] PEP 789 - Preventing task-cancellation bugs by limiting yield in async generators","excerpt":"Python Enhancement Proposal 789: 'Preventing task-cancellation bugs by limiting yield in async generators'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:23+0900","lastModifiedAt":"2025-09-27 14:04:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/789/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0788-pyinterpreterref_interpreter_references_in_the_c_api","title":"[Draft] PEP 788 - PyInterpreterRef: Interpreter References in the C API","excerpt":"Python Enhancement Proposal 788: 'PyInterpreterRef: Interpreter References in the C API'에 대한 한국어 번역입니다.","date":"2025-09-27 14:03:48+0900","lastModifiedAt":"2025-09-27 14:03:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/788/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0787-safer_subprocess_usage_using_t-strings","title":"[Deferred] PEP 787 - Safer subprocess usage using t-strings","excerpt":"Python Enhancement Proposal 787: 'Safer subprocess usage using t-strings'에 대한 한국어 번역입니다.","date":"2025-09-27 14:00:15+0900","lastModifiedAt":"2025-09-27 14:00:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/787/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0785-new_methods_for_easier_handling_ofexceptiongroups","title":"[Draft] PEP 785 - New methods for easier handling ofExceptionGroups","excerpt":"Python Enhancement Proposal 785: 'New methods for easier handling ofExceptionGroups'에 대한 한국어 번역입니다.","date":"2025-09-27 13:59:04+0900","lastModifiedAt":"2025-09-27 13:59:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/785/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0784-adding_zstandard_to_the_standard_library","title":"[Final] PEP 784 - Adding Zstandard to the standard library","excerpt":"Python Enhancement Proposal 784: 'Adding Zstandard to the standard library'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:47+0900","lastModifiedAt":"2025-09-27 13:58:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/784/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0783-emscripten_packaging","title":"[Draft] PEP 783 - Emscripten Packaging","excerpt":"Python Enhancement Proposal 783: 'Emscripten Packaging'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:16+0900","lastModifiedAt":"2025-09-27 13:58:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/783/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0782-add_pybyteswriter_c_api","title":"[Final] PEP 782 - Add PyBytesWriter C API","excerpt":"Python Enhancement Proposal 782: 'Add PyBytesWriter C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:01+0900","lastModifiedAt":"2025-09-27 13:58:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/782/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0781-maketype_checkinga_built-in_constant","title":"[Draft] PEP 781 - MakeTYPE_CHECKINGa built-in constant","excerpt":"Python Enhancement Proposal 781: 'MakeTYPE_CHECKINGa built-in constant'에 대한 한국어 번역입니다.","date":"2025-09-27 13:57:36+0900","lastModifiedAt":"2025-09-27 13:57:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/781/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0780-abi_features_as_environment_markers","title":"[Draft] PEP 780 - ABI features as environment markers","excerpt":"Python Enhancement Proposal 780: 'ABI features as environment markers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:57:13+0900","lastModifiedAt":"2025-09-27 13:57:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/780/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0779-criteria_for_supported_status_for_free-threaded_python","title":"[Accepted] PEP 779 - Criteria for supported status for free-threaded Python","excerpt":"Python Enhancement Proposal 779: 'Criteria for supported status for free-threaded Python'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:46+0900","lastModifiedAt":"2025-09-27 13:56:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/779/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0778-supporting_symlinks_in_wheels","title":"[Deferred] PEP 778 - Supporting Symlinks in Wheels","excerpt":"Python Enhancement Proposal 778: 'Supporting Symlinks in Wheels'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:34+0900","lastModifiedAt":"2025-09-27 13:56:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/778/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0777-how_to_re-invent_the_wheel","title":"[Draft] PEP 777 - How to Re-invent the Wheel","excerpt":"Python Enhancement Proposal 777: 'How to Re-invent the Wheel'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:09+0900","lastModifiedAt":"2025-09-27 13:56:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/777/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0776-emscripten_support","title":"[Draft] PEP 776 - Emscripten Support","excerpt":"Python Enhancement Proposal 776: 'Emscripten Support'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:47+0900","lastModifiedAt":"2025-09-27 13:55:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/776/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0775-make_zlib_required_to_build_cpython","title":"[Withdrawn] PEP 775 - Make zlib required to build CPython","excerpt":"Python Enhancement Proposal 775: 'Make zlib required to build CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:17+0900","lastModifiedAt":"2025-09-27 13:55:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/775/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0774-removing_the_llvm_requirement_for_jit_builds","title":"[Deferred] PEP 774 - Removing the LLVM requirement for JIT builds","excerpt":"Python Enhancement Proposal 774: 'Removing the LLVM requirement for JIT builds'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:06+0900","lastModifiedAt":"2025-09-27 13:55:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/774/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0773-a_python_installation_manager_for_windows","title":"[Accepted] PEP 773 - A Python Installation Manager for Windows","excerpt":"Python Enhancement Proposal 773: 'A Python Installation Manager for Windows'에 대한 한국어 번역입니다.","date":"2025-09-27 13:54:28+0900","lastModifiedAt":"2025-09-27 13:54:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/773/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0772-packaging_council_governance_process","title":"[Draft] PEP 772 - Packaging Council governance process","excerpt":"Python Enhancement Proposal 772: 'Packaging Council governance process'에 대한 한국어 번역입니다.","date":"2025-09-27 13:53:40+0900","lastModifiedAt":"2025-09-27 13:53:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/772/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0771-default_extras_for_python_software_packages","title":"[Draft] PEP 771 - Default Extras for Python Software Packages","excerpt":"Python Enhancement Proposal 771: 'Default Extras for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:52:58+0900","lastModifiedAt":"2025-09-27 13:52:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/771/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0770-improving_measurability_of_python_packages_with_software_bill-of-materials","title":"[Accepted] PEP 770 - Improving measurability of Python packages with Software Bill-of-Materials","excerpt":"Python Enhancement Proposal 770: 'Improving measurability of Python packages with Software Bill-of-Materials'에 대한 한국어 번역입니다.","date":"2025-09-27 13:51:21+0900","lastModifiedAt":"2025-09-27 13:51:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/770/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0769-add_a_default_keyword_argument_to_attrgetter_itemgetter_and_getitem","title":"[Rejected] PEP 769 - Add a ‘default’ keyword argument to ‘attrgetter’, ‘itemgetter’ and ‘getitem’","excerpt":"Python Enhancement Proposal 769: 'Add a ‘default’ keyword argument to ‘attrgetter’, ‘itemgetter’ and ‘getitem’'에 대한 한국어 번역입니다.","date":"2025-09-27 13:50:33+0900","lastModifiedAt":"2025-09-27 13:50:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/769/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0768-safe_external_debugger_interface_for_cpython","title":"[Accepted] PEP 768 - Safe external debugger interface for CPython","excerpt":"Python Enhancement Proposal 768: 'Safe external debugger interface for CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:50:05+0900","lastModifiedAt":"2025-09-27 13:50:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/768/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0767-annotating_read-only_attributes","title":"[Draft] PEP 767 - Annotating Read-Only Attributes","excerpt":"Python Enhancement Proposal 767: 'Annotating Read-Only Attributes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:49:33+0900","lastModifiedAt":"2025-09-27 13:49:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/767/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0766-explicit_priority_choices_among_multiple_indexes","title":"[Draft] PEP 766 - Explicit Priority Choices Among Multiple Indexes","excerpt":"Python Enhancement Proposal 766: 'Explicit Priority Choices Among Multiple Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:46:56+0900","lastModifiedAt":"2025-09-27 13:46:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/766/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0765-disallow_returnbreakcontinue_that_exit_a_finally_block","title":"[Final] PEP 765 - Disallow return/break/continue that exit a finally block","excerpt":"Python Enhancement Proposal 765: 'Disallow return/break/continue that exit a finally block'에 대한 한국어 번역입니다.","date":"2025-09-27 13:46:10+0900","lastModifiedAt":"2025-09-27 13:46:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/765/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0764-inline_typed_dictionaries","title":"[Draft] PEP 764 - Inline typed dictionaries","excerpt":"Python Enhancement Proposal 764: 'Inline typed dictionaries'에 대한 한국어 번역입니다.","date":"2025-09-27 13:45:41+0900","lastModifiedAt":"2025-09-27 13:45:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/764/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0763-limiting_deletions_on_pypi","title":"[Draft] PEP 763 - Limiting deletions on PyPI","excerpt":"Python Enhancement Proposal 763: 'Limiting deletions on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:45:17+0900","lastModifiedAt":"2025-09-27 13:45:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/763/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0762-repl-acing_the_default_repl","title":"[Final] PEP 762 - REPL-acing the default REPL","excerpt":"Python Enhancement Proposal 762: 'REPL-acing the default REPL'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:46+0900","lastModifiedAt":"2025-09-27 13:44:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/762/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0761-deprecating_pgp_signatures_for_cpython_artifacts","title":"[Active] PEP 761 - Deprecating PGP signatures for CPython artifacts","excerpt":"Python Enhancement Proposal 761: 'Deprecating PGP signatures for CPython artifacts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:29+0900","lastModifiedAt":"2025-09-27 13:44:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/761/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0760-no_more_bare_excepts","title":"[Withdrawn] PEP 760 - No More Bare Excepts","excerpt":"Python Enhancement Proposal 760: 'No More Bare Excepts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:07+0900","lastModifiedAt":"2025-09-27 13:44:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/760/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0759-external_wheel_hosting","title":"[Withdrawn] PEP 759 - External Wheel Hosting","excerpt":"Python Enhancement Proposal 759: 'External Wheel Hosting'에 대한 한국어 번역입니다.","date":"2025-09-27 13:43:53+0900","lastModifiedAt":"2025-09-27 13:43:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/759/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0758-allowexceptandexceptexpressions_without_parentheses","title":"[Final] PEP 758 - Allowexceptandexcept*expressions without parentheses","excerpt":"Python Enhancement Proposal 758: 'Allowexceptandexcept*expressions without parentheses'에 대한 한국어 번역입니다.","date":"2025-09-27 13:43:13+0900","lastModifiedAt":"2025-09-27 13:43:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/758/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0757-c_api_to_import-export_python_integers","title":"[Final] PEP 757 - C API to import-export Python integers","excerpt":"Python Enhancement Proposal 757: 'C API to import-export Python integers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:42:57+0900","lastModifiedAt":"2025-09-27 13:42:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/757/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0756-add_pyunicode_export_and_pyunicode_import_c_functions","title":"[Withdrawn] PEP 756 - Add PyUnicode_Export() and PyUnicode_Import() C functions","excerpt":"Python Enhancement Proposal 756: 'Add PyUnicode_Export() and PyUnicode_Import() C functions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:47+0900","lastModifiedAt":"2025-09-27 13:41:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/756/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0755-implicit_namespace_policy_for_pypi","title":"[Draft] PEP 755 - Implicit namespace policy for PyPI","excerpt":"Python Enhancement Proposal 755: 'Implicit namespace policy for PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:27+0900","lastModifiedAt":"2025-09-27 13:41:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/755/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0754-ieee_754_floating_point_special_values","title":"[Rejected] PEP 754 - IEEE 754 Floating Point Special Values","excerpt":"Python Enhancement Proposal 754: 'IEEE 754 Floating Point Special Values'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:12+0900","lastModifiedAt":"2025-09-27 13:41:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/754/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0753-uniform_project_urls_in_core_metadata","title":"[Accepted] PEP 753 - Uniform project URLs in core metadata","excerpt":"Python Enhancement Proposal 753: 'Uniform project URLs in core metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:57+0900","lastModifiedAt":"2025-09-27 13:40:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/753/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0752-implicit_namespaces_for_package_repositories","title":"[Draft] PEP 752 - Implicit namespaces for package repositories","excerpt":"Python Enhancement Proposal 752: 'Implicit namespaces for package repositories'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:26+0900","lastModifiedAt":"2025-09-27 13:40:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/752/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0751-a_file_format_to_record_python_dependencies_for_installation_reproducibility","title":"[Final] PEP 751 - A file format to record Python dependencies for installation reproducibility","excerpt":"Python Enhancement Proposal 751: 'A file format to record Python dependencies for installation reproducibility'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:01+0900","lastModifiedAt":"2025-09-27 13:40:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/751/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0750-template_strings","title":"[Final] PEP 750 - Template Strings","excerpt":"Python Enhancement Proposal 750: 'Template Strings'에 대한 한국어 번역입니다.","date":"2025-09-27 13:38:50+0900","lastModifiedAt":"2025-09-27 13:38:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/750/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0749-implementing_pep_649","title":"[Accepted] PEP 749 - Implementing PEP 649","excerpt":"Python Enhancement Proposal 749: 'Implementing PEP 649'에 대한 한국어 번역입니다.","date":"2025-09-27 13:37:09+0900","lastModifiedAt":"2025-09-27 13:37:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/749/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0748-a_unified_tls_api_for_python","title":"[Draft] PEP 748 - A Unified TLS API for Python","excerpt":"Python Enhancement Proposal 748: 'A Unified TLS API for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 13:35:46+0900","lastModifiedAt":"2025-09-27 13:35:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/748/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0747-annotating_type_forms","title":"[Draft] PEP 747 - Annotating Type Forms","excerpt":"Python Enhancement Proposal 747: 'Annotating Type Forms'에 대한 한국어 번역입니다.","date":"2025-09-27 13:35:04+0900","lastModifiedAt":"2025-09-27 13:35:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/747/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0746-type_checking_annotated_metadata","title":"[Draft] PEP 746 - Type checking Annotated metadata","excerpt":"Python Enhancement Proposal 746: 'Type checking Annotated metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:43+0900","lastModifiedAt":"2025-09-27 13:34:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/746/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0745-python_3.14_release_schedule","title":"[Active] PEP 745 - Python 3.14 Release Schedule","excerpt":"Python Enhancement Proposal 745: 'Python 3.14 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:29+0900","lastModifiedAt":"2025-09-27 13:34:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/745/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0744-jit_compilation","title":"[Draft] PEP 744 - JIT Compilation","excerpt":"Python Enhancement Proposal 744: 'JIT Compilation'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:20+0900","lastModifiedAt":"2025-09-27 13:34:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/744/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0743-add_py_compat_api_version_to_the_python_c_api","title":"[Draft] PEP 743 - Add Py_COMPAT_API_VERSION to the Python C API","excerpt":"Python Enhancement Proposal 743: 'Add Py_COMPAT_API_VERSION to the Python C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:33:51+0900","lastModifiedAt":"2025-09-27 13:33:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/743/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0742-narrowing_types_with_typeis","title":"[Final] PEP 742 - Narrowing types with TypeIs","excerpt":"Python Enhancement Proposal 742: 'Narrowing types with TypeIs'에 대한 한국어 번역입니다.","date":"2025-09-27 13:33:17+0900","lastModifiedAt":"2025-09-27 13:33:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/742/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0741-python_configuration_c_api","title":"[Final] PEP 741 - Python Configuration C API","excerpt":"Python Enhancement Proposal 741: 'Python Configuration C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:32:38+0900","lastModifiedAt":"2025-09-27 13:32:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/741/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0740-index_support_for_digital_attestations","title":"[Final] PEP 740 - Index support for digital attestations","excerpt":"Python Enhancement Proposal 740: 'Index support for digital attestations'에 대한 한국어 번역입니다.","date":"2025-09-27 13:31:46+0900","lastModifiedAt":"2025-09-27 13:31:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/740/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0739-build-details.json1.0_a_static_description_file_for_python_build_details","title":"[Accepted] PEP 739 - build-details.json1.0 — a static description file for Python build details","excerpt":"Python Enhancement Proposal 739: 'build-details.json1.0 — a static description file for Python build details'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:50+0900","lastModifiedAt":"2025-09-27 13:28:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/739/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0738-adding_android_as_a_supported_platform","title":"[Final] PEP 738 - Adding Android as a supported platform","excerpt":"Python Enhancement Proposal 738: 'Adding Android as a supported platform'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:23+0900","lastModifiedAt":"2025-09-27 13:28:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/738/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0737-c_api_to_format_a_type_fully_qualified_name","title":"[Final] PEP 737 - C API to format a type fully qualified name","excerpt":"Python Enhancement Proposal 737: 'C API to format a type fully qualified name'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:04+0900","lastModifiedAt":"2025-09-27 13:28:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/737/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0736-shorthand_syntax_for_keyword_arguments_at_invocation","title":"[Rejected] PEP 736 - Shorthand syntax for keyword arguments at invocation","excerpt":"Python Enhancement Proposal 736: 'Shorthand syntax for keyword arguments at invocation'에 대한 한국어 번역입니다.","date":"2025-09-27 13:27:17+0900","lastModifiedAt":"2025-09-27 13:27:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/736/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0735-dependency_groups_in_pyproject.toml","title":"[Final] PEP 735 - Dependency Groups in pyproject.toml","excerpt":"Python Enhancement Proposal 735: 'Dependency Groups in pyproject.toml'에 대한 한국어 번역입니다.","date":"2025-09-27 13:26:47+0900","lastModifiedAt":"2025-09-27 13:26:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/735/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0734-multiple_interpreters_in_the_stdlib","title":"[Final] PEP 734 - Multiple Interpreters in the Stdlib","excerpt":"Python Enhancement Proposal 734: 'Multiple Interpreters in the Stdlib'에 대한 한국어 번역입니다.","date":"2025-09-27 13:25:59+0900","lastModifiedAt":"2025-09-27 13:25:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/734/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0733-an_evaluation_of_pythons_public_c_api","title":"[Final] PEP 733 - An Evaluation of Python’s Public C API","excerpt":"Python Enhancement Proposal 733: 'An Evaluation of Python’s Public C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:23:51+0900","lastModifiedAt":"2025-09-27 13:23:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/733/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0732-the_python_documentation_editorial_board","title":"[Active] PEP 732 - The Python Documentation Editorial Board","excerpt":"Python Enhancement Proposal 732: 'The Python Documentation Editorial Board'에 대한 한국어 번역입니다.","date":"2025-09-27 13:23:00+0900","lastModifiedAt":"2025-09-27 13:23:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/732/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0731-c_api_working_group_charter","title":"[Active] PEP 731 - C API Working Group Charter","excerpt":"Python Enhancement Proposal 731: 'C API Working Group Charter'에 대한 한국어 번역입니다.","date":"2025-09-27 13:22:44+0900","lastModifiedAt":"2025-09-27 13:22:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/731/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0730-adding_ios_as_a_supported_platform","title":"[Final] PEP 730 - Adding iOS as a supported platform","excerpt":"Python Enhancement Proposal 730: 'Adding iOS as a supported platform'에 대한 한국어 번역입니다.","date":"2025-09-27 13:22:30+0900","lastModifiedAt":"2025-09-27 13:22:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/730/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0729-typing_governance_process","title":"[Active] PEP 729 - Typing governance process","excerpt":"Python Enhancement Proposal 729: 'Typing governance process'에 대한 한국어 번역입니다.","date":"2025-09-27 13:21:49+0900","lastModifiedAt":"2025-09-27 13:21:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/729/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0728-typeddict_with_typed_extra_items","title":"[Accepted] PEP 728 - TypedDict with Typed Extra Items","excerpt":"Python Enhancement Proposal 728: 'TypedDict with Typed Extra Items'에 대한 한국어 번역입니다.","date":"2025-09-27 13:20:42+0900","lastModifiedAt":"2025-09-27 13:20:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/728/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0727-documentation_in_annotated_metadata","title":"[Withdrawn] PEP 727 - Documentation in Annotated Metadata","excerpt":"Python Enhancement Proposal 727: 'Documentation in Annotated Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:19:38+0900","lastModifiedAt":"2025-09-27 13:19:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/727/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0726-module__setattr__and__delattr","title":"[Rejected] PEP 726 - Module__setattr__and__delattr__","excerpt":"Python Enhancement Proposal 726: 'Module__setattr__and__delattr__'에 대한 한국어 번역입니다.","date":"2025-09-27 13:19:13+0900","lastModifiedAt":"2025-09-27 13:19:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/726/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0725-specifying_external_dependencies_in_pyproject.toml","title":"[Draft] PEP 725 - Specifying external dependencies in pyproject.toml","excerpt":"Python Enhancement Proposal 725: 'Specifying external dependencies in pyproject.toml'에 대한 한국어 번역입니다.","date":"2025-09-27 13:18:53+0900","lastModifiedAt":"2025-09-27 13:18:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/725/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0724-stricter_type_guards","title":"[Withdrawn] PEP 724 - Stricter Type Guards","excerpt":"Python Enhancement Proposal 724: 'Stricter Type Guards'에 대한 한국어 번역입니다.","date":"2025-09-27 13:17:29+0900","lastModifiedAt":"2025-09-27 13:17:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/724/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0723-inline_script_metadata","title":"[Final] PEP 723 - Inline script metadata","excerpt":"Python Enhancement Proposal 723: 'Inline script metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:17:06+0900","lastModifiedAt":"2025-09-27 13:17:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/723/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0722-dependency_specification_for_single-file_scripts","title":"[Rejected] PEP 722 - Dependency specification for single-file scripts","excerpt":"Python Enhancement Proposal 722: 'Dependency specification for single-file scripts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:16:11+0900","lastModifiedAt":"2025-09-27 13:16:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/722/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0721-using_tarfile.data_filter_for_source_distribution_extraction","title":"[Final] PEP 721 - Using tarfile.data_filter for source distribution extraction","excerpt":"Python Enhancement Proposal 721: 'Using tarfile.data_filter for source distribution extraction'에 대한 한국어 번역입니다.","date":"2025-09-27 13:15:15+0900","lastModifiedAt":"2025-09-27 13:15:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/721/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0720-cross-compiling_python_packages","title":"[Draft] PEP 720 - Cross-compiling Python packages","excerpt":"Python Enhancement Proposal 720: 'Cross-compiling Python packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:14:54+0900","lastModifiedAt":"2025-09-27 13:14:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/720/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0719-python_3.13_release_schedule","title":"[Active] PEP 719 - Python 3.13 Release Schedule","excerpt":"Python Enhancement Proposal 719: 'Python 3.13 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 13:14:04+0900","lastModifiedAt":"2025-09-27 13:14:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/719/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0718-subscriptable_functions","title":"[Draft] PEP 718 - Subscriptable functions","excerpt":"Python Enhancement Proposal 718: 'Subscriptable functions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:52+0900","lastModifiedAt":"2025-09-27 13:13:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/718/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0715-disabling_bdist_egg_distribution_uploads_on_pypi","title":"[Final] PEP 715 - Disabling bdist_egg distribution uploads on PyPI","excerpt":"Python Enhancement Proposal 715: 'Disabling bdist_egg distribution uploads on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:33+0900","lastModifiedAt":"2025-09-27 13:13:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/715/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0714-rename_dist-info-metadata_in_the_simple_api","title":"[Accepted] PEP 714 - Rename dist-info-metadata in the Simple API","excerpt":"Python Enhancement Proposal 714: 'Rename dist-info-metadata in the Simple API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:16+0900","lastModifiedAt":"2025-09-27 13:13:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/714/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0713-callable_modules","title":"[Rejected] PEP 713 - Callable Modules","excerpt":"Python Enhancement Proposal 713: 'Callable Modules'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:51+0900","lastModifiedAt":"2025-09-27 13:12:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/713/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0712-adding_a_converter_parameter_to_dataclasses.field","title":"[Rejected] PEP 712 - Adding a “converter” parameter to dataclasses.field","excerpt":"Python Enhancement Proposal 712: 'Adding a “converter” parameter to dataclasses.field'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:37+0900","lastModifiedAt":"2025-09-27 13:12:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/712/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0711-pybi_a_standard_format_for_distributing_python_binaries","title":"[Draft] PEP 711 - PyBI: a standard format for distributing Python Binaries","excerpt":"Python Enhancement Proposal 711: 'PyBI: a standard format for distributing Python Binaries'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:07+0900","lastModifiedAt":"2025-09-27 13:12:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/711/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0710-recording_the_provenance_of_installed_packages","title":"[Draft] PEP 710 - Recording the provenance of installed packages","excerpt":"Python Enhancement Proposal 710: 'Recording the provenance of installed packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:11:44+0900","lastModifiedAt":"2025-09-27 13:11:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/710/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0709-inlined_comprehensions","title":"[Final] PEP 709 - Inlined comprehensions","excerpt":"Python Enhancement Proposal 709: 'Inlined comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:10:56+0900","lastModifiedAt":"2025-09-27 13:10:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/709/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0708-extending_the_repository_api_to_mitigate_dependency_confusion_attacks","title":"[Provisional] PEP 708 - Extending the Repository API to Mitigate Dependency Confusion Attacks","excerpt":"Python Enhancement Proposal 708: 'Extending the Repository API to Mitigate Dependency Confusion Attacks'에 대한 한국어 번역입니다.","date":"2025-09-27 13:10:38+0900","lastModifiedAt":"2025-09-27 13:10:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/708/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0707-a_simplified_signature_for___exit___and___aexit","title":"[Rejected] PEP 707 - A simplified signature for __exit__ and __aexit__","excerpt":"Python Enhancement Proposal 707: 'A simplified signature for __exit__ and __aexit__'에 대한 한국어 번역입니다.","date":"2025-09-27 13:09:56+0900","lastModifiedAt":"2025-09-27 13:09:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/707/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0706-filter_for_tarfile.extractall","title":"[Final] PEP 706 - Filter for tarfile.extractall","excerpt":"Python Enhancement Proposal 706: 'Filter for tarfile.extractall'에 대한 한국어 번역입니다.","date":"2025-09-27 13:09:29+0900","lastModifiedAt":"2025-09-27 13:09:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/706/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0705-typeddict_read-only_items","title":"[Final] PEP 705 - TypedDict: Read-only items","excerpt":"Python Enhancement Proposal 705: 'TypedDict: Read-only items'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:52+0900","lastModifiedAt":"2025-09-27 13:08:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/705/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0704-require_virtual_environments_by_default_for_package_installers","title":"[Withdrawn] PEP 704 - Require virtual environments by default for package installers","excerpt":"Python Enhancement Proposal 704: 'Require virtual environments by default for package installers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:22+0900","lastModifiedAt":"2025-09-27 13:08:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/704/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0703-making_the_global_interpreter_lock_optional_in_cpython","title":"[Accepted] PEP 703 - Making the Global Interpreter Lock Optional in CPython","excerpt":"Python Enhancement Proposal 703: 'Making the Global Interpreter Lock Optional in CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:02+0900","lastModifiedAt":"2025-09-27 13:08:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/703/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0702-marking_deprecations_using_the_type_system","title":"[Final] PEP 702 - Marking deprecations using the type system","excerpt":"Python Enhancement Proposal 702: 'Marking deprecations using the type system'에 대한 한국어 번역입니다.","date":"2025-09-27 13:07:08+0900","lastModifiedAt":"2025-09-27 13:07:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/702/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0701-syntactic_formalization_of_f-strings","title":"[Accepted] PEP 701 - Syntactic formalization of f-strings","excerpt":"Python Enhancement Proposal 701: 'Syntactic formalization of f-strings'에 대한 한국어 번역입니다.","date":"2025-09-27 13:06:50+0900","lastModifiedAt":"2025-09-27 13:06:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/701/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0700-additional_fields_for_the_simple_api_for_package_indexes","title":"[Final] PEP 700 - Additional Fields for the Simple API for Package Indexes","excerpt":"Python Enhancement Proposal 700: 'Additional Fields for the Simple API for Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:58+0900","lastModifiedAt":"2025-09-27 13:05:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/700/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0699-remove_private_dict_version_field_added_in_pep_509","title":"[Accepted] PEP 699 - Remove private dict version field added in PEP 509","excerpt":"Python Enhancement Proposal 699: 'Remove private dict version field added in PEP 509'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:33+0900","lastModifiedAt":"2025-09-27 13:05:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/699/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0698-override_decorator_for_static_typing","title":"[Final] PEP 698 - Override Decorator for Static Typing","excerpt":"Python Enhancement Proposal 698: 'Override Decorator for Static Typing'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:19+0900","lastModifiedAt":"2025-09-27 13:05:19+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/698/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0697-limited_c_api_for_extending_opaque_types","title":"[Final] PEP 697 - Limited C API for Extending Opaque Types","excerpt":"Python Enhancement Proposal 697: 'Limited C API for Extending Opaque Types'에 대한 한국어 번역입니다.","date":"2025-09-27 13:04:45+0900","lastModifiedAt":"2025-09-27 13:04:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/697/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0696-type_defaults_for_type_parameters","title":"[Final] PEP 696 - Type Defaults for Type Parameters","excerpt":"Python Enhancement Proposal 696: 'Type Defaults for Type Parameters'에 대한 한국어 번역입니다.","date":"2025-09-27 13:04:16+0900","lastModifiedAt":"2025-09-27 13:04:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/696/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0695-type_parameter_syntax","title":"[Final] PEP 695 - Type Parameter Syntax","excerpt":"Python Enhancement Proposal 695: 'Type Parameter Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 13:03:44+0900","lastModifiedAt":"2025-09-27 13:03:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/695/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0694-upload_2.0_api_for_python_package_indexes","title":"[Draft] PEP 694 - Upload 2.0 API for Python Package Indexes","excerpt":"Python Enhancement Proposal 694: 'Upload 2.0 API for Python Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:23:36+0900","lastModifiedAt":"2025-09-27 10:23:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/694/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0693-python_3.12_release_schedule","title":"[Active] PEP 693 - Python 3.12 Release Schedule","excerpt":"Python Enhancement Proposal 693: 'Python 3.12 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 10:22:22+0900","lastModifiedAt":"2025-09-27 10:22:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/693/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0692-using_typeddict_for_more_precise_kwargs_typing","title":"[Final] PEP 692 - Using TypedDict for more precise **kwargs typing","excerpt":"Python Enhancement Proposal 692: 'Using TypedDict for more precise **kwargs typing'에 대한 한국어 번역입니다.","date":"2025-09-27 10:21:57+0900","lastModifiedAt":"2025-09-27 10:21:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/692/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0691-json-based_simple_api_for_python_package_indexes","title":"[Accepted] PEP 691 - JSON-based Simple API for Python Package Indexes","excerpt":"Python Enhancement Proposal 691: 'JSON-based Simple API for Python Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:21:04+0900","lastModifiedAt":"2025-09-27 10:21:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/691/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0690-lazy_imports","title":"[Rejected] PEP 690 - Lazy Imports","excerpt":"Python Enhancement Proposal 690: 'Lazy Imports'에 대한 한국어 번역입니다.","date":"2025-09-27 10:19:51+0900","lastModifiedAt":"2025-09-27 10:19:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/690/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0689-unstable_c_api_tier","title":"[Final] PEP 689 - Unstable C API tier","excerpt":"Python Enhancement Proposal 689: 'Unstable C API tier'에 대한 한국어 번역입니다.","date":"2025-09-27 10:13:23+0900","lastModifiedAt":"2025-09-27 10:13:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/689/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0688-making_the_buffer_protocol_accessible_in_python","title":"[Final] PEP 688 - Making the buffer protocol accessible in Python","excerpt":"Python Enhancement Proposal 688: 'Making the buffer protocol accessible in Python'에 대한 한국어 번역입니다.","date":"2025-09-27 10:13:02+0900","lastModifiedAt":"2025-09-27 10:13:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/688/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0687-isolating_modules_in_the_standard_library","title":"[Accepted] PEP 687 - Isolating modules in the standard library","excerpt":"Python Enhancement Proposal 687: 'Isolating modules in the standard library'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:44+0900","lastModifiedAt":"2025-09-27 10:12:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/687/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0686-make_utf-8_mode_default","title":"[Accepted] PEP 686 - Make UTF-8 mode default","excerpt":"Python Enhancement Proposal 686: 'Make UTF-8 mode default'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:32+0900","lastModifiedAt":"2025-09-27 10:12:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/686/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0685-comparison_of_extra_names_for_optional_distribution_dependencies","title":"[Final] PEP 685 - Comparison of extra names for optional distribution dependencies","excerpt":"Python Enhancement Proposal 685: 'Comparison of extra names for optional distribution dependencies'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:15+0900","lastModifiedAt":"2025-09-27 10:12:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/685/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0684-a_per-interpreter_gil","title":"[Final] PEP 684 - A Per-Interpreter GIL","excerpt":"Python Enhancement Proposal 684: 'A Per-Interpreter GIL'에 대한 한국어 번역입니다.","date":"2025-09-27 10:11:59+0900","lastModifiedAt":"2025-09-27 10:11:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/684/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0683-immortal_objects_using_a_fixed_refcount","title":"[Final] PEP 683 - Immortal Objects, Using a Fixed Refcount","excerpt":"Python Enhancement Proposal 683: 'Immortal Objects, Using a Fixed Refcount'에 대한 한국어 번역입니다.","date":"2025-09-27 10:11:29+0900","lastModifiedAt":"2025-09-27 10:11:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/683/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0682-format_specifier_for_signed_zero","title":"[Final] PEP 682 - Format Specifier for Signed Zero","excerpt":"Python Enhancement Proposal 682: 'Format Specifier for Signed Zero'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:49+0900","lastModifiedAt":"2025-09-27 10:10:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/682/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0681-data_class_transforms","title":"[Final] PEP 681 - Data Class Transforms","excerpt":"Python Enhancement Proposal 681: 'Data Class Transforms'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:35+0900","lastModifiedAt":"2025-09-27 10:10:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/681/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0680-tomllib_support_for_parsing_toml_in_the_standard_library","title":"[Final] PEP 680 - tomllib: Support for Parsing TOML in the Standard Library","excerpt":"Python Enhancement Proposal 680: 'tomllib: Support for Parsing TOML in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:11+0900","lastModifiedAt":"2025-09-27 10:10:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/680/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0679-new_assert_statement_syntax_with_parentheses","title":"[Draft] PEP 679 - New assert statement syntax with parentheses","excerpt":"Python Enhancement Proposal 679: 'New assert statement syntax with parentheses'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:51+0900","lastModifiedAt":"2025-09-27 10:09:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/679/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0678-enriching_exceptions_with_notes","title":"[Final] PEP 678 - Enriching Exceptions with Notes","excerpt":"Python Enhancement Proposal 678: 'Enriching Exceptions with Notes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:33+0900","lastModifiedAt":"2025-09-27 10:09:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/678/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0677-callable_type_syntax","title":"[Rejected] PEP 677 - Callable Type Syntax","excerpt":"Python Enhancement Proposal 677: 'Callable Type Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:13+0900","lastModifiedAt":"2025-09-27 10:09:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/677/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0676-pep_infrastructure_process","title":"[Active] PEP 676 - PEP Infrastructure Process","excerpt":"Python Enhancement Proposal 676: 'PEP Infrastructure Process'에 대한 한국어 번역입니다.","date":"2025-09-27 10:08:50+0900","lastModifiedAt":"2025-09-27 10:08:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/676/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0675-arbitrary_literal_string_type","title":"[Final] PEP 675 - Arbitrary Literal String Type","excerpt":"Python Enhancement Proposal 675: 'Arbitrary Literal String Type'에 대한 한국어 번역입니다.","date":"2025-09-27 10:08:22+0900","lastModifiedAt":"2025-09-27 10:08:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/675/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0674-disallow_using_macros_as_l-values","title":"[Deferred] PEP 674 - Disallow using macros as l-values","excerpt":"Python Enhancement Proposal 674: 'Disallow using macros as l-values'에 대한 한국어 번역입니다.","date":"2025-09-27 10:07:42+0900","lastModifiedAt":"2025-09-27 10:07:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/674/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0673-self_type","title":"[Final] PEP 673 - Self Type","excerpt":"Python Enhancement Proposal 673: 'Self Type'에 대한 한국어 번역입니다.","date":"2025-09-27 10:07:14+0900","lastModifiedAt":"2025-09-27 10:07:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/673/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0672-unicode-related_security_considerations_for_python","title":"[Active] PEP 672 - Unicode-related Security Considerations for Python","excerpt":"Python Enhancement Proposal 672: 'Unicode-related Security Considerations for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 10:06:17+0900","lastModifiedAt":"2025-09-27 10:06:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/672/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0671-syntax_for_late-bound_function_argument_defaults","title":"[Draft] PEP 671 - Syntax for late-bound function argument defaults","excerpt":"Python Enhancement Proposal 671: 'Syntax for late-bound function argument defaults'에 대한 한국어 번역입니다.","date":"2025-09-27 10:05:44+0900","lastModifiedAt":"2025-09-27 10:05:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/671/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0670-convert_macros_to_functions_in_the_python_c_api","title":"[Final] PEP 670 - Convert macros to functions in the Python C API","excerpt":"Python Enhancement Proposal 670: 'Convert macros to functions in the Python C API'에 대한 한국어 번역입니다.","date":"2025-09-27 10:05:27+0900","lastModifiedAt":"2025-09-27 10:05:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/670/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0669-low_impact_monitoring_for_cpython","title":"[Final] PEP 669 - Low Impact Monitoring for CPython","excerpt":"Python Enhancement Proposal 669: 'Low Impact Monitoring for CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 10:04:48+0900","lastModifiedAt":"2025-09-27 10:04:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/669/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0668-marking_python_base_environments_as_externally_managed","title":"[Accepted] PEP 668 - Marking Python base environments as “externally managed”","excerpt":"Python Enhancement Proposal 668: 'Marking Python base environments as “externally managed”'에 대한 한국어 번역입니다.","date":"2025-09-27 10:04:11+0900","lastModifiedAt":"2025-09-27 10:04:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/668/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0667-consistent_views_of_namespaces","title":"[Final] PEP 667 - Consistent views of namespaces","excerpt":"Python Enhancement Proposal 667: 'Consistent views of namespaces'에 대한 한국어 번역입니다.","date":"2025-09-27 10:03:19+0900","lastModifiedAt":"2025-09-27 10:03:19+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/667/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0666-reject_foolish_indentation","title":"[Rejected] PEP 666 - Reject Foolish Indentation","excerpt":"Python Enhancement Proposal 666: 'Reject Foolish Indentation'에 대한 한국어 번역입니다.","date":"2025-09-27 10:02:26+0900","lastModifiedAt":"2025-09-27 10:02:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/666/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0665-a_file_format_to_list_python_dependencies_for_reproducibility_of_an_application","title":"[Rejected] PEP 665 - A file format to list Python dependencies for reproducibility of an application","excerpt":"Python Enhancement Proposal 665: 'A file format to list Python dependencies for reproducibility of an application'에 대한 한국어 번역입니다.","date":"2025-09-27 10:02:15+0900","lastModifiedAt":"2025-09-27 10:02:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/665/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0664-python_3.11_release_schedule","title":"[Active] PEP 664 - Python 3.11 Release Schedule","excerpt":"Python Enhancement Proposal 664: 'Python 3.11 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 09:57:22+0900","lastModifiedAt":"2025-09-27 09:57:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/664/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0663-standardizing_enum_str_repr_and_format_behaviors","title":"[Rejected] PEP 663 - Standardizing Enum str(), repr(), and format() behaviors","excerpt":"Python Enhancement Proposal 663: 'Standardizing Enum str(), repr(), and format() behaviors'에 대한 한국어 번역입니다.","date":"2025-09-27 09:57:11+0900","lastModifiedAt":"2025-09-27 09:57:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/663/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0662-editable_installs_via_virtual_wheels","title":"[Rejected] PEP 662 - Editable installs via virtual wheels","excerpt":"Python Enhancement Proposal 662: 'Editable installs via virtual wheels'에 대한 한국어 번역입니다.","date":"2025-09-27 09:56:52+0900","lastModifiedAt":"2025-09-27 09:56:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/662/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0661-sentinel_values","title":"[Deferred] PEP 661 - Sentinel Values","excerpt":"Python Enhancement Proposal 661: 'Sentinel Values'에 대한 한국어 번역입니다.","date":"2025-09-27 09:55:59+0900","lastModifiedAt":"2025-09-27 09:55:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/661/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0660-editable_installs_for_pyproject.toml_based_builds_wheel_based","title":"[Final] PEP 660 - Editable installs for pyproject.toml based builds (wheel based)","excerpt":"Python Enhancement Proposal 660: 'Editable installs for pyproject.toml based builds (wheel based)'에 대한 한국어 번역입니다.","date":"2025-09-27 09:55:28+0900","lastModifiedAt":"2025-09-27 09:55:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/660/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0659-specializing_adaptive_interpreter","title":"[Final] PEP 659 - Specializing Adaptive Interpreter","excerpt":"Python Enhancement Proposal 659: 'Specializing Adaptive Interpreter'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:44+0900","lastModifiedAt":"2025-09-27 09:54:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/659/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0658-serve_distribution_metadata_in_the_simple_repository_api","title":"[Accepted] PEP 658 - Serve Distribution Metadata in the Simple Repository API","excerpt":"Python Enhancement Proposal 658: 'Serve Distribution Metadata in the Simple Repository API'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:18+0900","lastModifiedAt":"2025-09-27 09:54:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/658/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0657-include_fine_grained_error_locations_in_tracebacks","title":"[Final] PEP 657 - Include Fine Grained Error Locations in Tracebacks","excerpt":"Python Enhancement Proposal 657: 'Include Fine Grained Error Locations in Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:06+0900","lastModifiedAt":"2025-09-27 09:54:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/657/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0656-platform_tag_for_linux_distributions_using_musl","title":"[Final] PEP 656 - Platform Tag for Linux Distributions Using Musl","excerpt":"Python Enhancement Proposal 656: 'Platform Tag for Linux Distributions Using Musl'에 대한 한국어 번역입니다.","date":"2025-09-27 09:53:36+0900","lastModifiedAt":"2025-09-27 09:53:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/656/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0655-marking_individual_typeddict_items_as_required_or_potentially-missing","title":"[Final] PEP 655 - Marking individual TypedDict items as required or potentially-missing","excerpt":"Python Enhancement Proposal 655: 'Marking individual TypedDict items as required or potentially-missing'에 대한 한국어 번역입니다.","date":"2025-09-27 09:53:14+0900","lastModifiedAt":"2025-09-27 09:53:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/655/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0654-exception_groups_and_except","title":"[Final] PEP 654 - Exception Groups and except*","excerpt":"Python Enhancement Proposal 654: 'Exception Groups and except*'에 대한 한국어 번역입니다.","date":"2025-09-27 09:52:52+0900","lastModifiedAt":"2025-09-27 09:52:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/654/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0653-precise_semantics_for_pattern_matching","title":"[Draft] PEP 653 - Precise Semantics for Pattern Matching","excerpt":"Python Enhancement Proposal 653: 'Precise Semantics for Pattern Matching'에 대한 한국어 번역입니다.","date":"2025-09-27 09:52:27+0900","lastModifiedAt":"2025-09-27 09:52:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/653/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0652-maintaining_the_stable_abi","title":"[Final] PEP 652 - Maintaining the Stable ABI","excerpt":"Python Enhancement Proposal 652: 'Maintaining the Stable ABI'에 대한 한국어 번역입니다.","date":"2025-09-27 01:41:49+0900","lastModifiedAt":"2025-09-27 01:41:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/652/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0651-robust_stack_overflow_handling","title":"[Rejected] PEP 651 - Robust Stack Overflow Handling","excerpt":"Python Enhancement Proposal 651: 'Robust Stack Overflow Handling'에 대한 한국어 번역입니다.","date":"2025-09-27 01:41:16+0900","lastModifiedAt":"2025-09-27 01:41:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/651/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0650-specifying_installer_requirements_for_python_projects","title":"[Withdrawn] PEP 650 - Specifying Installer Requirements for Python Projects","excerpt":"Python Enhancement Proposal 650: 'Specifying Installer Requirements for Python Projects'에 대한 한국어 번역입니다.","date":"2025-09-27 01:40:48+0900","lastModifiedAt":"2025-09-27 01:40:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/650/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0649-deferred_evaluation_of_annotations_using_descriptors","title":"[Accepted] PEP 649 - Deferred Evaluation Of Annotations Using Descriptors","excerpt":"Python Enhancement Proposal 649: 'Deferred Evaluation Of Annotations Using Descriptors'에 대한 한국어 번역입니다.","date":"2025-09-27 01:39:54+0900","lastModifiedAt":"2025-09-27 01:39:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/649/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0648-extensible_customizations_of_the_interpreter_at_startup","title":"[Rejected] PEP 648 - Extensible customizations of the interpreter at startup","excerpt":"Python Enhancement Proposal 648: 'Extensible customizations of the interpreter at startup'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:58+0900","lastModifiedAt":"2025-09-27 01:37:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/648/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0647-user-defined_type_guards","title":"[Final] PEP 647 - User-Defined Type Guards","excerpt":"Python Enhancement Proposal 647: 'User-Defined Type Guards'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:28+0900","lastModifiedAt":"2025-09-27 01:37:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/647/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0646-variadic_generics","title":"[Final] PEP 646 - Variadic Generics","excerpt":"Python Enhancement Proposal 646: 'Variadic Generics'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:00+0900","lastModifiedAt":"2025-09-27 01:37:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/646/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0645-allow_writing_optional_types_asx","title":"[Withdrawn] PEP 645 - Allow writing optional types asx?","excerpt":"Python Enhancement Proposal 645: 'Allow writing optional types asx?'에 대한 한국어 번역입니다.","date":"2025-09-27 01:34:51+0900","lastModifiedAt":"2025-09-27 01:34:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/645/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0644-require_openssl_1.1.1_or_newer","title":"[Final] PEP 644 - Require OpenSSL 1.1.1 or newer","excerpt":"Python Enhancement Proposal 644: 'Require OpenSSL 1.1.1 or newer'에 대한 한국어 번역입니다.","date":"2025-09-27 01:34:31+0900","lastModifiedAt":"2025-09-27 01:34:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/644/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0643-metadata_for_package_source_distributions","title":"[Final] PEP 643 - Metadata for Package Source Distributions","excerpt":"Python Enhancement Proposal 643: 'Metadata for Package Source Distributions'에 대한 한국어 번역입니다.","date":"2025-09-27 01:33:51+0900","lastModifiedAt":"2025-09-27 01:33:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/643/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0642-explicit_pattern_syntax_for_structural_pattern_matching","title":"[Rejected] PEP 642 - Explicit Pattern Syntax for Structural Pattern Matching","excerpt":"Python Enhancement Proposal 642: 'Explicit Pattern Syntax for Structural Pattern Matching'에 대한 한국어 번역입니다.","date":"2025-09-27 01:33:04+0900","lastModifiedAt":"2025-09-27 01:33:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/642/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0641-using_an_underscore_in_the_version_portion_of_python_3.10_compatibility_tags","title":"[Rejected] PEP 641 - Using an underscore in the version portion of Python 3.10 compatibility tags","excerpt":"Python Enhancement Proposal 641: 'Using an underscore in the version portion of Python 3.10 compatibility tags'에 대한 한국어 번역입니다.","date":"2025-09-27 01:32:00+0900","lastModifiedAt":"2025-09-27 01:32:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/641/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0640-unused_variable_syntax","title":"[Rejected] PEP 640 - Unused variable syntax","excerpt":"Python Enhancement Proposal 640: 'Unused variable syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 01:31:38+0900","lastModifiedAt":"2025-09-27 01:31:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/640/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0639-improving_license_clarity_with_better_package_metadata","title":"[Final] PEP 639 - Improving License Clarity with Better Package Metadata","excerpt":"Python Enhancement Proposal 639: 'Improving License Clarity with Better Package Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 01:31:09+0900","lastModifiedAt":"2025-09-27 01:31:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/639/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0638-syntactic_macros","title":"[Draft] PEP 638 - Syntactic Macros","excerpt":"Python Enhancement Proposal 638: 'Syntactic Macros'에 대한 한국어 번역입니다.","date":"2025-09-27 01:30:41+0900","lastModifiedAt":"2025-09-27 01:30:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/638/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0637-support_for_indexing_with_keyword_arguments","title":"[Rejected] PEP 637 - Support for indexing with keyword arguments","excerpt":"Python Enhancement Proposal 637: 'Support for indexing with keyword arguments'에 대한 한국어 번역입니다.","date":"2025-09-27 01:29:49+0900","lastModifiedAt":"2025-09-27 01:29:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/637/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0636-structural_pattern_matching_tutorial","title":"[Final] PEP 636 - Structural Pattern Matching: Tutorial","excerpt":"Python Enhancement Proposal 636: 'Structural Pattern Matching: Tutorial'에 대한 한국어 번역입니다.","date":"2025-09-27 01:29:01+0900","lastModifiedAt":"2025-09-27 01:29:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/636/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0635-structural_pattern_matching_motivation_and_rationale","title":"[Final] PEP 635 - Structural Pattern Matching: Motivation and Rationale","excerpt":"Python Enhancement Proposal 635: 'Structural Pattern Matching: Motivation and Rationale'에 대한 한국어 번역입니다.","date":"2025-09-27 01:26:51+0900","lastModifiedAt":"2025-09-27 01:26:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/635/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0634-structural_pattern_matching_specification","title":"[Final] PEP 634 - Structural Pattern Matching: Specification","excerpt":"Python Enhancement Proposal 634: 'Structural Pattern Matching: Specification'에 대한 한국어 번역입니다.","date":"2025-09-27 01:25:49+0900","lastModifiedAt":"2025-09-27 01:25:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/634/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0633-dependency_specification_in_pyproject.toml_using_an_exploded_toml_table","title":"[Rejected] PEP 633 - Dependency specification in pyproject.toml using an exploded TOML table","excerpt":"Python Enhancement Proposal 633: 'Dependency specification in pyproject.toml using an exploded TOML table'에 대한 한국어 번역입니다.","date":"2025-09-27 01:25:08+0900","lastModifiedAt":"2025-09-27 01:25:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/633/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0632-deprecate_distutils_module","title":"[Final] PEP 632 - Deprecate distutils module","excerpt":"Python Enhancement Proposal 632: 'Deprecate distutils module'에 대한 한국어 번역입니다.","date":"2025-09-27 01:16:21+0900","lastModifiedAt":"2025-09-27 01:16:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/632/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0631-dependency_specification_in_pyproject.toml_based_on_pep_508","title":"[Superseded] PEP 631 - Dependency specification in pyproject.toml based on PEP 508","excerpt":"Python Enhancement Proposal 631: 'Dependency specification in pyproject.toml based on PEP 508'에 대한 한국어 번역입니다.","date":"2025-09-27 01:15:39+0900","lastModifiedAt":"2025-09-27 01:15:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/631/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0630-isolating_extension_modules","title":"[Final] PEP 630 - Isolating Extension Modules","excerpt":"Python Enhancement Proposal 630: 'Isolating Extension Modules'에 대한 한국어 번역입니다.","date":"2025-09-27 01:13:23+0900","lastModifiedAt":"2025-09-27 01:13:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/630/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0355-path_-_object_oriented_filesystem_paths","title":"[Rejected] PEP 355 - Path - Object oriented filesystem paths","excerpt":"Python Enhancement Proposal 355: 'Path - Object oriented filesystem paths'에 대한 한국어 번역입니다.","date":"2025-09-27 01:12:42+0900","lastModifiedAt":"2025-09-27 01:12:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/355/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0343-the_with_statement","title":"[Final] PEP 343 - The “with” Statement","excerpt":"Python Enhancement Proposal 343: 'The “with” Statement'에 대한 한국어 번역입니다.","date":"2025-09-27 01:01:20+0900","lastModifiedAt":"2025-09-27 01:01:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/343/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0335-overloadable_boolean_operators","title":"[Rejected] PEP 335 - Overloadable Boolean Operators","excerpt":"Python Enhancement Proposal 335: 'Overloadable Boolean Operators'에 대한 한국어 번역입니다.","date":"2025-09-27 00:55:17+0900","lastModifiedAt":"2025-09-27 00:55:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/335/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0629-versioning_pypis_simple_api","title":"[Final] PEP 629 - Versioning PyPI’s Simple API","excerpt":"Python Enhancement Proposal 629: 'Versioning PyPI’s Simple API'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:52+0900","lastModifiedAt":"2025-09-27 00:30:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/629/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0628-addmath.tau","title":"[Final] PEP 628 - Addmath.tau","excerpt":"Python Enhancement Proposal 628: 'Addmath.tau'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:25+0900","lastModifiedAt":"2025-09-27 00:30:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/628/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0627-recording_installed_projects","title":"[Final] PEP 627 - Recording installed projects","excerpt":"Python Enhancement Proposal 627: 'Recording installed projects'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:10+0900","lastModifiedAt":"2025-09-27 00:30:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/627/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0626-precise_line_numbers_for_debugging_and_other_tools.","title":"[Final] PEP 626 - Precise line numbers for debugging and other tools.","excerpt":"Python Enhancement Proposal 626: 'Precise line numbers for debugging and other tools.'에 대한 한국어 번역입니다.","date":"2025-09-27 00:29:42+0900","lastModifiedAt":"2025-09-27 00:29:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/626/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0608-coordinated_python_release","title":"[Rejected] PEP 608 - Coordinated Python release","excerpt":"Python Enhancement Proposal 608: 'Coordinated Python release'에 대한 한국어 번역입니다.","date":"2025-09-27 00:19:32+0900","lastModifiedAt":"2025-09-27 00:19:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/608/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-27-pep-0599-the_manylinux2014_platform_tag","title":"[Superseded] PEP 599 - The manylinux2014 Platform Tag","excerpt":"Python Enhancement Proposal 599: 'The manylinux2014 Platform Tag'에 대한 한국어 번역입니다.","date":"2025-09-27 00:14:20+0900","lastModifiedAt":"2025-09-27 00:14:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/599/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0573-module_state_access_from_c_extension_methods","title":"[Final] PEP 573 - Module State Access from C Extension Methods","excerpt":"Python Enhancement Proposal 573: 'Module State Access from C Extension Methods'에 대한 한국어 번역입니다.","date":"2025-09-26 23:59:37+0900","lastModifiedAt":"2025-09-26 23:59:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/573/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0572-assignment_expressions","title":"[Final] PEP 572 - Assignment Expressions","excerpt":"Python Enhancement Proposal 572: 'Assignment Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:58:55+0900","lastModifiedAt":"2025-09-26 23:58:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/572/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0571-the_manylinux2010_platform_tag","title":"[Superseded] PEP 571 - The manylinux2010 Platform Tag","excerpt":"Python Enhancement Proposal 571: 'The manylinux2010 Platform Tag'에 대한 한국어 번역입니다.","date":"2025-09-26 23:56:56+0900","lastModifiedAt":"2025-09-26 23:56:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/571/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0570-python_positional-only_parameters","title":"[Final] PEP 570 - Python Positional-Only Parameters","excerpt":"Python Enhancement Proposal 570: 'Python Positional-Only Parameters'에 대한 한국어 번역입니다.","date":"2025-09-26 23:56:05+0900","lastModifiedAt":"2025-09-26 23:56:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/570/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0569-python_3.8_release_schedule","title":"[Final] PEP 569 - Python 3.8 Release Schedule","excerpt":"Python Enhancement Proposal 569: 'Python 3.8 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 23:51:49+0900","lastModifiedAt":"2025-09-26 23:51:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/569/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0568-generator-sensitivity_for_context_variables","title":"[Deferred] PEP 568 - Generator-sensitivity for Context Variables","excerpt":"Python Enhancement Proposal 568: 'Generator-sensitivity for Context Variables'에 대한 한국어 번역입니다.","date":"2025-09-26 23:51:20+0900","lastModifiedAt":"2025-09-26 23:51:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/568/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0567-context_variables","title":"[Final] PEP 567 - Context Variables","excerpt":"Python Enhancement Proposal 567: 'Context Variables'에 대한 한국어 번역입니다.","date":"2025-09-26 23:50:36+0900","lastModifiedAt":"2025-09-26 23:50:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/567/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0566-metadata_for_python_software_packages_2.1","title":"[Final] PEP 566 - Metadata for Python Software Packages 2.1","excerpt":"Python Enhancement Proposal 566: 'Metadata for Python Software Packages 2.1'에 대한 한국어 번역입니다.","date":"2025-09-26 23:49:52+0900","lastModifiedAt":"2025-09-26 23:49:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/566/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0565-show_deprecationwarning_in___main","title":"[Final] PEP 565 - Show DeprecationWarning in __main__","excerpt":"Python Enhancement Proposal 565: 'Show DeprecationWarning in __main__'에 대한 한국어 번역입니다.","date":"2025-09-26 23:49:30+0900","lastModifiedAt":"2025-09-26 23:49:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/565/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0564-add_new_time_functions_with_nanosecond_resolution","title":"[Final] PEP 564 - Add new time functions with nanosecond resolution","excerpt":"Python Enhancement Proposal 564: 'Add new time functions with nanosecond resolution'에 대한 한국어 번역입니다.","date":"2025-09-26 23:48:58+0900","lastModifiedAt":"2025-09-26 23:48:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/564/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0563-postponed_evaluation_of_annotations","title":"[Superseded] PEP 563 - Postponed Evaluation of Annotations","excerpt":"Python Enhancement Proposal 563: 'Postponed Evaluation of Annotations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:48:31+0900","lastModifiedAt":"2025-09-26 23:48:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/563/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0562-module___getattr___and___dir","title":"[Final] PEP 562 - Module __getattr__ and __dir__","excerpt":"Python Enhancement Proposal 562: 'Module __getattr__ and __dir__'에 대한 한국어 번역입니다.","date":"2025-09-26 23:47:20+0900","lastModifiedAt":"2025-09-26 23:47:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/562/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0561-distributing_and_packaging_type_information","title":"[Final] PEP 561 - Distributing and Packaging Type Information","excerpt":"Python Enhancement Proposal 561: 'Distributing and Packaging Type Information'에 대한 한국어 번역입니다.","date":"2025-09-26 23:46:31+0900","lastModifiedAt":"2025-09-26 23:46:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/561/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0560-core_support_for_typing_module_and_generic_types","title":"[Final] PEP 560 - Core support for typing module and generic types","excerpt":"Python Enhancement Proposal 560: 'Core support for typing module and generic types'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:50+0900","lastModifiedAt":"2025-09-26 23:45:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/560/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0559-built-in_noop","title":"[Rejected] PEP 559 - Built-in noop()","excerpt":"Python Enhancement Proposal 559: 'Built-in noop()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:26+0900","lastModifiedAt":"2025-09-26 23:45:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/559/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0558-defined_semantics_for_locals","title":"[Withdrawn] PEP 558 - Defined semantics for locals()","excerpt":"Python Enhancement Proposal 558: 'Defined semantics for locals()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:10+0900","lastModifiedAt":"2025-09-26 23:45:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/558/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0557-data_classes","title":"[Final] PEP 557 - Data Classes","excerpt":"Python Enhancement Proposal 557: 'Data Classes'에 대한 한국어 번역입니다.","date":"2025-09-26 23:42:21+0900","lastModifiedAt":"2025-09-26 23:42:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/557/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0556-threaded_garbage_collection","title":"[Deferred] PEP 556 - Threaded garbage collection","excerpt":"Python Enhancement Proposal 556: 'Threaded garbage collection'에 대한 한국어 번역입니다.","date":"2025-09-26 23:41:31+0900","lastModifiedAt":"2025-09-26 23:41:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/556/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0555-context-local_variables_contextvars","title":"[Withdrawn] PEP 555 - Context-local variables (contextvars)","excerpt":"Python Enhancement Proposal 555: 'Context-local variables (contextvars)'에 대한 한국어 번역입니다.","date":"2025-09-26 23:40:56+0900","lastModifiedAt":"2025-09-26 23:40:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/555/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0554-multiple_interpreters_in_the_stdlib","title":"[Superseded] PEP 554 - Multiple Interpreters in the Stdlib","excerpt":"Python Enhancement Proposal 554: 'Multiple Interpreters in the Stdlib'에 대한 한국어 번역입니다.","date":"2025-09-26 23:40:00+0900","lastModifiedAt":"2025-09-26 23:40:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/554/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0553-built-in_breakpoint","title":"[Final] PEP 553 - Built-in breakpoint()","excerpt":"Python Enhancement Proposal 553: 'Built-in breakpoint()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:38:53+0900","lastModifiedAt":"2025-09-26 23:38:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/553/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0552-deterministic_pycs","title":"[Final] PEP 552 - Deterministic pycs","excerpt":"Python Enhancement Proposal 552: 'Deterministic pycs'에 대한 한국어 번역입니다.","date":"2025-09-26 23:38:24+0900","lastModifiedAt":"2025-09-26 23:38:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/552/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0551-security_transparency_in_the_python_runtime","title":"[Withdrawn] PEP 551 - Security transparency in the Python runtime","excerpt":"Python Enhancement Proposal 551: 'Security transparency in the Python runtime'에 대한 한국어 번역입니다.","date":"2025-09-26 23:37:57+0900","lastModifiedAt":"2025-09-26 23:37:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/551/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0550-execution_context","title":"[Withdrawn] PEP 550 - Execution Context","excerpt":"Python Enhancement Proposal 550: 'Execution Context'에 대한 한국어 번역입니다.","date":"2025-09-26 23:36:24+0900","lastModifiedAt":"2025-09-26 23:36:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/550/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0549-instance_descriptors","title":"[Rejected] PEP 549 - Instance Descriptors","excerpt":"Python Enhancement Proposal 549: 'Instance Descriptors'에 대한 한국어 번역입니다.","date":"2025-09-26 23:35:20+0900","lastModifiedAt":"2025-09-26 23:35:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/549/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0548-more_flexible_loop_control","title":"[Rejected] PEP 548 - More Flexible Loop Control","excerpt":"Python Enhancement Proposal 548: 'More Flexible Loop Control'에 대한 한국어 번역입니다.","date":"2025-09-26 23:35:01+0900","lastModifiedAt":"2025-09-26 23:35:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/548/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0547-running_extension_modules_using_the_-m_option","title":"[Deferred] PEP 547 - Running extension modules using the -m option","excerpt":"Python Enhancement Proposal 547: 'Running extension modules using the -m option'에 대한 한국어 번역입니다.","date":"2025-09-26 23:34:23+0900","lastModifiedAt":"2025-09-26 23:34:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/547/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0546-backport_ssl.memorybio_and_ssl.sslobject_to_python_2.7","title":"[Rejected] PEP 546 - Backport ssl.MemoryBIO and ssl.SSLObject to Python 2.7","excerpt":"Python Enhancement Proposal 546: 'Backport ssl.MemoryBIO and ssl.SSLObject to Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 23:33:55+0900","lastModifiedAt":"2025-09-26 23:33:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/546/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0545-python_documentation_translations","title":"[Active] PEP 545 - Python Documentation Translations","excerpt":"Python Enhancement Proposal 545: 'Python Documentation Translations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:33:31+0900","lastModifiedAt":"2025-09-26 23:33:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/545/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0544-protocols_structural_subtyping_static_duck_typing","title":"[Final] PEP 544 - Protocols: Structural subtyping (static duck typing)","excerpt":"Python Enhancement Proposal 544: 'Protocols: Structural subtyping (static duck typing)'에 대한 한국어 번역입니다.","date":"2025-09-26 23:32:41+0900","lastModifiedAt":"2025-09-26 23:32:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/544/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0543-a_unified_tls_api_for_python","title":"[Withdrawn] PEP 543 - A Unified TLS API for Python","excerpt":"Python Enhancement Proposal 543: 'A Unified TLS API for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 23:30:50+0900","lastModifiedAt":"2025-09-26 23:30:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/543/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0542-dot_notation_assignment_in_function_header","title":"[Rejected] PEP 542 - Dot Notation Assignment In Function Header","excerpt":"Python Enhancement Proposal 542: 'Dot Notation Assignment In Function Header'에 대한 한국어 번역입니다.","date":"2025-09-26 23:30:04+0900","lastModifiedAt":"2025-09-26 23:30:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/542/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0541-package_index_name_retention","title":"[Final] PEP 541 - Package Index Name Retention","excerpt":"Python Enhancement Proposal 541: 'Package Index Name Retention'에 대한 한국어 번역입니다.","date":"2025-09-26 23:29:44+0900","lastModifiedAt":"2025-09-26 23:29:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/541/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0540-add_a_new_utf-8_mode","title":"[Final] PEP 540 - Add a new UTF-8 Mode","excerpt":"Python Enhancement Proposal 540: 'Add a new UTF-8 Mode'에 대한 한국어 번역입니다.","date":"2025-09-26 23:29:04+0900","lastModifiedAt":"2025-09-26 23:29:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/540/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0539-a_new_c-api_for_thread-local_storage_in_cpython","title":"[Final] PEP 539 - A New C-API for Thread-Local Storage in CPython","excerpt":"Python Enhancement Proposal 539: 'A New C-API for Thread-Local Storage in CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 23:28:37+0900","lastModifiedAt":"2025-09-26 23:28:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/539/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0538-coercing_the_legacy_c_locale_to_a_utf-8_based_locale","title":"[Final] PEP 538 - Coercing the legacy C locale to a UTF-8 based locale","excerpt":"Python Enhancement Proposal 538: 'Coercing the legacy C locale to a UTF-8 based locale'에 대한 한국어 번역입니다.","date":"2025-09-26 23:28:03+0900","lastModifiedAt":"2025-09-26 23:28:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/538/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0537-python_3.7_release_schedule","title":"[Final] PEP 537 - Python 3.7 Release Schedule","excerpt":"Python Enhancement Proposal 537: 'Python 3.7 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 23:27:21+0900","lastModifiedAt":"2025-09-26 23:27:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/537/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0536-final_grammar_for_literal_string_interpolation","title":"[Withdrawn] PEP 536 - Final Grammar for Literal String Interpolation","excerpt":"Python Enhancement Proposal 536: 'Final Grammar for Literal String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 23:26:52+0900","lastModifiedAt":"2025-09-26 23:26:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/536/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0535-rich_comparison_chaining","title":"[Deferred] PEP 535 - Rich comparison chaining","excerpt":"Python Enhancement Proposal 535: 'Rich comparison chaining'에 대한 한국어 번역입니다.","date":"2025-09-26 23:26:10+0900","lastModifiedAt":"2025-09-26 23:26:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/535/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0534-improved_errors_for_missing_standard_library_modules","title":"[Deferred] PEP 534 - Improved Errors for Missing Standard Library Modules","excerpt":"Python Enhancement Proposal 534: 'Improved Errors for Missing Standard Library Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 23:25:45+0900","lastModifiedAt":"2025-09-26 23:25:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/534/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0533-deterministic_cleanup_for_iterators","title":"[Deferred] PEP 533 - Deterministic cleanup for iterators","excerpt":"Python Enhancement Proposal 533: 'Deterministic cleanup for iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:25:05+0900","lastModifiedAt":"2025-09-26 23:25:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/533/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0532-a_circuit_breaking_protocol_and_binary_operators","title":"[Deferred] PEP 532 - A circuit breaking protocol and binary operators","excerpt":"Python Enhancement Proposal 532: 'A circuit breaking protocol and binary operators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:23:40+0900","lastModifiedAt":"2025-09-26 23:23:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/532/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0531-existence_checking_operators","title":"[Withdrawn] PEP 531 - Existence checking operators","excerpt":"Python Enhancement Proposal 531: 'Existence checking operators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:22:20+0900","lastModifiedAt":"2025-09-26 23:22:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/531/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0530-asynchronous_comprehensions","title":"[Final] PEP 530 - Asynchronous Comprehensions","excerpt":"Python Enhancement Proposal 530: 'Asynchronous Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:21:12+0900","lastModifiedAt":"2025-09-26 23:21:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/530/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0529-change_windows_filesystem_encoding_to_utf-8","title":"[Final] PEP 529 - Change Windows filesystem encoding to UTF-8","excerpt":"Python Enhancement Proposal 529: 'Change Windows filesystem encoding to UTF-8'에 대한 한국어 번역입니다.","date":"2025-09-26 23:20:53+0900","lastModifiedAt":"2025-09-26 23:20:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/529/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0528-change_windows_console_encoding_to_utf-8","title":"[Final] PEP 528 - Change Windows console encoding to UTF-8","excerpt":"Python Enhancement Proposal 528: 'Change Windows console encoding to UTF-8'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:55+0900","lastModifiedAt":"2025-09-26 23:19:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/528/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0527-removing_underused_file_typesextensions_on_pypi","title":"[Final] PEP 527 - Removing Un(der)used file types/extensions on PyPI","excerpt":"Python Enhancement Proposal 527: 'Removing Un(der)used file types/extensions on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:32+0900","lastModifiedAt":"2025-09-26 23:19:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/527/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0526-syntax_for_variable_annotations","title":"[Final] PEP 526 - Syntax for Variable Annotations","excerpt":"Python Enhancement Proposal 526: 'Syntax for Variable Annotations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:02+0900","lastModifiedAt":"2025-09-26 23:19:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/526/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0525-asynchronous_generators","title":"[Final] PEP 525 - Asynchronous Generators","excerpt":"Python Enhancement Proposal 525: 'Asynchronous Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:17:46+0900","lastModifiedAt":"2025-09-26 23:17:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/525/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0524-make_os.urandom_blocking_on_linux","title":"[Final] PEP 524 - Make os.urandom() blocking on Linux","excerpt":"Python Enhancement Proposal 524: 'Make os.urandom() blocking on Linux'에 대한 한국어 번역입니다.","date":"2025-09-26 23:16:54+0900","lastModifiedAt":"2025-09-26 23:16:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/524/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0523-adding_a_frame_evaluation_api_to_cpython","title":"[Final] PEP 523 - Adding a frame evaluation API to CPython","excerpt":"Python Enhancement Proposal 523: 'Adding a frame evaluation API to CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 23:16:26+0900","lastModifiedAt":"2025-09-26 23:16:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/523/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0522-allow_blockingioerror_in_security_sensitive_apis","title":"[Rejected] PEP 522 - Allow BlockingIOError in security sensitive APIs","excerpt":"Python Enhancement Proposal 522: 'Allow BlockingIOError in security sensitive APIs'에 대한 한국어 번역입니다.","date":"2025-09-26 23:15:31+0900","lastModifiedAt":"2025-09-26 23:15:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/522/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0521-managing_global_context_via_with_blocks_in_generators_and_coroutines","title":"[Withdrawn] PEP 521 - Managing global context via ‘with’ blocks in generators and coroutines","excerpt":"Python Enhancement Proposal 521: 'Managing global context via ‘with’ blocks in generators and coroutines'에 대한 한국어 번역입니다.","date":"2025-09-26 23:14:33+0900","lastModifiedAt":"2025-09-26 23:14:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/521/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0520-preserving_class_attribute_definition_order","title":"[Final] PEP 520 - Preserving Class Attribute Definition Order","excerpt":"Python Enhancement Proposal 520: 'Preserving Class Attribute Definition Order'에 대한 한국어 번역입니다.","date":"2025-09-26 23:13:52+0900","lastModifiedAt":"2025-09-26 23:13:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/520/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0519-adding_a_file_system_path_protocol","title":"[Final] PEP 519 - Adding a file system path protocol","excerpt":"Python Enhancement Proposal 519: 'Adding a file system path protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 23:13:13+0900","lastModifiedAt":"2025-09-26 23:13:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/519/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0518-specifying_minimum_build_system_requirements_for_python_projects","title":"[Final] PEP 518 - Specifying Minimum Build System Requirements for Python Projects","excerpt":"Python Enhancement Proposal 518: 'Specifying Minimum Build System Requirements for Python Projects'에 대한 한국어 번역입니다.","date":"2025-09-26 23:12:43+0900","lastModifiedAt":"2025-09-26 23:12:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/518/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0517-a_build-system_independent_format_for_source_trees","title":"[Final] PEP 517 - A build-system independent format for source trees","excerpt":"Python Enhancement Proposal 517: 'A build-system independent format for source trees'에 대한 한국어 번역입니다.","date":"2025-09-26 23:11:54+0900","lastModifiedAt":"2025-09-26 23:11:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/517/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0516-build_system_abstraction_for_pipconda_etc","title":"[Rejected] PEP 516 - Build system abstraction for pip/conda etc","excerpt":"Python Enhancement Proposal 516: 'Build system abstraction for pip/conda etc'에 대한 한국어 번역입니다.","date":"2025-09-26 23:11:03+0900","lastModifiedAt":"2025-09-26 23:11:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/516/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0515-underscores_in_numeric_literals","title":"[Final] PEP 515 - Underscores in Numeric Literals","excerpt":"Python Enhancement Proposal 515: 'Underscores in Numeric Literals'에 대한 한국어 번역입니다.","date":"2025-09-26 23:10:13+0900","lastModifiedAt":"2025-09-26 23:10:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/515/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0514-python_registration_in_the_windows_registry","title":"[Active] PEP 514 - Python registration in the Windows registry","excerpt":"Python Enhancement Proposal 514: 'Python registration in the Windows registry'에 대한 한국어 번역입니다.","date":"2025-09-26 23:09:52+0900","lastModifiedAt":"2025-09-26 23:09:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/514/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0513-a_platform_tag_for_portable_linux_built_distributions","title":"[Superseded] PEP 513 - A Platform Tag for Portable Linux Built Distributions","excerpt":"Python Enhancement Proposal 513: 'A Platform Tag for Portable Linux Built Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:03:09+0900","lastModifiedAt":"2025-09-26 23:03:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/513/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0512-migrating_from_hg.python.org_to_github","title":"[Final] PEP 512 - Migrating from hg.python.org to GitHub","excerpt":"Python Enhancement Proposal 512: 'Migrating from hg.python.org to GitHub'에 대한 한국어 번역입니다.","date":"2025-09-26 22:57:36+0900","lastModifiedAt":"2025-09-26 22:57:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/512/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0511-api_for_code_transformers","title":"[Rejected] PEP 511 - API for code transformers","excerpt":"Python Enhancement Proposal 511: 'API for code transformers'에 대한 한국어 번역입니다.","date":"2025-09-26 22:56:47+0900","lastModifiedAt":"2025-09-26 22:56:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/511/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0510-specialize_functions_with_guards","title":"[Rejected] PEP 510 - Specialize functions with guards","excerpt":"Python Enhancement Proposal 510: 'Specialize functions with guards'에 대한 한국어 번역입니다.","date":"2025-09-26 22:55:51+0900","lastModifiedAt":"2025-09-26 22:55:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/510/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0509-add_a_private_version_to_dict","title":"[Superseded] PEP 509 - Add a private version to dict","excerpt":"Python Enhancement Proposal 509: 'Add a private version to dict'에 대한 한국어 번역입니다.","date":"2025-09-26 22:55:06+0900","lastModifiedAt":"2025-09-26 22:55:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/509/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0508-dependency_specification_for_python_software_packages","title":"[Final] PEP 508 - Dependency specification for Python Software Packages","excerpt":"Python Enhancement Proposal 508: 'Dependency specification for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:54:08+0900","lastModifiedAt":"2025-09-26 22:54:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/508/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0507-migrate_cpython_to_git_and_gitlab","title":"[Rejected] PEP 507 - Migrate CPython to Git and GitLab","excerpt":"Python Enhancement Proposal 507: 'Migrate CPython to Git and GitLab'에 대한 한국어 번역입니다.","date":"2025-09-26 22:53:39+0900","lastModifiedAt":"2025-09-26 22:53:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/507/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0506-adding_a_secrets_module_to_the_standard_library","title":"[Final] PEP 506 - Adding A Secrets Module To The Standard Library","excerpt":"Python Enhancement Proposal 506: 'Adding A Secrets Module To The Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 22:53:00+0900","lastModifiedAt":"2025-09-26 22:53:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/506/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0505-none-aware_operators","title":"[Deferred] PEP 505 - None-aware operators","excerpt":"Python Enhancement Proposal 505: 'None-aware operators'에 대한 한국어 번역입니다.","date":"2025-09-26 22:52:11+0900","lastModifiedAt":"2025-09-26 22:52:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/505/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0504-using_the_system_rng_by_default","title":"[Withdrawn] PEP 504 - Using the System RNG by default","excerpt":"Python Enhancement Proposal 504: 'Using the System RNG by default'에 대한 한국어 번역입니다.","date":"2025-09-26 22:47:22+0900","lastModifiedAt":"2025-09-26 22:47:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/504/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0503-simple_repository_api","title":"[Final] PEP 503 - Simple Repository API","excerpt":"Python Enhancement Proposal 503: 'Simple Repository API'에 대한 한국어 번역입니다.","date":"2025-09-26 22:45:48+0900","lastModifiedAt":"2025-09-26 22:45:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/503/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0502-string_interpolation_-_extended_discussion","title":"[Rejected] PEP 502 - String Interpolation - Extended Discussion","excerpt":"Python Enhancement Proposal 502: 'String Interpolation - Extended Discussion'에 대한 한국어 번역입니다.","date":"2025-09-26 22:45:25+0900","lastModifiedAt":"2025-09-26 22:45:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/502/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0501-general_purpose_template_literal_strings","title":"[Withdrawn] PEP 501 - General purpose template literal strings","excerpt":"Python Enhancement Proposal 501: 'General purpose template literal strings'에 대한 한국어 번역입니다.","date":"2025-09-26 22:44:34+0900","lastModifiedAt":"2025-09-26 22:44:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/501/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0500-a_protocol_for_delegating_datetime_methods_to_their_tzinfo_implementations","title":"[Rejected] PEP 500 - A protocol for delegating datetime methods to their tzinfo implementations","excerpt":"Python Enhancement Proposal 500: 'A protocol for delegating datetime methods to their tzinfo implementations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:43:18+0900","lastModifiedAt":"2025-09-26 22:43:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/500/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0499-python-mfooshould_also_bindfooinsys.modules","title":"[Deferred] PEP 499 - python-mfooshould also bind'foo'insys.modules","excerpt":"Python Enhancement Proposal 499: 'python-mfooshould also bind'foo'insys.modules'에 대한 한국어 번역입니다.","date":"2025-09-26 22:42:49+0900","lastModifiedAt":"2025-09-26 22:42:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/499/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0498-literal_string_interpolation","title":"[Final] PEP 498 - Literal String Interpolation","excerpt":"Python Enhancement Proposal 498: 'Literal String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:42:20+0900","lastModifiedAt":"2025-09-26 22:42:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/498/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0497-a_standard_mechanism_for_backward_compatibility","title":"[Rejected] PEP 497 - A standard mechanism for backward compatibility","excerpt":"Python Enhancement Proposal 497: 'A standard mechanism for backward compatibility'에 대한 한국어 번역입니다.","date":"2025-09-26 22:41:11+0900","lastModifiedAt":"2025-09-26 22:41:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/497/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0496-environment_markers","title":"[Rejected] PEP 496 - Environment Markers","excerpt":"Python Enhancement Proposal 496: 'Environment Markers'에 대한 한국어 번역입니다.","date":"2025-09-26 22:40:36+0900","lastModifiedAt":"2025-09-26 22:40:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/496/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0495-local_time_disambiguation","title":"[Final] PEP 495 - Local Time Disambiguation","excerpt":"Python Enhancement Proposal 495: 'Local Time Disambiguation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:40:14+0900","lastModifiedAt":"2025-09-26 22:40:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/495/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0494-python_3.6_release_schedule","title":"[Final] PEP 494 - Python 3.6 Release Schedule","excerpt":"Python Enhancement Proposal 494: 'Python 3.6 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 22:39:23+0900","lastModifiedAt":"2025-09-26 22:39:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/494/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0493-https_verification_migration_tools_for_python_2.7","title":"[Final] PEP 493 - HTTPS verification migration tools for Python 2.7","excerpt":"Python Enhancement Proposal 493: 'HTTPS verification migration tools for Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 22:38:56+0900","lastModifiedAt":"2025-09-26 22:38:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/493/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0492-coroutines_with_async_and_await_syntax","title":"[Final] PEP 492 - Coroutines with async and await syntax","excerpt":"Python Enhancement Proposal 492: 'Coroutines with async and await syntax'에 대한 한국어 번역입니다.","date":"2025-09-26 22:37:54+0900","lastModifiedAt":"2025-09-26 22:37:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/492/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0491-the_wheel_binary_package_format_1.9","title":"[Deferred] PEP 491 - The Wheel Binary Package Format 1.9","excerpt":"Python Enhancement Proposal 491: 'The Wheel Binary Package Format 1.9'에 대한 한국어 번역입니다.","date":"2025-09-26 22:36:44+0900","lastModifiedAt":"2025-09-26 22:36:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/491/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0490-chain_exceptions_at_c_level","title":"[Rejected] PEP 490 - Chain exceptions at C level","excerpt":"Python Enhancement Proposal 490: 'Chain exceptions at C level'에 대한 한국어 번역입니다.","date":"2025-09-26 22:35:41+0900","lastModifiedAt":"2025-09-26 22:35:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/490/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0489-multi-phase_extension_module_initialization","title":"[Final] PEP 489 - Multi-phase extension module initialization","excerpt":"Python Enhancement Proposal 489: 'Multi-phase extension module initialization'에 대한 한국어 번역입니다.","date":"2025-09-26 22:35:12+0900","lastModifiedAt":"2025-09-26 22:35:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/489/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0488-elimination_of_pyo_files","title":"[Final] PEP 488 - Elimination of PYO files","excerpt":"Python Enhancement Proposal 488: 'Elimination of PYO files'에 대한 한국어 번역입니다.","date":"2025-09-26 22:34:01+0900","lastModifiedAt":"2025-09-26 22:34:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/488/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0487-simpler_customisation_of_class_creation","title":"[Final] PEP 487 - Simpler customisation of class creation","excerpt":"Python Enhancement Proposal 487: 'Simpler customisation of class creation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:33:21+0900","lastModifiedAt":"2025-09-26 22:33:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/487/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0486-make_the_python_launcher_aware_of_virtual_environments","title":"[Final] PEP 486 - Make the Python Launcher aware of virtual environments","excerpt":"Python Enhancement Proposal 486: 'Make the Python Launcher aware of virtual environments'에 대한 한국어 번역입니다.","date":"2025-09-26 22:33:02+0900","lastModifiedAt":"2025-09-26 22:33:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/486/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0485-a_function_for_testing_approximate_equality","title":"[Final] PEP 485 - A Function for testing approximate equality","excerpt":"Python Enhancement Proposal 485: 'A Function for testing approximate equality'에 대한 한국어 번역입니다.","date":"2025-09-26 22:32:41+0900","lastModifiedAt":"2025-09-26 22:32:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/485/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0484-type_hints","title":"[Final] PEP 484 - Type Hints","excerpt":"Python Enhancement Proposal 484: 'Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:32:07+0900","lastModifiedAt":"2025-09-26 22:32:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/484/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0483-the_theory_of_type_hints","title":"[Final] PEP 483 - The Theory of Type Hints","excerpt":"Python Enhancement Proposal 483: 'The Theory of Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:28:39+0900","lastModifiedAt":"2025-09-26 22:28:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/483/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0482-literature_overview_for_type_hints","title":"[Final] PEP 482 - Literature Overview for Type Hints","excerpt":"Python Enhancement Proposal 482: 'Literature Overview for Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:27:40+0900","lastModifiedAt":"2025-09-26 22:27:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/482/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0481-migrate_cpython_to_git_github_and_phabricator","title":"[Withdrawn] PEP 481 - Migrate CPython to Git, Github, and Phabricator","excerpt":"Python Enhancement Proposal 481: 'Migrate CPython to Git, Github, and Phabricator'에 대한 한국어 번역입니다.","date":"2025-09-26 22:27:20+0900","lastModifiedAt":"2025-09-26 22:27:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/481/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0480-surviving_a_compromise_of_pypi_end-to-end_signing_of_packages","title":"[Draft] PEP 480 - Surviving a Compromise of PyPI: End-to-end signing of packages","excerpt":"Python Enhancement Proposal 480: 'Surviving a Compromise of PyPI: End-to-end signing of packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:26:39+0900","lastModifiedAt":"2025-09-26 22:26:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/480/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0479-change_stopiteration_handling_inside_generators","title":"[Final] PEP 479 - Change StopIteration handling inside generators","excerpt":"Python Enhancement Proposal 479: 'Change StopIteration handling inside generators'에 대한 한국어 번역입니다.","date":"2025-09-26 22:23:28+0900","lastModifiedAt":"2025-09-26 22:23:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/479/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0478-python_3.5_release_schedule","title":"[Final] PEP 478 - Python 3.5 Release Schedule","excerpt":"Python Enhancement Proposal 478: 'Python 3.5 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:47+0900","lastModifiedAt":"2025-09-26 22:22:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/478/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0477-backport_ensurepip_pep_453_to_python_2.7","title":"[Final] PEP 477 - Backport ensurepip (PEP 453) to Python 2.7","excerpt":"Python Enhancement Proposal 477: 'Backport ensurepip (PEP 453) to Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:24+0900","lastModifiedAt":"2025-09-26 22:22:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/477/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0476-enabling_certificate_verification_by_default_for_stdlib_http_clients","title":"[Final] PEP 476 - Enabling certificate verification by default for stdlib http clients","excerpt":"Python Enhancement Proposal 476: 'Enabling certificate verification by default for stdlib http clients'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:04+0900","lastModifiedAt":"2025-09-26 22:22:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/476/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0475-retry_system_calls_failing_with_eintr","title":"[Final] PEP 475 - Retry system calls failing with EINTR","excerpt":"Python Enhancement Proposal 475: 'Retry system calls failing with EINTR'에 대한 한국어 번역입니다.","date":"2025-09-26 22:21:40+0900","lastModifiedAt":"2025-09-26 22:21:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/475/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0474-creating_forge.python.org","title":"[Withdrawn] PEP 474 - Creating forge.python.org","excerpt":"Python Enhancement Proposal 474: 'Creating forge.python.org'에 대한 한국어 번역입니다.","date":"2025-09-26 22:21:06+0900","lastModifiedAt":"2025-09-26 22:21:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/474/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0473-adding_structured_data_to_built-in_exceptions","title":"[Rejected] PEP 473 - Adding structured data to built-in exceptions","excerpt":"Python Enhancement Proposal 473: 'Adding structured data to built-in exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 22:19:47+0900","lastModifiedAt":"2025-09-26 22:19:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/473/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0472-support_for_indexing_with_keyword_arguments","title":"[Rejected] PEP 472 - Support for indexing with keyword arguments","excerpt":"Python Enhancement Proposal 472: 'Support for indexing with keyword arguments'에 대한 한국어 번역입니다.","date":"2025-09-26 22:19:17+0900","lastModifiedAt":"2025-09-26 22:19:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/472/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0471-os.scandir_function_a_better_and_faster_directory_iterator","title":"[Final] PEP 471 - os.scandir() function – a better and faster directory iterator","excerpt":"Python Enhancement Proposal 471: 'os.scandir() function – a better and faster directory iterator'에 대한 한국어 번역입니다.","date":"2025-09-26 22:18:05+0900","lastModifiedAt":"2025-09-26 22:18:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/471/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0470-removing_external_hosting_support_on_pypi","title":"[Final] PEP 470 - Removing External Hosting Support on PyPI","excerpt":"Python Enhancement Proposal 470: 'Removing External Hosting Support on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 22:17:18+0900","lastModifiedAt":"2025-09-26 22:17:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/470/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0469-migration_of_dict_iteration_code_to_python_3","title":"[Withdrawn] PEP 469 - Migration of dict iteration code to Python 3","excerpt":"Python Enhancement Proposal 469: 'Migration of dict iteration code to Python 3'에 대한 한국어 번역입니다.","date":"2025-09-26 22:15:48+0900","lastModifiedAt":"2025-09-26 22:15:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/469/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0468-preserving_the_order_of_kwargs_in_a_function.","title":"[Final] PEP 468 - Preserving the order of **kwargs in a function.","excerpt":"Python Enhancement Proposal 468: 'Preserving the order of **kwargs in a function.'에 대한 한국어 번역입니다.","date":"2025-09-26 22:15:02+0900","lastModifiedAt":"2025-09-26 22:15:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/468/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0467-minor_api_improvements_for_binary_sequences","title":"[Draft] PEP 467 - Minor API improvements for binary sequences","excerpt":"Python Enhancement Proposal 467: 'Minor API improvements for binary sequences'에 대한 한국어 번역입니다.","date":"2025-09-26 22:14:25+0900","lastModifiedAt":"2025-09-26 22:14:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/467/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0466-network_security_enhancements_for_python_2.7.x","title":"[Final] PEP 466 - Network Security Enhancements for Python 2.7.x","excerpt":"Python Enhancement Proposal 466: 'Network Security Enhancements for Python 2.7.x'에 대한 한국어 번역입니다.","date":"2025-09-26 22:13:59+0900","lastModifiedAt":"2025-09-26 22:13:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/466/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0465-a_dedicated_infix_operator_for_matrix_multiplication","title":"[Final] PEP 465 - A dedicated infix operator for matrix multiplication","excerpt":"Python Enhancement Proposal 465: 'A dedicated infix operator for matrix multiplication'에 대한 한국어 번역입니다.","date":"2025-09-26 22:12:46+0900","lastModifiedAt":"2025-09-26 22:12:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/465/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0464-removal_of_the_pypi_mirror_authenticity_api","title":"[Final] PEP 464 - Removal of the PyPI Mirror Authenticity API","excerpt":"Python Enhancement Proposal 464: 'Removal of the PyPI Mirror Authenticity API'에 대한 한국어 번역입니다.","date":"2025-09-26 22:11:47+0900","lastModifiedAt":"2025-09-26 22:11:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/464/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0463-exception-catching_expressions","title":"[Rejected] PEP 463 - Exception-catching expressions","excerpt":"Python Enhancement Proposal 463: 'Exception-catching expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 22:11:32+0900","lastModifiedAt":"2025-09-26 22:11:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/463/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0462-core_development_workflow_automation_for_cpython","title":"[Withdrawn] PEP 462 - Core development workflow automation for CPython","excerpt":"Python Enhancement Proposal 462: 'Core development workflow automation for CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 22:10:21+0900","lastModifiedAt":"2025-09-26 22:10:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/462/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0461-adding_formatting_to_bytes_and_bytearray","title":"[Final] PEP 461 - Adding % formatting to bytes and bytearray","excerpt":"Python Enhancement Proposal 461: 'Adding % formatting to bytes and bytearray'에 대한 한국어 번역입니다.","date":"2025-09-26 22:09:29+0900","lastModifiedAt":"2025-09-26 22:09:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/461/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0460-add_binary_interpolation_and_formatting","title":"[Withdrawn] PEP 460 - Add binary interpolation and formatting","excerpt":"Python Enhancement Proposal 460: 'Add binary interpolation and formatting'에 대한 한국어 번역입니다.","date":"2025-09-26 22:08:41+0900","lastModifiedAt":"2025-09-26 22:08:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/460/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0459-standard_metadata_extensions_for_python_software_packages","title":"[Withdrawn] PEP 459 - Standard Metadata Extensions for Python Software Packages","excerpt":"Python Enhancement Proposal 459: 'Standard Metadata Extensions for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:08:18+0900","lastModifiedAt":"2025-09-26 22:08:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/459/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0458-secure_pypi_downloads_with_signed_repository_metadata","title":"[Accepted] PEP 458 - Secure PyPI downloads with signed repository metadata","excerpt":"Python Enhancement Proposal 458: 'Secure PyPI downloads with signed repository metadata'에 대한 한국어 번역입니다.","date":"2025-09-26 22:07:13+0900","lastModifiedAt":"2025-09-26 22:07:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/458/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0457-notation_for_positional-only_parameters","title":"[Final] PEP 457 - Notation For Positional-Only Parameters","excerpt":"Python Enhancement Proposal 457: 'Notation For Positional-Only Parameters'에 대한 한국어 번역입니다.","date":"2025-09-26 22:06:22+0900","lastModifiedAt":"2025-09-26 22:06:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/457/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0456-secure_and_interchangeable_hash_algorithm","title":"[Final] PEP 456 - Secure and interchangeable hash algorithm","excerpt":"Python Enhancement Proposal 456: 'Secure and interchangeable hash algorithm'에 대한 한국어 번역입니다.","date":"2025-09-26 22:05:49+0900","lastModifiedAt":"2025-09-26 22:05:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/456/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0455-adding_a_key-transforming_dictionary_to_collections","title":"[Rejected] PEP 455 - Adding a key-transforming dictionary to collections","excerpt":"Python Enhancement Proposal 455: 'Adding a key-transforming dictionary to collections'에 대한 한국어 번역입니다.","date":"2025-09-26 22:05:06+0900","lastModifiedAt":"2025-09-26 22:05:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/455/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0454-add_a_new_tracemalloc_module_to_trace_python_memory_allocations","title":"[Final] PEP 454 - Add a new tracemalloc module to trace Python memory allocations","excerpt":"Python Enhancement Proposal 454: 'Add a new tracemalloc module to trace Python memory allocations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:04:39+0900","lastModifiedAt":"2025-09-26 22:04:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/454/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0453-explicit_bootstrapping_of_pip_in_python_installations","title":"[Final] PEP 453 - Explicit bootstrapping of pip in Python installations","excerpt":"Python Enhancement Proposal 453: 'Explicit bootstrapping of pip in Python installations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:03:55+0900","lastModifiedAt":"2025-09-26 22:03:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/453/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0452-api_for_cryptographic_hash_functions_v2.0","title":"[Final] PEP 452 - API for Cryptographic Hash Functions v2.0","excerpt":"Python Enhancement Proposal 452: 'API for Cryptographic Hash Functions v2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 22:03:16+0900","lastModifiedAt":"2025-09-26 22:03:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/452/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0451-a_modulespec_type_for_the_import_system","title":"[Final] PEP 451 - A ModuleSpec Type for the Import System","excerpt":"Python Enhancement Proposal 451: 'A ModuleSpec Type for the Import System'에 대한 한국어 번역입니다.","date":"2025-09-26 22:02:47+0900","lastModifiedAt":"2025-09-26 22:02:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/451/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0450-adding_a_statistics_module_to_the_standard_library","title":"[Final] PEP 450 - Adding A Statistics Module To The Standard Library","excerpt":"Python Enhancement Proposal 450: 'Adding A Statistics Module To The Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:57+0900","lastModifiedAt":"2025-09-26 22:00:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/450/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0449-removal_of_the_pypi_mirror_auto_discovery_and_naming_scheme","title":"[Final] PEP 449 - Removal of the PyPI Mirror Auto Discovery and Naming Scheme","excerpt":"Python Enhancement Proposal 449: 'Removal of the PyPI Mirror Auto Discovery and Naming Scheme'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:20+0900","lastModifiedAt":"2025-09-26 22:00:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/449/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0448-additional_unpacking_generalizations","title":"[Final] PEP 448 - Additional Unpacking Generalizations","excerpt":"Python Enhancement Proposal 448: 'Additional Unpacking Generalizations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:00+0900","lastModifiedAt":"2025-09-26 22:00:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/448/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0447-add___getdescriptor___method_to_metaclass","title":"[Deferred] PEP 447 - Add __getdescriptor__ method to metaclass","excerpt":"Python Enhancement Proposal 447: 'Add __getdescriptor__ method to metaclass'에 대한 한국어 번역입니다.","date":"2025-09-26 21:59:30+0900","lastModifiedAt":"2025-09-26 21:59:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/447/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0446-make_newly_created_file_descriptors_non-inheritable","title":"[Final] PEP 446 - Make newly created file descriptors non-inheritable","excerpt":"Python Enhancement Proposal 446: 'Make newly created file descriptors non-inheritable'에 대한 한국어 번역입니다.","date":"2025-09-26 21:58:51+0900","lastModifiedAt":"2025-09-26 21:58:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/446/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0445-add_new_apis_to_customize_python_memory_allocators","title":"[Final] PEP 445 - Add new APIs to customize Python memory allocators","excerpt":"Python Enhancement Proposal 445: 'Add new APIs to customize Python memory allocators'에 대한 한국어 번역입니다.","date":"2025-09-26 21:58:25+0900","lastModifiedAt":"2025-09-26 21:58:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/445/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0444-python_web3_interface","title":"[Deferred] PEP 444 - Python Web3 Interface","excerpt":"Python Enhancement Proposal 444: 'Python Web3 Interface'에 대한 한국어 번역입니다.","date":"2025-09-26 21:57:54+0900","lastModifiedAt":"2025-09-26 21:57:54+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/444/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0443-single-dispatch_generic_functions","title":"[Final] PEP 443 - Single-dispatch generic functions","excerpt":"Python Enhancement Proposal 443: 'Single-dispatch generic functions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:55:16+0900","lastModifiedAt":"2025-09-26 21:55:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/443/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0442-safe_object_finalization","title":"[Final] PEP 442 - Safe object finalization","excerpt":"Python Enhancement Proposal 442: 'Safe object finalization'에 대한 한국어 번역입니다.","date":"2025-09-26 21:54:42+0900","lastModifiedAt":"2025-09-26 21:54:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/442/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0441-improving_python_zip_application_support","title":"[Final] PEP 441 - Improving Python ZIP Application Support","excerpt":"Python Enhancement Proposal 441: 'Improving Python ZIP Application Support'에 대한 한국어 번역입니다.","date":"2025-09-26 21:54:04+0900","lastModifiedAt":"2025-09-26 21:54:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/441/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0440-version_identification_and_dependency_specification","title":"[Final] PEP 440 - Version Identification and Dependency Specification","excerpt":"Python Enhancement Proposal 440: 'Version Identification and Dependency Specification'에 대한 한국어 번역입니다.","date":"2025-09-26 21:53:38+0900","lastModifiedAt":"2025-09-26 21:53:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/440/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0439-inclusion_of_implicit_pip_bootstrap_in_python_installation","title":"[Rejected] PEP 439 - Inclusion of implicit pip bootstrap in Python installation","excerpt":"Python Enhancement Proposal 439: 'Inclusion of implicit pip bootstrap in Python installation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:52:55+0900","lastModifiedAt":"2025-09-26 21:52:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/439/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0438-transitioning_to_release-file_hosting_on_pypi","title":"[Superseded] PEP 438 - Transitioning to release-file hosting on PyPI","excerpt":"Python Enhancement Proposal 438: 'Transitioning to release-file hosting on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:52:24+0900","lastModifiedAt":"2025-09-26 21:52:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/438/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0437-a_dsl_for_specifying_signatures_annotations_and_argument_converters","title":"[Rejected] PEP 437 - A DSL for specifying signatures, annotations and argument converters","excerpt":"Python Enhancement Proposal 437: 'A DSL for specifying signatures, annotations and argument converters'에 대한 한국어 번역입니다.","date":"2025-09-26 21:51:38+0900","lastModifiedAt":"2025-09-26 21:51:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/437/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0436-the_argument_clinic_dsl","title":"[Final] PEP 436 - The Argument Clinic DSL","excerpt":"Python Enhancement Proposal 436: 'The Argument Clinic DSL'에 대한 한국어 번역입니다.","date":"2025-09-26 21:51:01+0900","lastModifiedAt":"2025-09-26 21:51:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/436/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0435-adding_an_enum_type_to_the_python_standard_library","title":"[Final] PEP 435 - Adding an Enum type to the Python standard library","excerpt":"Python Enhancement Proposal 435: 'Adding an Enum type to the Python standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:46:35+0900","lastModifiedAt":"2025-09-26 21:46:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/435/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0434-idle_enhancement_exception_for_all_branches","title":"[Active] PEP 434 - IDLE Enhancement Exception for All Branches","excerpt":"Python Enhancement Proposal 434: 'IDLE Enhancement Exception for All Branches'에 대한 한국어 번역입니다.","date":"2025-09-26 21:45:59+0900","lastModifiedAt":"2025-09-26 21:45:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/434/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0433-easier_suppression_of_file_descriptor_inheritance","title":"[Superseded] PEP 433 - Easier suppression of file descriptor inheritance","excerpt":"Python Enhancement Proposal 433: 'Easier suppression of file descriptor inheritance'에 대한 한국어 번역입니다.","date":"2025-09-26 21:45:26+0900","lastModifiedAt":"2025-09-26 21:45:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/433/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0432-restructuring_the_cpython_startup_sequence","title":"[Withdrawn] PEP 432 - Restructuring the CPython startup sequence","excerpt":"Python Enhancement Proposal 432: 'Restructuring the CPython startup sequence'에 대한 한국어 번역입니다.","date":"2025-09-26 21:44:16+0900","lastModifiedAt":"2025-09-26 21:44:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/432/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0431-time_zone_support_improvements","title":"[Superseded] PEP 431 - Time zone support improvements","excerpt":"Python Enhancement Proposal 431: 'Time zone support improvements'에 대한 한국어 번역입니다.","date":"2025-09-26 21:43:32+0900","lastModifiedAt":"2025-09-26 21:43:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/431/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0430-migrating_to_python_3_as_the_default_online_documentation","title":"[Final] PEP 430 - Migrating to Python 3 as the default online documentation","excerpt":"Python Enhancement Proposal 430: 'Migrating to Python 3 as the default online documentation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:43:01+0900","lastModifiedAt":"2025-09-26 21:43:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/430/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0429-python_3.4_release_schedule","title":"[Final] PEP 429 - Python 3.4 Release Schedule","excerpt":"Python Enhancement Proposal 429: 'Python 3.4 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:42:34+0900","lastModifiedAt":"2025-09-26 21:42:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/429/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0428-the_pathlib_module_object-oriented_filesystem_paths","title":"[Final] PEP 428 - The pathlib module – object-oriented filesystem paths","excerpt":"Python Enhancement Proposal 428: 'The pathlib module – object-oriented filesystem paths'에 대한 한국어 번역입니다.","date":"2025-09-26 21:42:10+0900","lastModifiedAt":"2025-09-26 21:42:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/428/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0427-the_wheel_binary_package_format_1.0","title":"[Final] PEP 427 - The Wheel Binary Package Format 1.0","excerpt":"Python Enhancement Proposal 427: 'The Wheel Binary Package Format 1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 21:41:28+0900","lastModifiedAt":"2025-09-26 21:41:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/427/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0426-metadata_for_python_software_packages_2.0","title":"[Withdrawn] PEP 426 - Metadata for Python Software Packages 2.0","excerpt":"Python Enhancement Proposal 426: 'Metadata for Python Software Packages 2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 21:40:33+0900","lastModifiedAt":"2025-09-26 21:40:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/426/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0425-compatibility_tags_for_built_distributions","title":"[Final] PEP 425 - Compatibility Tags for Built Distributions","excerpt":"Python Enhancement Proposal 425: 'Compatibility Tags for Built Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:40:01+0900","lastModifiedAt":"2025-09-26 21:40:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/425/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0424-a_method_for_exposing_a_length_hint","title":"[Final] PEP 424 - A method for exposing a length hint","excerpt":"Python Enhancement Proposal 424: 'A method for exposing a length hint'에 대한 한국어 번역입니다.","date":"2025-09-26 21:39:24+0900","lastModifiedAt":"2025-09-26 21:39:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/424/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0423-naming_conventions_and_recipes_related_to_packaging","title":"[Deferred] PEP 423 - Naming conventions and recipes related to packaging","excerpt":"Python Enhancement Proposal 423: 'Naming conventions and recipes related to packaging'에 대한 한국어 번역입니다.","date":"2025-09-26 21:39:09+0900","lastModifiedAt":"2025-09-26 21:39:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/423/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0422-simpler_customisation_of_class_creation","title":"[Withdrawn] PEP 422 - Simpler customisation of class creation","excerpt":"Python Enhancement Proposal 422: 'Simpler customisation of class creation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:38:32+0900","lastModifiedAt":"2025-09-26 21:38:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/422/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0421-adding_sys.implementation","title":"[Final] PEP 421 - Adding sys.implementation","excerpt":"Python Enhancement Proposal 421: 'Adding sys.implementation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:37:48+0900","lastModifiedAt":"2025-09-26 21:37:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/421/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0420-implicit_namespace_packages","title":"[Final] PEP 420 - Implicit Namespace Packages","excerpt":"Python Enhancement Proposal 420: 'Implicit Namespace Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 21:37:09+0900","lastModifiedAt":"2025-09-26 21:37:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/420/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0419-protecting_cleanup_statements_from_interruptions","title":"[Deferred] PEP 419 - Protecting cleanup statements from interruptions","excerpt":"Python Enhancement Proposal 419: 'Protecting cleanup statements from interruptions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:36:35+0900","lastModifiedAt":"2025-09-26 21:36:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/419/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0418-add_monotonic_time_performance_counter_and_process_time_functions","title":"[Final] PEP 418 - Add monotonic time, performance counter, and process time functions","excerpt":"Python Enhancement Proposal 418: 'Add monotonic time, performance counter, and process time functions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:35:49+0900","lastModifiedAt":"2025-09-26 21:35:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/418/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0417-including_mock_in_the_standard_library","title":"[Final] PEP 417 - Including mock in the Standard Library","excerpt":"Python Enhancement Proposal 417: 'Including mock in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:35:01+0900","lastModifiedAt":"2025-09-26 21:35:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/417/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0416-add_a_frozendict_builtin_type","title":"[Rejected] PEP 416 - Add a frozendict builtin type","excerpt":"Python Enhancement Proposal 416: 'Add a frozendict builtin type'에 대한 한국어 번역입니다.","date":"2025-09-26 21:34:46+0900","lastModifiedAt":"2025-09-26 21:34:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/416/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0415-implement_context_suppression_with_exception_attributes","title":"[Final] PEP 415 - Implement context suppression with exception attributes","excerpt":"Python Enhancement Proposal 415: 'Implement context suppression with exception attributes'에 대한 한국어 번역입니다.","date":"2025-09-26 21:34:10+0900","lastModifiedAt":"2025-09-26 21:34:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/415/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0414-explicit_unicode_literal_for_python_3.3","title":"[Final] PEP 414 - Explicit Unicode Literal for Python 3.3","excerpt":"Python Enhancement Proposal 414: 'Explicit Unicode Literal for Python 3.3'에 대한 한국어 번역입니다.","date":"2025-09-26 21:33:55+0900","lastModifiedAt":"2025-09-26 21:33:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/414/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0413-faster_evolution_of_the_python_standard_library","title":"[Withdrawn] PEP 413 - Faster evolution of the Python Standard Library","excerpt":"Python Enhancement Proposal 413: 'Faster evolution of the Python Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:33:07+0900","lastModifiedAt":"2025-09-26 21:33:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/413/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0412-key-sharing_dictionary","title":"[Final] PEP 412 - Key-Sharing Dictionary","excerpt":"Python Enhancement Proposal 412: 'Key-Sharing Dictionary'에 대한 한국어 번역입니다.","date":"2025-09-26 21:32:26+0900","lastModifiedAt":"2025-09-26 21:32:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/412/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0411-provisional_packages_in_the_python_standard_library","title":"[Superseded] PEP 411 - Provisional packages in the Python standard library","excerpt":"Python Enhancement Proposal 411: 'Provisional packages in the Python standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:31:48+0900","lastModifiedAt":"2025-09-26 21:31:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/411/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0410-use_decimal.decimal_type_for_timestamps","title":"[Rejected] PEP 410 - Use decimal.Decimal type for timestamps","excerpt":"Python Enhancement Proposal 410: 'Use decimal.Decimal type for timestamps'에 대한 한국어 번역입니다.","date":"2025-09-26 21:31:14+0900","lastModifiedAt":"2025-09-26 21:31:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/410/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0409-suppressing_exception_context","title":"[Final] PEP 409 - Suppressing exception context","excerpt":"Python Enhancement Proposal 409: 'Suppressing exception context'에 대한 한국어 번역입니다.","date":"2025-09-26 21:30:26+0900","lastModifiedAt":"2025-09-26 21:30:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/409/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0408-standard_library___preview___package","title":"[Rejected] PEP 408 - Standard library __preview__ package","excerpt":"Python Enhancement Proposal 408: 'Standard library __preview__ package'에 대한 한국어 번역입니다.","date":"2025-09-26 21:30:04+0900","lastModifiedAt":"2025-09-26 21:30:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/408/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0407-new_release_cycle_and_introducing_long-term_support_versions","title":"[Deferred] PEP 407 - New release cycle and introducing long-term support versions","excerpt":"Python Enhancement Proposal 407: 'New release cycle and introducing long-term support versions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:29:25+0900","lastModifiedAt":"2025-09-26 21:29:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/407/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0406-improved_encapsulation_of_import_state","title":"[Withdrawn] PEP 406 - Improved Encapsulation of Import State","excerpt":"Python Enhancement Proposal 406: 'Improved Encapsulation of Import State'에 대한 한국어 번역입니다.","date":"2025-09-26 21:29:05+0900","lastModifiedAt":"2025-09-26 21:29:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/406/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0405-python_virtual_environments","title":"[Final] PEP 405 - Python Virtual Environments","excerpt":"Python Enhancement Proposal 405: 'Python Virtual Environments'에 대한 한국어 번역입니다.","date":"2025-09-26 21:28:37+0900","lastModifiedAt":"2025-09-26 21:28:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/405/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0404-python_2.8_un-release_schedule","title":"[Final] PEP 404 - Python 2.8 Un-release Schedule","excerpt":"Python Enhancement Proposal 404: 'Python 2.8 Un-release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:27:55+0900","lastModifiedAt":"2025-09-26 21:27:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/404/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0403-general_purpose_decorator_clause_aka_in_clause","title":"[Deferred] PEP 403 - General purpose decorator clause (aka “@in” clause)","excerpt":"Python Enhancement Proposal 403: 'General purpose decorator clause (aka “@in” clause)'에 대한 한국어 번역입니다.","date":"2025-09-26 21:27:31+0900","lastModifiedAt":"2025-09-26 21:27:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/403/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0402-simplified_package_layout_and_partitioning","title":"[Rejected] PEP 402 - Simplified Package Layout and Partitioning","excerpt":"Python Enhancement Proposal 402: 'Simplified Package Layout and Partitioning'에 대한 한국어 번역입니다.","date":"2025-09-26 21:26:40+0900","lastModifiedAt":"2025-09-26 21:26:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/402/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0401-bdfl_retirement","title":"[April Fool!] PEP 401 - BDFL Retirement","excerpt":"Python Enhancement Proposal 401: 'BDFL Retirement'에 대한 한국어 번역입니다.","date":"2025-09-26 21:25:14+0900","lastModifiedAt":"2025-09-26 21:25:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/401/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0400-deprecate_codecs.streamreader_and_codecs.streamwriter","title":"[Deferred] PEP 400 - Deprecate codecs.StreamReader and codecs.StreamWriter","excerpt":"Python Enhancement Proposal 400: 'Deprecate codecs.StreamReader and codecs.StreamWriter'에 대한 한국어 번역입니다.","date":"2025-09-26 21:24:56+0900","lastModifiedAt":"2025-09-26 21:24:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/400/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0399-pure_pythonc_accelerator_module_compatibility_requirements","title":"[Final] PEP 399 - Pure Python/C Accelerator Module Compatibility Requirements","excerpt":"Python Enhancement Proposal 399: 'Pure Python/C Accelerator Module Compatibility Requirements'에 대한 한국어 번역입니다.","date":"2025-09-26 21:24:25+0900","lastModifiedAt":"2025-09-26 21:24:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/399/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0398-python_3.3_release_schedule","title":"[Final] PEP 398 - Python 3.3 Release Schedule","excerpt":"Python Enhancement Proposal 398: 'Python 3.3 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:23:58+0900","lastModifiedAt":"2025-09-26 21:23:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/398/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0397-python_launcher_for_windows","title":"[Final] PEP 397 - Python launcher for Windows","excerpt":"Python Enhancement Proposal 397: 'Python launcher for Windows'에 대한 한국어 번역입니다.","date":"2025-09-26 21:23:27+0900","lastModifiedAt":"2025-09-26 21:23:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/397/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0396-module_version_numbers","title":"[Withdrawn] PEP 396 - Module Version Numbers","excerpt":"Python Enhancement Proposal 396: 'Module Version Numbers'에 대한 한국어 번역입니다.","date":"2025-09-26 21:22:37+0900","lastModifiedAt":"2025-09-26 21:22:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/396/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0395-qualified_names_for_modules","title":"[Withdrawn] PEP 395 - Qualified Names for Modules","excerpt":"Python Enhancement Proposal 395: 'Qualified Names for Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 21:22:00+0900","lastModifiedAt":"2025-09-26 21:22:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/395/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0394-the_python_command_on_unix-like_systems","title":"[Active] PEP 394 - The “python” Command on Unix-Like Systems","excerpt":"Python Enhancement Proposal 394: 'The “python” Command on Unix-Like Systems'에 대한 한국어 번역입니다.","date":"2025-09-26 21:21:20+0900","lastModifiedAt":"2025-09-26 21:21:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/394/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0393-flexible_string_representation","title":"[Final] PEP 393 - Flexible String Representation","excerpt":"Python Enhancement Proposal 393: 'Flexible String Representation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:20:21+0900","lastModifiedAt":"2025-09-26 21:20:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/393/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0392-python_3.2_release_schedule","title":"[Final] PEP 392 - Python 3.2 Release Schedule","excerpt":"Python Enhancement Proposal 392: 'Python 3.2 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:19:41+0900","lastModifiedAt":"2025-09-26 21:19:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/392/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0391-dictionary-based_configuration_for_logging","title":"[Final] PEP 391 - Dictionary-Based Configuration For Logging","excerpt":"Python Enhancement Proposal 391: 'Dictionary-Based Configuration For Logging'에 대한 한국어 번역입니다.","date":"2025-09-26 21:19:23+0900","lastModifiedAt":"2025-09-26 21:19:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/391/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0390-static_metadata_for_distutils","title":"[Rejected] PEP 390 - Static metadata for Distutils","excerpt":"Python Enhancement Proposal 390: 'Static metadata for Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 21:09:17+0900","lastModifiedAt":"2025-09-26 21:09:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/390/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0389-argparse_-_new_command_line_parsing_module","title":"[Final] PEP 389 - argparse - New Command Line Parsing Module","excerpt":"Python Enhancement Proposal 389: 'argparse - New Command Line Parsing Module'에 대한 한국어 번역입니다.","date":"2025-09-26 21:08:50+0900","lastModifiedAt":"2025-09-26 21:08:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/389/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0387-backwards_compatibility_policy","title":"[Active] PEP 387 - Backwards Compatibility Policy","excerpt":"Python Enhancement Proposal 387: 'Backwards Compatibility Policy'에 대한 한국어 번역입니다.","date":"2025-09-26 21:07:43+0900","lastModifiedAt":"2025-09-26 21:07:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/387/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0386-changing_the_version_comparison_module_in_distutils","title":"[Superseded] PEP 386 - Changing the version comparison module in Distutils","excerpt":"Python Enhancement Proposal 386: 'Changing the version comparison module in Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 21:07:16+0900","lastModifiedAt":"2025-09-26 21:07:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/386/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0385-migrating_from_subversion_to_mercurial","title":"[Final] PEP 385 - Migrating from Subversion to Mercurial","excerpt":"Python Enhancement Proposal 385: 'Migrating from Subversion to Mercurial'에 대한 한국어 번역입니다.","date":"2025-09-26 21:05:29+0900","lastModifiedAt":"2025-09-26 21:05:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/385/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0384-defining_a_stable_abi","title":"[Final] PEP 384 - Defining a Stable ABI","excerpt":"Python Enhancement Proposal 384: 'Defining a Stable ABI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:04:32+0900","lastModifiedAt":"2025-09-26 21:04:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/384/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0383-non-decodable_bytes_in_system_character_interfaces","title":"[Final] PEP 383 - Non-decodable Bytes in System Character Interfaces","excerpt":"Python Enhancement Proposal 383: 'Non-decodable Bytes in System Character Interfaces'에 대한 한국어 번역입니다.","date":"2025-09-26 21:03:51+0900","lastModifiedAt":"2025-09-26 21:03:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/383/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0382-namespace_packages","title":"[Rejected] PEP 382 - Namespace Packages","excerpt":"Python Enhancement Proposal 382: 'Namespace Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 21:03:27+0900","lastModifiedAt":"2025-09-26 21:03:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/382/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0381-mirroring_infrastructure_for_pypi","title":"[Withdrawn] PEP 381 - Mirroring infrastructure for PyPI","excerpt":"Python Enhancement Proposal 381: 'Mirroring infrastructure for PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:02:58+0900","lastModifiedAt":"2025-09-26 21:02:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/381/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0380-syntax_for_delegating_to_a_subgenerator","title":"[Final] PEP 380 - Syntax for Delegating to a Subgenerator","excerpt":"Python Enhancement Proposal 380: 'Syntax for Delegating to a Subgenerator'에 대한 한국어 번역입니다.","date":"2025-09-26 21:02:19+0900","lastModifiedAt":"2025-09-26 21:02:19+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/380/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0379-adding_an_assignment_expression","title":"[Withdrawn] PEP 379 - Adding an Assignment Expression","excerpt":"Python Enhancement Proposal 379: 'Adding an Assignment Expression'에 대한 한국어 번역입니다.","date":"2025-09-26 21:01:34+0900","lastModifiedAt":"2025-09-26 21:01:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/379/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0378-format_specifier_for_thousands_separator","title":"[Final] PEP 378 - Format Specifier for Thousands Separator","excerpt":"Python Enhancement Proposal 378: 'Format Specifier for Thousands Separator'에 대한 한국어 번역입니다.","date":"2025-09-26 21:01:13+0900","lastModifiedAt":"2025-09-26 21:01:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/378/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0377-allow___enter___methods_to_skip_the_statement_body","title":"[Rejected] PEP 377 - Allow __enter__() methods to skip the statement body","excerpt":"Python Enhancement Proposal 377: 'Allow __enter__() methods to skip the statement body'에 대한 한국어 번역입니다.","date":"2025-09-26 21:00:13+0900","lastModifiedAt":"2025-09-26 21:00:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/377/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0376-database_of_installed_python_distributions","title":"[Final] PEP 376 - Database of Installed Python Distributions","excerpt":"Python Enhancement Proposal 376: 'Database of Installed Python Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:42+0900","lastModifiedAt":"2025-09-26 20:59:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/376/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0375-python_3.1_release_schedule","title":"[Final] PEP 375 - Python 3.1 Release Schedule","excerpt":"Python Enhancement Proposal 375: 'Python 3.1 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:16+0900","lastModifiedAt":"2025-09-26 20:59:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/375/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0374-choosing_a_distributed_vcs_for_the_python_project","title":"[Final] PEP 374 - Choosing a distributed VCS for the Python project","excerpt":"Python Enhancement Proposal 374: 'Choosing a distributed VCS for the Python project'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:02+0900","lastModifiedAt":"2025-09-26 20:59:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/374/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0373-python_2.7_release_schedule","title":"[Final] PEP 373 - Python 2.7 Release Schedule","excerpt":"Python Enhancement Proposal 373: 'Python 2.7 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 20:56:29+0900","lastModifiedAt":"2025-09-26 20:56:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/373/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0372-adding_an_ordered_dictionary_to_collections","title":"[Final] PEP 372 - Adding an ordered dictionary to collections","excerpt":"Python Enhancement Proposal 372: 'Adding an ordered dictionary to collections'에 대한 한국어 번역입니다.","date":"2025-09-26 20:56:08+0900","lastModifiedAt":"2025-09-26 20:56:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/372/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0371-addition_of_the_multiprocessing_package_to_the_standard_library","title":"[Final] PEP 371 - Addition of the multiprocessing package to the standard library","excerpt":"Python Enhancement Proposal 371: 'Addition of the multiprocessing package to the standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 20:55:33+0900","lastModifiedAt":"2025-09-26 20:55:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/371/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0370-per_user_site-packages_directory","title":"[Final] PEP 370 - Per user site-packages directory","excerpt":"Python Enhancement Proposal 370: 'Per user site-packages directory'에 대한 한국어 번역입니다.","date":"2025-09-26 20:54:08+0900","lastModifiedAt":"2025-09-26 20:54:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/370/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0369-post_import_hooks","title":"[Withdrawn] PEP 369 - Post import hooks","excerpt":"Python Enhancement Proposal 369: 'Post import hooks'에 대한 한국어 번역입니다.","date":"2025-09-26 20:53:39+0900","lastModifiedAt":"2025-09-26 20:53:39+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/369/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0368-standard_image_protocol_and_class","title":"[Deferred] PEP 368 - Standard image protocol and class","excerpt":"Python Enhancement Proposal 368: 'Standard image protocol and class'에 대한 한국어 번역입니다.","date":"2025-09-26 20:53:03+0900","lastModifiedAt":"2025-09-26 20:53:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/368/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0367-new_super","title":"[Superseded] PEP 367 - New Super","excerpt":"Python Enhancement Proposal 367: 'New Super'에 대한 한국어 번역입니다.","date":"2025-09-26 20:50:02+0900","lastModifiedAt":"2025-09-26 20:50:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/367/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0366-main_module_explicit_relative_imports","title":"[Final] PEP 366 - Main module explicit relative imports","excerpt":"Python Enhancement Proposal 366: 'Main module explicit relative imports'에 대한 한국어 번역입니다.","date":"2025-09-26 20:49:35+0900","lastModifiedAt":"2025-09-26 20:49:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/366/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0329-treating_builtins_as_constants_in_the_standard_library","title":"[Rejected] PEP 329 - Treating Builtins as Constants in the Standard Library","excerpt":"Python Enhancement Proposal 329: 'Treating Builtins as Constants in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 20:37:36+0900","lastModifiedAt":"2025-09-26 20:37:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/329/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0326-a_case_for_top_and_bottom_values","title":"[Rejected] PEP 326 - A Case for Top and Bottom Values","excerpt":"Python Enhancement Proposal 326: 'A Case for Top and Bottom Values'에 대한 한국어 번역입니다.","date":"2025-09-26 20:37:04+0900","lastModifiedAt":"2025-09-26 20:37:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/326/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0365-adding_the_pkg_resources_module","title":"[Rejected] PEP 365 - Adding the pkg_resources module","excerpt":"Python Enhancement Proposal 365: 'Adding the pkg_resources module'에 대한 한국어 번역입니다.","date":"2025-09-26 19:09:15+0900","lastModifiedAt":"2025-09-26 19:09:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/365/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0364-transitioning_to_the_py3k_standard_library","title":"[Withdrawn] PEP 364 - Transitioning to the Py3K Standard Library","excerpt":"Python Enhancement Proposal 364: 'Transitioning to the Py3K Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:59+0900","lastModifiedAt":"2025-09-26 19:08:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/364/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0363-syntax_for_dynamic_attribute_access","title":"[Rejected] PEP 363 - Syntax For Dynamic Attribute Access","excerpt":"Python Enhancement Proposal 363: 'Syntax For Dynamic Attribute Access'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:27+0900","lastModifiedAt":"2025-09-26 19:08:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/363/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0362-function_signature_object","title":"[Final] PEP 362 - Function Signature Object","excerpt":"Python Enhancement Proposal 362: 'Function Signature Object'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:03+0900","lastModifiedAt":"2025-09-26 19:08:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/362/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0361-python_2.6_and_3.0_release_schedule","title":"[Final] PEP 361 - Python 2.6 and 3.0 Release Schedule","excerpt":"Python Enhancement Proposal 361: 'Python 2.6 and 3.0 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 19:07:30+0900","lastModifiedAt":"2025-09-26 19:07:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/361/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0360-externally_maintained_packages","title":"[Final] PEP 360 - Externally Maintained Packages","excerpt":"Python Enhancement Proposal 360: 'Externally Maintained Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 19:07:10+0900","lastModifiedAt":"2025-09-26 19:07:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/360/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0359-the_make_statement","title":"[Withdrawn] PEP 359 - The “make” Statement","excerpt":"Python Enhancement Proposal 359: 'The “make” Statement'에 대한 한국어 번역입니다.","date":"2025-09-26 19:06:53+0900","lastModifiedAt":"2025-09-26 19:06:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/359/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0358-the_bytes_object","title":"[Final] PEP 358 - The “bytes” Object","excerpt":"Python Enhancement Proposal 358: 'The “bytes” Object'에 대한 한국어 번역입니다.","date":"2025-09-26 19:04:13+0900","lastModifiedAt":"2025-09-26 19:04:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/358/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0357-allowing_any_object_to_be_used_for_slicing","title":"[Final] PEP 357 - Allowing Any Object to be Used for Slicing","excerpt":"Python Enhancement Proposal 357: 'Allowing Any Object to be Used for Slicing'에 대한 한국어 번역입니다.","date":"2025-09-26 19:03:31+0900","lastModifiedAt":"2025-09-26 19:03:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/357/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0356-python_2.5_release_schedule","title":"[Final] PEP 356 - Python 2.5 Release Schedule","excerpt":"Python Enhancement Proposal 356: 'Python 2.5 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 19:03:07+0900","lastModifiedAt":"2025-09-26 19:03:07+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/356/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0354-enumerations_in_python","title":"[Superseded] PEP 354 - Enumerations in Python","excerpt":"Python Enhancement Proposal 354: 'Enumerations in Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:58+0900","lastModifiedAt":"2025-09-26 18:59:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/354/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0353-using_ssize_t_as_the_index_type","title":"[Final] PEP 353 - Using ssize_t as the index type","excerpt":"Python Enhancement Proposal 353: 'Using ssize_t as the index type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:36+0900","lastModifiedAt":"2025-09-26 18:59:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/353/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0352-required_superclass_for_exceptions","title":"[Final] PEP 352 - Required Superclass for Exceptions","excerpt":"Python Enhancement Proposal 352: 'Required Superclass for Exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:06+0900","lastModifiedAt":"2025-09-26 18:59:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/352/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0351-the_freeze_protocol","title":"[Rejected] PEP 351 - The freeze protocol","excerpt":"Python Enhancement Proposal 351: 'The freeze protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 18:58:26+0900","lastModifiedAt":"2025-09-26 18:58:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/351/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0350-codetags","title":"[Rejected] PEP 350 - Codetags","excerpt":"Python Enhancement Proposal 350: 'Codetags'에 대한 한국어 번역입니다.","date":"2025-09-26 18:58:11+0900","lastModifiedAt":"2025-09-26 18:58:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/350/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0349-allow_str_to_return_unicode_strings","title":"[Rejected] PEP 349 - Allow str() to return unicode strings","excerpt":"Python Enhancement Proposal 349: 'Allow str() to return unicode strings'에 대한 한국어 번역입니다.","date":"2025-09-26 18:57:18+0900","lastModifiedAt":"2025-09-26 18:57:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/349/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0348-exception_reorganization_for_python_3.0","title":"[Rejected] PEP 348 - Exception Reorganization for Python 3.0","excerpt":"Python Enhancement Proposal 348: 'Exception Reorganization for Python 3.0'에 대한 한국어 번역입니다.","date":"2025-09-26 18:56:58+0900","lastModifiedAt":"2025-09-26 18:56:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/348/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0347-migrating_the_python_cvs_to_subversion","title":"[Final] PEP 347 - Migrating the Python CVS to Subversion","excerpt":"Python Enhancement Proposal 347: 'Migrating the Python CVS to Subversion'에 대한 한국어 번역입니다.","date":"2025-09-26 18:56:24+0900","lastModifiedAt":"2025-09-26 18:56:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/347/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0346-user_defined_with_statements","title":"[Withdrawn] PEP 346 - User Defined (”with”) Statements","excerpt":"Python Enhancement Proposal 346: 'User Defined (”with”) Statements'에 대한 한국어 번역입니다.","date":"2025-09-26 18:55:56+0900","lastModifiedAt":"2025-09-26 18:55:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/346/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0345-metadata_for_python_software_packages_1.2","title":"[Superseded] PEP 345 - Metadata for Python Software Packages 1.2","excerpt":"Python Enhancement Proposal 345: 'Metadata for Python Software Packages 1.2'에 대한 한국어 번역입니다.","date":"2025-09-26 18:54:36+0900","lastModifiedAt":"2025-09-26 18:54:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/345/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0344-exception_chaining_and_embedded_tracebacks","title":"[Superseded] PEP 344 - Exception Chaining and Embedded Tracebacks","excerpt":"Python Enhancement Proposal 344: 'Exception Chaining and Embedded Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:54:00+0900","lastModifiedAt":"2025-09-26 18:54:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/344/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0342-coroutines_via_enhanced_generators","title":"[Final] PEP 342 - Coroutines via Enhanced Generators","excerpt":"Python Enhancement Proposal 342: 'Coroutines via Enhanced Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:58+0900","lastModifiedAt":"2025-09-26 18:48:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/342/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0341-unifying_try-except_and_try-finally","title":"[Final] PEP 341 - Unifying try-except and try-finally","excerpt":"Python Enhancement Proposal 341: 'Unifying try-except and try-finally'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:18+0900","lastModifiedAt":"2025-09-26 18:48:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/341/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0340-anonymous_block_statements","title":"[Rejected] PEP 340 - Anonymous Block Statements","excerpt":"Python Enhancement Proposal 340: 'Anonymous Block Statements'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:04+0900","lastModifiedAt":"2025-09-26 18:48:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/340/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0339-design_of_the_cpython_compiler","title":"[Withdrawn] PEP 339 - Design of the CPython Compiler","excerpt":"Python Enhancement Proposal 339: 'Design of the CPython Compiler'에 대한 한국어 번역입니다.","date":"2025-09-26 18:46:48+0900","lastModifiedAt":"2025-09-26 18:46:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/339/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0338-executing_modules_as_scripts","title":"[Final] PEP 338 - Executing modules as scripts","excerpt":"Python Enhancement Proposal 338: 'Executing modules as scripts'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:56+0900","lastModifiedAt":"2025-09-26 18:45:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/338/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0337-logging_usage_in_the_standard_library","title":"[Deferred] PEP 337 - Logging Usage in the Standard Library","excerpt":"Python Enhancement Proposal 337: 'Logging Usage in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:28+0900","lastModifiedAt":"2025-09-26 18:45:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/337/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0336-make_none_callable","title":"[Rejected] PEP 336 - Make None Callable","excerpt":"Python Enhancement Proposal 336: 'Make None Callable'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:11+0900","lastModifiedAt":"2025-09-26 18:45:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/336/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0334-simple_coroutines_via_suspenditeration","title":"[Withdrawn] PEP 334 - Simple Coroutines via SuspendIteration","excerpt":"Python Enhancement Proposal 334: 'Simple Coroutines via SuspendIteration'에 대한 한국어 번역입니다.","date":"2025-09-26 18:43:20+0900","lastModifiedAt":"2025-09-26 18:43:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/334/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0333-python_web_server_gateway_interface_v1.0","title":"[Final] PEP 333 - Python Web Server Gateway Interface v1.0","excerpt":"Python Enhancement Proposal 333: 'Python Web Server Gateway Interface v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 18:42:49+0900","lastModifiedAt":"2025-09-26 18:42:49+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/333/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0332-byte_vectors_and_stringunicode_unification","title":"[Rejected] PEP 332 - Byte vectors and String/Unicode Unification","excerpt":"Python Enhancement Proposal 332: 'Byte vectors and String/Unicode Unification'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:55+0900","lastModifiedAt":"2025-09-26 18:38:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/332/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0331-locale-independent_floatstring_conversions","title":"[Final] PEP 331 - Locale-Independent Float/String Conversions","excerpt":"Python Enhancement Proposal 331: 'Locale-Independent Float/String Conversions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:41+0900","lastModifiedAt":"2025-09-26 18:38:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/331/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0330-python_bytecode_verification","title":"[Rejected] PEP 330 - Python Bytecode Verification","excerpt":"Python Enhancement Proposal 330: 'Python Bytecode Verification'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:18+0900","lastModifiedAt":"2025-09-26 18:38:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/330/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0328-imports_multi-line_and_absoluterelative","title":"[Final] PEP 328 - Imports: Multi-Line and Absolute/Relative","excerpt":"Python Enhancement Proposal 328: 'Imports: Multi-Line and Absolute/Relative'에 대한 한국어 번역입니다.","date":"2025-09-26 18:36:33+0900","lastModifiedAt":"2025-09-26 18:36:33+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/328/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0327-decimal_data_type","title":"[Final] PEP 327 - Decimal Data Type","excerpt":"Python Enhancement Proposal 327: 'Decimal Data Type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:35:48+0900","lastModifiedAt":"2025-09-26 18:35:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/327/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0325-resource-release_support_for_generators","title":"[Rejected] PEP 325 - Resource-Release Support for Generators","excerpt":"Python Enhancement Proposal 325: 'Resource-Release Support for Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:57+0900","lastModifiedAt":"2025-09-26 18:31:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/325/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0324-subprocess_-_new_process_module","title":"[Final] PEP 324 - subprocess - New process module","excerpt":"Python Enhancement Proposal 324: 'subprocess - New process module'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:27+0900","lastModifiedAt":"2025-09-26 18:31:27+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/324/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0323-copyable_iterators","title":"[Deferred] PEP 323 - Copyable Iterators","excerpt":"Python Enhancement Proposal 323: 'Copyable Iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:01+0900","lastModifiedAt":"2025-09-26 18:31:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/323/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0322-reverse_iteration","title":"[Final] PEP 322 - Reverse Iteration","excerpt":"Python Enhancement Proposal 322: 'Reverse Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 18:30:23+0900","lastModifiedAt":"2025-09-26 18:30:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/322/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0321-datetime_parsing_and_formatting","title":"[Withdrawn] PEP 321 - Date/Time Parsing and Formatting","excerpt":"Python Enhancement Proposal 321: 'Date/Time Parsing and Formatting'에 대한 한국어 번역입니다.","date":"2025-09-26 18:30:01+0900","lastModifiedAt":"2025-09-26 18:30:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/321/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0320-python_2.4_release_schedule","title":"[Final] PEP 320 - Python 2.4 Release Schedule","excerpt":"Python Enhancement Proposal 320: 'Python 2.4 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 18:29:42+0900","lastModifiedAt":"2025-09-26 18:29:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/320/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0319-python_synchronizeasynchronize_block","title":"[Rejected] PEP 319 - Python Synchronize/Asynchronize Block","excerpt":"Python Enhancement Proposal 319: 'Python Synchronize/Asynchronize Block'에 대한 한국어 번역입니다.","date":"2025-09-26 18:29:20+0900","lastModifiedAt":"2025-09-26 18:29:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/319/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0318-decorators_for_functions_and_methods","title":"[Final] PEP 318 - Decorators for Functions and Methods","excerpt":"Python Enhancement Proposal 318: 'Decorators for Functions and Methods'에 대한 한국어 번역입니다.","date":"2025-09-26 18:28:53+0900","lastModifiedAt":"2025-09-26 18:28:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/318/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0317-eliminate_implicit_exception_instantiation","title":"[Rejected] PEP 317 - Eliminate Implicit Exception Instantiation","excerpt":"Python Enhancement Proposal 317: 'Eliminate Implicit Exception Instantiation'에 대한 한국어 번역입니다.","date":"2025-09-26 18:28:16+0900","lastModifiedAt":"2025-09-26 18:28:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/317/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0316-programming_by_contract_for_python","title":"[Deferred] PEP 316 - Programming by Contract for Python","excerpt":"Python Enhancement Proposal 316: 'Programming by Contract for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:27:40+0900","lastModifiedAt":"2025-09-26 18:27:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/316/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0315-enhanced_while_loop","title":"[Rejected] PEP 315 - Enhanced While Loop","excerpt":"Python Enhancement Proposal 315: 'Enhanced While Loop'에 대한 한국어 번역입니다.","date":"2025-09-26 18:13:38+0900","lastModifiedAt":"2025-09-26 18:13:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/315/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0314-metadata_for_python_software_packages_1.1","title":"[Superseded] PEP 314 - Metadata for Python Software Packages 1.1","excerpt":"Python Enhancement Proposal 314: 'Metadata for Python Software Packages 1.1'에 대한 한국어 번역입니다.","date":"2025-09-26 18:13:21+0900","lastModifiedAt":"2025-09-26 18:13:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/314/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0313-adding_roman_numeral_literals_to_python","title":"[Rejected] PEP 313 - Adding Roman Numeral Literals to Python","excerpt":"Python Enhancement Proposal 313: 'Adding Roman Numeral Literals to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:44+0900","lastModifiedAt":"2025-09-26 18:12:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/313/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0312-simple_implicit_lambda","title":"[Deferred] PEP 312 - Simple Implicit Lambda","excerpt":"Python Enhancement Proposal 312: 'Simple Implicit Lambda'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:24+0900","lastModifiedAt":"2025-09-26 18:12:24+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/312/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0311-simplified_global_interpreter_lock_acquisition_for_extensions","title":"[Final] PEP 311 - Simplified Global Interpreter Lock Acquisition for Extensions","excerpt":"Python Enhancement Proposal 311: 'Simplified Global Interpreter Lock Acquisition for Extensions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:03+0900","lastModifiedAt":"2025-09-26 18:12:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/311/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0310-reliable_acquisitionrelease_pairs","title":"[Rejected] PEP 310 - Reliable Acquisition/Release Pairs","excerpt":"Python Enhancement Proposal 310: 'Reliable Acquisition/Release Pairs'에 대한 한국어 번역입니다.","date":"2025-09-26 18:11:37+0900","lastModifiedAt":"2025-09-26 18:11:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/310/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0309-partial_function_application","title":"[Final] PEP 309 - Partial Function Application","excerpt":"Python Enhancement Proposal 309: 'Partial Function Application'에 대한 한국어 번역입니다.","date":"2025-09-26 18:11:12+0900","lastModifiedAt":"2025-09-26 18:11:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/309/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0308-conditional_expressions","title":"[Final] PEP 308 - Conditional Expressions","excerpt":"Python Enhancement Proposal 308: 'Conditional Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:10:46+0900","lastModifiedAt":"2025-09-26 18:10:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/308/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0307-extensions_to_the_pickle_protocol","title":"[Final] PEP 307 - Extensions to the pickle protocol","excerpt":"Python Enhancement Proposal 307: 'Extensions to the pickle protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 18:10:28+0900","lastModifiedAt":"2025-09-26 18:10:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/307/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0306-how_to_change_pythons_grammar","title":"[Withdrawn] PEP 306 - How to Change Python’s Grammar","excerpt":"Python Enhancement Proposal 306: 'How to Change Python’s Grammar'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:57+0900","lastModifiedAt":"2025-09-26 18:09:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/306/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0305-csv_file_api","title":"[Final] PEP 305 - CSV File API","excerpt":"Python Enhancement Proposal 305: 'CSV File API'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:42+0900","lastModifiedAt":"2025-09-26 18:09:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/305/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0304-controlling_generation_of_bytecode_files","title":"[Withdrawn] PEP 304 - Controlling Generation of Bytecode Files","excerpt":"Python Enhancement Proposal 304: 'Controlling Generation of Bytecode Files'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:05+0900","lastModifiedAt":"2025-09-26 18:09:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/304/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0303-extend_divmod_for_multiple_divisors","title":"[Rejected] PEP 303 - Extend divmod() for Multiple Divisors","excerpt":"Python Enhancement Proposal 303: 'Extend divmod() for Multiple Divisors'에 대한 한국어 번역입니다.","date":"2025-09-26 18:08:34+0900","lastModifiedAt":"2025-09-26 18:08:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/303/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0302-new_import_hooks","title":"[Final] PEP 302 - New Import Hooks","excerpt":"Python Enhancement Proposal 302: 'New Import Hooks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:08:12+0900","lastModifiedAt":"2025-09-26 18:08:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/302/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0301-package_index_and_metadata_for_distutils","title":"[Final] PEP 301 - Package Index and Metadata for Distutils","excerpt":"Python Enhancement Proposal 301: 'Package Index and Metadata for Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 18:07:29+0900","lastModifiedAt":"2025-09-26 18:07:29+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/301/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0299-special___main___function_in_modules","title":"[Rejected] PEP 299 - Special __main__() function in modules","excerpt":"Python Enhancement Proposal 299: 'Special __main__() function in modules'에 대한 한국어 번역입니다.","date":"2025-09-26 18:07:00+0900","lastModifiedAt":"2025-09-26 18:07:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/299/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0298-the_locked_buffer_interface","title":"[Withdrawn] PEP 298 - The Locked Buffer Interface","excerpt":"Python Enhancement Proposal 298: 'The Locked Buffer Interface'에 대한 한국어 번역입니다.","date":"2025-09-26 18:06:42+0900","lastModifiedAt":"2025-09-26 18:06:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/298/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0297-support_for_system_upgrades","title":"[Rejected] PEP 297 - Support for System Upgrades","excerpt":"Python Enhancement Proposal 297: 'Support for System Upgrades'에 대한 한국어 번역입니다.","date":"2025-09-26 18:06:11+0900","lastModifiedAt":"2025-09-26 18:06:11+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/297/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0296-adding_a_bytes_object_type","title":"[Withdrawn] PEP 296 - Adding a bytes Object Type","excerpt":"Python Enhancement Proposal 296: 'Adding a bytes Object Type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:57+0900","lastModifiedAt":"2025-09-26 18:05:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/296/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0295-interpretation_of_multiline_string_constants","title":"[Rejected] PEP 295 - Interpretation of multiline string constants","excerpt":"Python Enhancement Proposal 295: 'Interpretation of multiline string constants'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:22+0900","lastModifiedAt":"2025-09-26 18:05:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/295/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0294-type_names_in_the_types_module","title":"[Rejected] PEP 294 - Type Names in the types Module","excerpt":"Python Enhancement Proposal 294: 'Type Names in the types Module'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:08+0900","lastModifiedAt":"2025-09-26 18:05:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/294/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0293-codec_error_handling_callbacks","title":"[Final] PEP 293 - Codec Error Handling Callbacks","excerpt":"Python Enhancement Proposal 293: 'Codec Error Handling Callbacks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:55+0900","lastModifiedAt":"2025-09-26 18:04:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/293/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0292-simpler_string_substitutions","title":"[Final] PEP 292 - Simpler String Substitutions","excerpt":"Python Enhancement Proposal 292: 'Simpler String Substitutions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:32+0900","lastModifiedAt":"2025-09-26 18:04:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/292/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0291-backward_compatibility_for_the_python_2_standard_library","title":"[Superseded] PEP 291 - Backward Compatibility for the Python 2 Standard Library","excerpt":"Python Enhancement Proposal 291: 'Backward Compatibility for the Python 2 Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:05+0900","lastModifiedAt":"2025-09-26 18:04:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/291/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0290-code_migration_and_modernization","title":"[Active] PEP 290 - Code Migration and Modernization","excerpt":"Python Enhancement Proposal 290: 'Code Migration and Modernization'에 대한 한국어 번역입니다.","date":"2025-09-26 18:00:45+0900","lastModifiedAt":"2025-09-26 18:00:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/290/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0289-generator_expressions","title":"[Final] PEP 289 - Generator Expressions","excerpt":"Python Enhancement Proposal 289: 'Generator Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:59+0900","lastModifiedAt":"2025-09-26 17:59:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/289/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0288-generators_attributes_and_exceptions","title":"[Withdrawn] PEP 288 - Generators Attributes and Exceptions","excerpt":"Python Enhancement Proposal 288: 'Generators Attributes and Exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:36+0900","lastModifiedAt":"2025-09-26 17:59:36+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/288/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0287-restructuredtext_docstring_format","title":"[Active] PEP 287 - reStructuredText Docstring Format","excerpt":"Python Enhancement Proposal 287: 'reStructuredText Docstring Format'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:15+0900","lastModifiedAt":"2025-09-26 17:59:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/287/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0286-enhanced_argument_tuples","title":"[Deferred] PEP 286 - Enhanced Argument Tuples","excerpt":"Python Enhancement Proposal 286: 'Enhanced Argument Tuples'에 대한 한국어 번역입니다.","date":"2025-09-26 17:58:16+0900","lastModifiedAt":"2025-09-26 17:58:16+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/286/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0285-adding_a_bool_type","title":"[Final] PEP 285 - Adding a bool type","excerpt":"Python Enhancement Proposal 285: 'Adding a bool type'에 대한 한국어 번역입니다.","date":"2025-09-26 17:58:00+0900","lastModifiedAt":"2025-09-26 17:58:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/285/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0284-integer_for-loops","title":"[Rejected] PEP 284 - Integer for-loops","excerpt":"Python Enhancement Proposal 284: 'Integer for-loops'에 대한 한국어 번역입니다.","date":"2025-09-26 17:57:40+0900","lastModifiedAt":"2025-09-26 17:57:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/284/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0283-python_2.3_release_schedule","title":"[Final] PEP 283 - Python 2.3 Release Schedule","excerpt":"Python Enhancement Proposal 283: 'Python 2.3 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 17:57:14+0900","lastModifiedAt":"2025-09-26 17:57:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/283/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0282-a_logging_system","title":"[Final] PEP 282 - A Logging System","excerpt":"Python Enhancement Proposal 282: 'A Logging System'에 대한 한국어 번역입니다.","date":"2025-09-26 17:56:46+0900","lastModifiedAt":"2025-09-26 17:56:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/282/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0281-loop_counter_iteration_with_range_and_xrange","title":"[Rejected] PEP 281 - Loop Counter Iteration with range and xrange","excerpt":"Python Enhancement Proposal 281: 'Loop Counter Iteration with range and xrange'에 대한 한국어 번역입니다.","date":"2025-09-26 17:56:00+0900","lastModifiedAt":"2025-09-26 17:56:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/281/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0280-optimizing_access_to_globals","title":"[Deferred] PEP 280 - Optimizing access to globals","excerpt":"Python Enhancement Proposal 280: 'Optimizing access to globals'에 대한 한국어 번역입니다.","date":"2025-09-26 17:55:42+0900","lastModifiedAt":"2025-09-26 17:55:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/280/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0279-the_enumerate_built-in_function","title":"[Final] PEP 279 - The enumerate() built-in function","excerpt":"Python Enhancement Proposal 279: 'The enumerate() built-in function'에 대한 한국어 번역입니다.","date":"2025-09-26 17:55:04+0900","lastModifiedAt":"2025-09-26 17:55:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/279/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0278-universal_newline_support","title":"[Final] PEP 278 - Universal Newline Support","excerpt":"Python Enhancement Proposal 278: 'Universal Newline Support'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:43+0900","lastModifiedAt":"2025-09-26 17:54:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/278/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0277-unicode_file_name_support_for_windows_nt","title":"[Final] PEP 277 - Unicode file name support for Windows NT","excerpt":"Python Enhancement Proposal 277: 'Unicode file name support for Windows NT'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:18+0900","lastModifiedAt":"2025-09-26 17:54:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/277/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0276-simple_iterator_for_ints","title":"[Rejected] PEP 276 - Simple Iterator for ints","excerpt":"Python Enhancement Proposal 276: 'Simple Iterator for ints'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:04+0900","lastModifiedAt":"2025-09-26 17:54:04+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/276/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0275-switching_on_multiple_values","title":"[Rejected] PEP 275 - Switching on Multiple Values","excerpt":"Python Enhancement Proposal 275: 'Switching on Multiple Values'에 대한 한국어 번역입니다.","date":"2025-09-26 17:53:28+0900","lastModifiedAt":"2025-09-26 17:53:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/275/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0274-dict_comprehensions","title":"[Final] PEP 274 - Dict Comprehensions","excerpt":"Python Enhancement Proposal 274: 'Dict Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:53:00+0900","lastModifiedAt":"2025-09-26 17:53:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/274/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0273-import_modules_from_zip_archives","title":"[Final] PEP 273 - Import Modules from Zip Archives","excerpt":"Python Enhancement Proposal 273: 'Import Modules from Zip Archives'에 대한 한국어 번역입니다.","date":"2025-09-26 17:52:44+0900","lastModifiedAt":"2025-09-26 17:52:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/273/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0272-api_for_block_encryption_algorithms_v1.0","title":"[Final] PEP 272 - API for Block Encryption Algorithms v1.0","excerpt":"Python Enhancement Proposal 272: 'API for Block Encryption Algorithms v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:52:18+0900","lastModifiedAt":"2025-09-26 17:52:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/272/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0271-prefixing_sys.path_by_command_line_option","title":"[Rejected] PEP 271 - Prefixing sys.path by command line option","excerpt":"Python Enhancement Proposal 271: 'Prefixing sys.path by command line option'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:56+0900","lastModifiedAt":"2025-09-26 17:51:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/271/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0270-uniq_method_for_list_objects","title":"[Rejected] PEP 270 - uniq method for list objects","excerpt":"Python Enhancement Proposal 270: 'uniq method for list objects'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:43+0900","lastModifiedAt":"2025-09-26 17:51:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/270/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0269-pgen_module_for_python","title":"[Deferred] PEP 269 - Pgen Module for Python","excerpt":"Python Enhancement Proposal 269: 'Pgen Module for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:28+0900","lastModifiedAt":"2025-09-26 17:51:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/269/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0268-extended_http_functionality_and_webdav","title":"[Rejected] PEP 268 - Extended HTTP functionality and WebDAV","excerpt":"Python Enhancement Proposal 268: 'Extended HTTP functionality and WebDAV'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:10+0900","lastModifiedAt":"2025-09-26 17:51:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/268/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0267-optimized_access_to_module_namespaces","title":"[Deferred] PEP 267 - Optimized Access to Module Namespaces","excerpt":"Python Enhancement Proposal 267: 'Optimized Access to Module Namespaces'에 대한 한국어 번역입니다.","date":"2025-09-26 17:50:38+0900","lastModifiedAt":"2025-09-26 17:50:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/267/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0266-optimizing_global_variableattribute_access","title":"[Withdrawn] PEP 266 - Optimizing Global Variable/Attribute Access","excerpt":"Python Enhancement Proposal 266: 'Optimizing Global Variable/Attribute Access'에 대한 한국어 번역입니다.","date":"2025-09-26 17:49:31+0900","lastModifiedAt":"2025-09-26 17:49:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/266/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0265-sorting_dictionaries_by_value","title":"[Rejected] PEP 265 - Sorting Dictionaries by Value","excerpt":"Python Enhancement Proposal 265: 'Sorting Dictionaries by Value'에 대한 한국어 번역입니다.","date":"2025-09-26 17:48:48+0900","lastModifiedAt":"2025-09-26 17:48:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/265/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0264-future_statements_in_simulated_shells","title":"[Final] PEP 264 - Future statements in simulated shells","excerpt":"Python Enhancement Proposal 264: 'Future statements in simulated shells'에 대한 한국어 번역입니다.","date":"2025-09-26 17:47:23+0900","lastModifiedAt":"2025-09-26 17:47:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/264/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0263-defining_python_source_code_encodings","title":"[Final] PEP 263 - Defining Python Source Code Encodings","excerpt":"Python Enhancement Proposal 263: 'Defining Python Source Code Encodings'에 대한 한국어 번역입니다.","date":"2025-09-26 17:46:12+0900","lastModifiedAt":"2025-09-26 17:46:12+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/263/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0262-a_database_of_installed_python_packages","title":"[Rejected] PEP 262 - A Database of Installed Python Packages","excerpt":"Python Enhancement Proposal 262: 'A Database of Installed Python Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 17:44:46+0900","lastModifiedAt":"2025-09-26 17:44:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/262/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0261-support_for_wide_unicode_characters","title":"[Final] PEP 261 - Support for “wide” Unicode characters","excerpt":"Python Enhancement Proposal 261: 'Support for “wide” Unicode characters'에 대한 한국어 번역입니다.","date":"2025-09-26 17:43:32+0900","lastModifiedAt":"2025-09-26 17:43:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/261/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0260-simplify_xrange","title":"[Final] PEP 260 - Simplify xrange()","excerpt":"Python Enhancement Proposal 260: 'Simplify xrange()'에 대한 한국어 번역입니다.","date":"2025-09-26 17:42:10+0900","lastModifiedAt":"2025-09-26 17:42:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/260/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0259-omit_printing_newline_after_newline","title":"[Rejected] PEP 259 - Omit printing newline after newline","excerpt":"Python Enhancement Proposal 259: 'Omit printing newline after newline'에 대한 한국어 번역입니다.","date":"2025-09-26 17:41:02+0900","lastModifiedAt":"2025-09-26 17:41:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/259/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0258-docutils_design_specification","title":"[Rejected] PEP 258 - Docutils Design Specification","excerpt":"Python Enhancement Proposal 258: 'Docutils Design Specification'에 대한 한국어 번역입니다.","date":"2025-09-26 17:39:51+0900","lastModifiedAt":"2025-09-26 17:39:51+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/258/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0257-docstring_conventions","title":"[Active] PEP 257 - Docstring Conventions","excerpt":"Python Enhancement Proposal 257: 'Docstring Conventions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:37:43+0900","lastModifiedAt":"2025-09-26 17:37:43+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/257/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0256-docstring_processing_system_framework","title":"[Rejected] PEP 256 - Docstring Processing System Framework","excerpt":"Python Enhancement Proposal 256: 'Docstring Processing System Framework'에 대한 한국어 번역입니다.","date":"2025-09-26 17:36:23+0900","lastModifiedAt":"2025-09-26 17:36:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/256/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0255-simple_generators","title":"[Final] PEP 255 - Simple Generators","excerpt":"Python Enhancement Proposal 255: 'Simple Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 17:35:06+0900","lastModifiedAt":"2025-09-26 17:35:06+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/255/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0254-making_classes_look_more_like_types","title":"[Rejected] PEP 254 - Making Classes Look More Like Types","excerpt":"Python Enhancement Proposal 254: 'Making Classes Look More Like Types'에 대한 한국어 번역입니다.","date":"2025-09-26 17:33:28+0900","lastModifiedAt":"2025-09-26 17:33:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/254/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0253-subtyping_built-in_types","title":"[Final] PEP 253 - Subtyping Built-in Types","excerpt":"Python Enhancement Proposal 253: 'Subtyping Built-in Types'에 대한 한국어 번역입니다.","date":"2025-09-26 17:32:18+0900","lastModifiedAt":"2025-09-26 17:32:18+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/253/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0252-making_types_look_more_like_classes","title":"[Final] PEP 252 - Making Types Look More Like Classes","excerpt":"Python Enhancement Proposal 252: 'Making Types Look More Like Classes'에 대한 한국어 번역입니다.","date":"2025-09-26 17:30:03+0900","lastModifiedAt":"2025-09-26 17:30:03+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/252/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0251-python_2.2_release_schedule","title":"[Final] PEP 251 - Python 2.2 Release Schedule","excerpt":"Python Enhancement Proposal 251: 'Python 2.2 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 17:27:59+0900","lastModifiedAt":"2025-09-26 17:27:59+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/251/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0250-using_site-packages_on_windows","title":"[Final] PEP 250 - Using site-packages on Windows","excerpt":"Python Enhancement Proposal 250: 'Using site-packages on Windows'에 대한 한국어 번역입니다.","date":"2025-09-26 17:26:48+0900","lastModifiedAt":"2025-09-26 17:26:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/250/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0249-python_database_api_specification_v2.0","title":"[Final] PEP 249 - Python Database API Specification v2.0","excerpt":"Python Enhancement Proposal 249: 'Python Database API Specification v2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:25:38+0900","lastModifiedAt":"2025-09-26 17:25:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/249/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0248-python_database_api_specification_v1.0","title":"[Final] PEP 248 - Python Database API Specification v1.0","excerpt":"Python Enhancement Proposal 248: 'Python Database API Specification v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:23:30+0900","lastModifiedAt":"2025-09-26 17:23:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/248/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0247-api_for_cryptographic_hash_functions","title":"[Final] PEP 247 - API for Cryptographic Hash Functions","excerpt":"Python Enhancement Proposal 247: 'API for Cryptographic Hash Functions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:22:10+0900","lastModifiedAt":"2025-09-26 17:22:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/247/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0246-object_adaptation","title":"[Rejected] PEP 246 - Object Adaptation","excerpt":"Python Enhancement Proposal 246: 'Object Adaptation'에 대한 한국어 번역입니다.","date":"2025-09-26 17:20:56+0900","lastModifiedAt":"2025-09-26 17:20:56+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/246/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0245-python_interface_syntax","title":"[Rejected] PEP 245 - Python Interface Syntax","excerpt":"Python Enhancement Proposal 245: 'Python Interface Syntax'에 대한 한국어 번역입니다.","date":"2025-09-26 17:16:57+0900","lastModifiedAt":"2025-09-26 17:16:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/245/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0244-thedirectivestatement","title":"[Rejected] PEP 244 - Thedirectivestatement","excerpt":"Python Enhancement Proposal 244: 'Thedirectivestatement'에 대한 한국어 번역입니다.","date":"2025-09-26 17:15:30+0900","lastModifiedAt":"2025-09-26 17:15:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/244/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0243-module_repository_upload_mechanism","title":"[Withdrawn] PEP 243 - Module Repository Upload Mechanism","excerpt":"Python Enhancement Proposal 243: 'Module Repository Upload Mechanism'에 대한 한국어 번역입니다.","date":"2025-09-26 17:14:15+0900","lastModifiedAt":"2025-09-26 17:14:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/243/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0242-numeric_kinds","title":"[Withdrawn] PEP 242 - Numeric Kinds","excerpt":"Python Enhancement Proposal 242: 'Numeric Kinds'에 대한 한국어 번역입니다.","date":"2025-09-26 17:13:00+0900","lastModifiedAt":"2025-09-26 17:13:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/242/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0241-metadata_for_python_software_packages","title":"[Superseded] PEP 241 - Metadata for Python Software Packages","excerpt":"Python Enhancement Proposal 241: 'Metadata for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 17:11:45+0900","lastModifiedAt":"2025-09-26 17:11:45+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/241/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0240-adding_a_rational_literal_to_python","title":"[Rejected] PEP 240 - Adding a Rational Literal to Python","excerpt":"Python Enhancement Proposal 240: 'Adding a Rational Literal to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:10:32+0900","lastModifiedAt":"2025-09-26 17:10:32+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/240/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0239-adding_a_rational_type_to_python","title":"[Rejected] PEP 239 - Adding a Rational Type to Python","excerpt":"Python Enhancement Proposal 239: 'Adding a Rational Type to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:09:22+0900","lastModifiedAt":"2025-09-26 17:09:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/239/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0238-changing_the_division_operator","title":"[Final] PEP 238 - Changing the Division Operator","excerpt":"Python Enhancement Proposal 238: 'Changing the Division Operator'에 대한 한국어 번역입니다.","date":"2025-09-26 17:08:05+0900","lastModifiedAt":"2025-09-26 17:08:05+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/238/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0237-unifying_long_integers_and_integers","title":"[Final] PEP 237 - Unifying Long Integers and Integers","excerpt":"Python Enhancement Proposal 237: 'Unifying Long Integers and Integers'에 대한 한국어 번역입니다.","date":"2025-09-26 17:06:31+0900","lastModifiedAt":"2025-09-26 17:06:31+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/237/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0236-back_to_the___future","title":"[Final] PEP 236 - Back to the __future__","excerpt":"Python Enhancement Proposal 236: 'Back to the __future__'에 대한 한국어 번역입니다.","date":"2025-09-26 17:05:02+0900","lastModifiedAt":"2025-09-26 17:05:02+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/236/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0235-import_on_case-insensitive_platforms","title":"[Final] PEP 235 - Import on Case-Insensitive Platforms","excerpt":"Python Enhancement Proposal 235: 'Import on Case-Insensitive Platforms'에 대한 한국어 번역입니다.","date":"2025-09-26 17:03:34+0900","lastModifiedAt":"2025-09-26 17:03:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/235/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0234-iterators","title":"[Final] PEP 234 - Iterators","excerpt":"Python Enhancement Proposal 234: 'Iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 17:02:22+0900","lastModifiedAt":"2025-09-26 17:02:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/234/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0233-python_online_help","title":"[Deferred] PEP 233 - Python Online Help","excerpt":"Python Enhancement Proposal 233: 'Python Online Help'에 대한 한국어 번역입니다.","date":"2025-09-26 17:00:50+0900","lastModifiedAt":"2025-09-26 17:00:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/233/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0232-function_attributes","title":"[Final] PEP 232 - Function Attributes","excerpt":"Python Enhancement Proposal 232: 'Function Attributes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:59:42+0900","lastModifiedAt":"2025-09-26 16:59:42+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/232/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0231-findattr","title":"[Rejected] PEP 231 - __findattr__()","excerpt":"Python Enhancement Proposal 231: '__findattr__()'에 대한 한국어 번역입니다.","date":"2025-09-26 16:58:10+0900","lastModifiedAt":"2025-09-26 16:58:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/231/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0230-warning_framework","title":"[Final] PEP 230 - Warning Framework","excerpt":"Python Enhancement Proposal 230: 'Warning Framework'에 대한 한국어 번역입니다.","date":"2025-09-26 16:53:00+0900","lastModifiedAt":"2025-09-26 16:53:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/230/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0229-using_distutils_to_build_python","title":"[Final] PEP 229 - Using Distutils to Build Python","excerpt":"Python Enhancement Proposal 229: 'Using Distutils to Build Python'에 대한 한국어 번역입니다.","date":"2025-09-26 16:51:00+0900","lastModifiedAt":"2025-09-26 16:51:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/229/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0228-reworking_pythons_numeric_model","title":"[Withdrawn] PEP 228 - Reworking Python’s Numeric Model","excerpt":"Python Enhancement Proposal 228: 'Reworking Python’s Numeric Model'에 대한 한국어 번역입니다.","date":"2025-09-26 16:49:50+0900","lastModifiedAt":"2025-09-26 16:49:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/228/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0227-statically_nested_scopes","title":"[Final] PEP 227 - Statically Nested Scopes","excerpt":"Python Enhancement Proposal 227: 'Statically Nested Scopes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:48:35+0900","lastModifiedAt":"2025-09-26 16:48:35+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/227/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0226-python_2.1_release_schedule","title":"[Final] PEP 226 - Python 2.1 Release Schedule","excerpt":"Python Enhancement Proposal 226: 'Python 2.1 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:47:09+0900","lastModifiedAt":"2025-09-26 16:47:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/226/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0225-elementwiseobjectwise_operators","title":"[Rejected] PEP 225 - Elementwise/Objectwise Operators","excerpt":"Python Enhancement Proposal 225: 'Elementwise/Objectwise Operators'에 대한 한국어 번역입니다.","date":"2025-09-26 16:45:47+0900","lastModifiedAt":"2025-09-26 16:45:47+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/225/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0224-attribute_docstrings","title":"[Rejected] PEP 224 - Attribute Docstrings","excerpt":"Python Enhancement Proposal 224: 'Attribute Docstrings'에 대한 한국어 번역입니다.","date":"2025-09-26 16:43:48+0900","lastModifiedAt":"2025-09-26 16:43:48+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/224/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0223-change_the_meaning_ofxescapes","title":"[Final] PEP 223 - Change the Meaning of '\\x' Escapes","excerpt":"Python Enhancement Proposal 223: 'Change the Meaning of '\\x' Escapes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:42:30+0900","lastModifiedAt":"2025-09-26 16:42:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/223/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0222-web_library_enhancements","title":"[Deferred] PEP 222 - Web Library Enhancements","excerpt":"Python Enhancement Proposal 222: 'Web Library Enhancements'에 대한 한국어 번역입니다.","date":"2025-09-26 16:41:15+0900","lastModifiedAt":"2025-09-26 16:41:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/222/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0221-import_as","title":"[Final] PEP 221 - Import As","excerpt":"Python Enhancement Proposal 221: 'Import As'에 대한 한국어 번역입니다.","date":"2025-09-26 16:39:52+0900","lastModifiedAt":"2025-09-26 16:39:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/221/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0220-coroutines_generators_continuations","title":"[Rejected] PEP 220 - Coroutines, Generators, Continuations","excerpt":"Python Enhancement Proposal 220: 'Coroutines, Generators, Continuations'에 대한 한국어 번역입니다.","date":"2025-09-26 16:38:41+0900","lastModifiedAt":"2025-09-26 16:38:41+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/220/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0219-stackless_python","title":"[Deferred] PEP 219 - Stackless Python","excerpt":"Python Enhancement Proposal 219: 'Stackless Python'에 대한 한국어 번역입니다.","date":"2025-09-26 16:37:30+0900","lastModifiedAt":"2025-09-26 16:37:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/219/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0218-adding_a_built-in_set_object_type","title":"[Final] PEP 218 - Adding a Built-In Set Object Type","excerpt":"Python Enhancement Proposal 218: 'Adding a Built-In Set Object Type'에 대한 한국어 번역입니다.","date":"2025-09-26 16:36:13+0900","lastModifiedAt":"2025-09-26 16:36:13+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/218/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0217-display_hook_for_interactive_use","title":"[Final] PEP 217 - Display Hook for Interactive Use","excerpt":"Python Enhancement Proposal 217: 'Display Hook for Interactive Use'에 대한 한국어 번역입니다.","date":"2025-09-26 16:34:58+0900","lastModifiedAt":"2025-09-26 16:34:58+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/217/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0216-docstring_format","title":"[Withdrawn] PEP 216 - Docstring Format","excerpt":"Python Enhancement Proposal 216: 'Docstring Format'에 대한 한국어 번역입니다.","date":"2025-09-26 16:33:50+0900","lastModifiedAt":"2025-09-26 16:33:50+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/216/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0215-string_interpolation","title":"[Superseded] PEP 215 - String Interpolation","excerpt":"Python Enhancement Proposal 215: 'String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 16:32:37+0900","lastModifiedAt":"2025-09-26 16:32:37+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/215/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0214-extended_print_statement","title":"[Final] PEP 214 - Extended Print Statement","excerpt":"Python Enhancement Proposal 214: 'Extended Print Statement'에 대한 한국어 번역입니다.","date":"2025-09-26 16:31:25+0900","lastModifiedAt":"2025-09-26 16:31:25+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/214/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0213-attribute_access_handlers","title":"[Deferred] PEP 213 - Attribute Access Handlers","excerpt":"Python Enhancement Proposal 213: 'Attribute Access Handlers'에 대한 한국어 번역입니다.","date":"2025-09-26 16:29:57+0900","lastModifiedAt":"2025-09-26 16:29:57+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/213/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0212-loop_counter_iteration","title":"[Rejected] PEP 212 - Loop Counter Iteration","excerpt":"Python Enhancement Proposal 212: 'Loop Counter Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:28:28+0900","lastModifiedAt":"2025-09-26 16:28:28+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/212/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0211-adding_a_new_outer_product_operator","title":"[Rejected] PEP 211 - Adding A New Outer Product Operator","excerpt":"Python Enhancement Proposal 211: 'Adding A New Outer Product Operator'에 대한 한국어 번역입니다.","date":"2025-09-26 16:27:14+0900","lastModifiedAt":"2025-09-26 16:27:14+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/211/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0210-decoupling_the_interpreter_loop","title":"[Rejected] PEP 210 - Decoupling the Interpreter Loop","excerpt":"Python Enhancement Proposal 210: 'Decoupling the Interpreter Loop'에 대한 한국어 번역입니다.","date":"2025-09-26 16:25:55+0900","lastModifiedAt":"2025-09-26 16:25:55+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/210/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0209-multi-dimensional_arrays","title":"[Withdrawn] PEP 209 - Multi-dimensional Arrays","excerpt":"Python Enhancement Proposal 209: 'Multi-dimensional Arrays'에 대한 한국어 번역입니다.","date":"2025-09-26 16:21:26+0900","lastModifiedAt":"2025-09-26 16:21:26+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/209/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0208-reworking_the_coercion_model","title":"[Final] PEP 208 - Reworking the Coercion Model","excerpt":"Python Enhancement Proposal 208: 'Reworking the Coercion Model'에 대한 한국어 번역입니다.","date":"2025-09-26 16:20:00+0900","lastModifiedAt":"2025-09-26 16:20:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/208/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0207-rich_comparisons","title":"[Final] PEP 207 - Rich Comparisons","excerpt":"Python Enhancement Proposal 207: 'Rich Comparisons'에 대한 한국어 번역입니다.","date":"2025-09-26 16:18:40+0900","lastModifiedAt":"2025-09-26 16:18:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/207/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0206-python_advanced_library","title":"[Withdrawn] PEP 206 - Python Advanced Library","excerpt":"Python Enhancement Proposal 206: 'Python Advanced Library'에 대한 한국어 번역입니다.","date":"2025-09-26 16:17:22+0900","lastModifiedAt":"2025-09-26 16:17:22+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/206/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0205-weak_references","title":"[Final] PEP 205 - Weak References","excerpt":"Python Enhancement Proposal 205: 'Weak References'에 대한 한국어 번역입니다.","date":"2025-09-26 16:16:10+0900","lastModifiedAt":"2025-09-26 16:16:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/205/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0204-range_literals","title":"[Rejected] PEP 204 - Range Literals","excerpt":"Python Enhancement Proposal 204: 'Range Literals'에 대한 한국어 번역입니다.","date":"2025-09-26 16:14:46+0900","lastModifiedAt":"2025-09-26 16:14:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/204/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0203-augmented_assignments","title":"[Final] PEP 203 - Augmented Assignments","excerpt":"Python Enhancement Proposal 203: 'Augmented Assignments'에 대한 한국어 번역입니다.","date":"2025-09-26 16:13:23+0900","lastModifiedAt":"2025-09-26 16:13:23+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/203/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0202-list_comprehensions","title":"[Final] PEP 202 - List Comprehensions","excerpt":"Python Enhancement Proposal 202: 'List Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 16:12:08+0900","lastModifiedAt":"2025-09-26 16:12:08+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/202/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0201-lockstep_iteration","title":"[Final] PEP 201 - Lockstep Iteration","excerpt":"Python Enhancement Proposal 201: 'Lockstep Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:10:52+0900","lastModifiedAt":"2025-09-26 16:10:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/201/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0200-python_2.0_release_schedule","title":"[Final] PEP 200 - Python 2.0 Release Schedule","excerpt":"Python Enhancement Proposal 200: 'Python 2.0 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:09:15+0900","lastModifiedAt":"2025-09-26 16:09:15+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/200/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0160-python_1.6_release_schedule","title":"[Final] PEP 160 - Python 1.6 Release Schedule","excerpt":"Python Enhancement Proposal 160: 'Python 1.6 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:07:53+0900","lastModifiedAt":"2025-09-26 16:07:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/160/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0103-collecting_information_about_git","title":"[Withdrawn] PEP 103 - Collecting information about git","excerpt":"Python Enhancement Proposal 103: 'Collecting information about git'에 대한 한국어 번역입니다.","date":"2025-09-26 16:06:44+0900","lastModifiedAt":"2025-09-26 16:06:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/103/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0102-doing_python_micro_releases","title":"[Superseded] PEP 102 - Doing Python Micro Releases","excerpt":"Python Enhancement Proposal 102: 'Doing Python Micro Releases'에 대한 한국어 번역입니다.","date":"2025-09-26 16:04:40+0900","lastModifiedAt":"2025-09-26 16:04:40+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/102/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0101-doing_python_releases_101","title":"[Active] PEP 101 - Doing Python Releases 101","excerpt":"Python Enhancement Proposal 101: 'Doing Python Releases 101'에 대한 한국어 번역입니다.","date":"2025-09-26 16:02:21+0900","lastModifiedAt":"2025-09-26 16:02:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/101/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0100-python_unicode_integration","title":"[Final] PEP 100 - Python Unicode Integration","excerpt":"Python Enhancement Proposal 100: 'Python Unicode Integration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:00:20+0900","lastModifiedAt":"2025-09-26 16:00:20+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/100/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0042-feature_requests","title":"[Withdrawn] PEP 42 - Feature Requests","excerpt":"Python Enhancement Proposal 42: 'Feature Requests'에 대한 한국어 번역입니다.","date":"2025-09-26 15:56:44+0900","lastModifiedAt":"2025-09-26 15:56:44+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/42/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0020-the_zen_of_python","title":"[Active] PEP 20 - The Zen of Python","excerpt":"Python Enhancement Proposal 20: 'The Zen of Python'에 대한 한국어 번역입니다.","date":"2025-09-26 15:55:10+0900","lastModifiedAt":"2025-09-26 15:55:10+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/20/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0013-python_language_governance","title":"[Active] PEP 13 - Python Language Governance","excerpt":"Python Enhancement Proposal 13: 'Python Language Governance'에 대한 한국어 번역입니다.","date":"2025-09-26 15:54:00+0900","lastModifiedAt":"2025-09-26 15:54:00+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/13/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0012-sample_restructuredtext_pep_template","title":"[Active] PEP 12 - Sample reStructuredText PEP Template","excerpt":"Python Enhancement Proposal 12: 'Sample reStructuredText PEP Template'에 대한 한국어 번역입니다.","date":"2025-09-26 15:52:34+0900","lastModifiedAt":"2025-09-26 15:52:34+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/12/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0011-cpython_platform_support","title":"[Active] PEP 11 - CPython platform support","excerpt":"Python Enhancement Proposal 11: 'CPython platform support'에 대한 한국어 번역입니다.","date":"2025-09-26 15:49:46+0900","lastModifiedAt":"2025-09-26 15:49:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/11/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0010-voting_guidelines","title":"[Active] PEP 10 - Voting Guidelines","excerpt":"Python Enhancement Proposal 10: 'Voting Guidelines'에 대한 한국어 번역입니다.","date":"2025-09-26 15:48:17+0900","lastModifiedAt":"2025-09-26 15:48:17+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/10/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0009-sample_plaintext_pep_template","title":"[Withdrawn] PEP 9 - Sample Plaintext PEP Template","excerpt":"Python Enhancement Proposal 9: 'Sample Plaintext PEP Template'에 대한 한국어 번역입니다.","date":"2025-09-26 15:47:09+0900","lastModifiedAt":"2025-09-26 15:47:09+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/9/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0008-style_guide_for_python_code","title":"[Active] PEP 8 - Style Guide for Python Code","excerpt":"Python Enhancement Proposal 8: 'Style Guide for Python Code'에 대한 한국어 번역입니다.","date":"2025-09-26 15:45:52+0900","lastModifiedAt":"2025-09-26 15:45:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/8/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0007-style_guide_for_c_code","title":"[Active] PEP 7 - Style Guide for C Code","excerpt":"Python Enhancement Proposal 7: 'Style Guide for C Code'에 대한 한국어 번역입니다.","date":"2025-09-26 15:43:52+0900","lastModifiedAt":"2025-09-26 15:43:52+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/7/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0006-bug_fix_releases","title":"[Superseded] PEP 6 - Bug Fix Releases","excerpt":"Python Enhancement Proposal 6: 'Bug Fix Releases'에 대한 한국어 번역입니다.","date":"2025-09-26 15:42:30+0900","lastModifiedAt":"2025-09-26 15:42:30+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/6/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0005-guidelines_for_language_evolution","title":"[Superseded] PEP 5 - Guidelines for Language Evolution","excerpt":"Python Enhancement Proposal 5: 'Guidelines for Language Evolution'에 대한 한국어 번역입니다.","date":"2025-09-26 15:40:01+0900","lastModifiedAt":"2025-09-26 15:40:01+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/5/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0004-deprecation_of_standard_modules","title":"[Active] PEP 4 - Deprecation of Standard Modules","excerpt":"Python Enhancement Proposal 4: 'Deprecation of Standard Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 15:38:53+0900","lastModifiedAt":"2025-09-26 15:38:53+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/4/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0003-guidelines_for_handling_bug_reports","title":"[Withdrawn] PEP 3 - Guidelines for Handling Bug Reports","excerpt":"Python Enhancement Proposal 3: 'Guidelines for Handling Bug Reports'에 대한 한국어 번역입니다.","date":"2025-09-26 15:37:46+0900","lastModifiedAt":"2025-09-26 15:37:46+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/3/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0002-procedure_for_adding_new_modules","title":"[Active] PEP 2 - Procedure for Adding New Modules","excerpt":"Python Enhancement Proposal 2: 'Procedure for Adding New Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 15:36:38+0900","lastModifiedAt":"2025-09-26 15:36:38+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/2/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-09-26-pep-0001-pep_purpose_and_guidelines","title":"[Active] PEP 1 - PEP Purpose and Guidelines","excerpt":"Python Enhancement Proposal 1: 'PEP Purpose and Guidelines'에 대한 한국어 번역입니다.","date":"2025-09-26 15:31:21+0900","lastModifiedAt":"2025-09-26 15:31:21+0900","categories":["Python","PEP"],"tags":["Python","PEP","Translation"],"permalink":"/python/pep/1/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-When_Judgment_Becomes_Noise_How_Design_Failures_in_LLM_Judge_Benchmarks_Silently_Undermine_Validity","title":"[논문리뷰] When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity","excerpt":"John P Dickerson이 [arXiv]에 게시한 'When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","LLM Judge","Benchmark Evaluation","Validity","Reliability","Psychometrics","Factor Analysis","Schema Adherence","ELO Ranking"],"permalink":"/ai/review/2025-9-26-When_Judgment_Becomes_Noise_How_Design_Failures_in_LLM_Judge_Benchmarks_Silently_Undermine_Validity/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-VCRL_Variance-based_Curriculum_Reinforcement_Learning_for_Large_Language_Models","title":"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models","excerpt":"Yuewei Zhang이 [arXiv]에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Curriculum Learning","Large Language Models","Mathematical Reasoning","Variance-based Sampling","Replay Learning","Policy Optimization"],"permalink":"/ai/review/2025-9-26-VCRL_Variance-based_Curriculum_Reinforcement_Learning_for_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-V-GameGym_Visual_Game_Generation_for_Code_Large_Language_Models","title":"[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models","excerpt":"Shawn Guo이 [arXiv]에 게시한 'V-GameGym: Visual Game Generation for Code Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Code Large Language Models","Visual Game Generation","Benchmark","Pygame","Multimodal Evaluation","Software Engineering","AI-assisted Game Development"],"permalink":"/ai/review/2025-9-26-V-GameGym_Visual_Game_Generation_for_Code_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Understanding_the_Thinking_Process_of_Reasoning_Models_A_Perspective_from_Schoenfelds_Episode_Theory","title":"[논문리뷰] Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory","excerpt":"Yanbin Fu이 [arXiv]에 게시한 'Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Cognitive Science","Schoenfeld's Episode Theory","Math Problem Solving","Chain-of-Thought","Behavioral Analysis","Dataset Annotation"],"permalink":"/ai/review/2025-9-26-Understanding_the_Thinking_Process_of_Reasoning_Models_A_Perspective_from_Schoenfelds_Episode_Theory/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-TrustJudge_Inconsistencies_of_LLM-as-a-Judge_and_How_to_Alleviate_Them","title":"[논문리뷰] TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them","excerpt":"Zhuohao Yu이 [arXiv]에 게시한 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","LLM-as-a-Judge","Evaluation Frameworks","Inconsistency Reduction","Probabilistic Scoring","Transitivity","Information Loss","Perplexity","Large Language Models"],"permalink":"/ai/review/2025-9-26-TrustJudge_Inconsistencies_of_LLM-as-a-Judge_and_How_to_Alleviate_Them/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Tree_Search_for_LLM_Agent_Reinforcement_Learning","title":"[논문리뷰] Tree Search for LLM Agent Reinforcement Learning","excerpt":"Xiangxiang Chu이 [arXiv]에 게시한 'Tree Search for LLM Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Tree Search","Policy Optimization","Preference Learning","Sparse Rewards","Multi-turn Tasks"],"permalink":"/ai/review/2025-9-26-Tree_Search_for_LLM_Agent_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Thinking_While_Listening_Simple_Test_Time_Scaling_For_Audio_Classification","title":"[논문리뷰] Thinking While Listening: Simple Test Time Scaling For Audio Classification","excerpt":"Mert Pilanci이 [arXiv]에 게시한 'Thinking While Listening: Simple Test Time Scaling For Audio Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Audio Classification","Test-Time Scaling","Reasoning Traces","Large Language Models (LLMs)","Transformer Architectures","Zero-shot Reasoning","Computational Efficiency"],"permalink":"/ai/review/2025-9-26-Thinking_While_Listening_Simple_Test_Time_Scaling_For_Audio_Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Thinking_Augmented_Pre-training","title":"[논문리뷰] Thinking Augmented Pre-training","excerpt":"Furu Wei이 [arXiv]에 게시한 'Thinking Augmented Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Pre-training","Data Augmentation","Reasoning","Data Efficiency","Thinking Trajectories"],"permalink":"/ai/review/2025-9-26-Thinking_Augmented_Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-The_Unanticipated_Asymmetry_Between_Perceptual_Optimization_and_Assessment","title":"[논문리뷰] The Unanticipated Asymmetry Between Perceptual Optimization and Assessment","excerpt":"Du Chen이 [arXiv]에 게시한 'The Unanticipated Asymmetry Between Perceptual Optimization and Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Perceptual Optimization","Image Quality Assessment (IQA)","Adversarial Training","Discriminators","Super-Resolution","Fidelity Metrics","Deep Learning"],"permalink":"/ai/review/2025-9-26-The_Unanticipated_Asymmetry_Between_Perceptual_Optimization_and_Assessment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-StyleBench_Evaluating_thinking_styles_in_Large_Language_Models","title":"[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models","excerpt":"Javad Lavaei이 [arXiv]에 게시한 'StyleBench: Evaluating thinking styles in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Large Language Models","Reasoning Strategies","Prompt Engineering","LLM Evaluation","Benchmark","Thinking Styles","Scaling Laws","Meta-Reasoning"],"permalink":"/ai/review/2025-9-26-StyleBench_Evaluating_thinking_styles_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Seedream_4.0_Toward_Next-generation_Multimodal_Image_Generation","title":"[논문리뷰] Seedream 4.0: Toward Next-generation Multimodal Image Generation","excerpt":"Yunpeng Chen이 [arXiv]에 게시한 'Seedream 4.0: Toward Next-generation Multimodal Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Multimodal Image Generation","Diffusion Transformer","VAE","Image Editing","Text-to-Image","Model Acceleration","Human Evaluation"],"permalink":"/ai/review/2025-9-26-Seedream_4.0_Toward_Next-generation_Multimodal_Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-SciReasoner_Laying_the_Scientific_Reasoning_Ground_Across_Disciplines","title":"[논문리뷰] SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines","excerpt":"Jiabei Xiao이 [arXiv]에 게시한 'SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Scientific Reasoning","Foundation Models","Multi-modal Learning","Cross-domain Generalization","Chain-of-Thought","Reinforcement Learning","Scientific Discovery","Molecular Design"],"permalink":"/ai/review/2025-9-26-SciReasoner_Laying_the_Scientific_Reasoning_Ground_Across_Disciplines/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-SceneWeaver_All-in-One_3D_Scene_Synthesis_with_an_Extensible_and_Self-Reflective_Agent","title":"[논문리뷰] SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent","excerpt":"Siyuan Huang이 [arXiv]에 게시한 'SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","3D Scene Synthesis","Agentic Framework","LLMs","Self-Reflection","Tool-Use","Physical Plausibility","Iterative Refinement","Embodied AI"],"permalink":"/ai/review/2025-9-26-SceneWeaver_All-in-One_3D_Scene_Synthesis_with_an_Extensible_and_Self-Reflective_Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-ScaleDiff_Scaling_Difficult_Problems_for_Advanced_Mathematical_Reasoning","title":"[논문리뷰] ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning","excerpt":"Yu Li이 [arXiv]에 게시한 'ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Large Reasoning Models (LRMs)","Difficulty Scaling","Data Augmentation","Supervised Fine-Tuning (SFT)","Problem Generation","Solution Distillation"],"permalink":"/ai/review/2025-9-26-ScaleDiff_Scaling_Difficult_Problems_for_Advanced_Mathematical_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-SD3.5-Flash_Distribution-Guided_Distillation_of_Generative_Flows","title":"[논문리뷰] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows","excerpt":"Yi-Zhe Song이 [arXiv]에 게시한 'SD3.5-Flash: Distribution-Guided Distillation of Generative Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Generative AI","Image Generation","Diffusion Models","Rectified Flow","Model Distillation","Few-Step Generation","Computational Efficiency","Prompt Alignment"],"permalink":"/ai/review/2025-9-26-SD3.5-Flash_Distribution-Guided_Distillation_of_Generative_Flows/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Residual_Off-Policy_RL_for_Finetuning_Behavior_Cloning_Policies","title":"[논문리뷰] Residual Off-Policy RL for Finetuning Behavior Cloning Policies","excerpt":"Pieter Abbeel이 [arXiv]에 게시한 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Behavior Cloning (BC)","Residual Learning","Off-Policy RL","Robot Manipulation","Real-World Robotics","High-DoF Systems","Sample Efficiency"],"permalink":"/ai/review/2025-9-26-Residual_Off-Policy_RL_for_Finetuning_Behavior_Cloning_Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Recon-Act_A_Self-Evolving_Multi-Agent_Browser-Use_System_via_Web_Reconnaissance_Tool_Generation_and_Task_Execution","title":"[논문리뷰] Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution","excerpt":"Jinjie Gu이 [arXiv]에 게시한 'Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Browser Automation","Web Reconnaissance","Tool Generation","Task Execution","Self-Evolving AI","LLM/VLM","VisualWebArena"],"permalink":"/ai/review/2025-9-26-Recon-Act_A_Self-Evolving_Multi-Agent_Browser-Use_System_via_Web_Reconnaissance_Tool_Generation_and_Task_Execution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Quantized_Visual_Geometry_Grounded_Transformer","title":"[논문리뷰] Quantized Visual Geometry Grounded Transformer","excerpt":"Yuqi Li이 [arXiv]에 게시한 'Quantized Visual Geometry Grounded Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Quantization","Post-Training Quantization","3D Reconstruction","Visual Transformer","Model Compression","Efficient Inference","Hadamard Rotation","Calibration Sampling"],"permalink":"/ai/review/2025-9-26-Quantized_Visual_Geometry_Grounded_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-MOSS-ChatV_Reinforcement_Learning_with_Process_Reasoning_Reward_for_Video_Temporal_Reasoning","title":"[논문리뷰] MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning","excerpt":"Junyan Zhang이 [arXiv]에 게시한 'MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Video Temporal Reasoning","Reinforcement Learning","Process Supervision","Dynamic Time Warping","Multimodal Large Language Models","Video State Prediction","Reward Hacking"],"permalink":"/ai/review/2025-9-26-MOSS-ChatV_Reinforcement_Learning_with_Process_Reasoning_Reward_for_Video_Temporal_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-MMR1_Enhancing_Multimodal_Reasoning_with_Variance-Aware_Sampling_and_Open_Resources","title":"[논문리뷰] MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources","excerpt":"Jing Wang이 [arXiv]에 게시한 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Reinforcement Learning","Variance-Aware Sampling","Gradient Vanishing","Data Curation","Chain-of-Thought","GRPO"],"permalink":"/ai/review/2025-9-26-MMR1_Enhancing_Multimodal_Reasoning_with_Variance-Aware_Sampling_and_Open_Resources/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-MI-Fuse_Label_Fusion_for_Unsupervised_Domain_Adaptation_with_Closed-Source_Large-Audio_Language_Model","title":"[논문리뷰] MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model","excerpt":"Hung-yi Lee이 [arXiv]에 게시한 'MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Speech Emotion Recognition","Source-Free Unsupervised Domain Adaptation","Large Audio-Language Models","Label Fusion","Mutual Information","API-Only Models","Domain Mismatch"],"permalink":"/ai/review/2025-9-26-MI-Fuse_Label_Fusion_for_Unsupervised_Domain_Adaptation_with_Closed-Source_Large-Audio_Language_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Interactive_Recommendation_Agent_with_Active_User_Commands","title":"[논문리뷰] Interactive Recommendation Agent with Active User Commands","excerpt":"Xueyang Feng이 [arXiv]에 게시한 'Interactive Recommendation Agent with Active User Commands' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Interactive Recommendation","Large Language Models","Multi-Agent System","Natural Language Processing","Knowledge Distillation","User Control"],"permalink":"/ai/review/2025-9-26-Interactive_Recommendation_Agent_with_Active_User_Commands/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Hunyuan3D-Omni_A_Unified_Framework_for_Controllable_Generation_of_3D_Assets","title":"[논문리뷰] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets","excerpt":"Bowen Zhang이 [arXiv]에 게시한 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","3D Generation","Controllable Generation","Multi-modal Conditioning","Diffusion Models","Point Clouds","Voxels","Bounding Boxes","Skeletons","Hunyuan3D"],"permalink":"/ai/review/2025-9-26-Hunyuan3D-Omni_A_Unified_Framework_for_Controllable_Generation_of_3D_Assets/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Does_FLUX_Already_Know_How_to_Perform_Physically_Plausible_Image_Composition","title":"[논문리뷰] Does FLUX Already Know How to Perform Physically Plausible Image Composition?","excerpt":"Chen Zhao이 [arXiv]에 게시한 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Image Composition","Diffusion Models","Training-Free","Physically Plausible","FLUX","Adapter","Guidance","Benchmark"],"permalink":"/ai/review/2025-9-26-Does_FLUX_Already_Know_How_to_Perform_Physically_Plausible_Image_Composition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Discrete_Diffusion_for_Reflective_Vision-Language-Action_Models_in_Autonomous_Driving","title":"[논문리뷰] Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving","excerpt":"Hang Zhao이 [arXiv]에 게시한 'Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Autonomous Driving","Vision-Language-Action Models","Discrete Diffusion","Reflection Mechanism","Trajectory Generation","Safety Constraints","Imitation Learning"],"permalink":"/ai/review/2025-9-26-Discrete_Diffusion_for_Reflective_Vision-Language-Action_Models_in_Autonomous_Driving/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-CHARM_Control-point-based_3D_Anime_Hairstyle_Auto-Regressive_Modeling","title":"[논문리뷰] CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling","excerpt":"Yushi Bai이 [arXiv]에 게시한 'CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","3D Anime Hairstyle","Autoregressive Modeling","Control Points","Parametric Representation","Transformer","Generative AI","Dataset (AnimeHair)","Computer Graphics"],"permalink":"/ai/review/2025-9-26-CHARM_Control-point-based_3D_Anime_Hairstyle_Auto-Regressive_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-CE-GPPO_Controlling_Entropy_via_Gradient-Preserving_Clipping_Policy_Optimization_in_Reinforcement_Learning","title":"[논문리뷰] CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning","excerpt":"Wenping Hu이 [arXiv]에 게시한 'CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Policy Optimization","PPO","Entropy Control","Gradient Clipping","Exploration-Exploitation"],"permalink":"/ai/review/2025-9-26-CE-GPPO_Controlling_Entropy_via_Gradient-Preserving_Clipping_Policy_Optimization_in_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Blueprints_of_Trust_AI_System_Cards_for_End_to_End_Transparency_and_Governance","title":"[논문리뷰] Blueprints of Trust: AI System Cards for End to End Transparency and Governance","excerpt":"Roman Zhukov이 [arXiv]에 게시한 'Blueprints of Trust: AI System Cards for End to End Transparency and Governance' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","AI Governance","Transparency","AI System Card","Hazard-Aware System Card","Data Provenance","AI Safety","AI Risk Management","ISO/IEC 42001"],"permalink":"/ai/review/2025-9-26-Blueprints_of_Trust_AI_System_Cards_for_End_to_End_Transparency_and_Governance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-Behind_RoPE_How_Does_Causal_Mask_Encode_Positional_Information","title":"[논문리뷰] Behind RoPE: How Does Causal Mask Encode Positional Information?","excerpt":"Yeyun Gong이 [arXiv]에 게시한 'Behind RoPE: How Does Causal Mask Encode Positional Information?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Transformer Decoder","Causal Mask","Positional Encoding","RoPE","Attention Mechanism","Length Generalization","Large Language Models"],"permalink":"/ai/review/2025-9-26-Behind_RoPE_How_Does_Causal_Mask_Encode_Positional_Information/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-BESPOKE_Benchmark_for_Search-Augmented_Large_Language_Model_Personalization_via_Diagnostic_Feedback","title":"[논문리뷰] BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback","excerpt":"Dongha Lee이 [arXiv]에 게시한 'BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","Search-Augmented LLMs","Personalization","Benchmark","Diagnostic Feedback","User History","Evaluation Framework","RAG"],"permalink":"/ai/review/2025-9-26-BESPOKE_Benchmark_for_Search-Augmented_Large_Language_Model_Personalization_via_Diagnostic_Feedback/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-26-AutoIntent_AutoML_for_Text_Classification","title":"[논문리뷰] AutoIntent: AutoML for Text Classification","excerpt":"Denis Kuznetsov이 [arXiv]에 게시한 'AutoIntent: AutoML for Text Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","lastModifiedAt":"2025-09-26 13:35:32+0900","categories":["Review"],"tags":["Review","AutoML","Text Classification","Intent Classification","Transformer Embeddings","Out-of-Scope Detection","Multi-label Classification","Few-shot Learning","Sklearn-like Interface"],"permalink":"/ai/review/2025-9-26-AutoIntent_AutoML_for_Text_Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-Video_models_are_zero-shot_learners_and_reasoners","title":"[논문리뷰] Video models are zero-shot learners and reasoners","excerpt":"rgeirhos이 [arXiv]에 게시한 'Video models are zero-shot learners and reasoners' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Video Models","Zero-shot Learning","Visual Reasoning","Foundation Models","Generative AI","Perception","Manipulation","Modeling"],"permalink":"/ai/review/2025-9-25-Video_models_are_zero-shot_learners_and_reasoners/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-SIM-CoT_Supervised_Implicit_Chain-of-Thought","title":"[논문리뷰] SIM-CoT: Supervised Implicit Chain-of-Thought","excerpt":"Yuhang Cao이 [arXiv]에 게시한 'SIM-CoT: Supervised Implicit Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Implicit Reasoning","Chain-of-Thought","LLM","Latent Space","Supervised Learning","Model Stability","Interpretability"],"permalink":"/ai/review/2025-9-25-SIM-CoT_Supervised_Implicit_Chain-of-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-PhysCtrl_Generative_Physics_for_Controllable_and_Physics-Grounded_Video_Generation","title":"[논문리뷰] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation","excerpt":"Yiming Huang이 [arXiv]에 게시한 'PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Video Generation","Physics-Grounded","Controllable Generation","Diffusion Models","Point Cloud Trajectories","Material Simulation","Generative Physics"],"permalink":"/ai/review/2025-9-25-PhysCtrl_Generative_Physics_for_Controllable_and_Physics-Grounded_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-On_the_Use_of_Agentic_Coding_An_Empirical_Study_of_Pull_Requests_on_GitHub","title":"[논문리뷰] On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub","excerpt":"Hajimu Iida이 [arXiv]에 게시한 'On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Agentic Coding","AI Agents","Large Language Models","GitHub Pull Requests","Software Engineering","Empirical Study","Code Generation","Software Development"],"permalink":"/ai/review/2025-9-25-On_the_Use_of_Agentic_Coding_An_Empirical_Study_of_Pull_Requests_on_GitHub/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-Logics-Parsing_Technical_Report","title":"[논문리뷰] Logics-Parsing Technical Report","excerpt":"Fan Yang이 [arXiv]에 게시한 'Logics-Parsing Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Document Parsing","Large Vision-Language Models (LVLM)","Reinforcement Learning (RL)","Layout Analysis","Reading Order","Supervised Fine-Tuning (SFT)","HTML Annotation","Benchmarking"],"permalink":"/ai/review/2025-9-25-Logics-Parsing_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-Lavida-O_Elastic_Large_Masked_Diffusion_Models_for_Unified_Multimodal_Understanding_and_Generation","title":"[논문리뷰] Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation","excerpt":"Zhe Lin이 [arXiv]에 게시한 'Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Multimodal AI","Masked Diffusion Models","Image Understanding","Image Generation","Image Editing","Object Grounding","ElasticMoT","Self-reflection"],"permalink":"/ai/review/2025-9-25-Lavida-O_Elastic_Large_Masked_Diffusion_Models_for_Unified_Multimodal_Understanding_and_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-LLMs4All_A_Review_on_Large_Language_Models_for_Research_and_Applications_in_Academic_Disciplines","title":"[논문리뷰] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines","excerpt":"Yanfang이 [arXiv]에 게시한 'LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Large Language Models","Generative AI","Academic Disciplines","LLM Applications","Review","Cross-disciplinary Research","Benchmarks"],"permalink":"/ai/review/2025-9-25-LLMs4All_A_Review_on_Large_Language_Models_for_Research_and_Applications_in_Academic_Disciplines/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-EmbeddingGemma_Powerful_and_Lightweight_Text_Representations","title":"[논문리뷰] EmbeddingGemma: Powerful and Lightweight Text Representations","excerpt":"Marksherwood이 [arXiv]에 게시한 'EmbeddingGemma: Powerful and Lightweight Text Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Text Embeddings","Lightweight Models","Encoder-Decoder","Knowledge Distillation","Model Souping","Quantization","Multilingual","Gemma"],"permalink":"/ai/review/2025-9-25-EmbeddingGemma_Powerful_and_Lightweight_Text_Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-EditVerse_Unifying_Image_and_Video_Editing_and_Generation_with_In-Context_Learning","title":"[논문리뷰] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning","excerpt":"Tianyu Wang이 [arXiv]에 게시한 'EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Unified Multimodal Model","In-Context Learning","Image and Video Editing","Video Generation","Full Self-Attention","Rotary Positional Embedding","Cross-Modal Knowledge Transfer"],"permalink":"/ai/review/2025-9-25-EditVerse_Unifying_Image_and_Video_Editing_and_Generation_with_In-Context_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-25-Advancing_Speech_Understanding_in_Speech-Aware_Language_Models_with_GRPO","title":"[논문리뷰] Advancing Speech Understanding in Speech-Aware Language Models with GRPO","excerpt":"Avihu이 [arXiv]에 게시한 'Advancing Speech Understanding in Speech-Aware Language Models with GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","lastModifiedAt":"2025-09-25 13:08:16+0900","categories":["Review"],"tags":["Review","Speech-Aware Language Models","SALLMs","GRPO","Reinforcement Learning","Speech Understanding","Spoken Question Answering","Automatic Speech Translation","BLEU Metric"],"permalink":"/ai/review/2025-9-25-Advancing_Speech_Understanding_in_Speech-Aware_Language_Models_with_GRPO/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-Zero-Shot_Multi-Spectral_Learning_Reimagining_a_Generalist_Multimodal_Gemini_2.5_Model_for_Remote_Sensing_Applications","title":"[논문리뷰] Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications","excerpt":"Genady Beryozkin이 [arXiv]에 게시한 'Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Remote Sensing","Zero-Shot Learning","Multimodal Models","Multi-spectral Imagery","Gemini 2.5","Prompt Engineering","Land Cover Classification","Pseudo-Image"],"permalink":"/ai/review/2025-9-24-Zero-Shot_Multi-Spectral_Learning_Reimagining_a_Generalist_Multimodal_Gemini_2.5_Model_for_Remote_Sensing_Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-What_Characterizes_Effective_Reasoning_Revisiting_Length_Review_and_Structure_of_CoT","title":"[논문리뷰] What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT","excerpt":"Anthony Hartshorn이 [arXiv]에 게시한 'What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Chain-of-Thought","Reasoning Effectiveness","Large Reasoning Models","Failed-Step Fraction","Test-time Scaling","Reasoning Graph","Model Evaluation"],"permalink":"/ai/review/2025-9-24-What_Characterizes_Effective_Reasoning_Revisiting_Length_Review_and_Structure_of_CoT/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-VolSplat_Rethinking_Feed-Forward_3D_Gaussian_Splatting_with_Voxel-Aligned_Prediction","title":"[논문리뷰] VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction","excerpt":"Haoxiao Wang이 [arXiv]에 게시한 'VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting","Novel View Synthesis","Voxel-Aligned Prediction","Feed-Forward Reconstruction","Multi-View Consistency","Scene Representation","Computer Vision"],"permalink":"/ai/review/2025-9-24-VolSplat_Rethinking_Feed-Forward_3D_Gaussian_Splatting_with_Voxel-Aligned_Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-VIR-Bench_Evaluating_Geospatial_and_Temporal_Understanding_of_MLLMs_via_Travel_Video_Itinerary_Reconstruction","title":"[논문리뷰] VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction","excerpt":"So Fukuda이 [arXiv]에 게시한 'VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Video Understanding","Geospatial Reasoning","Temporal Reasoning","Travel Itinerary Reconstruction","Benchmark","Agent System","VLOG"],"permalink":"/ai/review/2025-9-24-VIR-Bench_Evaluating_Geospatial_and_Temporal_Understanding_of_MLLMs_via_Travel_Video_Itinerary_Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-Reinforcement_Learning_on_Pre-Training_Data","title":"[논문리뷰] Reinforcement Learning on Pre-Training Data","excerpt":"Evander Yang이 [arXiv]에 게시한 'Reinforcement Learning on Pre-Training Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Pre-training","Large Language Models","Self-supervised Learning","Scaling Laws","Next-segment Reasoning","Reward Modeling"],"permalink":"/ai/review/2025-9-24-Reinforcement_Learning_on_Pre-Training_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-OpenGVL_-_Benchmarking_Visual_Temporal_Progress_for_Data_Curation","title":"[논문리뷰] OpenGVL - Benchmarking Visual Temporal Progress for Data Curation","excerpt":"Viktor Petrenko이 [arXiv]에 게시한 'OpenGVL - Benchmarking Visual Temporal Progress for Data Curation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Robotics Data Curation","Visual Temporal Progress","Generative Value Learning (GVL)","Vision-Language Models (VLMs)","Benchmark","Task Progress Prediction","Value-Order Correlation (VOC)"],"permalink":"/ai/review/2025-9-24-OpenGVL_-_Benchmarking_Visual_Temporal_Progress_for_Data_Curation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-MiniCPM-V_4.5_Cooking_Efficient_MLLMs_via_Architecture_Data_and_Training_Recipe","title":"[논문리뷰] MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe","excerpt":"Wenshuo Ma이 [arXiv]에 게시한 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","MLLM Efficiency","Multimodal Transformer","3D-Resampler","Document AI","Hybrid Reinforcement Learning","Video Understanding","Efficient Inference"],"permalink":"/ai/review/2025-9-24-MiniCPM-V_4.5_Cooking_Efficient_MLLMs_via_Architecture_Data_and_Training_Recipe/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-MAPO_Mixed_Advantage_Policy_Optimization","title":"[논문리뷰] MAPO: Mixed Advantage Policy Optimization","excerpt":"Xuankun Rong이 [arXiv]에 게시한 'MAPO: Mixed Advantage Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Foundation Models","Policy Optimization","Advantage Function","Trajectory Certainty","Multimodal Reasoning","GRPO"],"permalink":"/ai/review/2025-9-24-MAPO_Mixed_Advantage_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-Lyra_Generative_3D_Scene_Reconstruction_via_Video_Diffusion_Model_Self-Distillation","title":"[논문리뷰] Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation","excerpt":"Yifeng Jiang이 [arXiv]에 게시한 'Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Generative AI","3D Scene Reconstruction","Video Diffusion Models","Self-Distillation","3D Gaussian Splatting","Dynamic 4D Generation","Monocular Input"],"permalink":"/ai/review/2025-9-24-Lyra_Generative_3D_Scene_Reconstruction_via_Video_Diffusion_Model_Self-Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-Large_Language_Models_Discriminate_Against_Speakers_of_German_Dialects","title":"[논문리뷰] Large Language Models Discriminate Against Speakers of German Dialects","excerpt":"Katharina von der Wense이 [arXiv]에 게시한 'Large Language Models Discriminate Against Speakers of German Dialects' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Large Language Models","Bias","German Dialects","Sociolinguistics","Stereotypes","Implicit Association Test","Decision Making"],"permalink":"/ai/review/2025-9-24-Large_Language_Models_Discriminate_Against_Speakers_of_German_Dialects/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-Hyper-Bagel_A_Unified_Acceleration_Framework_for_Multimodal_Understanding_and_Generation","title":"[논문리뷰] Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation","excerpt":"Jianbin Zheng이 [arXiv]에 게시한 'Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Multimodal AI","Acceleration Framework","Speculative Decoding","Diffusion Distillation","Unified Models","Text-to-Image Generation","Image Editing","Computational Efficiency"],"permalink":"/ai/review/2025-9-24-Hyper-Bagel_A_Unified_Acceleration_Framework_for_Multimodal_Understanding_and_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-HyRF_Hybrid_Radiance_Fields_for_Memory-efficient_and_High-quality_Novel_View_Synthesis","title":"[논문리뷰] HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis","excerpt":"Dan Xu이 [arXiv]에 게시한 'HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Novel View Synthesis","3D Gaussian Splatting (3DGS)","Neural Radiance Fields (NeRF)","Memory Efficiency","High-Quality Rendering","Hybrid Representation","Real-time Rendering"],"permalink":"/ai/review/2025-9-24-HyRF_Hybrid_Radiance_Fields_for_Memory-efficient_and_High-quality_Novel_View_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-GeoSVR_Taming_Sparse_Voxels_for_Geometrically_Accurate_Surface_Reconstruction","title":"[논문리뷰] GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction","excerpt":"Jin Zheng이 [arXiv]에 게시한 'GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Surface Reconstruction","Sparse Voxels","Geometric Accuracy","Neural Radiance Fields","3D Gaussian Splatting","Monocular Depth","Voxel Uncertainty"],"permalink":"/ai/review/2025-9-24-GeoSVR_Taming_Sparse_Voxels_for_Geometrically_Accurate_Surface_Reconstruction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-Do_You_Need_Proprioceptive_States_in_Visuomotor_Policies","title":"[논문리뷰] Do You Need Proprioceptive States in Visuomotor Policies?","excerpt":"Yushen Liang이 [arXiv]에 게시한 'Do You Need Proprioceptive States in Visuomotor Policies?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Visuomotor Policies","Spatial Generalization","Imitation Learning","Proprioception","State-free Policies","Robot Manipulation","End-Effector Control","Data Efficiency"],"permalink":"/ai/review/2025-9-24-Do_You_Need_Proprioceptive_States_in_Visuomotor_Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-CAR-Flow_Condition-Aware_Reparameterization_Aligns_Source_and_Target_for_Better_Flow_Matching","title":"[논문리뷰] CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching","excerpt":"Rui Qian이 [arXiv]에 게시한 'CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Flow Matching","Conditional Generative Models","Reparameterization","Mode Collapse","Image Generation","Latent Space Alignment","Diffusion Models"],"permalink":"/ai/review/2025-9-24-CAR-Flow_Condition-Aware_Reparameterization_Aligns_Source_and_Target_for_Better_Flow_Matching/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-24-Baseer_A_Vision-Language_Model_for_Arabic_Document-to-Markdown_OCR","title":"[논문리뷰] Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR","excerpt":"Zeina Aldallal이 [arXiv]에 게시한 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","lastModifiedAt":"2025-09-24 13:14:19+0900","categories":["Review"],"tags":["Review","Arabic OCR","Vision-Language Model","Fine-tuning","Document Understanding","Markdown Conversion","Benchmark"],"permalink":"/ai/review/2025-9-24-Baseer_A_Vision-Language_Model_for_Arabic_Document-to-Markdown_OCR/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-When_Big_Models_Train_Small_Ones_Label-Free_Model_Parity_Alignment_for_Efficient_Visual_Question_Answering_using_Small_VLMs","title":"[논문리뷰] When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs","excerpt":"Anand Mishra이 [arXiv]에 게시한 'When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","VQA","Small VLMs","Large VLMs","Knowledge Transfer","Pseudo-labeling","Label-Free Learning","Model Parity Alignment","Computational Efficiency"],"permalink":"/ai/review/2025-9-23-When_Big_Models_Train_Small_Ones_Label-Free_Model_Parity_Alignment_for_Efficient_Visual_Question_Answering_using_Small_VLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-VideoFrom3D_3D_Scene_Video_Generation_via_Complementary_Image_and_Video_Diffusion_Models","title":"[논문리뷰] VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models","excerpt":"Sunghyun Cho이 [arXiv]에 게시한 'VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Video Diffusion","Image Diffusion","Generative Models","Computer Graphics","Temporal Consistency","Sparse Anchor Views"],"permalink":"/ai/review/2025-9-23-VideoFrom3D_3D_Scene_Video_Generation_via_Complementary_Image_and_Video_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-VaseVQA_Multimodal_Agent_and_Benchmark_for_Ancient_Greek_Pottery","title":"[논문리뷰] VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery","excerpt":"Shiya Huang이 [arXiv]에 게시한 'VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Visual Question Answering","Reinforcement Learning","Cultural Heritage","Ancient Greek Pottery","Supervised Fine-Tuning","Benchmark"],"permalink":"/ai/review/2025-9-23-VaseVQA_Multimodal_Agent_and_Benchmark_for_Ancient_Greek_Pottery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Understanding_Embedding_Scaling_in_Collaborative_Filtering","title":"[논문리뷰] Understanding Embedding Scaling in Collaborative Filtering","excerpt":"Yonghui Yang이 [arXiv]에 게시한 'Understanding Embedding Scaling in Collaborative Filtering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Collaborative Filtering","Embedding Scaling","Noise Robustness","Recommender Systems","Graph Neural Networks","Self-supervised Learning","Performance Degradation"],"permalink":"/ai/review/2025-9-23-Understanding_Embedding_Scaling_in_Collaborative_Filtering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Turk-LettuceDetect_A_Hallucination_Detection_Models_for_Turkish_RAG_Applications","title":"[논문리뷰] Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications","excerpt":"Fatma Betül Terzioğlu이 [arXiv]에 게시한 'Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Hallucination Detection","Retrieval Augmented Generation","Large Language Models","Turkish NLP","Token Classification","ModernBERT","Low-Resource Languages"],"permalink":"/ai/review/2025-9-23-Turk-LettuceDetect_A_Hallucination_Detection_Models_for_Turkish_RAG_Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-TempSamp-R1_Effective_Temporal_Sampling_with_Reinforcement_Fine-Tuning_for_Video_LLMs","title":"[논문리뷰] TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs","excerpt":"Shaohui Jiao이 [arXiv]에 게시한 'TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Video LLMs","Temporal Grounding","Reinforcement Learning","Off-policy Learning","Reward Shaping","Chain-of-Thought","Multimodal LLMs"],"permalink":"/ai/review/2025-9-23-TempSamp-R1_Effective_Temporal_Sampling_with_Reinforcement_Fine-Tuning_for_Video_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Synthetic_bootstrapped_pretraining","title":"[논문리뷰] Synthetic bootstrapped pretraining","excerpt":"Emmanuel Candès이 [arXiv]에 게시한 'Synthetic bootstrapped pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Language Model Pretraining","Synthetic Data","Inter-document Correlation","Data Augmentation","Transformer","Bootstrapping","Concept Learning"],"permalink":"/ai/review/2025-9-23-Synthetic_bootstrapped_pretraining/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-SWE-Bench_Pro_Can_AI_Agents_Solve_Long-Horizon_Software_Engineering_Tasks","title":"[논문리뷰] SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?","excerpt":"Yannis Yiming He이 [arXiv]에 게시한 'SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","AI Agents","Software Engineering","LLMs","Code Generation","Benchmark","Contamination Resistance","Long-Horizon Tasks","Enterprise Software"],"permalink":"/ai/review/2025-9-23-SWE-Bench_Pro_Can_AI_Agents_Solve_Long-Horizon_Software_Engineering_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-SCAN_Self-Denoising_Monte_Carlo_Annotation_for_Robust_Process_Reward_Learning","title":"[논문리뷰] SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning","excerpt":"Zhaopeng Tu이 [arXiv]에 게시한 'SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Process Reward Models","Monte Carlo Annotation","Noise Denoising","Robust Learning","Self-Supervision","Mathematical Reasoning","Large Language Models"],"permalink":"/ai/review/2025-9-23-SCAN_Self-Denoising_Monte_Carlo_Annotation_for_Robust_Process_Reward_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Reasoning_Core_A_Scalable_RL_Environment_for_LLM_Symbolic_Reasoning","title":"[논문리뷰] Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning","excerpt":"Damien Sileo이 [arXiv]에 게시한 'Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Symbolic AI","Reinforcement Learning","Procedural Content Generation","Verifiable Rewards","Adaptive Curricula","First-Order Logic","PDDL Planning"],"permalink":"/ai/review/2025-9-23-Reasoning_Core_A_Scalable_RL_Environment_for_LLM_Symbolic_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Qwen3-Omni_Technical_Report","title":"[논문리뷰] Qwen3-Omni Technical Report","excerpt":"Lhma-aslp이 [arXiv]에 게시한 'Qwen3-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Model","Thinker-Talker Architecture","Mixture-of-Experts","Low-latency","Audio Understanding","Cross-modal Reasoning","State-of-the-Art","Real-time Interaction"],"permalink":"/ai/review/2025-9-23-Qwen3-Omni_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-QWHA_Quantization-Aware_Walsh-Hadamard_Adaptation_for_Parameter-Efficient_Fine-Tuning_on_Large_Language_Models","title":"[논문리뷰] QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models","excerpt":"Jae-Joon Kim이 [arXiv]에 게시한 'QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","LLM Fine-tuning","Quantization-Aware PEFT","Walsh-Hadamard Transform","Sparse Adaptation","Low-bit Quantization","Parameter-Efficient Learning"],"permalink":"/ai/review/2025-9-23-QWHA_Quantization-Aware_Walsh-Hadamard_Adaptation_for_Parameter-Efficient_Fine-Tuning_on_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-OmniInsert_Mask-Free_Video_Insertion_of_Any_Reference_via_Diffusion_Transformer_Models","title":"[논문리뷰] OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models","excerpt":"Pengze Zhang이 [arXiv]에 게시한 'OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Video Insertion","Diffusion Models","Diffusion Transformers","Mask-Free","Data Augmentation","Progressive Training","Preference Optimization","Video Generation"],"permalink":"/ai/review/2025-9-23-OmniInsert_Mask-Free_Video_Insertion_of_Any_Reference_via_Diffusion_Transformer_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-MetaEmbed_Scaling_Multimodal_Retrieval_at_Test-Time_with_Flexible_Late_Interaction","title":"[논문리뷰] MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction","excerpt":"Xintao Chen이 [arXiv]에 게시한 'MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Retrieval","Late Interaction","Meta Tokens","Matryoshka Representation Learning","Test-Time Scaling","Vision-Language Models","Dense Retrieval","Efficiency"],"permalink":"/ai/review/2025-9-23-MetaEmbed_Scaling_Multimodal_Retrieval_at_Test-Time_with_Flexible_Late_Interaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Mano_Report","title":"[논문리뷰] Mano Report","excerpt":"Minghui Wu이 [arXiv]에 게시한 'Mano Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","GUI Agent","Multi-modal Foundation Model","Reinforcement Learning","Supervised Fine-tuning","Simulated Environment","Data Generation","Error Recovery","Web Automation"],"permalink":"/ai/review/2025-9-23-Mano_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-LIMI_Less_is_More_for_Agency","title":"[논문리뷰] LIMI: Less is More for Agency","excerpt":"happyZYM이 [arXiv]에 게시한 'LIMI: Less is More for Agency' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","AI Agency","Data Curation","Less Is More","Agentic Intelligence","Foundation Models","Evaluation Benchmark","Efficiency Principle","Large Language Models"],"permalink":"/ai/review/2025-9-23-LIMI_Less_is_More_for_Agency/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-GeoPQA_Bridging_the_Visual_Perception_Gap_in_MLLMs_for_Geometric_Reasoning","title":"[논문리뷰] GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning","excerpt":"Hou Pong Chan이 [arXiv]에 게시한 'GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Geometric Reasoning","Visual Perception","Reinforcement Learning (RL)","Two-stage Training","GeoPQA Benchmark","Perceptual Bottleneck"],"permalink":"/ai/review/2025-9-23-GeoPQA_Bridging_the_Visual_Perception_Gap_in_MLLMs_for_Geometric_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-From_Uniform_to_Heterogeneous_Tailoring_Policy_Optimization_to_Every_Tokens_Nature","title":"[논문리뷰] From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature","excerpt":"Bin Cui이 [arXiv]에 게시한 'From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Policy Optimization","Token Heterogeneity","Adaptive Sampling","Advantage Redistribution","Asymmetric Clipping","Entropy-based RL"],"permalink":"/ai/review/2025-9-23-From_Uniform_to_Heterogeneous_Tailoring_Policy_Optimization_to_Every_Tokens_Nature/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-From_Hugging_Face_to_GitHub_Tracing_License_Drift_in_the_Open-Source_AI_Ecosystem","title":"[논문리뷰] From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem","excerpt":"Ahmed E. Hassan이 [arXiv]에 게시한 'From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Open-Source AI","License Compliance","License Drift","AI Supply Chain","Hugging Face","GitHub","LicenseRec","Legal Risk"],"permalink":"/ai/review/2025-9-23-From_Hugging_Face_to_GitHub_Tracing_License_Drift_in_the_Open-Source_AI_Ecosystem/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-FlagEval_Findings_Report_A_Preliminary_Evaluation_of_Large_Reasoning_Models_on_Automatically_Verifiable_Textual_and_Visual_Questions","title":"[논문리뷰] FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions","excerpt":"tengdai722이 [arXiv]에 게시한 'FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","LLM Evaluation","Multimodal AI","Reasoning Behaviors","Hallucination","Contamination-Free","AI Safety","Instruction Following"],"permalink":"/ai/review/2025-9-23-FlagEval_Findings_Report_A_Preliminary_Evaluation_of_Large_Reasoning_Models_on_Automatically_Verifiable_Textual_and_Visual_Questions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-EpiCache_Episodic_KV_Cache_Management_for_Long_Conversational_Question_Answering","title":"[논문리뷰] EpiCache: Episodic KV Cache Management for Long Conversational Question Answering","excerpt":"Minsik Cho이 [arXiv]에 게시한 'EpiCache: Episodic KV Cache Management for Long Conversational Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","KV Cache Management","Long Conversational QA","LLMs","Memory Efficiency","Episodic Clustering","Block Prefill Eviction","Sensitivity-aware Allocation"],"permalink":"/ai/review/2025-9-23-EpiCache_Episodic_KV_Cache_Management_for_Long_Conversational_Question_Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-DiffusionNFT_Online_Diffusion_Reinforcement_with_Forward_Process","title":"[논문리뷰] DiffusionNFT: Online Diffusion Reinforcement with Forward Process","excerpt":"Qinsheng Zhang이 [arXiv]에 게시한 'DiffusionNFT: Online Diffusion Reinforcement with Forward Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Diffusion Models","Reinforcement Learning","Online RL","Flow Matching","Forward Process","CFG-free","Image Generation","Negative-Aware FineTuning"],"permalink":"/ai/review/2025-9-23-DiffusionNFT_Online_Diffusion_Reinforcement_with_Forward_Process/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-DIWALI_-_Diversity_and_Inclusivity_aWare_cuLture_specific_Items_for_India_Dataset_and_Assessment_of_LLMs_for_Cultural_Text_Adaptation_in_Indian_Context","title":"[논문리뷰] DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context","excerpt":"Maunendra Sankar Desarkar이 [arXiv]에 게시한 'DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Cultural Adaptation","Large Language Models","Indian Culture","Dataset Creation","CSI","Human Evaluation","LLM Evaluation","Cultural Bias"],"permalink":"/ai/review/2025-9-23-DIWALI_-_Diversity_and_Inclusivity_aWare_cuLture_specific_Items_for_India_Dataset_and_Assessment_of_LLMs_for_Cultural_Text_Adaptation_in_Indian_Context/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Cross-Attention_is_Half_Explanation_in_Speech-to-Text_Models","title":"[논문리뷰] Cross-Attention is Half Explanation in Speech-to-Text Models","excerpt":"Luisa Bentivogli이 [arXiv]에 게시한 'Cross-Attention is Half Explanation in Speech-to-Text Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Cross-attention","Speech-to-Text (S2T)","Explainable AI (XAI)","Saliency Maps","Feature Attribution","Transformer","Context Mixing","Correlation"],"permalink":"/ai/review/2025-9-23-Cross-Attention_is_Half_Explanation_in_Speech-to-Text_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-ContextFlow_Training-Free_Video_Object_Editing_via_Adaptive_Context_Enrichment","title":"[논문리뷰] ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment","excerpt":"Yue Ma이 [arXiv]에 게시한 'ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Video Object Editing","Training-Free","Diffusion Transformers","Rectified Flow","Adaptive Context Enrichment","Guidance Responsiveness","Temporal Consistency","Image-to-Video"],"permalink":"/ai/review/2025-9-23-ContextFlow_Training-Free_Video_Object_Editing_via_Adaptive_Context_Enrichment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-CodeFuse-CR-Bench_A_Comprehensiveness-aware_Benchmark_for_End-to-End_Code_Review_Evaluation_in_Python_Projects","title":"[논문리뷰] CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects","excerpt":"Hang Yu이 [arXiv]에 게시한 'CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Code Review","LLMs","Benchmark","Python Projects","End-to-End Evaluation","Context-Awareness","Software Engineering","LLM-as-a-Judge"],"permalink":"/ai/review/2025-9-23-CodeFuse-CR-Bench_A_Comprehensiveness-aware_Benchmark_for_End-to-End_Code_Review_Evaluation_in_Python_Projects/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-ByteWrist_A_Parallel_Robotic_Wrist_Enabling_Flexible_and_Anthropomorphic_Motion_for_Confined_Spaces","title":"[논문리뷰] ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces","excerpt":"Jiafeng Xu이 [arXiv]에 게시한 'ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Robotics","Parallel Manipulator","Robotic Wrist","Confined Space Manipulation","Kinematics","Anthropomorphic Robot","Robot Design"],"permalink":"/ai/review/2025-9-23-ByteWrist_A_Parallel_Robotic_Wrist_Enabling_Flexible_and_Anthropomorphic_Motion_for_Confined_Spaces/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-AuditoryBench_Can_Language_Models_Understand_Auditory_Knowledge_without_Hearing","title":"[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?","excerpt":"Jaeho Lee이 [arXiv]에 게시한 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Auditory Knowledge","Large Language Models","Multimodal Reasoning","Benchmark","Chain-of-Thought","Auditory Imagination","Text-only Reasoning"],"permalink":"/ai/review/2025-9-23-AuditoryBench_Can_Language_Models_Understand_Auditory_Knowledge_without_Hearing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-Analyzing_the_Effects_of_Supervised_Fine-Tuning_on_Model_Knowledge_from_Token_and_Parameter_Levels","title":"[논문리뷰] Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels","excerpt":"Qi Zhang이 [arXiv]에 게시한 'Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Supervised Fine-Tuning (SFT)","Large Language Models (LLMs)","Model Knowledge","Closed-Book Question Answering (CBQA)","Parameter Restoration","Kullback-Leibler Divergence","Knowledge Forgetting"],"permalink":"/ai/review/2025-9-23-Analyzing_the_Effects_of_Supervised_Fine-Tuning_on_Model_Knowledge_from_Token_and_Parameter_Levels/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-23-ARE_Scaling_Up_Agent_Environments_and_Evaluations","title":"[논문리뷰] ARE: Scaling Up Agent Environments and Evaluations","excerpt":"Matteo Bettini이 [arXiv]에 게시한 'ARE: Scaling Up Agent Environments and Evaluations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","lastModifiedAt":"2025-09-23 13:36:03+0900","categories":["Review"],"tags":["Review","Agent Environments","Agent Evaluation","LLM Agents","Asynchronous Systems","Reinforcement Learning","Tool Use","Multi-agent Collaboration","Benchmark"],"permalink":"/ai/review/2025-9-23-ARE_Scaling_Up_Agent_Environments_and_Evaluations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-WhisTLE_Deeply_Supervised_Text-Only_Domain_Adaptation_for_Pretrained_Speech_Recognition_Transformers","title":"[논문리뷰] WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers","excerpt":"Karun Kumar이 [arXiv]에 게시한 'WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","ASR","Domain Adaptation","Text-Only Training","Transformer","Variational Autoencoder","Deep Supervision","Whisper","Encoder-Decoder Models"],"permalink":"/ai/review/2025-9-22-WhisTLE_Deeply_Supervised_Text-Only_Domain_Adaptation_for_Pretrained_Speech_Recognition_Transformers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-Video2Roleplay_A_Multimodal_Dataset_and_Framework_for_Video-Guided_Role-playing_Agents","title":"[논문리뷰] Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents","excerpt":"Chao Zhang이 [arXiv]에 게시한 'Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Role-playing Agents (RPAs)","Multimodal AI","Video Understanding","Large Language Models (LLMs)","Dataset Creation","Dynamic Role Profiles","Adaptive Temporal Sampling","Fine-tuning"],"permalink":"/ai/review/2025-9-22-Video2Roleplay_A_Multimodal_Dataset_and_Framework_for_Video-Guided_Role-playing_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-SPATIALGEN_Layout-guided_3D_Indoor_Scene_Generation","title":"[논문리뷰] SPATIALGEN: Layout-guided 3D Indoor Scene Generation","excerpt":"Yongsen Mao이 [arXiv]에 게시한 'SPATIALGEN: Layout-guided 3D Indoor Scene Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Layout Guidance","Diffusion Models","Multi-view Synthesis","Synthetic Dataset","Indoor Environments","Gaussian Splatting","Semantic Consistency"],"permalink":"/ai/review/2025-9-22-SPATIALGEN_Layout-guided_3D_Indoor_Scene_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-RPG_A_Repository_Planning_Graph_for_Unified_and_Scalable_Codebase_Generation","title":"[논문리뷰] RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation","excerpt":"Steven Liu이 [arXiv]에 게시한 'RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Code Generation","LLMs","Repository Planning","Graph-based Representation","Software Engineering","Agent Frameworks","Scalable Codebase"],"permalink":"/ai/review/2025-9-22-RPG_A_Repository_Planning_Graph_for_Unified_and_Scalable_Codebase_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-RGB-Only_Supervised_Camera_Parameter_Optimization_in_Dynamic_Scenes","title":"[논문리뷰] RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes","excerpt":"Narendra Ahuja이 [arXiv]에 게시한 'RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Camera Parameter Optimization","Dynamic Scenes","RGB-Only Supervision","Structure from Motion","Outlier Robustness","3D Gaussian Splatting","Two-stage Optimization","Point Tracking"],"permalink":"/ai/review/2025-9-22-RGB-Only_Supervised_Camera_Parameter_Optimization_in_Dynamic_Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-MANZANO_A_Simple_and_Scalable_Unified_Multimodal_Model_with_a_Hybrid_Vision_Tokenizer","title":"[논문리뷰] MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer","excerpt":"jialingt이 [arXiv]에 게시한 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Hybrid Tokenizer","Text-to-Image Generation","Visual Question Answering","Autoregressive Model","Diffusion Decoder","Unified Architecture","Model Scaling"],"permalink":"/ai/review/2025-9-22-MANZANO_A_Simple_and_Scalable_Unified_Multimodal_Model_with_a_Hybrid_Vision_Tokenizer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-Lynx_Towards_High-Fidelity_Personalized_Video_Generation","title":"[논문리뷰] Lynx: Towards High-Fidelity Personalized Video Generation","excerpt":"Linjie Luo이 [arXiv]에 게시한 'Lynx: Towards High-Fidelity Personalized Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Personalized Video Generation","Diffusion Transformer","Identity Preservation","Video Synthesis","Adapter Networks","Facial Recognition","Cross-Attention"],"permalink":"/ai/review/2025-9-22-Lynx_Towards_High-Fidelity_Personalized_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-Latent_Zoning_Network_A_Unified_Principle_for_Generative_Modeling_Representation_Learning_and_Classification","title":"[논문리뷰] Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification","excerpt":"Wenyu Wang이 [arXiv]에 게시한 'Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Generative Modeling","Representation Learning","Classification","Unified Framework","Latent Space","Flow Matching","Deep Learning","Image Generation"],"permalink":"/ai/review/2025-9-22-Latent_Zoning_Network_A_Unified_Principle_for_Generative_Modeling_Representation_Learning_and_Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-Do_You_Hear_What_I_Mean_Quantifying_the_Instruction-Perception_Gap_in_Instruction-Guided_Expressive_Text-To-Speech_Systems","title":"[논문리뷰] Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems","excerpt":"Hung-yi Lee이 [arXiv]에 게시한 'Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Instruction-Guided TTS","Expressive Speech Synthesis","Human Perception","Subjective Evaluation","Controllability","Instruction Following","Evaluation Metrics"],"permalink":"/ai/review/2025-9-22-Do_You_Hear_What_I_Mean_Quantifying_the_Instruction-Perception_Gap_in_Instruction-Guided_Expressive_Text-To-Speech_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-BaseReward_A_Strong_Baseline_for_Multimodal_Reward_Model","title":"[논문리뷰] BaseReward: A Strong Baseline for Multimodal Reward Model","excerpt":"jianfeipan이 [arXiv]에 게시한 'BaseReward: A Strong Baseline for Multimodal Reward Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Multimodal Reward Model","MLLM Alignment","RLHF","Reward Head Architecture","Data Curation","Ensemble Methods","BaseReward"],"permalink":"/ai/review/2025-9-22-BaseReward_A_Strong_Baseline_for_Multimodal_Reward_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-BTL-UI_Blink-Think-Link_Reasoning_Model_for_GUI_Agent","title":"[논문리뷰] BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent","excerpt":"Jiahui Yang이 [arXiv]에 게시한 'BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","GUI Agent","Human-GUI Interaction","Cognitive Modeling","Reinforcement Learning","Multimodal Large Language Models","Attention Mechanisms","Action Planning"],"permalink":"/ai/review/2025-9-22-BTL-UI_Blink-Think-Link_Reasoning_Model_for_GUI_Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-Ask-to-Clarify_Resolving_Instruction_Ambiguity_through_Multi-turn_Dialogue","title":"[논문리뷰] Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue","excerpt":"Hui Zhang이 [arXiv]에 게시한 'Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Embodied AI","Human-Robot Interaction","Multi-turn Dialogue","Instruction Following","Vision-Language Models","Diffusion Models","Ambiguity Resolution","Low-level Actions"],"permalink":"/ai/review/2025-9-22-Ask-to-Clarify_Resolving_Instruction_Ambiguity_through_Multi-turn_Dialogue/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-22-A_Vision-Language-Action-Critic_Model_for_Robotic_Real-World_Reinforcement_Learning","title":"[논문리뷰] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning","excerpt":"Jiangmiao이 [arXiv]에 게시한 'A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","lastModifiedAt":"2025-09-22 13:11:29+0900","categories":["Review"],"tags":["Review","Robotics","Reinforcement Learning (RL)","Vision-Language-Action (VLA) Models","Reward Modeling","Human-in-the-Loop","Dense Rewards","Generalization","Autoregressive Models"],"permalink":"/ai/review/2025-9-22-A_Vision-Language-Action-Critic_Model_for_Robotic_Real-World_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-WorldForge_Unlocking_Emergent_3D4D_Generation_in_Video_Diffusion_Model_via_Training-Free_Guidance","title":"[논문리뷰] WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance","excerpt":"Ruibo Li이 [arXiv]에 게시한 'WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Video Diffusion Models","3D/4D Generation","Training-Free Guidance","Camera Trajectory Control","Novel View Synthesis","Geometric Consistency","Inference-Time Optimization"],"permalink":"/ai/review/2025-9-19-WorldForge_Unlocking_Emergent_3D4D_Generation_in_Video_Diffusion_Model_via_Training-Free_Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-Unleashing_the_Potential_of_Multimodal_LLMs_for_Zero-Shot_Spatio-Temporal_Video_Grounding","title":"[논문리뷰] Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding","excerpt":"Rynson W. H. Lau이 [arXiv]에 게시한 'Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Spatio-Temporal Video Grounding","Multimodal Large Language Models","Zero-Shot Learning","Visual Grounding","Decomposed Spatio-Temporal Highlighting","Logit-Guided Re-attention","Temporal-Augmented Assembling"],"permalink":"/ai/review/2025-9-19-Unleashing_the_Potential_of_Multimodal_LLMs_for_Zero-Shot_Spatio-Temporal_Video_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-Understand_Before_You_Generate_Self-Guided_Training_for_Autoregressive_Image_Generation","title":"[논문리뷰] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation","excerpt":"Xihui Liu이 [arXiv]에 게시한 'Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Image Generation","Self-Supervised Learning","Visual Understanding","Masked Image Modeling","Contrastive Learning","Next-Token Prediction","LlamaGen"],"permalink":"/ai/review/2025-9-19-Understand_Before_You_Generate_Self-Guided_Training_for_Autoregressive_Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-ScaleCUA_Scaling_Open-Source_Computer_Use_Agents_with_Cross-Platform_Data","title":"[논문리뷰] ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data","excerpt":"Zehao Li이 [arXiv]에 게시한 'ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Computer Use Agents","Vision-Language Models","Cross-Platform Data","GUI Automation","Data Scaling","Open-Source","Task Completion","GUI Grounding"],"permalink":"/ai/review/2025-9-19-ScaleCUA_Scaling_Open-Source_Computer_Use_Agents_with_Cross-Platform_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-RynnVLA-001_Using_Human_Demonstrations_to_Improve_Robot_Manipulation","title":"[논문리뷰] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation","excerpt":"SpaceProduct이 [arXiv]에 게시한 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA) Model","Robot Manipulation","Human Demonstrations","Video Generative Pretraining","Ego-Centric Video","Trajectory Prediction","ActionVAE","Transformer"],"permalink":"/ai/review/2025-9-19-RynnVLA-001_Using_Human_Demonstrations_to_Improve_Robot_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-RecoWorld_Building_Simulated_Environments_for_Agentic_Recommender_Systems","title":"[논문리뷰] RecoWorld: Building Simulated Environments for Agentic Recommender Systems","excerpt":"Mingyuan Wu이 [arXiv]에 게시한 'RecoWorld: Building Simulated Environments for Agentic Recommender Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Agentic Recommender Systems","Simulated Environments","LLM-driven Simulation","Multi-turn Interaction","Reinforcement Learning","User Retention","Instruction Following","Multi-agent Systems"],"permalink":"/ai/review/2025-9-19-RecoWorld_Building_Simulated_Environments_for_Agentic_Recommender_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-Reasoning_over_Boundaries_Enhancing_Specification_Alignment_via_Test-time_Delibration","title":"[논문리뷰] Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration","excerpt":"Zhilin Wang이 [arXiv]에 게시한 'Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","LLMs","Specification Alignment","Test-Time Deliberation","Safety-Behavior Trade-off","ALIGN3","SPECBENCH","Prompt Engineering"],"permalink":"/ai/review/2025-9-19-Reasoning_over_Boundaries_Enhancing_Specification_Alignment_via_Test-time_Delibration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-MultiEdit_Advancing_Instruction-based_Image_Editing_on_Diverse_and_Challenging_Tasks","title":"[논문리뷰] MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks","excerpt":"Xijun Gu이 [arXiv]에 게시한 'MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Instruction-based Image Editing","Dataset","Multi-modal LLM","Image Generation","Style Transfer","Multi-task Learning","Fine-tuning"],"permalink":"/ai/review/2025-9-19-MultiEdit_Advancing_Instruction-based_Image_Editing_on_Diverse_and_Challenging_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-Mind_the_Gap_A_Closer_Look_at_Tokenization_for_Multiple-Choice_Question_Answering_with_LLMs","title":"[논문리뷰] Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs","excerpt":"Katharina von der Wense이 [arXiv]에 게시한 'Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Multiple-Choice QA","Tokenization","Prompt Sensitivity","Accuracy","Calibration","Model Ranking"],"permalink":"/ai/review/2025-9-19-Mind_the_Gap_A_Closer_Look_at_Tokenization_for_Multiple-Choice_Question_Answering_with_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-FlowRL_Matching_Reward_Distributions_for_LLM_Reasoning","title":"[논문리뷰] FlowRL: Matching Reward Distributions for LLM Reasoning","excerpt":"Hengli Li이 [arXiv]에 게시한 'FlowRL: Matching Reward Distributions for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reward Distribution Matching","GFlowNets","Mode Collapse","Diverse Reasoning","Flow-Balanced Optimization"],"permalink":"/ai/review/2025-9-19-FlowRL_Matching_Reward_Distributions_for_LLM_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-FinSearchComp_Towards_a_Realistic_Expert-Level_Evaluation_of_Financial_Search_and_Reasoning","title":"[논문리뷰] FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning","excerpt":"Jiashuo Liu이 [arXiv]에 게시한 'FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Financial LLMs","Agent Benchmarking","Open-domain Search","Financial Reasoning","Time-Sensitive Data","Multi-hop QA","Tool Use"],"permalink":"/ai/review/2025-9-19-FinSearchComp_Towards_a_Realistic_Expert-Level_Evaluation_of_Financial_Search_and_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-FSG-Net_Frequency-Spatial_Synergistic_Gated_Network_for_High-Resolution_Remote_Sensing_Change_Detection","title":"[논문리뷰] FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection","excerpt":"Zhewei Zhang이 [arXiv]에 게시한 'FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Change Detection","Remote Sensing","Frequency-Spatial Analysis","Wavelet Transform","Attention Mechanism","Gated Fusion","Deep Learning"],"permalink":"/ai/review/2025-9-19-FSG-Net_Frequency-Spatial_Synergistic_Gated_Network_for_High-Resolution_Remote_Sensing_Change_Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-Evolving_Language_Models_without_Labels_Majority_Drives_Selection_Novelty_Promotes_Variation","title":"[논문리뷰] Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation","excerpt":"Kishan Panaganti이 [arXiv]에 게시한 'Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Label-free Reinforcement Learning","LLMs","Self-improvement","Entropy Collapse","Novelty Reward","Test-Time RL","GRPO","Evolutionary Computing Principles"],"permalink":"/ai/review/2025-9-19-Evolving_Language_Models_without_Labels_Majority_Drives_Selection_Novelty_Promotes_Variation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-EchoVLM_Dynamic_Mixture-of-Experts_Vision-Language_Model_for_Universal_Ultrasound_Intelligence","title":"[논문리뷰] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence","excerpt":"Qinghua Huang이 [arXiv]에 게시한 'EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Ultrasound Imaging","Medical Diagnosis","Mixture-of-Experts (MoE)","Instruction Tuning","Multimodal AI","Report Generation","VQA"],"permalink":"/ai/review/2025-9-19-EchoVLM_Dynamic_Mixture-of-Experts_Vision-Language_Model_for_Universal_Ultrasound_Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-19-AToken_A_Unified_Tokenizer_for_Vision","title":"[논문리뷰] AToken: A Unified Tokenizer for Vision","excerpt":"Mingze Xu이 [arXiv]에 게시한 'AToken: A Unified Tokenizer for Vision' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","lastModifiedAt":"2025-09-19 13:12:21+0900","categories":["Review"],"tags":["Review","Unified Visual Tokenizer","Multimodal AI","Transformer Architecture","4D Representation","Adversarial-free Training","Reconstruction","Semantic Understanding","Generative Models"],"permalink":"/ai/review/2025-9-19-AToken_A_Unified_Tokenizer_for_Vision/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-Wan-Animate_Unified_Character_Animation_and_Replacement_with_Holistic_Replication","title":"[논문리뷰] Wan-Animate: Unified Character Animation and Replacement with Holistic Replication","excerpt":"Mingyang Huang이 [arXiv]에 게시한 'Wan-Animate: Unified Character Animation and Replacement with Holistic Replication' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Character Animation","Video Replacement","Diffusion Models","Transformer","DiT","Relighting LoRA","Holistic Replication","Open-Source"],"permalink":"/ai/review/2025-9-18-Wan-Animate_Unified_Character_Animation_and_Replacement_with_Holistic_Replication/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-THOR_Tool-Integrated_Hierarchical_Optimization_via_RL_for_Mathematical_Reasoning","title":"[논문리뷰] THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning","excerpt":"Yicheng Pan이 [arXiv]에 게시한 'THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Mathematical Reasoning","Tool-Integrated Reasoning","Reinforcement Learning","Hierarchical Optimization","Self-Correction","Large Language Models","Code Generation"],"permalink":"/ai/review/2025-9-18-THOR_Tool-Integrated_Hierarchical_Optimization_via_RL_for_Mathematical_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-SteeringControl_Holistic_Evaluation_of_Alignment_Steering_in_LLMs","title":"[논문리뷰] SteeringControl: Holistic Evaluation of Alignment Steering in LLMs","excerpt":"Zhun Wang이 [arXiv]에 게시한 'SteeringControl: Holistic Evaluation of Alignment Steering in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","LLM Alignment","Representation Steering","Benchmark","Behavioral Entanglement","Bias Mitigation","Harmful Generation","Hallucination Control","Modular Framework"],"permalink":"/ai/review/2025-9-18-SteeringControl_Holistic_Evaluation_of_Alignment_Steering_in_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-Scrub_It_Out_Erasing_Sensitive_Memorization_in_Code_Language_Models_via_Machine_Unlearning","title":"[논문리뷰] Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning","excerpt":"Zhou Yang이 [arXiv]에 게시한 'Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Code Language Models","Machine Unlearning","Sensitive Memorization","Privacy","Gradient Ascent","Model Utility","Code Generation"],"permalink":"/ai/review/2025-9-18-Scrub_It_Out_Erasing_Sensitive_Memorization_in_Code_Language_Models_via_Machine_Unlearning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-SAIL-VL2_Technical_Report","title":"[논문리뷰] SAIL-VL2 Technical Report","excerpt":"Zijian Kang이 [arXiv]에 게시한 'SAIL-VL2 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Vision-Language Model","Multimodal Understanding","Mixture-of-Experts","Progressive Training","Data Curation","Supervised Fine-tuning","Reinforcement Learning","SAIL-ViT"],"permalink":"/ai/review/2025-9-18-SAIL-VL2_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-PANORAMA_The_Rise_of_Omnidirectional_Vision_in_the_Embodied_AI_Era","title":"[논문리뷰] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era","excerpt":"Zihao Dongfang이 [arXiv]에 게시한 'PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Omnidirectional Vision","Embodied AI","Panoramic Perception","Multi-modal Learning","Dataset Development","Robot Navigation","Spatial Reasoning","System Architecture"],"permalink":"/ai/review/2025-9-18-PANORAMA_The_Rise_of_Omnidirectional_Vision_in_the_Embodied_AI_Era/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-MARS2_2025_Challenge_on_Multimodal_Reasoning_Datasets_Methods_Results_Discussion_and_Outlook","title":"[논문리뷰] MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook","excerpt":"Bowen Zhou이 [arXiv]에 게시한 'MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Large Language Models (LLMs)","Multimodal Large Language Models (MLLMs)","Visual Grounding","Visual Question Answering","Advertisement Video Analysis","Real-world Scenarios","Challenge Benchmark"],"permalink":"/ai/review/2025-9-18-MARS2_2025_Challenge_on_Multimodal_Reasoning_Datasets_Methods_Results_Discussion_and_Outlook/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-Improving_Context_Fidelity_via_Native_Retrieval-Augmented_Reasoning","title":"[논문리뷰] Improving Context Fidelity via Native Retrieval-Augmented Reasoning","excerpt":"Xiangru Tang이 [arXiv]에 게시한 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Context Fidelity","Retrieval-Augmented Generation (RAG)","Large Language Models (LLMs)","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Hallucination","Question Answering","In-context Retrieval","Curriculum Learning"],"permalink":"/ai/review/2025-9-18-Improving_Context_Fidelity_via_Native_Retrieval-Augmented_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-Hala_Technical_Report_Building_Arabic-Centric_Instruction_Translation_Models_at_Scale","title":"[논문리뷰] Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale","excerpt":"Bernard Ghanem이 [arXiv]에 게시한 'Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Arabic NLP","Instruction Tuning","Machine Translation","Large Language Models","FP8 Quantization","Data Bootstrapping","Model Merging","Language-Centric AI"],"permalink":"/ai/review/2025-9-18-Hala_Technical_Report_Building_Arabic-Centric_Instruction_Translation_Models_at_Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-18-GenExam_A_Multidisciplinary_Text-to-Image_Exam","title":"[논문리뷰] GenExam: A Multidisciplinary Text-to-Image Exam","excerpt":"Yu Qiao이 [arXiv]에 게시한 'GenExam: A Multidisciplinary Text-to-Image Exam' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","lastModifiedAt":"2025-09-18 13:07:00+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Multidisciplinary","Benchmark","Evaluation","AGI","Reasoning","Scoring System","Visual Question Answering"],"permalink":"/ai/review/2025-9-18-GenExam_A_Multidisciplinary_Text-to-Image_Exam/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-WebWeaver_Structuring_Web-Scale_Evidence_with_Dynamic_Outlines_for_Open-Ended_Deep_Research","title":"[논문리뷰] WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research","excerpt":"Houquan Zhou이 [arXiv]에 게시한 'WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Open-Ended Deep Research","LLM Agents","Dynamic Outline","Evidence Acquisition","Hierarchical Writing","Memory Bank","State-of-the-Art","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-9-17-WebWeaver_Structuring_Web-Scale_Evidence_with_Dynamic_Outlines_for_Open-Ended_Deep_Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-WebSailor-V2_Bridging_the_Chasm_to_Proprietary_Agents_via_Synthetic_Data_and_Scalable_Reinforcement_Learning","title":"[논문리뷰] WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning","excerpt":"Huifeng Yin이 [arXiv]에 게시한 'WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Web Agents","Reinforcement Learning","Synthetic Data","Knowledge Graphs","LLMs","Supervised Fine-Tuning","Sim-to-Real Transfer","Agentic AI"],"permalink":"/ai/review/2025-9-17-WebSailor-V2_Bridging_the_Chasm_to_Proprietary_Agents_via_Synthetic_Data_and_Scalable_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-WebResearcher_Unleashing_unbounded_reasoning_capability_in_Long-Horizon_Agents","title":"[논문리뷰] WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents","excerpt":"Wenbiao Yin이 [arXiv]에 게시한 'WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Agentic AI","Deep Research","Iterative Reasoning","Long-Horizon Tasks","Context Management","Data Synthesis","Tool-Augmented LLMs","Markov Decision Process"],"permalink":"/ai/review/2025-9-17-WebResearcher_Unleashing_unbounded_reasoning_capability_in_Long-Horizon_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Towards_General_Agentic_Intelligence_via_Environment_Scaling","title":"[논문리뷰] Towards General Agentic Intelligence via Environment Scaling","excerpt":"Guangyu Li이 [arXiv]에 게시한 'Towards General Agentic Intelligence via Environment Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Agentic AI","Environment Scaling","Function Calling","Tool Use","Large Language Models","Synthetic Data Generation","Supervised Fine-tuning"],"permalink":"/ai/review/2025-9-17-Towards_General_Agentic_Intelligence_via_Environment_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Single-stream_Policy_Optimization","title":"[논문리뷰] Single-stream Policy Optimization","excerpt":"Zihan Ding이 [arXiv]에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Optimization","Policy Gradient","Variance Reduction","Adaptive Sampling","Scalability","Agentic Systems","RLVR"],"permalink":"/ai/review/2025-9-17-Single-stream_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Scaling_Agents_via_Continual_Pre-training","title":"[논문리뷰] Scaling Agents via Continual Pre-training","excerpt":"Guangyu Li이 [arXiv]에 게시한 'Scaling Agents via Continual Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Agentic LLMs","Continual Pre-training","Deep Research Agents","Tool Use","Multi-step Reasoning","Data Synthesis","Scaling Laws"],"permalink":"/ai/review/2025-9-17-Scaling_Agents_via_Continual_Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-ReSum_Unlocking_Long-Horizon_Search_Intelligence_via_Context_Summarization","title":"[논문리뷰] ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization","excerpt":"Litu Ou이 [arXiv]에 게시한 'ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","LLM Agents","Context Management","Summarization","ReAct","Reinforcement Learning","Web Search","Long-Horizon Reasoning"],"permalink":"/ai/review/2025-9-17-ReSum_Unlocking_Long-Horizon_Search_Intelligence_via_Context_Summarization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Optimal_Brain_Restoration_for_Joint_Quantization_and_Sparsification_of_LLMs","title":"[논문리뷰] Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs","excerpt":"Luca Benini이 [arXiv]에 게시한 'Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","LLM Compression","Quantization","Sparsification","Post-training Quantization","Hessian-based Optimization","Error Compensation","Low-bit LLMs"],"permalink":"/ai/review/2025-9-17-Optimal_Brain_Restoration_for_Joint_Quantization_and_Sparsification_of_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Multiple_Instance_Learning_Framework_with_Masked_Hard_Instance_Mining_for_Gigapixel_Histopathology_Image_Analysis","title":"[논문리뷰] Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis","excerpt":"Bo Liu이 [arXiv]에 게시한 'Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Multiple Instance Learning","Hard Instance Mining","Computational Pathology","Whole Slide Images","Masked Learning","Siamese Network","Medical Image Analysis"],"permalink":"/ai/review/2025-9-17-Multiple_Instance_Learning_Framework_with_Masked_Hard_Instance_Mining_for_Gigapixel_Histopathology_Image_Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Multimodal_Reasoning_for_Science_Technical_Report_and_1st_Place_Solution_to_the_ICML_2025_SeePhys_Challenge","title":"[논문리뷰] Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge","excerpt":"Wentao Zhang이 [arXiv]에 게시한 'Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Multimodal Reasoning","Science AI","Caption-assisted Reasoning","SeePhys Challenge","Large Language Models","Visual Question Answering","Physics Problems","Cross-modal Alignment"],"permalink":"/ai/review/2025-9-17-Multimodal_Reasoning_for_Science_Technical_Report_and_1st_Place_Solution_to_the_ICML_2025_SeePhys_Challenge/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Hunyuan3D_Studio_End-to-End_AI_Pipeline_for_Game-Ready_3D_Asset_Generation","title":"[논문리뷰] Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation","excerpt":"Lixin Xu이 [arXiv]에 게시한 'Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","3D Asset Generation","AI Pipeline","Generative AI","Game Development","Diffusion Models","Neural Modules","Retopology","UV Unwrapping"],"permalink":"/ai/review/2025-9-17-Hunyuan3D_Studio_End-to-End_AI_Pipeline_for_Game-Ready_3D_Asset_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-Exact_Coset_Sampling_for_Quantum_Lattice_Algorithms","title":"[논문리뷰] Exact Coset Sampling for Quantum Lattice Algorithms","excerpt":"Yifan Zhang이 [arXiv]에 게시한 'Exact Coset Sampling for Quantum Lattice Algorithms' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Quantum Algorithms","Lattice Problems","Coset Sampling","Quantum Fourier Transform (QFT)","Modular Arithmetic","Quantum Cryptography","Exact Sampling"],"permalink":"/ai/review/2025-9-17-Exact_Coset_Sampling_for_Quantum_Lattice_Algorithms/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-EconProver_Towards_More_Economical_Test-Time_Scaling_for_Automated_Theorem_Proving","title":"[논문리뷰] EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving","excerpt":"Shansan Gong이 [arXiv]에 게시한 'EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","LLM","Test-Time Scaling","Chain-of-Thought","Reinforcement Learning","Efficiency Optimization","Token Cost","Sampling Cost","Dynamic CoT Switching"],"permalink":"/ai/review/2025-9-17-EconProver_Towards_More_Economical_Test-Time_Scaling_for_Automated_Theorem_Proving/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-17-3D_Aware_Region_Prompted_Vision_Language_Model","title":"[논문리뷰] 3D Aware Region Prompted Vision Language Model","excerpt":"Xiaolong Li이 [arXiv]에 게시한 '3D Aware Region Prompted Vision Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","lastModifiedAt":"2025-09-17 13:16:01+0900","categories":["Review"],"tags":["Review","3D Vision","Vision-Language Models","Spatial Reasoning","Region Prompting","Multi-view Learning","Depth Estimation","Unified Representation","Generative AI"],"permalink":"/ai/review/2025-9-17-3D_Aware_Region_Prompted_Vision_Language_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-UI-S1_Advancing_GUI_Automation_via_Semi-online_Reinforcement_Learning","title":"[논문리뷰] UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning","excerpt":"Yongliang Shen이 [arXiv]에 게시한 'UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","GUI Automation","Reinforcement Learning","Semi-online RL","Offline RL","Online RL","Patch Module","Multi-turn Interaction","Large Language Models"],"permalink":"/ai/review/2025-9-16-UI-S1_Advancing_GUI_Automation_via_Semi-online_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-SearchInstruct_Enhancing_Domain_Adaptation_via_Retrieval-Based_Instruction_Dataset_Creation","title":"[논문리뷰] SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation","excerpt":"Heshaam Faili이 [arXiv]에 게시한 'SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","LLM","Instruction Tuning","Domain Adaptation","Retrieval-Augmented Generation","Dataset Creation","Model Editing","Supervised Fine-Tuning"],"permalink":"/ai/review/2025-9-16-SearchInstruct_Enhancing_Domain_Adaptation_via_Retrieval-Based_Instruction_Dataset_Creation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-PersonaX_Multimodal_Datasets_with_LLM-Inferred_Behavior_Traits","title":"[논문리뷰] PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits","excerpt":"Zhenhao Chen이 [arXiv]에 게시한 'PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Multimodal Dataset","LLM Inference","Behavioral Traits","Causal Representation Learning","Big Five","Multimodal AI","Causal Discovery","Human-Computer Interaction"],"permalink":"/ai/review/2025-9-16-PersonaX_Multimodal_Datasets_with_LLM-Inferred_Behavior_Traits/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-OmniWorld_A_Multi-Domain_and_Multi-Modal_Dataset_for_4D_World_Modeling","title":"[논문리뷰] OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling","excerpt":"Yang Zhou이 [arXiv]에 게시한 'OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","4D World Modeling","Multi-Modal Dataset","Multi-Domain Data","Geometric Foundation Models","Video Generation","Spatio-Temporal Data","Dataset Benchmark"],"permalink":"/ai/review/2025-9-16-OmniWorld_A_Multi-Domain_and_Multi-Modal_Dataset_for_4D_World_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-Measuring_Epistemic_Humility_in_Multimodal_Large_Language_Models","title":"[논문리뷰] Measuring Epistemic Humility in Multimodal Large Language Models","excerpt":"Kaiyang Zhou이 [arXiv]에 게시한 'Measuring Epistemic Humility in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Hallucination","Epistemic Humility","Benchmark","False-Option Rejection","Visual Question Answering","Scene Graph"],"permalink":"/ai/review/2025-9-16-Measuring_Epistemic_Humility_in_Multimodal_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-Lost_in_Embeddings_Information_Loss_in_Vision-Language_Models","title":"[논문리뷰] Lost in Embeddings: Information Loss in Vision-Language Models","excerpt":"Ivan Vulić이 [arXiv]에 게시한 'Lost in Embeddings: Information Loss in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Information Loss","Embeddings","Connectors","k-NN Overlap Ratio","Embedding Reconstruction","Multimodal AI"],"permalink":"/ai/review/2025-9-16-Lost_in_Embeddings_Information_Loss_in_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-Look_Again_Think_Slowly_Enhancing_Visual_Reflection_in_Vision-Language_Models","title":"[논문리뷰] Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models","excerpt":"Shuo Ren이 [arXiv]에 게시한 'Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Visual Reasoning","Reflection","Reinforcement Learning","Visual Attention","Slow Thinking","Multimodal Agents"],"permalink":"/ai/review/2025-9-16-Look_Again_Think_Slowly_Enhancing_Visual_Reflection_in_Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-Locality_in_Image_Diffusion_Models_Emerges_from_Data_Statistics","title":"[논문리뷰] Locality in Image Diffusion Models Emerges from Data Statistics","excerpt":"Vincent Sitzmann이 [arXiv]에 게시한 'Locality in Image Diffusion Models Emerges from Data Statistics' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Diffusion Models","Locality","Data Statistics","Optimal Denoiser","Wiener Filter","Sensitivity Fields","Generative Models","Inductive Bias"],"permalink":"/ai/review/2025-9-16-Locality_in_Image_Diffusion_Models_Emerges_from_Data_Statistics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-Learning_to_Optimize_Multi-Objective_Alignment_Through_Dynamic_Reward_Weighting","title":"[논문리뷰] Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting","excerpt":"Changlong Yu이 [arXiv]에 게시한 'Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Multi-objective Reinforcement Learning","LLM Alignment","Dynamic Reward Weighting","Pareto Front Optimization","Hypervolume Indicator","Gradient-based Optimization","Online RL"],"permalink":"/ai/review/2025-9-16-Learning_to_Optimize_Multi-Objective_Alignment_Through_Dynamic_Reward_Weighting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-LazyDrag_Enabling_Stable_Drag-Based_Editing_on_Multi-Modal_Diffusion_Transformers_via_Explicit_Correspondence","title":"[논문리뷰] LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence","excerpt":"Lionel M. Ni이 [arXiv]에 게시한 'LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Image Editing","Diffusion Models","Multi-Modal Transformers","Drag-based Editing","Explicit Correspondence","Attention Control","Identity Preservation","Training-Free"],"permalink":"/ai/review/2025-9-16-LazyDrag_Enabling_Stable_Drag-Based_Editing_on_Multi-Modal_Diffusion_Transformers_via_Explicit_Correspondence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-InternScenes_A_Large-scale_Simulatable_Indoor_Scene_Dataset_with_Realistic_Layouts","title":"[논문리뷰] InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts","excerpt":"Wenzhe Cai이 [arXiv]에 게시한 'InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Embodied AI","3D Scene Dataset","Simulation Environment","Scene Generation","Point-Goal Navigation","Realistic Layouts","Object Interaction","Real-to-Sim"],"permalink":"/ai/review/2025-9-16-InternScenes_A_Large-scale_Simulatable_Indoor_Scene_Dataset_with_Realistic_Layouts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-GAPrune_Gradient-Alignment_Pruning_for_Domain-Aware_Embeddings","title":"[논문리뷰] GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings","excerpt":"Yixuan Tang이 [arXiv]에 게시한 'GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Model Pruning","Domain Adaptation","Embedding Models","Gradient Alignment","Fisher Information","Model Compression","LLMs"],"permalink":"/ai/review/2025-9-16-GAPrune_Gradient-Alignment_Pruning_for_Domain-Aware_Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-EthicsMH_A_Pilot_Benchmark_for_Ethical_Reasoning_in_Mental_Health_AI","title":"[논문리뷰] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI","excerpt":"UVSKKR이 [arXiv]에 게시한 'EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Ethical Reasoning","Mental Health AI","Benchmark Dataset","Large Language Models","AI Ethics","Clinical Decision Support","Human-in-the-loop"],"permalink":"/ai/review/2025-9-16-EthicsMH_A_Pilot_Benchmark_for_Ethical_Reasoning_in_Mental_Health_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-Dr.V_A_Hierarchical_Perception-Temporal-Cognition_Framework_to_Diagnose_Video_Hallucination_by_Fine-grained_Spatial-Temporal_Grounding","title":"[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding","excerpt":"Li Zheng이 [arXiv]에 게시한 'Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Video Hallucination","Large Video Models (LVMs)","Hierarchical Reasoning","Spatial-Temporal Grounding","Diagnostic Framework","Benchmark Dataset","Multimodal AI"],"permalink":"/ai/review/2025-9-16-Dr.V_A_Hierarchical_Perception-Temporal-Cognition_Framework_to_Diagnose_Video_Hallucination_by_Fine-grained_Spatial-Temporal_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-16-CognitiveSky_Scalable_Sentiment_and_Narrative_Analysis_for_Decentralized_Social_Media","title":"[논문리뷰] CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media","excerpt":"Subasish Das이 [arXiv]에 게시한 'CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","lastModifiedAt":"2025-09-16 13:16:41+0900","categories":["Review"],"tags":["Review","Sentiment Analysis","Narrative Analysis","Decentralized Social Media","Bluesky","Transformer Models","Topic Modeling","Real-time Processing","Data Visualization"],"permalink":"/ai/review/2025-9-16-CognitiveSky_Scalable_Sentiment_and_Narrative_Analysis_for_Decentralized_Social_Media/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-X-Part_high_fidelity_and_structure_coherent_shape_decomposition","title":"[논문리뷰] X-Part: high fidelity and structure coherent shape decomposition","excerpt":"Yunhan Yang이 [arXiv]에 게시한 'X-Part: high fidelity and structure coherent shape decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","3D Shape Decomposition","Diffusion Models","Part-level Generation","Controllable Generation","Bounding Box Prompts","Semantic Features","Interactive Editing","Generative AI"],"permalink":"/ai/review/2025-9-15-X-Part_high_fidelity_and_structure_coherent_shape_decomposition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-Virtual_Agent_Economies","title":"[논문리뷰] Virtual Agent Economies","excerpt":"William A. Cunningham이 [arXiv]에 게시한 'Virtual Agent Economies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","AI Agents","Virtual Economy","Multi-Agent Systems","Economic Mechanisms","Governance","Blockchain","Resource Allocation","Agent Alignment"],"permalink":"/ai/review/2025-9-15-Virtual_Agent_Economies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-VStyle_A_Benchmark_for_Voice_Style_Adaptation_with_Spoken_Instructions","title":"[논문리뷰] VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions","excerpt":"Dong Zhang이 [arXiv]에 게시한 'VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Voice Style Adaptation","Spoken Language Models","Benchmark","LALM-as-a-Judge","Speech Generation","Multilingual","Evaluation Framework"],"permalink":"/ai/review/2025-9-15-VStyle_A_Benchmark_for_Voice_Style_Adaptation_with_Spoken_Instructions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-The_Illusion_of_Diminishing_Returns_Measuring_Long_Horizon_Execution_in_LLMs","title":"[논문리뷰] The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs","excerpt":"Jonas Geiping이 [arXiv]에 게시한 'The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Large Language Models","Long-Horizon Tasks","Execution Capability","Scaling Laws","Self-Conditioning","Thinking Models","Agentic AI"],"permalink":"/ai/review/2025-9-15-The_Illusion_of_Diminishing_Returns_Measuring_Long_Horizon_Execution_in_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-QuantAgent_Price-Driven_Multi-Agent_LLMs_for_High-Frequency_Trading","title":"[논문리뷰] QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading","excerpt":"Chenyu You이 [arXiv]에 게시한 'QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","High-Frequency Trading","Multi-Agent Systems","Large Language Models","Technical Analysis","Algorithmic Trading","Financial Reasoning","Price-Driven Signals"],"permalink":"/ai/review/2025-9-15-QuantAgent_Price-Driven_Multi-Agent_LLMs_for_High-Frequency_Trading/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-MCP-AgentBench_Evaluating_Real-World_Language_Agent_Performance_with_MCP-Mediated_Tools","title":"[논문리뷰] MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools","excerpt":"Xiaorui Wang이 [arXiv]에 게시한 'MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Language Agents","Tool Use","Benchmarks","Model Context Protocol (MCP)","LLM Evaluation","Agentic AI","Real-World Performance"],"permalink":"/ai/review/2025-9-15-MCP-AgentBench_Evaluating_Real-World_Language_Agent_Performance_with_MCP-Mediated_Tools/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-LoFT_Parameter-Efficient_Fine-Tuning_for_Long-tailed_Semi-Supervised_Learning_in_Open-World_Scenarios","title":"[논문리뷰] LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios","excerpt":"Bing Su이 [arXiv]에 게시한 'LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Long-tailed Learning","Semi-Supervised Learning","Parameter-Efficient Fine-Tuning","Foundation Models","Open-World Scenarios","OOD Detection","Confidence Calibration"],"permalink":"/ai/review/2025-9-15-LoFT_Parameter-Efficient_Fine-Tuning_for_Long-tailed_Semi-Supervised_Learning_in_Open-World_Scenarios/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-IntrEx_A_Dataset_for_Modeling_Engagement_in_Educational_Conversations","title":"[논문리뷰] IntrEx: A Dataset for Modeling Engagement in Educational Conversations","excerpt":"Gabriele Pergola이 [arXiv]에 게시한 'IntrEx: A Dataset for Modeling Engagement in Educational Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Educational Dialogue","Engagement Modeling","Dataset Annotation","Second Language Learning","Human Feedback","LLM Alignment","Readability Metrics"],"permalink":"/ai/review/2025-9-15-IntrEx_A_Dataset_for_Modeling_Engagement_in_Educational_Conversations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-Inpainting-Guided_Policy_Optimization_for_Diffusion_Large_Language_Models","title":"[논문리뷰] Inpainting-Guided Policy Optimization for Diffusion Large Language Models","excerpt":"Chenyu Wang이 [arXiv]에 게시한 'Inpainting-Guided Policy Optimization for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Reinforcement Learning","Inpainting","Policy Optimization","Exploration","Mathematical Reasoning","GRPO"],"permalink":"/ai/review/2025-9-15-Inpainting-Guided_Policy_Optimization_for_Diffusion_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-InfGen_A_Resolution-Agnostic_Paradigm_for_Scalable_Image_Synthesis","title":"[논문리뷰] InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis","excerpt":"Song Guo이 [arXiv]에 게시한 'InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Image Synthesis","Resolution-Agnostic","Diffusion Models","Latent Space","VAE Decoder","High-Resolution Image Generation","Generative AI","Transformer Architecture"],"permalink":"/ai/review/2025-9-15-InfGen_A_Resolution-Agnostic_Paradigm_for_Scalable_Image_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-HANRAG_Heuristic_Accurate_Noise-resistant_Retrieval-Augmented_Generation_for_Multi-hop_Question_Answering","title":"[논문리뷰] HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering","excerpt":"Zhehao Tan이 [arXiv]에 게시한 'HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation","Multi-hop QA","Noise Resistance","LLM","Query Decomposition","Adaptive Retrieval","Heuristic Framework","Revelator"],"permalink":"/ai/review/2025-9-15-HANRAG_Heuristic_Accurate_Noise-resistant_Retrieval-Augmented_Generation_for_Multi-hop_Question_Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-FLOWER_Democratizing_Generalist_Robot_Policies_with_Efficient_Vision-Language-Action_Flow_Policies","title":"[논문리뷰] FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies","excerpt":"Fabian Otto이 [arXiv]에 게시한 'FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Generalist Robot Policies","Vision-Language-Action Models","Efficient AI","Imitation Learning","Diffusion Models","Intermediate Fusion","Robotics"],"permalink":"/ai/review/2025-9-15-FLOWER_Democratizing_Generalist_Robot_Policies_with_Efficient_Vision-Language-Action_Flow_Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-15-CMHG_A_Dataset_and_Benchmark_for_Headline_Generation_of_Minority_Languages_in_China","title":"[논문리뷰] CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China","excerpt":"XU Han이 [arXiv]에 게시한 'CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","lastModifiedAt":"2025-09-15 13:12:08+0900","categories":["Review"],"tags":["Review","Headline Generation","Minority Languages","Low-Resource NLP","Dataset","Benchmark","Natural Language Generation","Chinese Minority Languages"],"permalink":"/ai/review/2025-9-15-CMHG_A_Dataset_and_Benchmark_for_Headline_Generation_of_Minority_Languages_in_China/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-Visual_Programmability_A_Guide_for_Code-as-Thought_in_Chart_Understanding","title":"[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding","excerpt":"Ethan Chern이 [arXiv]에 게시한 'Visual Programmability: A Guide for Code-as-Thought in Chart Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Visual Programmability","Code-as-Thought (CaT)","Chart Understanding","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Adaptive Reasoning","Dual-Reward System","Multimodal AI"],"permalink":"/ai/review/2025-9-12-Visual_Programmability_A_Guide_for_Code-as-Thought_in_Chart_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-VLA-Adapter_An_Effective_Paradigm_for_Tiny-Scale_Vision-Language-Action_Model","title":"[논문리뷰] VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model","excerpt":"Zirui Ge이 [arXiv]에 게시한 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","Multimodal Learning","Efficient AI","Model Adaptation","Bridge Attention","Low-resource Training"],"permalink":"/ai/review/2025-9-12-VLA-Adapter_An_Effective_Paradigm_for_Tiny-Scale_Vision-Language-Action_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-The_Choice_of_Divergence_A_Neglected_Key_to_Mitigating_Diversity_Collapse_in_Reinforcement_Learning_with_Verifiable_Reward","title":"[논문리뷰] The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward","excerpt":"Xiaoyu Tan이 [arXiv]에 게시한 'The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models (LLMs)","Diversity Collapse","f-divergence","Forward-KL","JS-divergence","Pass@k","Catastrophic Forgetting"],"permalink":"/ai/review/2025-9-12-The_Choice_of_Divergence_A_Neglected_Key_to_Mitigating_Diversity_Collapse_in_Reinforcement_Learning_with_Verifiable_Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-SpatialVID_A_Large-Scale_Video_Dataset_with_Spatial_Annotations","title":"[논문리뷰] SpatialVID: A Large-Scale Video Dataset with Spatial Annotations","excerpt":"Jian Gao이 [arXiv]에 게시한 'SpatialVID: A Large-Scale Video Dataset with Spatial Annotations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Video Dataset","Spatial Annotation","Camera Pose Estimation","Depth Map","Structured Caption","Motion Instruction","3D Vision","World Modeling"],"permalink":"/ai/review/2025-9-12-SpatialVID_A_Large-Scale_Video_Dataset_with_Spatial_Annotations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-SimpleVLA-RL_Scaling_VLA_Training_via_Reinforcement_Learning","title":"[논문리뷰] SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning","excerpt":"Zhaohui Yang이 [arXiv]에 게시한 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Vision-Language-Action (VLA) Models","Robotic Manipulation","Data Scarcity","Generalization","Sim-to-Real Transfer","Online RL","Long-Horizon Planning"],"permalink":"/ai/review/2025-9-12-SimpleVLA-RL_Scaling_VLA_Training_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-Reasoning_Introduces_New_Poisoning_Attacks_Yet_Makes_Them_More_Complicated","title":"[논문리뷰] Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated","excerpt":"Jamie Hayes이 [arXiv]에 게시한 'Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","LLM Security","Data Poisoning","Chain-of-Thought","Reasoning Models","Backdoor Attacks","CoT Unfaithfulness","Emergent Robustness"],"permalink":"/ai/review/2025-9-12-Reasoning_Introduces_New_Poisoning_Attacks_Yet_Makes_Them_More_Complicated/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-OmniEVA_Embodied_Versatile_Planner_via_Task-Adaptive_3D-Grounded_and_Embodiment-aware_Reasoning","title":"[논문리뷰] OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning","excerpt":"Yuzheng Zhuang이 [arXiv]에 게시한 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Embodied AI","Multimodal LLMs","3D Grounding","Task-Adaptive Reasoning","Embodiment-Aware Planning","Robotics","Spatial Reasoning"],"permalink":"/ai/review/2025-9-12-OmniEVA_Embodied_Versatile_Planner_via_Task-Adaptive_3D-Grounded_and_Embodiment-aware_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-Modality_Alignment_with_Multi-scale_Bilateral_Attention_for_Multimodal_Recommendation","title":"[논문리뷰] Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation","excerpt":"Dong-Ho Lee이 [arXiv]에 게시한 'Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Multimodal Recommendation","Modality Alignment","Attention Mechanism","Dilated Convolution","Maximum Mean Discrepancy","Contrastive Learning","Dimensionality Reduction"],"permalink":"/ai/review/2025-9-12-Modality_Alignment_with_Multi-scale_Bilateral_Attention_for_Multimodal_Recommendation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-LoCoBench_A_Benchmark_for_Long-Context_Large_Language_Models_in_Complex_Software_Engineering","title":"[논문리뷰] LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering","excerpt":"Jianguo Zhang이 [arXiv]에 게시한 'LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Long-Context LLMs","Software Engineering","Code Evaluation","Benchmark","Multi-file Reasoning","Architectural Understanding","Context Length","Software Development Lifecycle","Metrics"],"permalink":"/ai/review/2025-9-12-LoCoBench_A_Benchmark_for_Long-Context_Large_Language_Models_in_Complex_Software_Engineering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-Kling-Avatar_Grounding_Multimodal_Instructions_for_Cascaded_Long-Duration_Avatar_Animation_Synthesis","title":"[논문리뷰] Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis","excerpt":"Wentao Hu이 [arXiv]에 게시한 'Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Avatar Animation","Multimodal Instructions","Long-Duration Video Generation","MLLM Director","Cascaded Framework","Lip Synchronization","Instruction Grounding","Video Diffusion Transformers"],"permalink":"/ai/review/2025-9-12-Kling-Avatar_Grounding_Multimodal_Instructions_for_Cascaded_Long-Duration_Avatar_Animation_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-HuMo_Human-Centric_Video_Generation_via_Collaborative_Multi-Modal_Conditioning","title":"[논문리뷰] HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning","excerpt":"Zhuowei Chen이 [arXiv]에 게시한 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Human-Centric Video Generation","Multimodal Conditioning","Text-to-Video","Image-to-Video","Audio-to-Video","Diffusion Models","Subject Preservation","Audio-Visual Synchronization","Progressive Training"],"permalink":"/ai/review/2025-9-12-HuMo_Human-Centric_Video_Generation_via_Collaborative_Multi-Modal_Conditioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-Harnessing_Uncertainty_Entropy-Modulated_Policy_Gradients_for_Long-Horizon_LLM_Agents","title":"[논문리뷰] Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents","excerpt":"Xintao Wang이 [arXiv]에 게시한 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Policy Gradients","Entropy Modulation","Credit Assignment","Uncertainty","Long-Horizon Tasks","Self-Calibrating Gradient Scaling"],"permalink":"/ai/review/2025-9-12-Harnessing_Uncertainty_Entropy-Modulated_Policy_Gradients_for_Long-Horizon_LLM_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-Gradient-Attention_Guided_Dual-Masking_Synergetic_Framework_for_Robust_Text-based_Person_Retrieval","title":"[논문리뷰] Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval","excerpt":"Kaicheng Yang이 [arXiv]에 게시한 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Text-based Person Retrieval","CLIP","MLLM","Data Curation","Dual-Masking","Gradient-Attention","WebPerson Dataset"],"permalink":"/ai/review/2025-9-12-Gradient-Attention_Guided_Dual-Masking_Synergetic_Framework_for_Robust_Text-based_Person_Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-FLUX-Reason-6M_PRISM-Bench_A_Million-Scale_Text-to-Image_Reasoning_Dataset_and_Comprehensive_Benchmark","title":"[논문리뷰] FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark","excerpt":"Shuai Bai이 [arXiv]에 게시한 'FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reasoning Dataset","Benchmark","Generation Chain-of-Thought","Vision-Language Model","Image Aesthetics","Prompt Alignment"],"permalink":"/ai/review/2025-9-12-FLUX-Reason-6M_PRISM-Bench_A_Million-Scale_Text-to-Image_Reasoning_Dataset_and_Comprehensive_Benchmark/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-EchoX_Towards_Mitigating_Acoustic-Semantic_Gap_via_Echo_Training_for_Speech-to-Speech_LLMs","title":"[논문리뷰] EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs","excerpt":"Kaiqi Kou이 [arXiv]에 게시한 'EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Speech-to-Speech LLMs","Acoustic-Semantic Gap","Echo Training","Unit Language","Streaming Inference","Knowledge-based QA"],"permalink":"/ai/review/2025-9-12-EchoX_Towards_Mitigating_Acoustic-Semantic_Gap_via_Echo_Training_for_Speech-to-Speech_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-Can_Understanding_and_Generation_Truly_Benefit_Together_--_or_Just_Coexist","title":"[논문리뷰] Can Understanding and Generation Truly Benefit Together -- or Just Coexist?","excerpt":"Hui Han이 [arXiv]에 게시한 'Can Understanding and Generation Truly Benefit Together -- or Just Coexist?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Multimodal Understanding","Multimodal Generation","Unified Models","Auto-Encoder","Reinforcement Learning","Image-to-Text","Text-to-Image","Reconstruction Fidelity"],"permalink":"/ai/review/2025-9-12-Can_Understanding_and_Generation_Truly_Benefit_Together_--_or_Just_Coexist/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-12-2D_Gaussian_Splatting_with_Semantic_Alignment_for_Image_Inpainting","title":"[논문리뷰] 2D Gaussian Splatting with Semantic Alignment for Image Inpainting","excerpt":"Guangming Lu이 [arXiv]에 게시한 '2D Gaussian Splatting with Semantic Alignment for Image Inpainting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","lastModifiedAt":"2025-09-12 13:12:46+0900","categories":["Review"],"tags":["Review","Image Inpainting","2D Gaussian Splatting","Semantic Alignment","DINO Features","Patch-level Rasterization","Continuous Representation","Generative Models"],"permalink":"/ai/review/2025-9-12-2D_Gaussian_Splatting_with_Semantic_Alignment_for_Image_Inpainting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-think_So_lets_replace_this_phrase_with_insult..._think_Lessons_learned_from_generation_of_toxic_texts_with_LLMs","title":"[논문리뷰] <think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs","excerpt":"Alexander Panchenko이 [arXiv]에 게시한 '<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Toxic Text Generation","LLMs","Text Detoxification","Lexical Diversity","Synthetic Data","Human Annotation","Style Transfer"],"permalink":"/ai/review/2025-9-11-think_So_lets_replace_this_phrase_with_insult..._think_Lessons_learned_from_generation_of_toxic_texts_with_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-RewardDance_Reward_Scaling_in_Visual_Generation","title":"[논문리뷰] RewardDance: Reward Scaling in Visual Generation","excerpt":"Liang Li이 [arXiv]에 게시한 'RewardDance: Reward Scaling in Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Reward Model","Visual Generation","RLHF","VLM","Reward Scaling","Reward Hacking","Generative Paradigm","Context Scaling","Text-to-Image","Text-to-Video"],"permalink":"/ai/review/2025-9-11-RewardDance_Reward_Scaling_in_Visual_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-P3-SAM_Native_3D_Part_Segmentation","title":"[논문리뷰] P3-SAM: Native 3D Part Segmentation","excerpt":"Yunhan Yang이 [arXiv]에 게시한 'P3-SAM: Native 3D Part Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","3D Part Segmentation","Point Cloud Segmentation","Prompt-based Segmentation","Deep Learning","Transformer","Interactive Segmentation","Automatic Segmentation","Native 3D"],"permalink":"/ai/review/2025-9-11-P3-SAM_Native_3D_Part_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-Hunyuan-MT_Technical_Report","title":"[논문리뷰] Hunyuan-MT Technical Report","excerpt":"Yang Du이 [arXiv]에 게시한 'Hunyuan-MT Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Machine Translation","Large Language Model","Multilingual","Low-Resource Languages","Reinforcement Learning","Weak-to-Strong Learning","Slow Thinking"],"permalink":"/ai/review/2025-9-11-Hunyuan-MT_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-HumanAgencyBench_Scalable_Evaluation_of_Human_Agency_Support_in_AI_Assistants","title":"[논문리뷰] HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants","excerpt":"Jacy Reese Anthis이 [arXiv]에 게시한 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Human Agency","AI Assistants","LLM Evaluation","Benchmark","Sociotechnical AI","AI Alignment","Scalable Evaluation"],"permalink":"/ai/review/2025-9-11-HumanAgencyBench_Scalable_Evaluation_of_Human_Agency_Support_in_AI_Assistants/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-EnvX_Agentize_Everything_with_Agentic_AI","title":"[논문리뷰] EnvX: Agentize Everything with Agentic AI","excerpt":"Wenzheng Tom Tang이 [arXiv]에 게시한 'EnvX: Agentize Everything with Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Agentic AI","Multi-Agent Systems","Code Repository","Agentization","Natural Language Interaction","Agent-to-Agent Protocol","LLM-based Agents"],"permalink":"/ai/review/2025-9-11-EnvX_Agentize_Everything_with_Agentic_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-AgentGym-RL_Training_LLM_Agents_for_Long-Horizon_Decision_Making_through_Multi-Turn_Reinforcement_Learning","title":"[논문리뷰] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning","excerpt":"Honglin Guo이 [arXiv]에 게시한 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","LLM Agents","Reinforcement Learning","Multi-Turn Interaction","Long-Horizon Decision Making","Agent Framework","Exploration-Exploitation","Progressive Scaling"],"permalink":"/ai/review/2025-9-11-AgentGym-RL_Training_LLM_Agents_for_Long-Horizon_Decision_Making_through_Multi-Turn_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-A_Survey_of_Reinforcement_Learning_for_Large_Reasoning_Models","title":"[논문리뷰] A Survey of Reinforcement Learning for Large Reasoning Models","excerpt":"Runze Liu이 [arXiv]에 게시한 'A Survey of Reinforcement Learning for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Reasoning Models","LLMs","Reward Design","Policy Optimization","Verifiable Rewards","Agentic AI","Multimodal AI"],"permalink":"/ai/review/2025-9-11-A_Survey_of_Reinforcement_Learning_for_Large_Reasoning_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-11-3D_and_4D_World_Modeling_A_Survey","title":"[논문리뷰] 3D and 4D World Modeling: A Survey","excerpt":"Ao Liang이 [arXiv]에 게시한 '3D and 4D World Modeling: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","lastModifiedAt":"2025-09-11 13:02:36+0900","categories":["Review"],"tags":["Review","3D World Modeling","4D World Modeling","Generative Models","Predictive Models","LiDAR","Occupancy Grids","Video Generation","Autonomous Driving","Robotics"],"permalink":"/ai/review/2025-9-11-3D_and_4D_World_Modeling_A_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-ΔL_Normalization_Rethink_Loss_Aggregation_in_RLVR","title":"[논문리뷰] ΔL Normalization: Rethink Loss Aggregation in RLVR","excerpt":"Lili Qiu이 [arXiv]에 게시한 'ΔL Normalization: Rethink Loss Aggregation in RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLMs","Gradient Variance","Loss Aggregation","Unbiased Estimator","RLVR","Policy Gradient","Normalization"],"permalink":"/ai/review/2025-9-10-ΔL_Normalization_Rethink_Loss_Aggregation_in_RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Visual_Representation_Alignment_for_Multimodal_Large_Language_Models","title":"[논문리뷰] Visual Representation Alignment for Multimodal Large Language Models","excerpt":"Heeseong Shin이 [arXiv]에 게시한 'Visual Representation Alignment for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Visual Representation Alignment","Foundation Models","Regularization","Fine-grained Visual Understanding","Spatial Reasoning","Object Counting","Vision-Language Models"],"permalink":"/ai/review/2025-9-10-Visual_Representation_Alignment_for_Multimodal_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-UMO_Scaling_Multi-Identity_Consistency_for_Image_Customization_via_Matching_Reward","title":"[논문리뷰] UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward","excerpt":"Fei Ding이 [arXiv]에 게시한 'UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Image Customization","Multi-Identity Generation","Identity Consistency","Identity Confusion","Reinforcement Learning","Diffusion Models","Matching Reward","Global Assignment"],"permalink":"/ai/review/2025-9-10-UMO_Scaling_Multi-Identity_Consistency_for_Image_Customization_via_Matching_Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Staying_in_the_Sweet_Spot_Responsive_Reasoning_Evolution_via_Capability-Adaptive_Hint_Scaffolding","title":"[논문리뷰] Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding","excerpt":"Yongcheng Zeng이 [arXiv]에 게시한 'Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","RLVR","LLM Reasoning","Adaptive Learning","Hint Scaffolding","Item Response Theory","Exploration Efficiency","Problem Difficulty","Policy Optimization"],"permalink":"/ai/review/2025-9-10-Staying_in_the_Sweet_Spot_Responsive_Reasoning_Evolution_via_Capability-Adaptive_Hint_Scaffolding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-SimpleQA_Verified_A_Reliable_Factuality_Benchmark_to_Measure_Parametric_Knowledge","title":"[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge","excerpt":"Dipanjan Das이 [arXiv]에 게시한 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","LLM Factuality","Parametric Knowledge","Benchmark","Question Answering","Data Curation","Evaluation Metrics","Hallucination Mitigation","Large Language Models"],"permalink":"/ai/review/2025-9-10-SimpleQA_Verified_A_Reliable_Factuality_Benchmark_to_Measure_Parametric_Knowledge/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Reconstruction_Alignment_Improves_Unified_Multimodal_Models","title":"[논문리뷰] Reconstruction Alignment Improves Unified Multimodal Models","excerpt":"XuDong Wang이 [arXiv]에 게시한 'Reconstruction Alignment Improves Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Unified Multimodal Models","Image Generation","Image Editing","Post-training","Self-supervised Learning","Reconstruction Alignment","Visual Embeddings"],"permalink":"/ai/review/2025-9-10-Reconstruction_Alignment_Improves_Unified_Multimodal_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Q-Sched_Pushing_the_Boundaries_of_Few-Step_Diffusion_Models_with_Quantization-Aware_Scheduling","title":"[논문리뷰] Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling","excerpt":"Diana Marculescu이 [arXiv]에 게시한 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Quantization","Few-Step Generation","Model Compression","Noise Scheduling","Post-Training Quantization","Image Quality Metrics","Latent Consistency Models"],"permalink":"/ai/review/2025-9-10-Q-Sched_Pushing_the_Boundaries_of_Few-Step_Diffusion_Models_with_Quantization-Aware_Scheduling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Parallel-R1_Towards_Parallel_Thinking_via_Reinforcement_Learning","title":"[논문리뷰] Parallel-R1: Towards Parallel Thinking via Reinforcement Learning","excerpt":"Xinyu Yang이 [arXiv]에 게시한 'Parallel-R1: Towards Parallel Thinking via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Large Language Models","Parallel Thinking","Reinforcement Learning","Mathematical Reasoning","Progressive Curriculum","Reward Design","Exploration Scaffold"],"permalink":"/ai/review/2025-9-10-Parallel-R1_Towards_Parallel_Thinking_via_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Mini-o3_Scaling_Up_Reasoning_Patterns_and_Interaction_Turns_for_Visual_Search","title":"[논문리뷰] Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search","excerpt":"Tianjian Li이 [arXiv]에 게시한 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Visual Search","Multi-Turn Reasoning","Reinforcement Learning","Tool-Integrated Agents","Exploratory Reasoning","Data Augmentation","Over-turn Masking","Visual Language Models"],"permalink":"/ai/review/2025-9-10-Mini-o3_Scaling_Up_Reasoning_Patterns_and_Interaction_Turns_for_Visual_Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Language_Self-Play_For_Data-Free_Training","title":"[논문리뷰] Language Self-Play For Data-Free Training","excerpt":"Vijai Mohan이 [arXiv]에 게시한 'Language Self-Play For Data-Free Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Self-Play","Data-Free Training","Instruction Following","Adversarial Training","Reward Modeling"],"permalink":"/ai/review/2025-9-10-Language_Self-Play_For_Data-Free_Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-F1_A_Vision-Language-Action_Model_Bridging_Understanding_and_Generation_to_Actions","title":"[논문리뷰] F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions","excerpt":"Zherui Qiu이 [arXiv]에 게시한 'F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Vision-Language-Action","Embodied AI","Visual Foresight","Predictive Inverse Dynamics","Mixture-of-Transformer","Robot Manipulation","Multi-stage Training","Generalization"],"permalink":"/ai/review/2025-9-10-F1_A_Vision-Language-Action_Model_Bridging_Understanding_and_Generation_to_Actions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Directly_Aligning_the_Full_Diffusion_Trajectory_with_Fine-Grained_Human_Preference","title":"[논문리뷰] Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference","excerpt":"Yingfang Zhang이 [arXiv]에 게시한 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Reinforcement Learning","Human Preference","Text-to-Image Generation","Reward Hacking","Direct-Align","SRPO","Fine-Grained Control","Flow Matching Models"],"permalink":"/ai/review/2025-9-10-Directly_Aligning_the_Full_Diffusion_Trajectory_with_Fine-Grained_Human_Preference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Curia_A_Multi-Modal_Foundation_Model_for_Radiology","title":"[논문리뷰] Curia: A Multi-Modal Foundation Model for Radiology","excerpt":"Elodie Ferreres이 [arXiv]에 게시한 'Curia: A Multi-Modal Foundation Model for Radiology' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Foundation Model","Radiology","Computed Tomography (CT)","Magnetic Resonance Imaging (MRI)","Self-supervised Learning","Vision Transformer","Cross-Modality Generalization"],"permalink":"/ai/review/2025-9-10-Curia_A_Multi-Modal_Foundation_Model_for_Radiology/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-10-Causal_Attention_with_Lookahead_Keys","title":"[논문리뷰] Causal Attention with Lookahead Keys","excerpt":"Quanquan Gu이 [arXiv]에 게시한 'Causal Attention with Lookahead Keys' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","lastModifiedAt":"2025-09-10 13:11:01+0900","categories":["Review"],"tags":["Review","Causal Attention","Lookahead Keys","Autoregressive Modeling","Language Models","Transformer","Perplexity Reduction","Parallel Training","Efficient Inference"],"permalink":"/ai/review/2025-9-10-Causal_Attention_with_Lookahead_Keys/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-WebExplorer_Explore_and_Evolve_for_Training_Long-Horizon_Web_Agents","title":"[논문리뷰] WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents","excerpt":"Aili Chen이 [arXiv]에 게시한 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Web Agents","Long-Horizon Reasoning","Large Language Models (LLMs)","Data Generation","Reinforcement Learning (RL)","Supervised Fine-tuning (SFT)","Web Navigation","Information Retrieval"],"permalink":"/ai/review/2025-9-9-WebExplorer_Explore_and_Evolve_for_Training_Long-Horizon_Web_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-UniVerse-1_Unified_Audio-Video_Generation_via_Stitching_of_Experts","title":"[논문리뷰] UniVerse-1: Unified Audio-Video Generation via Stitching of Experts","excerpt":"Xinyao Liao이 [arXiv]에 게시한 'UniVerse-1: Unified Audio-Video Generation via Stitching of Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Unified Audio-Video Generation","Stitching of Experts (SoE)","Multimodal Diffusion","Online Annotation","Cross-modal Noise Correlation","Foundation Models","Verse-Bench"],"permalink":"/ai/review/2025-9-9-UniVerse-1_Unified_Audio-Video_Generation_via_Stitching_of_Experts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Test-Time_Scaling_in_Reasoning_Models_Is_Not_Effective_for_Knowledge-Intensive_Tasks_Yet","title":"[논문리뷰] Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet","excerpt":"See-Kiong Ng이 [arXiv]에 게시한 'Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Test-Time Scaling","Reasoning Models","Knowledge-Intensive Tasks","Hallucinations","Factual Accuracy","Chain-of-Thought","Large Language Models"],"permalink":"/ai/review/2025-9-9-Test-Time_Scaling_in_Reasoning_Models_Is_Not_Effective_for_Knowledge-Intensive_Tasks_Yet/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Scaling_up_Multi-Turn_Off-Policy_RL_and_Multi-Agent_Tree_Search_for_LLM_Step-Provers","title":"[논문리뷰] Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers","excerpt":"Xia Xiao이 [arXiv]에 게시한 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","LLM Step-Provers","Reinforcement Learning (RL)","Off-Policy RL","Multi-Agent Systems","Tree Search","Automated Theorem Proving (ATP)","Formal Mathematics","AlphaZero"],"permalink":"/ai/review/2025-9-9-Scaling_up_Multi-Turn_Off-Policy_RL_and_Multi-Agent_Tree_Search_for_LLM_Step-Provers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Saturation-Driven_Dataset_Generation_for_LLM_Mathematical_Reasoning_in_the_TPTP_Ecosystem","title":"[논문리뷰] Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem","excerpt":"Damien Sileo이 [arXiv]에 게시한 'Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","LLM","Mathematical Reasoning","Synthetic Data Generation","TPTP Ecosystem","Saturation Proving","Proof Graph Reconstruction","Data Augmentation"],"permalink":"/ai/review/2025-9-9-Saturation-Driven_Dataset_Generation_for_LLM_Mathematical_Reasoning_in_the_TPTP_Ecosystem/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Rtextbf2AI_Towards_Resistant_and_Resilient_AI_in_an_Evolving_World","title":"[논문리뷰] R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World","excerpt":"Bowen Zhou이 [arXiv]에 게시한 'R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","AI Safety","Resistant AI","Resilient AI","Coevolution","Fast-Slow Models","Adversarial Training","Continual Learning","AGI Alignment"],"permalink":"/ai/review/2025-9-9-Rtextbf2AI_Towards_Resistant_and_Resilient_AI_in_an_Evolving_World/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Revolutionizing_Reinforcement_Learning_Framework_for_Diffusion_Large_Language_Models","title":"[논문리뷰] Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models","excerpt":"Ke Shen이 [arXiv]에 게시한 'Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Reinforcement Learning","Trajectory-aware RL","Value Model","Masked Diffusion Models","Large Language Models","Reasoning Tasks","Code Generation"],"permalink":"/ai/review/2025-9-9-Revolutionizing_Reinforcement_Learning_Framework_for_Diffusion_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Reverse-Engineered_Reasoning_for_Open-Ended_Generation","title":"[논문리뷰] Reverse-Engineered Reasoning for Open-Ended Generation","excerpt":"Wangchunshu Zhou이 [arXiv]에 게시한 'Reverse-Engineered Reasoning for Open-Ended Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Deep Reasoning","Open-Ended Generation","Reverse-Engineered Reasoning (REER)","LLMs","Synthetic Data","Iterative Refinement","Perplexity Minimization","DeepWriting-20K"],"permalink":"/ai/review/2025-9-9-Reverse-Engineered_Reasoning_for_Open-Ended_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Reinforcement_Learning_Foundations_for_Deep_Research_Systems_A_Survey","title":"[논문리뷰] Reinforcement Learning Foundations for Deep Research Systems: A Survey","excerpt":"Wei Han이 [arXiv]에 게시한 'Reinforcement Learning Foundations for Deep Research Systems: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Deep Research Systems","Agentic AI","Tool Use","Hierarchical Agents","Reward Design","Multimodal AI","RL Frameworks"],"permalink":"/ai/review/2025-9-9-Reinforcement_Learning_Foundations_for_Deep_Research_Systems_A_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Reinforced_Visual_Perception_with_Tools","title":"[논문리뷰] Reinforced Visual Perception with Tools","excerpt":"Mingyang Fu이 [arXiv]에 게시한 'Reinforced Visual Perception with Tools' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Visual Reasoning","Multimodal LLMs","Reinforcement Learning","Tool Usage","Perception-heavy Benchmarks","GRPO","Vision Tools"],"permalink":"/ai/review/2025-9-9-Reinforced_Visual_Perception_with_Tools/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Paper2Agent_Reimagining_Research_Papers_As_Interactive_and_Reliable_AI_Agents","title":"[논문리뷰] Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents","excerpt":"James Zou이 [arXiv]에 게시한 'Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","AI Agents","Research Reproducibility","Scientific Communication","Model Context Protocol (MCP)","Natural Language Interaction","Genomics","Single-Cell Analysis","Spatial Transcriptomics"],"permalink":"/ai/review/2025-9-9-Paper2Agent_Reimagining_Research_Papers_As_Interactive_and_Reliable_AI_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-MAS-Bench_A_Unified_Benchmark_for_Shortcut-Augmented_Hybrid_Mobile_GUI_Agents","title":"[논문리뷰] MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents","excerpt":"Zhengxi Lu이 [arXiv]에 게시한 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Mobile GUI Agents","Hybrid Automation","Shortcut Generation","Benchmark","Task Efficiency","LLM-based Agents","Mobile Robotics"],"permalink":"/ai/review/2025-9-9-MAS-Bench_A_Unified_Benchmark_for_Shortcut-Augmented_Hybrid_Mobile_GUI_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Llama-GENBA-10B_A_Trilingual_Large_Language_Model_for_German_English_and_Bavarian","title":"[논문리뷰] Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian","excerpt":"Hoi-Fong Mak이 [arXiv]에 게시한 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Multilingual LLM","Low-Resource Language","German","Bavarian Dialect","Cross-Lingual Transfer","Continuous Pretraining","Llama-3.1","Model Expansion"],"permalink":"/ai/review/2025-9-9-Llama-GENBA-10B_A_Trilingual_Large_Language_Model_for_German_English_and_Bavarian/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Interleaving_Reasoning_for_Better_Text-to-Image_Generation","title":"[논문리뷰] Interleaving Reasoning for Better Text-to-Image Generation","excerpt":"Shixiang Tang이 [arXiv]에 게시한 'Interleaving Reasoning for Better Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Interleaving Reasoning","Multimodal Learning","Visual Quality","Fine-grained Detail","Diffusion Models","Self-Correction"],"permalink":"/ai/review/2025-9-9-Interleaving_Reasoning_for_Better_Text-to-Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Focusing_by_Contrastive_Attention_Enhancing_VLMs_Visual_Reasoning","title":"[논문리뷰] Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning","excerpt":"Baolong Bi이 [arXiv]에 게시한 'Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Visual Reasoning","Attention Mechanisms","Contrastive Learning","Noise Suppression","Visual Complexity","Training-Free"],"permalink":"/ai/review/2025-9-9-Focusing_by_Contrastive_Attention_Enhancing_VLMs_Visual_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Easier_Painting_Than_Thinking_Can_Text-to-Image_Models_Set_the_Stage_but_Not_Direct_the_Play","title":"[논문리뷰] Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?","excerpt":"Rui Chen이 [arXiv]에 게시한 'Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","T2I Benchmarking","Compositional Reasoning","Deductive Inference","Inductive Inference","Abductive Inference","MLLM Evaluation"],"permalink":"/ai/review/2025-9-9-Easier_Painting_Than_Thinking_Can_Text-to-Image_Models_Set_the_Stage_but_Not_Direct_the_Play/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-Does_DINOv3_Set_a_New_Medical_Vision_Standard","title":"[논문리뷰] Does DINOv3 Set a New Medical Vision Standard?","excerpt":"Bailiang Jian이 [arXiv]에 게시한 'Does DINOv3 Set a New Medical Vision Standard?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Medical Imaging","Foundation Models","DINOv3","Self-Supervised Learning","Vision Transformer","2D/3D Classification","Segmentation","Domain Adaptation","Scaling Laws"],"permalink":"/ai/review/2025-9-9-Does_DINOv3_Set_a_New_Medical_Vision_Standard/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-9-D-HUMOR_Dark_Humor_Understanding_via_Multimodal_Open-ended_Reasoning","title":"[논문리뷰] D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning","excerpt":"Dhanvin Sanjay Namboodiri이 [arXiv]에 게시한 'D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","lastModifiedAt":"2025-09-09 13:19:09+0900","categories":["Review"],"tags":["Review","Dark Humor Detection","Multimodal Reasoning","Vision-Language Models (VLMs)","Iterative Reasoning Refinement","Meme Analysis","Content Moderation","Cross-Modal Attention","Dataset Annotation"],"permalink":"/ai/review/2025-9-9-D-HUMOR_Dark_Humor_Understanding_via_Multimodal_Open-ended_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-WinT3R_Window-Based_Streaming_Reconstruction_with_Camera_Token_Pool","title":"[논문리뷰] WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool","excerpt":"Wenzheng Chang이 [arXiv]에 게시한 'WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Online 3D Reconstruction","Camera Pose Estimation","Streaming Reconstruction","Sliding Window","Camera Token Pool","Real-time Performance","Computer Vision"],"permalink":"/ai/review/2025-9-8-WinT3R_Window-Based_Streaming_Reconstruction_with_Camera_Token_Pool/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-WildScore_Benchmarking_MLLMs_in-the-Wild_Symbolic_Music_Reasoning","title":"[논문리뷰] WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning","excerpt":"Amit Namburi이 [arXiv]에 게시한 'WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Symbolic Music Reasoning","Music Score Analysis","Benchmarking","Visual Question Answering","In-the-Wild Data","Music Theory"],"permalink":"/ai/review/2025-9-8-WildScore_Benchmarking_MLLMs_in-the-Wild_Symbolic_Music_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-Why_Language_Models_Hallucinate","title":"[논문리뷰] Why Language Models Hallucinate","excerpt":"Edwin Zhang이 [arXiv]에 게시한 'Why Language Models Hallucinate' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Language Models","Hallucination","Pretraining","Post-training","Evaluation Metrics","Binary Classification","Uncertainty Quantification","Calibration"],"permalink":"/ai/review/2025-9-8-Why_Language_Models_Hallucinate/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-U-ARM_Ultra_low-cost_general_teleoperation_interface_for_robot_manipulation","title":"[논문리뷰] U-ARM : Ultra low-cost general teleoperation interface for robot manipulation","excerpt":"Junda Huang이 [arXiv]에 게시한 'U-ARM : Ultra low-cost general teleoperation interface for robot manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Teleoperation","Robot Manipulation","Low-Cost Hardware","3D Printing","Leader-Follower System","Data Collection","Robotics Interface","Open Source"],"permalink":"/ai/review/2025-9-8-U-ARM_Ultra_low-cost_general_teleoperation_interface_for_robot_manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-Symbolic_Graphics_Programming_with_Large_Language_Models","title":"[논문리뷰] Symbolic Graphics Programming with Large Language Models","excerpt":"Kaipeng Zhang이 [arXiv]에 게시한 'Symbolic Graphics Programming with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Symbolic Graphics Programming","Large Language Models","Reinforcement Learning","SVG Generation","Text-to-Image Synthesis","Cross-Modal Alignment","Program Synthesis"],"permalink":"/ai/review/2025-9-8-Symbolic_Graphics_Programming_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-Set_Block_Decoding_is_a_Language_Model_Inference_Accelerator","title":"[논문리뷰] Set Block Decoding is a Language Model Inference Accelerator","excerpt":"Jeremy Reizenstein이 [arXiv]에 게시한 'Set Block Decoding is a Language Model Inference Accelerator' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Language Model Inference","Acceleration","Set Block Decoding","Next Token Prediction","Masked Token Prediction","Parallel Decoding","KV-caching","Diffusion Models"],"permalink":"/ai/review/2025-9-8-Set_Block_Decoding_is_a_Language_Model_Inference_Accelerator/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-On_Robustness_and_Reliability_of_Benchmark-Based_Evaluation_of_LLMs","title":"[논문리뷰] On Robustness and Reliability of Benchmark-Based Evaluation of LLMs","excerpt":"Kevin Roitero이 [arXiv]에 게시한 'On Robustness and Reliability of Benchmark-Based Evaluation of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Model Robustness","Benchmark Reliability","Paraphrasing","Linguistic Variability","Generalization","Question Answering"],"permalink":"/ai/review/2025-9-8-On_Robustness_and_Reliability_of_Benchmark-Based_Evaluation_of_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-MedVista3D_Vision-Language_Modeling_for_Reducing_Diagnostic_Errors_in_3D_CT_Disease_Detection_Understanding_and_Reporting","title":"[논문리뷰] MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting","excerpt":"Vanessa Wildman이 [arXiv]에 게시한 'MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","3D CT","Vision-Language Model","Medical Imaging","Diagnostic Error Reduction","Multi-scale Alignment","Semantic Enrichment","Radiology Reporting","Zero-shot Learning"],"permalink":"/ai/review/2025-9-8-MedVista3D_Vision-Language_Modeling_for_Reducing_Diagnostic_Errors_in_3D_CT_Disease_Detection_Understanding_and_Reporting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-LuxDiT_Lighting_Estimation_with_Video_Diffusion_Transformer","title":"[논문리뷰] LuxDiT: Lighting Estimation with Video Diffusion Transformer","excerpt":"Sanja Fidler이 [arXiv]에 게시한 'LuxDiT: Lighting Estimation with Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Lighting Estimation","HDR Environment Map","Diffusion Models","Video Transformer","Low-Rank Adaptation","Generative Models","Synthetic Data"],"permalink":"/ai/review/2025-9-8-LuxDiT_Lighting_Estimation_with_Video_Diffusion_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-LatticeWorld_A_Multimodal_Large_Language_Model-Empowered_Framework_for_Interactive_Complex_World_Generation","title":"[논문리뷰] LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation","excerpt":"Zhan Zhao이 [arXiv]에 게시한 'LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Multimodal LLM","3D World Generation","Unreal Engine 5","Procedural Content Generation","Interactive Environments","Sim-to-Real","Spatial Understanding","Multimodal Input"],"permalink":"/ai/review/2025-9-8-LatticeWorld_A_Multimodal_Large_Language_Model-Empowered_Framework_for_Interactive_Complex_World_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-Bootstrapping_Task_Spaces_for_Self-Improvement","title":"[논문리뷰] Bootstrapping Task Spaces for Self-Improvement","excerpt":"Yoram Bachrach이 [arXiv]에 게시한 'Bootstrapping Task Spaces for Self-Improvement' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Self-Improvement","Autocurriculum","Task-Space Exploration","Inference-Time Iteration","Policy Optimization"],"permalink":"/ai/review/2025-9-8-Bootstrapping_Task_Spaces_for_Self-Improvement/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-8-Behavioral_Fingerprinting_of_Large_Language_Models","title":"[논문리뷰] Behavioral Fingerprinting of Large Language Models","excerpt":"Xing Li이 [arXiv]에 게시한 'Behavioral Fingerprinting of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","lastModifiedAt":"2025-09-08 13:10:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Behavioral Evaluation","Model Alignment","Sycophancy","World Model Brittleness","Metacognition","Personality Profiling"],"permalink":"/ai/review/2025-9-8-Behavioral_Fingerprinting_of_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Video-MTR_Reinforced_Multi-Turn_Reasoning_for_Long_Video_Understanding","title":"[논문리뷰] Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding","excerpt":"Lionel Ni이 [arXiv]에 게시한 'Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Long Video Understanding","Reinforcement Learning","Multi-Turn Reasoning","MLLMs","Video Segment Selection","Bi-level Reward","Question Answering"],"permalink":"/ai/review/2025-9-5-Video-MTR_Reinforced_Multi-Turn_Reasoning_for_Long_Video_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Transition_Models_Rethinking_the_Generative_Learning_Objective","title":"[논문리뷰] Transition Models: Rethinking the Generative Learning Objective","excerpt":"Yangguang Li이 [arXiv]에 게시한 'Transition Models: Rethinking the Generative Learning Objective' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Generative Models","Diffusion Models","Training Objective","Continuous-Time Dynamics","State Transition","Few-Step Generation","Scalable Training","Image Generation"],"permalink":"/ai/review/2025-9-5-Transition_Models_Rethinking_the_Generative_Learning_Objective/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Towards_a_Unified_View_of_Large_Language_Model_Post-Training","title":"[논문리뷰] Towards a Unified View of Large Language Model Post-Training","excerpt":"Hongyi Liu이 [arXiv]에 게시한 'Towards a Unified View of Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Post-Training","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Policy Gradient","Unified Framework","Hybrid Algorithms","Bias-Variance Tradeoff"],"permalink":"/ai/review/2025-9-5-Towards_a_Unified_View_of_Large_Language_Model_Post-Training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-NER_Retriever_Zero-Shot_Named_Entity_Retrieval_with_Type-Aware_Embeddings","title":"[논문리뷰] NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings","excerpt":"Oren Glickman이 [arXiv]에 게시한 'NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Named Entity Retrieval","Zero-Shot Learning","Type-Aware Embeddings","Large Language Models (LLMs)","Contrastive Learning","Internal Representations","Information Retrieval"],"permalink":"/ai/review/2025-9-5-NER_Retriever_Zero-Shot_Named_Entity_Retrieval_with_Type-Aware_Embeddings/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Inverse_IFEval_Can_LLMs_Unlearn_Stubborn_Training_Conventions_to_Follow_Real_Instructions","title":"[논문리뷰] Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?","excerpt":"Yu Fu이 [arXiv]에 게시한 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLMs","Instruction Following","Benchmark","Cognitive Inertia","Out-of-Distribution","Supervised Fine-Tuning","Evaluation","Robustness"],"permalink":"/ai/review/2025-9-5-Inverse_IFEval_Can_LLMs_Unlearn_Stubborn_Training_Conventions_to_Follow_Real_Instructions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-From_Editor_to_Dense_Geometry_Estimator","title":"[논문리뷰] From Editor to Dense Geometry Estimator","excerpt":"Lang Nie이 [arXiv]에 게시한 'From Editor to Dense Geometry Estimator' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Dense Geometry Estimation","Diffusion Transformer","Image Editing","Zero-shot Learning","Depth Estimation","Normal Estimation","Flow Matching","Logarithmic Quantization"],"permalink":"/ai/review/2025-9-5-From_Editor_to_Dense_Geometry_Estimator/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Few-step_Flow_for_3D_Generation_via_Marginal-Data_Transport_Distillation","title":"[논문리뷰] Few-step Flow for 3D Generation via Marginal-Data Transport Distillation","excerpt":"Lingxi Xie이 [arXiv]에 게시한 'Few-step Flow for 3D Generation via Marginal-Data Transport Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","3D Generation","Flow-based Models","Model Distillation","Few-step Sampling","Marginal-Data Transport","Velocity Matching","Velocity Distillation"],"permalink":"/ai/review/2025-9-5-Few-step_Flow_for_3D_Generation_via_Marginal-Data_Transport_Distillation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-False_Sense_of_Security_Why_Probing-based_Malicious_Input_Detection_Fails_to_Generalize","title":"[논문리뷰] False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize","excerpt":"Muhao Chen이 [arXiv]에 게시한 'False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Safety","Malicious Input Detection","Probing Classifiers","Out-of-Distribution Generalization","Superficial Patterns","Instructional Patterns","Trigger Words","AI Safety"],"permalink":"/ai/review/2025-9-5-False_Sense_of_Security_Why_Probing-based_Malicious_Input_Detection_Fails_to_Generalize/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Durian_Dual_Reference-guided_Portrait_Animation_with_Attribute_Transfer","title":"[논문리뷰] Durian: Dual Reference-guided Portrait Animation with Attribute Transfer","excerpt":"Hanbyul Joo이 [arXiv]에 게시한 'Durian: Dual Reference-guided Portrait Animation with Attribute Transfer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Portrait Animation","Attribute Transfer","Diffusion Models","Dual Reference Networks","Zero-shot Learning","Self-Reconstruction","Facial Editing"],"permalink":"/ai/review/2025-9-5-Durian_Dual_Reference-guided_Portrait_Animation_with_Attribute_Transfer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Drivel-ology_Challenging_LLMs_with_Interpreting_Nonsense_with_Depth","title":"[논문리뷰] Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth","excerpt":"Chi-Li Chen이 [arXiv]에 게시한 'Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","Large Language Models","Pragmatic Understanding","Drivelology","Benchmark Dataset","Multilingual NLP","Semantic Reasoning","Contextual Inference"],"permalink":"/ai/review/2025-9-5-Drivel-ology_Challenging_LLMs_with_Interpreting_Nonsense_with_Depth/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Drawing2CAD_Sequence-to-Sequence_Learning_for_CAD_Generation_from_Vector_Drawings","title":"[논문리뷰] Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings","excerpt":"Meie Fang이 [arXiv]에 게시한 'Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","CAD Generation","Vector Graphics","Sequence-to-Sequence Learning","Transformer Architecture","Engineering Drawings","Multi-modal Learning","Soft Target Loss","Dual Decoder"],"permalink":"/ai/review/2025-9-5-Drawing2CAD_Sequence-to-Sequence_Learning_for_CAD_Generation_from_Vector_Drawings/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-Delta_Activations_A_Representation_for_Finetuned_Large_Language_Models","title":"[논문리뷰] Delta Activations: A Representation for Finetuned Large Language Models","excerpt":"Ser-Nam Lim이 [arXiv]에 게시한 'Delta Activations: A Representation for Finetuned Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Embedding","Delta Activations","Finetuned Models","Model Representation","Model Clustering","Additive Property","Task Embedding","Model Merging"],"permalink":"/ai/review/2025-9-5-Delta_Activations_A_Representation_for_Finetuned_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-5-DeepResearch_Arena_The_First_Exam_of_LLMs_Research_Abilities_via_Seminar-Grounded_Tasks","title":"[논문리뷰] DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks","excerpt":"Jiaxuan Lu이 [arXiv]에 게시한 'DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","lastModifiedAt":"2025-09-05 13:07:20+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Research Agents","Benchmark","Multi-Agent System","Seminar-Grounded Tasks","Data Leakage Prevention","Ill-Structured Problems"],"permalink":"/ai/review/2025-9-5-DeepResearch_Arena_The_First_Exam_of_LLMs_Research_Abilities_via_Seminar-Grounded_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-4-Robix_A_Unified_Model_for_Robot_Interaction_Reasoning_and_Planning","title":"[논문리뷰] Robix: A Unified Model for Robot Interaction, Reasoning and Planning","excerpt":"Zixuan Wang이 [arXiv]에 게시한 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Robot Learning","Vision-Language Models (VLMs)","Embodied AI","Human-Robot Interaction (HRI)","Task Planning","Reinforcement Learning (RL)","Chain-of-Thought (CoT) Reasoning","Robotics"],"permalink":"/ai/review/2025-9-4-Robix_A_Unified_Model_for_Robot_Interaction_Reasoning_and_Planning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-4-Open_Data_Synthesis_For_Deep_Research","title":"[논문리뷰] Open Data Synthesis For Deep Research","excerpt":"Zheng Liu이 [arXiv]에 게시한 'Open Data Synthesis For Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Data Synthesis","Deep Research","Hierarchical Constraint Satisfaction Problems","Large Language Models","Agentic AI","Reinforcement Learning","Question Answering"],"permalink":"/ai/review/2025-9-4-Open_Data_Synthesis_For_Deep_Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-4-Mixture_of_Global_and_Local_Experts_with_Diffusion_Transformer_for_Controllable_Face_Generation","title":"[논문리뷰] Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation","excerpt":"Kai Li이 [arXiv]에 게시한 'Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Diffusion Transformer","Mixture of Experts","Controllable Generation","Face Generation","Multimodal Synthesis","Semantic Control","Image Generation"],"permalink":"/ai/review/2025-9-4-Mixture_of_Global_and_Local_Experts_with_Diffusion_Transformer_for_Controllable_Face_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-4-MOSAIC_Multi-Subject_Personalized_Generation_via_Correspondence-Aware_Alignment_and_Disentanglement","title":"[논문리뷰] MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement","excerpt":"Hualiang Wang이 [arXiv]에 게시한 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Multi-Subject Generation","Personalized Image Synthesis","Semantic Correspondence","Attention Disentanglement","Diffusion Models","Identity Preservation","Dataset"],"permalink":"/ai/review/2025-9-4-MOSAIC_Multi-Subject_Personalized_Generation_via_Correspondence-Aware_Alignment_and_Disentanglement/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-4-LMEnt_A_Suite_for_Analyzing_Knowledge_in_Language_Models_from_Pretraining_Data_to_Representations","title":"[논문리뷰] LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations","excerpt":"Yoav Gur-Arieh이 [arXiv]에 게시한 'LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","lastModifiedAt":"2025-09-04 12:56:15+0900","categories":["Review"],"tags":["Review","Language Models","Knowledge Acquisition","Pretraining Data","Entity Linking","Coreference Resolution","Information Retrieval","Model Analysis","Checkpoints"],"permalink":"/ai/review/2025-9-4-LMEnt_A_Suite_for_Analyzing_Knowledge_in_Language_Models_from_Pretraining_Data_to_Representations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-ViSTA-SLAM_Visual_SLAM_with_Symmetric_Two-view_Association","title":"[논문리뷰] ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association","excerpt":"Daniel Cremers이 [arXiv]에 게시한 'ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Monocular SLAM","Dense Reconstruction","Neural Networks","Pose Graph Optimization","Intrinsics-free","Real-time","Two-view Association"],"permalink":"/ai/review/2025-9-3-ViSTA-SLAM_Visual_SLAM_with_Symmetric_Two-view_Association/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-VerlTool_Towards_Holistic_Agentic_Reinforcement_Learning_with_Tool_Use","title":"[논문리뷰] VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use","excerpt":"Zhiheng Lyu이 [arXiv]에 게시한 'VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Tool Use","Large Language Models","Reinforcement Learning from Verifiable Rewards (RLVR)","Asynchronous Execution","Multi-modal AI","Framework"],"permalink":"/ai/review/2025-9-3-VerlTool_Towards_Holistic_Agentic_Reinforcement_Learning_with_Tool_Use/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Universal_Deep_Research_Bring_Your_Own_Model_and_Strategy","title":"[논문리뷰] Universal Deep Research: Bring Your Own Model and Strategy","excerpt":"Pavlo Molchanov이 [arXiv]에 게시한 'Universal Deep Research: Bring Your Own Model and Strategy' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Agentic Systems","Language Models (LLMs)","Research Automation","Customizable Strategies","Code Generation","Deep Research","User-Defined Agents","Sandboxed Execution"],"permalink":"/ai/review/2025-9-3-Universal_Deep_Research_Bring_Your_Own_Model_and_Strategy/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-UI-TARS-2_Technical_Report_Advancing_GUI_Agent_with_Multi-Turn_Reinforcement_Learning","title":"[논문리뷰] UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning","excerpt":"Haoyang Zou이 [arXiv]에 게시한 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","GUI Agent","Multi-Turn RL","Reinforcement Learning","Data Flywheel","Agent Framework","Hybrid Environments","Parameter Interpolation"],"permalink":"/ai/review/2025-9-3-UI-TARS-2_Technical_Report_Advancing_GUI_Agent_with_Multi-Turn_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Towards_More_Diverse_and_Challenging_Pre-training_for_Point_Cloud_Learning_Self-Supervised_Cross_Reconstruction_with_Decoupled_Views","title":"[논문리뷰] Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views","excerpt":"Junchi Yan이 [arXiv]에 게시한 'Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Point Cloud Learning","Self-Supervised Learning","Cross Reconstruction","Decoupled Views","Generative Models","Positional Encoding","3D Vision"],"permalink":"/ai/review/2025-9-3-Towards_More_Diverse_and_Challenging_Pre-training_for_Point_Cloud_Learning_Self-Supervised_Cross_Reconstruction_with_Decoupled_Views/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-The_Landscape_of_Agentic_Reinforcement_Learning_for_LLMs_A_Survey","title":"[논문리뷰] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","excerpt":"Hejia Geng이 [arXiv]에 게시한 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Large Language Models","LLM Agents","Sequential Decision Making","Policy Optimization","Tool Use","Dynamic Environments","Autonomous AI"],"permalink":"/ai/review/2025-9-3-The_Landscape_of_Agentic_Reinforcement_Learning_for_LLMs_A_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-The_Gold_Medals_in_an_Empty_Room_Diagnosing_Metalinguistic_Reasoning_in_LLMs_with_Camlang","title":"[논문리뷰] The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang","excerpt":"Solomon Tsai이 [arXiv]에 게시한 'The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","LLMs","Metalinguistic Reasoning","Constructed Language","Camlang","Second Language Acquisition","Zero-shot Learning","Natural Language Understanding","Commonsense Reasoning"],"permalink":"/ai/review/2025-9-3-The_Gold_Medals_in_an_Empty_Room_Diagnosing_Metalinguistic_Reasoning_in_LLMs_with_Camlang/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-SimpleTIR_End-to-End_Reinforcement_Learning_for_Multi-Turn_Tool-Integrated_Reasoning","title":"[논문리뷰] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning","excerpt":"Qian Liu이 [arXiv]에 게시한 'SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Tool-Integrated Reasoning","Multi-turn Reasoning","Gradient Explosion","Training Stability","Trajectory Filtering","Zero RL"],"permalink":"/ai/review/2025-9-3-SimpleTIR_End-to-End_Reinforcement_Learning_for_Multi-Turn_Tool-Integrated_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-SQL-of-Thought_Multi-agentic_Text-to-SQL_with_Guided_Error_Correction","title":"[논문리뷰] SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction","excerpt":"bindsch이 [arXiv]에 게시한 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Text-to-SQL","Multi-agent Systems","Chain-of-Thought","Error Correction","Large Language Models","Query Planning","Database Interaction"],"permalink":"/ai/review/2025-9-3-SQL-of-Thought_Multi-agentic_Text-to-SQL_with_Guided_Error_Correction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Reasoning_Vectors_Transferring_Chain-of-Thought_Capabilities_via_Task_Arithmetic","title":"[논문리뷰] Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic","excerpt":"Bernard Ghanem이 [arXiv]에 게시한 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reasoning Vectors","Task Arithmetic","Chain-of-Thought","LLMs","Reinforcement Learning","Model Merging","Parameter Transfer"],"permalink":"/ai/review/2025-9-3-Reasoning_Vectors_Transferring_Chain-of-Thought_Capabilities_via_Task_Arithmetic/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-POINTS-Reader_Distillation-Free_Adaptation_of_Vision-Language_Models_for_Document_Conversion","title":"[논문리뷰] POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion","excerpt":"Haicheng Wang이 [arXiv]에 게시한 'POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","문서 변환","시각-언어 모델","자가 개선","합성 데이터","증류 없는 학습","OCR","멀티모달 AI","데이터 필터링"],"permalink":"/ai/review/2025-9-3-POINTS-Reader_Distillation-Free_Adaptation_of_Vision-Language_Models_for_Document_Conversion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-OpenVision_2_A_Family_of_Generative_Pretrained_Visual_Encoders_for_Multimodal_Learning","title":"[논문리뷰] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning","excerpt":"Zirui Wang이 [arXiv]에 게시한 'OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Multimodal Learning","Vision Encoder","Generative Pretraining","Captioning Loss","Training Efficiency","Image-Text Models","Large Language Models"],"permalink":"/ai/review/2025-9-3-OpenVision_2_A_Family_of_Generative_Pretrained_Visual_Encoders_for_Multimodal_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-MobiAgent_A_Systematic_Framework_for_Customizable_Mobile_Agents","title":"[논문리뷰] MobiAgent: A Systematic Framework for Customizable Mobile Agents","excerpt":"Wangbo Gong이 [arXiv]에 게시한 'MobiAgent: A Systematic Framework for Customizable Mobile Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Mobile Agents","GUI Agents","Vision-Language Models","Agent Acceleration","Benchmarking","Reinforcement Learning","Data Collection"],"permalink":"/ai/review/2025-9-3-MobiAgent_A_Systematic_Framework_for_Customizable_Mobile_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Metis_Training_Large_Language_Models_with_Advanced_Low-Bit_Quantization","title":"[논문리뷰] Metis: Training Large Language Models with Advanced Low-Bit Quantization","excerpt":"Hengjie Cao이 [arXiv]에 게시한 'Metis: Training Large Language Models with Advanced Low-Bit Quantization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Low-Bit Quantization","LLMs","Spectral Decomposition","Anisotropy","Adaptive Learning Rate","Regularization","FP8 Training","FP4 Training"],"permalink":"/ai/review/2025-9-3-Metis_Training_Large_Language_Models_with_Advanced_Low-Bit_Quantization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-MedDINOv3_How_to_adapt_vision_foundation_models_for_medical_image_segmentation","title":"[논문리뷰] MedDINOv3: How to adapt vision foundation models for medical image segmentation?","excerpt":"Xiaofeng Yang이 [arXiv]에 게시한 'MedDINOv3: How to adapt vision foundation models for medical image segmentation?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Medical Image Segmentation","Vision Foundation Models","Self-supervised Learning","Vision Transformers (ViT)","Domain Adaptation","DINOv3","CT Imaging"],"permalink":"/ai/review/2025-9-3-MedDINOv3_How_to_adapt_vision_foundation_models_for_medical_image_segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-M3Ret_Unleashing_Zero-shot_Multimodal_Medical_Image_Retrieval_via_Self-Supervision","title":"[논문리뷰] M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision","excerpt":"Yan-Jie Zhou이 [arXiv]에 게시한 'M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Medical Image Retrieval","Self-Supervised Learning","Multimodal","Zero-shot","Foundation Models","MAE","SimDINO","Vision Transformer"],"permalink":"/ai/review/2025-9-3-M3Ret_Unleashing_Zero-shot_Multimodal_Medical_Image_Retrieval_via_Self-Supervision/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-LLaVA-Critic-R1_Your_Critic_Model_is_Secretly_a_Strong_Policy_Model","title":"[논문리뷰] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model","excerpt":"Jianwei Yang이 [arXiv]에 게시한 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Critic Models","Policy Models","Reinforcement Learning (RL)","Self-Criticism","Multimodal Reasoning","Preference Learning","Generative Models"],"permalink":"/ai/review/2025-9-3-LLaVA-Critic-R1_Your_Critic_Model_is_Secretly_a_Strong_Policy_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Kwai_Keye-VL_1.5_Technical_Report","title":"[논문리뷰] Kwai Keye-VL 1.5 Technical Report","excerpt":"SXxtyz이 [arXiv]에 게시한 'Kwai Keye-VL 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Video Understanding","Slow-Fast Encoding","Long Context","Chain-of-Thought","Reinforcement Learning","Human Alignment","Native-Resolution Vision Encoder"],"permalink":"/ai/review/2025-9-3-Kwai_Keye-VL_1.5_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Jointly_Reinforcing_Diversity_and_Quality_in_Language_Model_Generations","title":"[논문리뷰] Jointly Reinforcing Diversity and Quality in Language Model Generations","excerpt":"Tianlu이 [arXiv]에 게시한 'Jointly Reinforcing Diversity and Quality in Language Model Generations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Language Models","Diversity Optimization","Quality Enhancement","Semantic Clustering","Post-training","Generative AI"],"permalink":"/ai/review/2025-9-3-Jointly_Reinforcing_Diversity_and_Quality_in_Language_Model_Generations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Improving_Large_Vision_and_Language_Models_by_Learning_from_a_Panel_of_Peers","title":"[논문리뷰] Improving Large Vision and Language Models by Learning from a Panel of Peers","excerpt":"Simon Jenni이 [arXiv]에 게시한 'Improving Large Vision and Language Models by Learning from a Panel of Peers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Large Vision and Language Models (LVLMs)","Self-Improvement","Peer Learning","Preference Alignment","Reward Modeling","Multimodal Learning","Knowledge Transfer"],"permalink":"/ai/review/2025-9-3-Improving_Large_Vision_and_Language_Models_by_Learning_from_a_Panel_of_Peers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Implicit_Actor_Critic_Coupling_via_a_Supervised_Learning_Framework_for_RLVR","title":"[논문리뷰] Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR","excerpt":"Lu Wang이 [arXiv]에 게시한 'Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","RLVR","Large Language Models","Actor-Critic","Supervised Learning","Mathematical Reasoning","Policy Optimization","Cross-Entropy Loss"],"permalink":"/ai/review/2025-9-3-Implicit_Actor_Critic_Coupling_via_a_Supervised_Learning_Framework_for_RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-GenCompositor_Generative_Video_Compositing_with_Diffusion_Transformer","title":"[논문리뷰] GenCompositor: Generative Video Compositing with Diffusion Transformer","excerpt":"Lingen Li이 [arXiv]에 게시한 'GenCompositor: Generative Video Compositing with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Video Compositing","Diffusion Transformer","Generative Models","Video Editing","Position Embedding","Diffusion Models","Masked Token Injection","Video Harmonization"],"permalink":"/ai/review/2025-9-3-GenCompositor_Generative_Video_Compositing_with_Diffusion_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-FlashAdventure_A_Benchmark_for_GUI_Agents_Solving_Full_Story_Arcs_in_Diverse_Adventure_Games","title":"[논문리뷰] FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games","excerpt":"Dongmin Park이 [arXiv]에 게시한 'FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","GUI Agents","Adventure Games","Benchmark","Full Story Arc","Observation-Behavior Gap","LLMs","Automated Evaluation"],"permalink":"/ai/review/2025-9-3-FlashAdventure_A_Benchmark_for_GUI_Agents_Solving_Full_Story_Arcs_in_Diverse_Adventure_Games/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-FastFit_Accelerating_Multi-Reference_Virtual_Try-On_via_Cacheable_Diffusion_Models","title":"[논문리뷰] FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models","excerpt":"Zhen Wang이 [arXiv]에 게시한 'FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Diffusion Models","Cacheable Architecture","Multi-Reference","Semi-Attention","Efficiency","Image Synthesis"],"permalink":"/ai/review/2025-9-3-FastFit_Accelerating_Multi-Reference_Virtual_Try-On_via_Cacheable_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Fantastic_Pretraining_Optimizers_and_Where_to_Find_Them","title":"[논문리뷰] Fantastic Pretraining Optimizers and Where to Find Them","excerpt":"Percy Liang이 [arXiv]에 게시한 'Fantastic Pretraining Optimizers and Where to Find Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Deep Learning Optimizers","Large Language Models","Hyperparameter Tuning","Pretraining Speedup","Scaling Laws","AdamW","Matrix-based Optimizers","Data-to-Model Ratio"],"permalink":"/ai/review/2025-9-3-Fantastic_Pretraining_Optimizers_and_Where_to_Find_Them/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-ELV-Halluc_Benchmarking_Semantic_Aggregation_Hallucinations_in_Long_Video_Understanding","title":"[논문리뷰] ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding","excerpt":"Xuanyu Zheng이 [arXiv]에 게시한 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Long Video Understanding","Hallucination","Semantic Aggregation","Video MLLM","Benchmark","DPO","Positional Encoding","VideoQA"],"permalink":"/ai/review/2025-9-3-ELV-Halluc_Benchmarking_Semantic_Aggregation_Hallucinations_in_Long_Video_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Discrete_Noise_Inversion_for_Next-scale_Autoregressive_Text-based_Image_Editing","title":"[논문리뷰] Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing","excerpt":"Amin Heyrani Nobar이 [arXiv]에 게시한 'Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Image Editing","Autoregressive Models","Noise Inversion","Text-to-Image","Gumbel-max Trick","Training-free","Location-aware Argmax Inversion"],"permalink":"/ai/review/2025-9-3-Discrete_Noise_Inversion_for_Next-scale_Autoregressive_Text-based_Image_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-DCPO_Dynamic_Clipping_Policy_Optimization","title":"[논문리뷰] DCPO: Dynamic Clipping Policy Optimization","excerpt":"Kai Lu이 [arXiv]에 게시한 'DCPO: Dynamic Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM","Policy Optimization","Dynamic Clipping","Advantage Standardization","RLVR","Reasoning"],"permalink":"/ai/review/2025-9-3-DCPO_Dynamic_Clipping_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-C-DiffDet_Fusing_Global_Scene_Context_with_Generative_Denoising_for_High-Fidelity_Object_Detection","title":"[논문리뷰] C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection","excerpt":"Vito Renó이 [arXiv]에 게시한 'C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Object Detection","Diffusion Model","Global Scene Context","Context-Aware Fusion","Fine-grained Detection","Automotive Damage Assessment","Generative Denoising","Cross-Attention"],"permalink":"/ai/review/2025-9-3-C-DiffDet_Fusing_Global_Scene_Context_with_Generative_Denoising_for_High-Fidelity_Object_Detection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Benchmarking_Optimizers_for_Large_Language_Model_Pretraining","title":"[논문리뷰] Benchmarking Optimizers for Large Language Model Pretraining","excerpt":"mjaggi이 [arXiv]에 게시한 'Benchmarking Optimizers for Large Language Model Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","LLM Optimizers","Benchmarking","Hyperparameter Tuning","AdamW","AdEMAMix","MARS","Mixture of Experts (MoE)","Weight Decay"],"permalink":"/ai/review/2025-9-3-Benchmarking_Optimizers_for_Large_Language_Model_Pretraining/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Baichuan-M2_Scaling_Medical_Capability_with_Large_Verifier_System","title":"[논문리뷰] Baichuan-M2: Scaling Medical Capability with Large Verifier System","excerpt":"Jayok6이 [arXiv]에 게시한 'Baichuan-M2: Scaling Medical Capability with Large Verifier System' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Medical AI","LLM","Reinforcement Learning","Verifier System","Patient Simulator","Clinical Rubrics","Baichuan-M2","HealthBench"],"permalink":"/ai/review/2025-9-3-Baichuan-M2_Scaling_Medical_Capability_with_Large_Verifier_System/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-Attributes_as_Textual_Genes_Leveraging_LLMs_as_Genetic_Algorithm_Simulators_for_Conditional_Synthetic_Data_Generation","title":"[논문리뷰] Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation","excerpt":"Xiaolei Huang이 [arXiv]에 게시한 'Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Synthetic Data Generation","Large Language Models (LLMs)","Genetic Algorithms","Textual Data Augmentation","Active Learning","NLP","Data Diversity"],"permalink":"/ai/review/2025-9-3-Attributes_as_Textual_Genes_Leveraging_LLMs_as_Genetic_Algorithm_Simulators_for_Conditional_Synthetic_Data_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-3-AMBEDKAR-A_Multi-level_Bias_Elimination_through_a_Decoding_Approach_with_Knowledge_Augmentation_for_Robust_Constitutional_Alignment_of_Language_Models","title":"[논문리뷰] AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models","excerpt":"Rahul Karthikeyan이 [arXiv]에 게시한 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","lastModifiedAt":"2025-09-03 13:36:21+0900","categories":["Review"],"tags":["Review","Bias Mitigation","Large Language Models","Speculative Decoding","Constitutional AI","Fairness","Inference-Time Control","Indian Sociocultural Context"],"permalink":"/ai/review/2025-9-3-AMBEDKAR-A_Multi-level_Bias_Elimination_through_a_Decoding_Approach_with_Knowledge_Augmentation_for_Robust_Constitutional_Alignment_of_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-2-UI-Level_Evaluation_of_ALLaM_34B_Measuring_an_Arabic-Centric_LLM_via_HUMAIN_Chat","title":"[논문리뷰] UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat","excerpt":"Omartificial-Intelligence-Space이 [arXiv]에 게시한 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Arabic LLM","UI-level Evaluation","ALLaM 34B","HUMAIN Chat","Dialectal Arabic","LLM as a Judge","Safety Evaluation"],"permalink":"/ai/review/2025-9-2-UI-Level_Evaluation_of_ALLaM_34B_Measuring_an_Arabic-Centric_LLM_via_HUMAIN_Chat/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-2-T2R-bench_A_Benchmark_for_Generating_Article-Level_Reports_from_Real_World_Industrial_Tables","title":"[논문리뷰] T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables","excerpt":"Yu Zhao이 [arXiv]에 게시한 'T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Table-to-Report Generation","Large Language Models (LLMs)","Benchmark Dataset","Industrial Applications","Table Reasoning","Evaluation Metrics","Real-world Data"],"permalink":"/ai/review/2025-9-2-T2R-bench_A_Benchmark_for_Generating_Article-Level_Reports_from_Real_World_Industrial_Tables/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-2-PVPO_Pre-Estimated_Value-Based_Policy_Optimization_for_Agentic_Reasoning","title":"[논문리뷰] PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning","excerpt":"Yuewei Zhang이 [arXiv]에 게시한 'PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Critic-Free RL","Agentic Reasoning","Policy Optimization","Large Language Models (LLMs)","Advantage Estimation","Group Sampling","Static Value Estimation"],"permalink":"/ai/review/2025-9-2-PVPO_Pre-Estimated_Value-Based_Policy_Optimization_for_Agentic_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-2-No_Label_Left_Behind_A_Unified_Surface_Defect_Detection_Model_for_all_Supervision_Regimes","title":"[논문리뷰] No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes","excerpt":"Danijel Skočaj이 [arXiv]에 게시한 'No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Surface Defect Detection","Anomaly Detection","Mixed Supervision","Deep Learning","Industrial Inspection","Unified Model"],"permalink":"/ai/review/2025-9-2-No_Label_Left_Behind_A_Unified_Surface_Defect_Detection_Model_for_all_Supervision_Regimes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-2-How_Can_Input_Reformulation_Improve_Tool_Usage_Accuracy_in_a_Complex_Dynamic_Environment_A_Study_on_τ-bench","title":"[논문리뷰] How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench","excerpt":"Jayanth Srinivasa이 [arXiv]에 게시한 'How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","LLM Agents","Tool Use","Function Calling","Input Reformulation","Dynamic Environments","τ-bench","Context Engineering","Multi-Agent Framework"],"permalink":"/ai/review/2025-9-2-How_Can_Input_Reformulation_Improve_Tool_Usage_Accuracy_in_a_Complex_Dynamic_Environment_A_Study_on_τ-bench/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-2-From_reactive_to_cognitive_brain-inspired_spatial_intelligence_for_embodied_agents","title":"[논문리뷰] From reactive to cognitive: brain-inspired spatial intelligence for embodied agents","excerpt":"Songming Liu이 [arXiv]에 게시한 'From reactive to cognitive: brain-inspired spatial intelligence for embodied agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","lastModifiedAt":"2025-09-02 13:01:41+0900","categories":["Review"],"tags":["Review","Spatial Cognition","Embodied Agents","Brain-inspired AI","Cognitive Map","Spatial Memory","MLLMs","Navigation"],"permalink":"/ai/review/2025-9-2-From_reactive_to_cognitive_brain-inspired_spatial_intelligence_for_embodied_agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-UItron_Foundational_GUI_Agent_with_Advanced_Perception_and_Planning","title":"[논문리뷰] UItron: Foundational GUI Agent with Advanced Perception and Planning","excerpt":"Yufeng Zhong이 [arXiv]에 게시한 'UItron: Foundational GUI Agent with Advanced Perception and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","GUI Agent","Foundational Model","Multimodal LLM","Perception","Planning","Reinforcement Learning","Data Engineering","Chinese App Scenarios"],"permalink":"/ai/review/2025-9-1-UItron_Foundational_GUI_Agent_with_Advanced_Perception_and_Planning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-TiKMiX_Take_Data_Influence_into_Dynamic_Mixture_for_Language_Model_Pre-training","title":"[논문리뷰] TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training","excerpt":"Jiyao Deng이 [arXiv]에 게시한 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Language Model Pre-training","Dynamic Data Mixing","Data Influence","Group Influence","Optimization","Regression Model","LLM Training"],"permalink":"/ai/review/2025-9-1-TiKMiX_Take_Data_Influence_into_Dynamic_Mixture_for_Language_Model_Pre-training/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-Think_in_Games_Learning_to_Reason_in_Games_via_Reinforcement_Learning_with_Large_Language_Models","title":"[논문리뷰] Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models","excerpt":"Yifan Lu이 [arXiv]에 게시한 'Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Game AI","Procedural Knowledge","Declarative Knowledge","Explainable AI","Strategic Decision-Making"],"permalink":"/ai/review/2025-9-1-Think_in_Games_Learning_to_Reason_in_Games_via_Reinforcement_Learning_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-TalkVid_A_Large-Scale_Diversified_Dataset_for_Audio-Driven_Talking_Head_Synthesis","title":"[논문리뷰] TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis","excerpt":"Pengcheng Chen이 [arXiv]에 게시한 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Audio-Driven Talking Head Synthesis","Large-Scale Dataset","Data Diversity","Data Curation","Evaluation Benchmark","Generalization Gap","Algorithmic Fairness"],"permalink":"/ai/review/2025-9-1-TalkVid_A_Large-Scale_Diversified_Dataset_for_Audio-Driven_Talking_Head_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-R-4B_Incentivizing_General-Purpose_Auto-Thinking_Capability_in_MLLMs_via_Bi-Mode_Annealing_and_Reinforce_Learning","title":"[논문리뷰] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning","excerpt":"Han Hu이 [arXiv]에 게시한 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Auto-Thinking","Reinforcement Learning (RL)","Bi-mode Annealing","Bi-mode Policy Optimization (BPO)","General-Purpose AI","Reasoning","Efficiency"],"permalink":"/ai/review/2025-9-1-R-4B_Incentivizing_General-Purpose_Auto-Thinking_Capability_in_MLLMs_via_Bi-Mode_Annealing_and_Reinforce_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-Morae_Proactively_Pausing_UI_Agents_for_User_Choices","title":"[논문리뷰] Morae: Proactively Pausing UI Agents for User Choices","excerpt":"Amy Pavel이 [arXiv]에 게시한 'Morae: Proactively Pausing UI Agents for User Choices' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","UI Agents","Accessibility","Human-Agent Interaction","Mixed-Initiative AI","Large Multimodal Models","Proactive AI","User Choice","Blind and Low-Vision Users"],"permalink":"/ai/review/2025-9-1-Morae_Proactively_Pausing_UI_Agents_for_User_Choices/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-Mimicking_the_Physicists_EyeA_VLM-centric_Approach_for_Physics_Formula_Discovery","title":"[논문리뷰] Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery","excerpt":"Wenjie Zhou이 [arXiv]에 게시한 'Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Physics Formula Discovery","Multimodal AI","Vision-Language Models","Symbolic Regression","Causal Chain of Thought","Reinforcement Learning","Agentic AI"],"permalink":"/ai/review/2025-9-1-Mimicking_the_Physicists_EyeA_VLM-centric_Approach_for_Physics_Formula_Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-HERMES_Human-to-Robot_Embodied_Learning_from_Multi-Source_Motion_Data_for_Mobile_Dexterous_Manipulation","title":"[논문리뷰] HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation","excerpt":"Tianhai Liang이 [arXiv]에 게시한 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Dexterous Manipulation","Mobile Manipulation","Human-to-Robot Learning","Sim2Real","Reinforcement Learning","Depth Image","Visual Localization","Bimanual Control"],"permalink":"/ai/review/2025-9-1-HERMES_Human-to-Robot_Embodied_Learning_from_Multi-Source_Motion_Data_for_Mobile_Dexterous_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-EmbodiedOneVision_Interleaved_Vision-Text-Action_Pretraining_for_General_Robot_Control","title":"[논문리뷰] EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control","excerpt":"Zhaoqing Chen이 [arXiv]에 게시한 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Embodied AI","Robot Control","Vision-Language-Action Models","Multimodal Pretraining","Flow Matching","Foundation Models","Generalization","Real-world Robotics"],"permalink":"/ai/review/2025-9-1-EmbodiedOneVision_Interleaved_Vision-Text-Action_Pretraining_for_General_Robot_Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-Efficient_Code_Embeddings_from_Code_Generation_Models","title":"[논문리뷰] Efficient Code Embeddings from Code Generation Models","excerpt":"Han Xiao이 [arXiv]에 게시한 'Efficient Code Embeddings from Code Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Code Embeddings","Code Generation Models","Autoregressive Backbones","Last-Token Pooling","Instruction Tuning","Contrastive Learning","Retrieval-Augmented Generation","MTEB Benchmark"],"permalink":"/ai/review/2025-9-1-Efficient_Code_Embeddings_from_Code_Generation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-Droplet3D_Commonsense_Priors_from_Videos_Facilitate_3D_Generation","title":"[논문리뷰] Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation","excerpt":"Qi Jia이 [arXiv]에 게시한 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","3D Generation","Video Diffusion Models","Spatial Consistency","Semantic Knowledge","Multi-view Synthesis","Large-scale Dataset","Image-to-3D","Text-to-3D"],"permalink":"/ai/review/2025-9-1-Droplet3D_Commonsense_Priors_from_Videos_Facilitate_3D_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-CLIPSym_Delving_into_Symmetry_Detection_with_CLIP","title":"[논문리뷰] CLIPSym: Delving into Symmetry Detection with CLIP","excerpt":"Raymond A. Yeh이 [arXiv]에 게시한 'CLIPSym: Delving into Symmetry Detection with CLIP' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Symmetry Detection","Vision-Language Models","CLIP","Equivariant Networks","Prompt Engineering","Geometric Deep Learning"],"permalink":"/ai/review/2025-9-1-CLIPSym_Delving_into_Symmetry_Detection_with_CLIP/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-A_Survey_of_Scientific_Large_Language_Models_From_Data_Foundations_to_Agent_Frontiers","title":"[논문리뷰] A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers","excerpt":"Jiamin Wu이 [arXiv]에 게시한 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Scientific LLMs","AI for Science","Scientific Data","Agentic AI","Multimodal Integration","Knowledge Representation","Autonomous Discovery","Data Ecosystems"],"permalink":"/ai/review/2025-9-1-A_Survey_of_Scientific_Large_Language_Models_From_Data_Foundations_to_Agent_Frontiers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-AHELM_A_Holistic_Evaluation_of_Audio-Language_Models","title":"[논문리뷰] AHELM: A Holistic Evaluation of Audio-Language Models","excerpt":"Siwei Yang이 [arXiv]에 게시한 'AHELM: A Holistic Evaluation of Audio-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","Audio-Language Models","Holistic Evaluation","Benchmarking","Multimodality","Fairness","Robustness","Reasoning","Bias Detection"],"permalink":"/ai/review/2025-9-1-AHELM_A_Holistic_Evaluation_of_Audio-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-9-1-A.S.E_A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code","title":"[논문리뷰] A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code","excerpt":"Libo Chen이 [arXiv]에 게시한 'A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","lastModifiedAt":"2025-09-01 13:14:34+0900","categories":["Review"],"tags":["Review","AI-Generated Code Security","LLM Evaluation","Repository-Level Benchmark","Code Security","Vulnerability Detection","Static Analysis","Reproducibility","Context-Awareness"],"permalink":"/ai/review/2025-9-1-A.S.E_A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-rStar2-Agent_Agentic_Reasoning_Technical_Report","title":"[논문리뷰] rStar2-Agent: Agentic Reasoning Technical Report","excerpt":"Weijiang Xu이 [arXiv]에 게시한 'rStar2-Agent: Agentic Reasoning Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Agentic Reinforcement Learning","Math Reasoning","Code Interpreter","Tool Use","GRPO-RoC","LLM Training Efficiency","Self-Reflection"],"permalink":"/ai/review/2025-8-29-rStar2-Agent_Agentic_Reasoning_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-USO_Unified_Style_and_Subject-Driven_Generation_via_Disentangled_and_Reward_Learning","title":"[논문리뷰] USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning","excerpt":"Jiahe Tian이 [arXiv]에 게시한 'USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Style-Driven Generation","Subject-Driven Generation","Disentangled Representation","Reward Learning","Cross-Task Learning","Diffusion Models","Image Customization","Unified Framework"],"permalink":"/ai/review/2025-8-29-USO_Unified_Style_and_Subject-Driven_Generation_via_Disentangled_and_Reward_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-Turning_the_Spell_Around_Lightweight_Alignment_Amplification_via_Rank-One_Safety_Injection","title":"[논문리뷰] Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection","excerpt":"Bernard Ghanem이 [arXiv]에 게시한 'Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","LLM Safety","Alignment Amplification","Rank-One Update","Mechanistic Interpretability","Weight Steering","Jailbreak Robustness","Fine-tuning-free","Safety Injection"],"permalink":"/ai/review/2025-8-29-Turning_the_Spell_Around_Lightweight_Alignment_Amplification_via_Rank-One_Safety_Injection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-TCIA_A_Task-Centric_Instruction_Augmentation_Method_for_Instruction_Finetuning","title":"[논문리뷰] TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning","excerpt":"Simin Ma이 [arXiv]에 게시한 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Instruction Augmentation","Fine-tuning","Large Language Models","Task-Centric","Data Diversity","Task Alignment","Breadth-First Search","Constraint Generation"],"permalink":"/ai/review/2025-8-29-TCIA_A_Task-Centric_Instruction_Augmentation_Method_for_Instruction_Finetuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-ROSE_Remove_Objects_with_Side_Effects_in_Videos","title":"[논문리뷰] ROSE: Remove Objects with Side Effects in Videos","excerpt":"Hantang Liu이 [arXiv]에 게시한 'ROSE: Remove Objects with Side Effects in Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Video Object Removal","Side Effects","3D Rendering","Diffusion Transformer","Video Inpainting","Synthetic Data","Difference Mask"],"permalink":"/ai/review/2025-8-29-ROSE_Remove_Objects_with_Side_Effects_in_Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-Provable_Benefits_of_In-Tool_Learning_for_Large_Language_Models","title":"[논문리뷰] Provable Benefits of In-Tool Learning for Large Language Models","excerpt":"Vivien Cabannes이 [arXiv]에 게시한 'Provable Benefits of In-Tool Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Large Language Models","In-Tool Learning","In-Weight Learning","Factual Recall","Retrieval-Augmented Generation","Scaling Laws","Parameter Efficiency","Catastrophic Forgetting"],"permalink":"/ai/review/2025-8-29-Provable_Benefits_of_In-Tool_Learning_for_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-Pref-GRPO_Pairwise_Preference_Reward-based_GRPO_for_Stable_Text-to-Image_Reinforcement_Learning","title":"[논문리뷰] Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning","excerpt":"Jiazi Bu이 [arXiv]에 게시한 'Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Text-to-Image Generation","GRPO","Reward Hacking","Pairwise Preference","Reward Model","Stable Optimization","UniGenBench"],"permalink":"/ai/review/2025-8-29-Pref-GRPO_Pairwise_Preference_Reward-based_GRPO_for_Stable_Text-to-Image_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-Persuasion_Dynamics_in_LLMs_Investigating_Robustness_and_Adaptability_in_Knowledge_and_Safety_with_DuET-PD","title":"[논문리뷰] Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD","excerpt":"Roy Ka-Wei Lee이 [arXiv]에 게시한 'Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Persuasion Dynamics","Large Language Models (LLMs)","Robustness","Gullibility","Receptiveness","Direct Preference Optimization (DPO)","Safety Alignment","Multi-turn Dialogue"],"permalink":"/ai/review/2025-8-29-Persuasion_Dynamics_in_LLMs_Investigating_Robustness_and_Adaptability_in_Knowledge_and_Safety_with_DuET-PD/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-OneReward_Unified_Mask-Guided_Image_Generation_via_Multi-Task_Human_Preference_Learning","title":"[논문리뷰] OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning","excerpt":"Yitong Wang이 [arXiv]에 게시한 'OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Image Generation","Mask-Guided Editing","Reinforcement Learning","Human Preference Learning","Vision-Language Models","Multi-Task Learning","Flow Matching"],"permalink":"/ai/review/2025-8-29-OneReward_Unified_Mask-Guided_Image_Generation_via_Multi-Task_Human_Preference_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-OnGoal_Tracking_and_Visualizing_Conversational_Goals_in_Multi-Turn_Dialogue_with_Large_Language_Models","title":"[논문리뷰] OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models","excerpt":"Alex Endert이 [arXiv]에 게시한 'OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Human-Computer Interaction (HCI)","Conversational AI","Goal Tracking","Visualization","Multi-Turn Dialogue","User Interface Design","Sensemaking"],"permalink":"/ai/review/2025-8-29-OnGoal_Tracking_and_Visualizing_Conversational_Goals_in_Multi-Turn_Dialogue_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-Multi-View_3D_Point_Tracking","title":"[논문리뷰] Multi-View 3D Point Tracking","excerpt":"Irem Demir이 [arXiv]에 게시한 'Multi-View 3D Point Tracking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","3D Point Tracking","Multi-View","Transformer","kNN Correlation","Depth Estimation","Dynamic Scenes","Occlusion Handling","Feature Fusion"],"permalink":"/ai/review/2025-8-29-Multi-View_3D_Point_Tracking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-Mixture_of_Contexts_for_Long_Video_Generation","title":"[논문리뷰] Mixture of Contexts for Long Video Generation","excerpt":"Junfei Xiao이 [arXiv]에 게시한 'Mixture of Contexts for Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Long Video Generation","Diffusion Transformers (DiT)","Sparse Attention","Context Routing","Memory Management","Generative Models","Video Synthesis"],"permalink":"/ai/review/2025-8-29-Mixture_of_Contexts_for_Long_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-MCP-Bench_Benchmarking_Tool-Using_LLM_Agents_with_Complex_Real-World_Tasks_via_MCP_Servers","title":"[논문리뷰] MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers","excerpt":"Shashank Biju이 [arXiv]에 게시한 'MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","LLM Agents","Tool Use","Benchmarking","Model Context Protocol (MCP)","Cross-Domain Orchestration","Fuzzy Instructions","Multi-Step Tasks","Real-World Scenarios"],"permalink":"/ai/review/2025-8-29-MCP-Bench_Benchmarking_Tool-Using_LLM_Agents_with_Complex_Real-World_Tasks_via_MCP_Servers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-FakeParts_a_New_Family_of_AI-Generated_DeepFakes","title":"[논문리뷰] FakeParts: a New Family of AI-Generated DeepFakes","excerpt":"Xi Wang이 [arXiv]에 게시한 'FakeParts: a New Family of AI-Generated DeepFakes' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Deepfake Detection","Partial Deepfakes","AI-Generated Video","Benchmark Dataset","Video Forensics","Generative Models","Manipulation Detection","Human Perception"],"permalink":"/ai/review/2025-8-29-FakeParts_a_New_Family_of_AI-Generated_DeepFakes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-DressDance_Dress_up_and_Dance_as_You_Like_It_-_Technical_Preview","title":"[논문리뷰] Dress&Dance: Dress up and Dance as You Like It - Technical Preview","excerpt":"Yu-Xiong Wang이 [arXiv]에 게시한 'Dress&Dance: Dress up and Dance as You Like It - Technical Preview' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Video Diffusion","Multi-modal Conditioning","Garment Transfer","Pose Animation","Generative AI","Fashion Tech","CondNet"],"permalink":"/ai/review/2025-8-29-DressDance_Dress_up_and_Dance_as_You_Like_It_-_Technical_Preview/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-Collaborative_Multi-Modal_Coding_for_High-Quality_3D_Generation","title":"[논문리뷰] Collaborative Multi-Modal Coding for High-Quality 3D Generation","excerpt":"Ziwei Liu이 [arXiv]에 게시한 'Collaborative Multi-Modal Coding for High-Quality 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","3D Generation","Multi-modal Learning","Diffusion Models","Triplane Representation","Collaborative Coding","Image-to-3D","Latent Space"],"permalink":"/ai/review/2025-8-29-Collaborative_Multi-Modal_Coding_for_High-Quality_3D_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-CogVLA_Cognition-Aligned_Vision-Language-Action_Model_via_Instruction-Driven_Routing_Sparsification","title":"[논문리뷰] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification","excerpt":"Liqiang Nie이 [arXiv]에 게시한 'CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Model","Sparsification","Instruction-Driven Routing","Cognition-Aligned AI","Robotics","Computational Efficiency","Multimodal AI"],"permalink":"/ai/review/2025-8-29-CogVLA_Cognition-Aligned_Vision-Language-Action_Model_via_Instruction-Driven_Routing_Sparsification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-29-AWorld_Orchestrating_the_Training_Recipe_for_Agentic_AI","title":"[논문리뷰] AWorld: Orchestrating the Training Recipe for Agentic AI","excerpt":"Qintong Wu이 [arXiv]에 게시한 'AWorld: Orchestrating the Training Recipe for Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","lastModifiedAt":"2025-08-29 13:14:44+0900","categories":["Review"],"tags":["Review","Agentic AI","Reinforcement Learning","Distributed Systems","Experience Generation","LLM Fine-tuning","GAIA Benchmark","Scalability","AWORLD Framework"],"permalink":"/ai/review/2025-8-29-AWorld_Orchestrating_the_Training_Recipe_for_Agentic_AI/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Taming_the_Chaos_Coordinated_Autoscaling_for_Heterogeneous_and_Disaggregated_LLM_Inference","title":"[논문리뷰] Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference","excerpt":"Chunlei Han이 [arXiv]에 게시한 'Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","LLM Inference","Autoscaling","Disaggregated Architecture","Heterogeneous Hardware","Resource Management","Topology-aware Scheduling","GPU Utilization"],"permalink":"/ai/review/2025-8-28-Taming_the_Chaos_Coordinated_Autoscaling_for_Heterogeneous_and_Disaggregated_LLM_Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-StepWiser_Stepwise_Generative_Judges_for_Wiser_Reasoning","title":"[논문리뷰] StepWiser: Stepwise Generative Judges for Wiser Reasoning","excerpt":"Olga Golovneva이 [arXiv]에 게시한 'StepWiser: Stepwise Generative Judges for Wiser Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Process Reward Models","Reinforcement Learning","Generative Judges","Stepwise Feedback","Chain-of-Thought","Meta-Reasoning"],"permalink":"/ai/review/2025-8-28-StepWiser_Stepwise_Generative_Judges_for_Wiser_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Self-Rewarding_Vision-Language_Model_via_Reasoning_Decomposition","title":"[논문리뷰] Self-Rewarding Vision-Language Model via Reasoning Decomposition","excerpt":"Zhenwen Liang이 [arXiv]에 게시한 'Self-Rewarding Vision-Language Model via Reasoning Decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Reinforcement Learning","Self-Rewarding","Reasoning Decomposition","Visual Perception","Language Reasoning","Hallucinations","Language Shortcuts"],"permalink":"/ai/review/2025-8-28-Self-Rewarding_Vision-Language_Model_via_Reasoning_Decomposition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Predicting_the_Order_of_Upcoming_Tokens_Improves_Language_Modeling","title":"[논문리뷰] Predicting the Order of Upcoming Tokens Improves Language Modeling","excerpt":"Alham Fikri Aji이 [arXiv]에 게시한 'Predicting the Order of Upcoming Tokens Improves Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Language Modeling","Next-Token Prediction","Multi-Token Prediction","Token Order Prediction","Auxiliary Objective","Learning-to-Rank","Transformer","Large Language Models"],"permalink":"/ai/review/2025-8-28-Predicting_the_Order_of_Upcoming_Tokens_Improves_Language_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-MotionFlux_Efficient_Text-Guided_Motion_Generation_through_Rectified_Flow_Matching_and_Preference_Alignment","title":"[논문리뷰] MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment","excerpt":"An-An Liu이 [arXiv]에 게시한 'MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Text-Guided Motion Generation","Rectified Flow Matching","Preference Alignment","Human Motion Synthesis","Real-time AI","Transformer Architecture","Self-supervised Learning"],"permalink":"/ai/review/2025-8-28-MotionFlux_Efficient_Text-Guided_Motion_Generation_through_Rectified_Flow_Matching_and_Preference_Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Mind_the_Third_Eye_Benchmarking_Privacy_Awareness_in_MLLM-powered_Smartphone_Agents","title":"[논문리뷰] Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents","excerpt":"Yue Yao이 [arXiv]에 게시한 'Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Multimodal LLMs (MLLMs)","Smartphone Agents","Privacy Awareness","Benchmarking","Sensitive Data Detection","Risk Assessment","UI Automation"],"permalink":"/ai/review/2025-8-28-Mind_the_Third_Eye_Benchmarking_Privacy_Awareness_in_MLLM-powered_Smartphone_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-MIDAS_Multimodal_Interactive_Digital-human_Synthesis_via_Real-time_Autoregressive_Video_Generation","title":"[논문리뷰] MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation","excerpt":"Yan Zhou이 [arXiv]에 게시한 'MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Multimodal Generation","Digital Human Synthesis","Real-time Video Generation","Autoregressive LLM","Diffusion Models","Deep Compression Autoencoder","Exposure Bias Mitigation","Streaming Inference"],"permalink":"/ai/review/2025-8-28-MIDAS_Multimodal_Interactive_Digital-human_Synthesis_via_Real-time_Autoregressive_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Gaze_into_the_Heart_A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation","title":"[논문리뷰] Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation","excerpt":"Anton Ivaschenko이 [arXiv]에 게시한 'Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","rPPG","Multi-View Video Dataset","Health Biomarkers","Physiological Monitoring","Deep Learning","Telemedicine","Biosignals"],"permalink":"/ai/review/2025-8-28-Gaze_into_the_Heart_A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Discrete_Diffusion_VLA_Bringing_Discrete_Diffusion_to_Action_Decoding_in_Vision-Language-Action_Policies","title":"[논문리뷰] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies","excerpt":"Sitong Mao이 [arXiv]에 게시한 'Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Discrete Diffusion","Action Decoding","Transformer","Robot Control","Masked Modeling","Adaptive Decoding","Reinforcement Learning"],"permalink":"/ai/review/2025-8-28-Discrete_Diffusion_VLA_Bringing_Discrete_Diffusion_to_Action_Decoding_in_Vision-Language-Action_Policies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Diffusion_Language_Models_Know_the_Answer_Before_Decoding","title":"[논문리뷰] Diffusion Language Models Know the Answer Before Decoding","excerpt":"Shilin Yan이 [arXiv]에 게시한 'Diffusion Language Models Know the Answer Before Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","DLM Acceleration","Early Answer Convergence","Early Commit Decoding","Confidence Gap","Inference Speedup","Training-Free"],"permalink":"/ai/review/2025-8-28-Diffusion_Language_Models_Know_the_Answer_Before_Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-DeepScholar-Bench_A_Live_Benchmark_and_Automated_Evaluation_for_Generative_Research_Synthesis","title":"[논문리뷰] DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis","excerpt":"Ion Stoica이 [arXiv]에 게시한 'DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Generative Research Synthesis","Live Benchmark","Automated Evaluation","LLM-as-a-judge","Related Work Generation","Retrieval-Augmented Generation","Verifiability"],"permalink":"/ai/review/2025-8-28-DeepScholar-Bench_A_Live_Benchmark_and_Automated_Evaluation_for_Generative_Research_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-CODA_Coordinating_the_Cerebrum_and_Cerebellum_for_a_Dual-Brain_Computer_Use_Agent_with_Decoupled_Reinforcement_Learning","title":"[논문리뷰] CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning","excerpt":"Jianze Liang이 [arXiv]에 게시한 'CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","GUI Agents","Reinforcement Learning","Planner-Executor Architecture","Decoupled Training","Large Vision-Language Models","Specialization","Generalization","Computer Use Agent"],"permalink":"/ai/review/2025-8-28-CODA_Coordinating_the_Cerebrum_and_Cerebellum_for_a_Dual-Brain_Computer_Use_Agent_with_Decoupled_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-Beyond_Transcription_Mechanistic_Interpretability_in_ASR","title":"[논문리뷰] Beyond Transcription: Mechanistic Interpretability in ASR","excerpt":"Aviv Shamsian이 [arXiv]에 게시한 'Beyond Transcription: Mechanistic Interpretability in ASR' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","ASR","Mechanistic Interpretability","Logit Lens","Linear Probing","Activation Patching","Hallucinations","Repetitions","Encoder-Decoder"],"permalink":"/ai/review/2025-8-28-Beyond_Transcription_Mechanistic_Interpretability_in_ASR/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-28-AudioStory_Generating_Long-Form_Narrative_Audio_with_Large_Language_Models","title":"[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models","excerpt":"Yixiao Ge이 [arXiv]에 게시한 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","lastModifiedAt":"2025-08-28 13:10:39+0900","categories":["Review"],"tags":["Review","Text-to-Audio","Long-Form Audio Generation","Large Language Models","Narrative Reasoning","Diffusion Models","Multimodal AI","Progressive Training"],"permalink":"/ai/review/2025-8-28-AudioStory_Generating_Long-Form_Narrative_Audio_with_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Wan-S2V_Audio-Driven_Cinematic_Video_Generation","title":"[논문리뷰] Wan-S2V: Audio-Driven Cinematic Video Generation","excerpt":"Chaonan Ji이 [arXiv]에 게시한 'Wan-S2V: Audio-Driven Cinematic Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Audio-Driven Video Generation","Cinematic Video","Diffusion Models","Transformer Architecture","Long Video Consistency","Human Animation","Multimodal Control","Data Curation"],"permalink":"/ai/review/2025-8-27-Wan-S2V_Audio-Driven_Cinematic_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-VoxHammer_Training-Free_Precise_and_Coherent_3D_Editing_in_Native_3D_Space","title":"[논문리뷰] VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space","excerpt":"Rui Chen이 [arXiv]에 게시한 'VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Editing","Training-Free","Diffusion Models","Latent Space","3D Inversion","Contextual Feature Replacement","3D Consistency","Edit3D-Bench"],"permalink":"/ai/review/2025-8-27-VoxHammer_Training-Free_Precise_and_Coherent_3D_Editing_in_Native_3D_Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-VibeVoice_Technical_Report","title":"[논문리뷰] VibeVoice Technical Report","excerpt":"Yaoyao Chang이 [arXiv]에 게시한 'VibeVoice Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Speech Synthesis","Long-form Audio","Multi-speaker","Next-token Diffusion","Speech Tokenizer","Large Language Model","Variational Autoencoder","Audio Compression"],"permalink":"/ai/review/2025-8-27-VibeVoice_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Unraveling_the_cognitive_patterns_of_Large_Language_Models_through_module_communities","title":"[논문리뷰] Unraveling the cognitive patterns of Large Language Models through module communities","excerpt":"Jianxi Gao이 [arXiv]에 게시한 'Unraveling the cognitive patterns of Large Language Models through module communities' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Network Community Structure","Cognitive Skills","AI Interpretability","Module Communities","Fine-tuning","Neural Plasticity"],"permalink":"/ai/review/2025-8-27-Unraveling_the_cognitive_patterns_of_Large_Language_Models_through_module_communities/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-UltraMemV2_Memory_Networks_Scaling_to_120B_Parameters_with_Superior_Long-Context_Learning","title":"[논문리뷰] UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning","excerpt":"Ran Guo이 [arXiv]에 게시한 'UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Memory Networks","Mixture of Experts (MoE)","Long-Context Learning","Sparse Models","Transformer Architecture","LLMs","Efficient Inference"],"permalink":"/ai/review/2025-8-27-UltraMemV2_Memory_Networks_Scaling_to_120B_Parameters_with_Superior_Long-Context_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-TreePO_Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic_Tree-based_Modeling","title":"[논문리뷰] TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling","excerpt":"Zhoufutu Wen이 [arXiv]에 게시한 'TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Policy Optimization","Large Language Models","Inference Efficiency","Tree Search","Segment-level Decoding","Advantage Estimation","Reasoning"],"permalink":"/ai/review/2025-8-27-TreePO_Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic_Tree-based_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Training_Language_Model_Agents_to_Find_Vulnerabilities_with_CTF-Dojo","title":"[논문리뷰] Training Language Model Agents to Find Vulnerabilities with CTF-Dojo","excerpt":"Zijian Wang이 [arXiv]에 게시한 'Training Language Model Agents to Find Vulnerabilities with CTF-Dojo' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","LLM Agents","Cybersecurity","CTF Challenges","Vulnerability Detection","Execution Environments","Docker","Automated Training","Verifiable Feedback"],"permalink":"/ai/review/2025-8-27-Training_Language_Model_Agents_to_Find_Vulnerabilities_with_CTF-Dojo/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-ThinkDial_An_Open_Recipe_for_Controlling_Reasoning_Effort_in_Large_Language_Models","title":"[논문리뷰] ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models","excerpt":"Jiangjie Chen이 [arXiv]에 게시한 'ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","LLMs","Controllable Reasoning","Computational Efficiency","Reinforcement Learning","Supervised Fine-tuning","Reasoning Compression","Budget-Aware Training"],"permalink":"/ai/review/2025-8-27-ThinkDial_An_Open_Recipe_for_Controlling_Reasoning_Effort_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Spacer_Towards_Engineered_Scientific_Inspiration","title":"[논문리뷰] Spacer: Towards Engineered Scientific Inspiration","excerpt":"zerojun48이 [arXiv]에 게시한 'Spacer: Towards Engineered Scientific Inspiration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Scientific Discovery","Large Language Models (LLMs)","Decontextualization","Keyword Graph","Multi-Agent System","Scientific Ideation","Research Automation","Inspiration Engine"],"permalink":"/ai/review/2025-8-27-Spacer_Towards_Engineered_Scientific_Inspiration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-ReportBench_Evaluating_Deep_Research_Agents_via_Academic_Survey_Tasks","title":"[논문리뷰] ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks","excerpt":"Kai Jia이 [arXiv]에 게시한 'ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Deep Research Agents","LLM Evaluation","Academic Survey","Factual Accuracy","Citation Verification","Report Generation","Benchmark","Hallucination"],"permalink":"/ai/review/2025-8-27-ReportBench_Evaluating_Deep_Research_Agents_via_Academic_Survey_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-QueryBandits_for_Hallucination_Mitigation_Exploiting_Semantic_Features_for_No-Regret_Rewriting","title":"[논문리뷰] QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting","excerpt":"Manuela Veloso이 [arXiv]에 게시한 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Hallucination Mitigation","Large Language Models","Contextual Bandits","Query Rewriting","Semantic Features","No-Regret Learning"],"permalink":"/ai/review/2025-8-27-QueryBandits_for_Hallucination_Mitigation_Exploiting_Semantic_Features_for_No-Regret_Rewriting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Pixie_Fast_and_Generalizable_Supervised_Learning_of_3D_Physics_from_Pixels","title":"[논문리뷰] Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels","excerpt":"Dinesh Jayaraman이 [arXiv]에 게시한 'Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Physics Prediction","Supervised Learning","CLIP Features","Neural Radiance Fields","Material Point Method","PIXIEVERSE Dataset","Zero-Shot Generalization"],"permalink":"/ai/review/2025-8-27-Pixie_Fast_and_Generalizable_Supervised_Learning_of_3D_Physics_from_Pixels/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Optimal_Sparsity_of_Mixture-of-Experts_Language_Models_for_Reasoning_Tasks","title":"[논문리뷰] Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks","excerpt":"Daisuke Nohara이 [arXiv]에 게시한 'Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","Sparsity","Scaling Laws","Reasoning Tasks","Memorization","Large Language Models","Generalization Gap","Top-k Routing"],"permalink":"/ai/review/2025-8-27-Optimal_Sparsity_of_Mixture-of-Experts_Language_Models_for_Reasoning_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-OmniHuman-1.5_Instilling_an_Active_Mind_in_Avatars_via_Cognitive_Simulation","title":"[논문리뷰] OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation","excerpt":"Jiaqi Yang이 [arXiv]에 게시한 'OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Video Avatar Generation","Cognitive Simulation","Multimodal Large Language Models (MLLMs)","Diffusion Transformers (DiT)","Multimodal Fusion","Human Motion Synthesis","Contextual Animation"],"permalink":"/ai/review/2025-8-27-OmniHuman-1.5_Instilling_an_Active_Mind_in_Avatars_via_Cognitive_Simulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-ObjFiller-3D_Consistent_Multi-view_3D_Inpainting_via_Video_Diffusion_Models","title":"[논문리뷰] ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models","excerpt":"Beiqi Chen이 [arXiv]에 게시한 'ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Inpainting","Multi-view Consistency","Video Diffusion Models","3D Object Completion","Generative Models","LoRA","3D Gaussian Splatting"],"permalink":"/ai/review/2025-8-27-ObjFiller-3D_Consistent_Multi-view_3D_Inpainting_via_Video_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-MovieCORE_COgnitive_REasoning_in_Movies","title":"[논문리뷰] MovieCORE: COgnitive REasoning in Movies","excerpt":"Hung-Ting Su이 [arXiv]에 게시한 'MovieCORE: COgnitive REasoning in Movies' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Video Question Answering (VQA)","Cognitive Reasoning","System-2 Thinking","Multi-agent LLMs","Dataset Creation","Movie Understanding","Cinematic Content","Agentic Enhancement"],"permalink":"/ai/review/2025-8-27-MovieCORE_COgnitive_REasoning_in_Movies/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-FastMeshEfficient_Artistic_Mesh_Generation_via_Component_Decoupling","title":"[논문리뷰] FastMesh:Efficient Artistic Mesh Generation via Component Decoupling","excerpt":"Xingang Pan이 [arXiv]에 게시한 'FastMesh:Efficient Artistic Mesh Generation via Component Decoupling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","3D Mesh Generation","Component Decoupling","Autoregressive Models","Bidirectional Transformer","Fidelity Enhancement","Prediction Filtering","Token Efficiency","Artistic Meshes"],"permalink":"/ai/review/2025-8-27-FastMeshEfficient_Artistic_Mesh_Generation_via_Component_Decoupling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Demystifying_Scientific_Problem-Solving_in_LLMs_by_Probing_Knowledge_and_Reasoning","title":"[논문리뷰] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning","excerpt":"Arman Cohan이 [arXiv]에 게시한 'Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Scientific Reasoning","Knowledge Retrieval","Reasoning Probing","Benchmarks","Chain-of-Thought","Fine-tuning"],"permalink":"/ai/review/2025-8-27-Demystifying_Scientific_Problem-Solving_in_LLMs_by_Probing_Knowledge_and_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-ClaimGen-CN_A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation","title":"[논문리뷰] ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation","excerpt":"Kun Kuang이 [arXiv]에 게시한 'ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Legal AI","Natural Language Processing","Claim Generation","Chinese Legal Dataset","Factuality","Clarity","Large Language Models","Zero-shot Evaluation"],"permalink":"/ai/review/2025-8-27-ClaimGen-CN_A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-CineScale_Free_Lunch_in_High-Resolution_Cinematic_Visual_Generation","title":"[논문리뷰] CineScale: Free Lunch in High-Resolution Cinematic Visual Generation","excerpt":"Ziwei Liu이 [arXiv]에 게시한 'CineScale: Free Lunch in High-Resolution Cinematic Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Diffusion Models","High-Resolution Generation","Image Generation","Video Generation","UNet Architecture","DiT Architecture","Scale Fusion","LoRA Fine-tuning"],"permalink":"/ai/review/2025-8-27-CineScale_Free_Lunch_in_High-Resolution_Cinematic_Visual_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-CMPhysBench_A_Benchmark_for_Evaluating_Large_Language_Models_in_Condensed_Matter_Physics","title":"[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics","excerpt":"Dongchen Huang이 [arXiv]에 게시한 'CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Large Language Models","Condensed Matter Physics","Benchmark","Scientific Reasoning","Evaluation Metric","Expression Edit Distance","Problem Solving"],"permalink":"/ai/review/2025-8-27-CMPhysBench_A_Benchmark_for_Evaluating_Large_Language_Models_in_Condensed_Matter_Physics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-27-Autoregressive_Universal_Video_Segmentation_Model","title":"[논문리뷰] Autoregressive Universal Video Segmentation Model","excerpt":"Albert Gu이 [arXiv]에 게시한 'Autoregressive Universal Video Segmentation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","lastModifiedAt":"2025-08-27 13:22:18+0900","categories":["Review"],"tags":["Review","Video Segmentation","Autoregressive Model","Universal Model","State Space Models","Mamba","Parallel Training","Streaming Video","Deep Learning"],"permalink":"/ai/review/2025-8-27-Autoregressive_Universal_Video_Segmentation_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-Visual-CoG_Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation","title":"[논문리뷰] Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation","excerpt":"Haoxiang Shi이 [arXiv]에 게시한 'Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reinforcement Learning","Chain of Thought","Multimodal LLMs","Stage-Aware Rewards","Semantic Reasoning","Generative AI"],"permalink":"/ai/review/2025-8-26-Visual-CoG_Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-UQ_Assessing_Language_Models_on_Unsolved_Questions","title":"[논문리뷰] UQ: Assessing Language Models on Unsolved Questions","excerpt":"Wei Liu이 [arXiv]에 게시한 'UQ: Assessing Language Models on Unsolved Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Unsolved Questions","AI Benchmark","Oracle-Free Validation","Generator-Validator Gap","Community Evaluation","Stack Exchange"],"permalink":"/ai/review/2025-8-26-UQ_Assessing_Language_Models_on_Unsolved_Questions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-TaDiCodec_Text-aware_Diffusion_Speech_Tokenizer_for_Speech_Language_Modeling","title":"[논문리뷰] TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling","excerpt":"Jiaqi Li이 [arXiv]에 게시한 'TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Speech Tokenizer","Diffusion Model","Text-to-Speech","Speech Language Modeling","Low Bitrate Codec","End-to-End Training","Binary Spherical Quantization"],"permalink":"/ai/review/2025-8-26-TaDiCodec_Text-aware_Diffusion_Speech_Tokenizer_for_Speech_Language_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-T2I-ReasonBench_Benchmarking_Reasoning-Informed_Text-to-Image_Generation","title":"[논문리뷰] T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation","excerpt":"Xihui Liu이 [arXiv]에 게시한 'T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Reasoning Benchmark","Idiom Interpretation","Textual Image Design","Entity Reasoning","Scientific Reasoning","Multimodal LLM Evaluation"],"permalink":"/ai/review/2025-8-26-T2I-ReasonBench_Benchmarking_Reasoning-Informed_Text-to-Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-SpotEdit_Evaluating_Visually-Guided_Image_Editing_Methods","title":"[논문리뷰] SpotEdit: Evaluating Visually-Guided Image Editing Methods","excerpt":"Ersin Yumer이 [arXiv]에 게시한 'SpotEdit: Evaluating Visually-Guided Image Editing Methods' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Visually-Guided Image Editing","Multimodal Models","Benchmark","Hallucination","Diffusion Models","Autoregressive Models","Evaluation Metrics"],"permalink":"/ai/review/2025-8-26-SpotEdit_Evaluating_Visually-Guided_Image_Editing_Methods/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-ST-Raptor_LLM-Powered_Semi-Structured_Table_Question_Answering","title":"[논문리뷰] ST-Raptor: LLM-Powered Semi-Structured Table Question Answering","excerpt":"Wei Zhou이 [arXiv]에 게시한 'ST-Raptor: LLM-Powered Semi-Structured Table Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Semi-structured Tables","Question Answering","LLMs","Hierarchical Orthogonal Tree","Table Layout Understanding","Pipeline Generation","Verification Mechanism"],"permalink":"/ai/review/2025-8-26-ST-Raptor_LLM-Powered_Semi-Structured_Table_Question_Answering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-PosterGen_Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs","title":"[논문리뷰] PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs","excerpt":"Chenyu You이 [arXiv]에 게시한 'PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Multi-Agent LLMs","Academic Poster Generation","Aesthetic Design","Layout Optimization","Typography","Color Palette","VLM-as-Judge","Content Fidelity"],"permalink":"/ai/review/2025-8-26-PosterGen_Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-Neither_Valid_nor_Reliable_Investigating_the_Use_of_LLMs_as_Judges","title":"[논문리뷰] Neither Valid nor Reliable? Investigating the Use of LLMs as Judges","excerpt":"Golnoosh Farnadi이 [arXiv]에 게시한 'Neither Valid nor Reliable? Investigating the Use of LLMs as Judges' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","LLMs as Judges","NLG Evaluation","Measurement Theory","Validity","Reliability","Evaluation Bias","Scalability","Responsible AI"],"permalink":"/ai/review/2025-8-26-Neither_Valid_nor_Reliable_Investigating_the_Use_of_LLMs_as_Judges/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-MeshSplat_Generalizable_Sparse-View_Surface_Reconstruction_via_Gaussian_Splatting","title":"[논문리뷰] MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting","excerpt":"Yanzhe Liang이 [arXiv]에 게시한 'MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Sparse-View","Surface Reconstruction","Gaussian Splatting","2DGS","Novel View Synthesis","Generalizable","Mesh Extraction","3D Vision"],"permalink":"/ai/review/2025-8-26-MeshSplat_Generalizable_Sparse-View_Surface_Reconstruction_via_Gaussian_Splatting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-MV-RAG_Retrieval_Augmented_Multiview_Diffusion","title":"[논문리뷰] MV-RAG: Retrieval Augmented Multiview Diffusion","excerpt":"sagiebenaim이 [arXiv]에 게시한 'MV-RAG: Retrieval Augmented Multiview Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Retrieval Augmented Generation","Multiview Diffusion","Text-to-3D Generation","Out-of-Domain","Image Retrieval","3D Consistency","Diffusion Models","Hybrid Training"],"permalink":"/ai/review/2025-8-26-MV-RAG_Retrieval_Augmented_Multiview_Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-MEENA_PersianMMMU_Multimodal-Multilingual_Educational_Exams_for_N-level_Assessment","title":"[논문리뷰] MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment","excerpt":"Doratossadat Dastgheib이 [arXiv]에 게시한 'MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Multimodal Language Models","Multilingual Benchmarking","Persian Language","Educational Assessment","Vision-Language Models","Cultural Nuance","Reasoning Tasks"],"permalink":"/ai/review/2025-8-26-MEENA_PersianMMMU_Multimodal-Multilingual_Educational_Exams_for_N-level_Assessment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-Limitations_of_Normalization_in_Attention_Mechanism","title":"[논문리뷰] Limitations of Normalization in Attention Mechanism","excerpt":"Radu State이 [arXiv]에 게시한 'Limitations of Normalization in Attention Mechanism' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Attention Mechanism","Normalization","Softmax","Transformer Models","Gradient Sensitivity","Token Separability","Context Length","GPT-2"],"permalink":"/ai/review/2025-8-26-Limitations_of_Normalization_in_Attention_Mechanism/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-InternVL3.5_Advancing_Open-Source_Multimodal_Models_in_Versatility_Reasoning_and_Efficiency","title":"[논문리뷰] InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency","excerpt":"jinglinglin이 [arXiv]에 게시한 'InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Reinforcement Learning","Inference Efficiency","Vision-Language Models","Open-Source","Versatility","Reasoning"],"permalink":"/ai/review/2025-8-26-InternVL3.5_Advancing_Open-Source_Multimodal_Models_in_Versatility_Reasoning_and_Efficiency/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-German4All_-_A_Dataset_and_Model_for_Readability-Controlled_Paraphrasing_in_German","title":"[논문리뷰] German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German","excerpt":"Cristian-George Craciun이 [arXiv]에 게시한 'German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Text Simplification","Paraphrasing","Readability Control","German NLP","Dataset Generation","LLM Distillation","Multi-level Text Generation","Accessibility"],"permalink":"/ai/review/2025-8-26-German4All_-_A_Dataset_and_Model_for_Readability-Controlled_Paraphrasing_in_German/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-Explain_Before_You_Answer_A_Survey_on_Compositional_Visual_Reasoning","title":"[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning","excerpt":"Xin Zheng이 [arXiv]에 게시한 'Explain Before You Answer: A Survey on Compositional Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Compositional Visual Reasoning","Multimodal AI","Vision-Language Models","Large Language Models","Chain-of-Thought","Tool Learning","Agentic AI","Survey"],"permalink":"/ai/review/2025-8-26-Explain_Before_You_Answer_A_Survey_on_Compositional_Visual_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-Breaking_the_Exploration_Bottleneck_Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reasoning","title":"[논문리뷰] Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning","excerpt":"Jiale Zhao이 [arXiv]에 게시한 'Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Exploration Bottleneck","Instructional Scaffolding","Rubric-based Rewards","General Reasoning","RL with Verifiable Rewards","Policy Optimization"],"permalink":"/ai/review/2025-8-26-Breaking_the_Exploration_Bottleneck_Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-26-Beyond_Memorization_Extending_Reasoning_Depth_with_Recurrence_Memory_and_Test-Time_Compute_Scaling","title":"[논문리뷰] Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling","excerpt":"Daniil Orel이 [arXiv]에 게시한 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","lastModifiedAt":"2025-08-26 13:21:57+0900","categories":["Review"],"tags":["Review","Reasoning Depth","Cellular Automata","Transformer Architectures","Recurrence","Adaptive Computation Time","Chain-of-Thought","Reinforcement Learning","Generalization"],"permalink":"/ai/review/2025-8-26-Beyond_Memorization_Extending_Reasoning_Depth_with_Recurrence_Memory_and_Test-Time_Compute_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-TPLA_Tensor_Parallel_Latent_Attention_for_Efficient_Disaggregated_Prefill_Decode_Inference","title":"[논문리뷰] TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference","excerpt":"Di Yin이 [arXiv]에 게시한 'TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Inference","Tensor Parallelism","KV Cache Optimization","Latent Attention","Memory Efficiency","Decoding Speedup","Prefill/Decode Separation","Reparameterization"],"permalink":"/ai/review/2025-8-25-TPLA_Tensor_Parallel_Latent_Attention_for_Efficient_Disaggregated_Prefill_Decode_Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-Selective_Contrastive_Learning_for_Weakly_Supervised_Affordance_Grounding","title":"[논문리뷰] Selective Contrastive Learning for Weakly Supervised Affordance Grounding","excerpt":"Jae-Pil Heo이 [arXiv]에 게시한 'Selective Contrastive Learning for Weakly Supervised Affordance Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Weakly Supervised Learning","Affordance Grounding","Contrastive Learning","CLIP","Part Discovery","Object Localization","DINO","Generative Models"],"permalink":"/ai/review/2025-8-25-Selective_Contrastive_Learning_for_Weakly_Supervised_Affordance_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-Learnable_SMPLify_A_Neural_Solution_for_Optimization-Free_Human_Pose_Inverse_Kinematics","title":"[논문리뷰] Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics","excerpt":"Xiao Sun이 [arXiv]에 게시한 'Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Inverse Kinematics","Human Pose Estimation","SMPL Model","Neural Networks","Optimization-Free","Residual Learning","Data-Driven"],"permalink":"/ai/review/2025-8-25-Learnable_SMPLify_A_Neural_Solution_for_Optimization-Free_Human_Pose_Inverse_Kinematics/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-Jailbreaking_Commercial_Black-Box_LLMs_with_Explicitly_Harmful_Prompts","title":"[논문리뷰] Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts","excerpt":"Liming Fang이 [arXiv]에 게시한 'Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Jailbreaking","Red Teaming","Malicious Content Detection","Developer Messages","D-Attack","DH-CoT","Adversarial Attacks","Dataset Cleaning"],"permalink":"/ai/review/2025-8-25-Jailbreaking_Commercial_Black-Box_LLMs_with_Explicitly_Harmful_Prompts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-InMind_Evaluating_LLMs_in_Capturing_and_Applying_Individual_Human_Reasoning_Styles","title":"[논문리뷰] InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles","excerpt":"Diping Song이 [arXiv]에 게시한 'InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Human Reasoning Styles","Social Deduction Games","Theory of Mind","Adaptive Reasoning","Avalon Game","Cognitive Grounding"],"permalink":"/ai/review/2025-8-25-InMind_Evaluating_LLMs_in_Capturing_and_Applying_Individual_Human_Reasoning_Styles/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-End-to-End_Agentic_RAG_System_Training_for_Traceable_Diagnostic_Reasoning","title":"[논문리뷰] End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning","excerpt":"Pengcheng Qiu이 [arXiv]에 게시한 'End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Agentic RAG","Medical Diagnosis","Reinforcement Learning","Traceable AI","Large Language Models","Clinical Decision Support","Out-of-Distribution Generalization","Reward Design"],"permalink":"/ai/review/2025-8-25-End-to-End_Agentic_RAG_System_Training_for_Traceable_Diagnostic_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-EgoTwin_Dreaming_Body_and_View_in_First_Person","title":"[논문리뷰] EgoTwin: Dreaming Body and View in First Person","excerpt":"Wentao Wang이 [arXiv]에 게시한 'EgoTwin: Dreaming Body and View in First Person' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Egocentric Video Generation","Human Motion Synthesis","Diffusion Transformers","Multimodal Generation","Viewpoint Alignment","Causal Interplay","First-Person Vision"],"permalink":"/ai/review/2025-8-25-EgoTwin_Dreaming_Body_and_View_in_First_Person/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-Do_What_Teaching_Vision-Language-Action_Models_to_Reject_the_Impossible","title":"[논문리뷰] Do What? Teaching Vision-Language-Action Models to Reject the Impossible","excerpt":"Roei Herzig이 [arXiv]에 게시한 'Do What? Teaching Vision-Language-Action Models to Reject the Impossible' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Robotics","False Premise Detection","Instruction Following","Human-Robot Interaction","Clarification","Instruction Tuning"],"permalink":"/ai/review/2025-8-25-Do_What_Teaching_Vision-Language-Action_Models_to_Reject_the_Impossible/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-CRISP_Persistent_Concept_Unlearning_via_Sparse_Autoencoders","title":"[논문리뷰] CRISP: Persistent Concept Unlearning via Sparse Autoencoders","excerpt":"Yonatan Belinkov이 [arXiv]에 게시한 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Concept Unlearning","Sparse Autoencoders (SAEs)","LLMs","Parameter-Efficient Fine-Tuning","Model Interpretability","Safety-Critical AI","Feature Suppression","WMDP Benchmark"],"permalink":"/ai/review/2025-8-25-CRISP_Persistent_Concept_Unlearning_via_Sparse_Autoencoders/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-CARFT_Boosting_LLM_Reasoning_via_Contrastive_Learning_with_Annotated_Chain-of-Thought-based_Reinforced_Fine-Tuning","title":"[논문리뷰] CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning","excerpt":"Yulun Zhang이 [arXiv]에 게시한 'CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Contrastive Learning","Reinforcement Learning","Fine-tuning","Chain-of-Thought (CoT)","Annotated Data","Model Stability"],"permalink":"/ai/review/2025-8-25-CARFT_Boosting_LLM_Reasoning_via_Contrastive_Learning_with_Annotated_Chain-of-Thought-based_Reinforced_Fine-Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-Beyond_Pass1_Self-Play_with_Variational_Problem_Synthesis_Sustains_RLVR","title":"[논문리뷰] Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR","excerpt":"Ying Nian Wu이 [arXiv]에 게시한 'Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Self-Play","Variational Problem Synthesis","Policy Entropy","Pass@k","Reasoning Benchmarks"],"permalink":"/ai/review/2025-8-25-Beyond_Pass1_Self-Play_with_Variational_Problem_Synthesis_Sustains_RLVR/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-AgentScope_1.0_A_Developer-Centric_Framework_for_Building_Agentic_Applications","title":"[논문리뷰] AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications","excerpt":"Liuyi Yao이 [arXiv]에 게시한 'AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","LLM Agents","Agentic Applications","ReAct Paradigm","Framework","Tool Use","Multi-Agent Systems","Developer Experience","Evaluation"],"permalink":"/ai/review/2025-8-25-AgentScope_1.0_A_Developer-Centric_Framework_for_Building_Agentic_Applications/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-25-AetherCode_Evaluating_LLMs_Ability_to_Win_In_Premier_Programming_Competitions","title":"[논문리뷰] AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions","excerpt":"Yidi Du이 [arXiv]에 게시한 'AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","lastModifiedAt":"2025-08-25 13:13:07+0900","categories":["Review"],"tags":["Review","Competitive Programming","LLM Evaluation","Code Reasoning","Benchmark","Test Case Generation","Programming Competitions","Algorithmic Problems"],"permalink":"/ai/review/2025-8-25-AetherCode_Evaluating_LLMs_Ability_to_Win_In_Premier_Programming_Competitions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-aiXiv_A_Next-Generation_Open_Access_Ecosystem_for_Scientific_Discovery_Generated_by_AI_Scientists","title":"[논문리뷰] aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists","excerpt":"Heng Zhang이 [arXiv]에 게시한 'aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","AI Agents","Open Access","Scientific Discovery","Peer Review","LLMs","Multi-agent Systems","Prompt Injection","Iterative Refinement"],"permalink":"/ai/review/2025-8-22-aiXiv_A_Next-Generation_Open_Access_Ecosystem_for_Scientific_Discovery_Generated_by_AI_Scientists/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-When_and_What_Diffusion-Grounded_VideoLLM_with_Entity_Aware_Segmentation_for_Long_Video_Understanding","title":"[논문리뷰] When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding","excerpt":"Rui Guo이 [arXiv]에 게시한 'When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Video-LLM","Diffusion Model","Temporal Grounding","Object Segmentation","Long Video Understanding","Multimodal AI","Video Question Answering"],"permalink":"/ai/review/2025-8-22-When_and_What_Diffusion-Grounded_VideoLLM_with_Entity_Aware_Segmentation_for_Long_Video_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-Waver_Wave_Your_Way_to_Lifelike_Video_Generation","title":"[논문리뷰] Waver: Wave Your Way to Lifelike Video Generation","excerpt":"Yifu Zhang이 [arXiv]에 게시한 'Waver: Wave Your Way to Lifelike Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Video Generation","Foundation Model","Diffusion Model","Transformer","Text-to-Video","Image-to-Video","Super-Resolution","Data Curation"],"permalink":"/ai/review/2025-8-22-Waver_Wave_Your_Way_to_Lifelike_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-Snap-Snap_Taking_Two_Images_to_Reconstruct_3D_Human_Gaussians_in_Milliseconds","title":"[논문리뷰] Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds","excerpt":"Chuiyun Wu이 [arXiv]에 게시한 'Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","3D Human Reconstruction","Gaussian Splatting","Sparse View","Two-Image Input","Real-time Inference","Point Cloud Prediction","Feed-forward Network"],"permalink":"/ai/review/2025-8-22-Snap-Snap_Taking_Two_Images_to_Reconstruct_3D_Human_Gaussians_in_Milliseconds/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-SceneGen_Single-Image_3D_Scene_Generation_in_One_Feedforward_Pass","title":"[논문리뷰] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass","excerpt":"Ya Zhang이 [arXiv]에 게시한 'SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","3D Scene Generation","Single-Image Input","Feedforward Networks","Diffusion Models","Geometric Modeling","Texture Synthesis","Transformer","Feature Aggregation"],"permalink":"/ai/review/2025-8-22-SceneGen_Single-Image_3D_Scene_Generation_in_One_Feedforward_Pass/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-Mobile-Agent-v3_Foundamental_Agents_for_GUI_Automation","title":"[논문리뷰] Mobile-Agent-v3: Foundamental Agents for GUI Automation","excerpt":"Haowei Liu이 [arXiv]에 게시한 'Mobile-Agent-v3: Foundamental Agents for GUI Automation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","GUI Automation","Multimodal Agents","Foundational Models","Reinforcement Learning","Large Language Models","Cross-Platform","Self-Supervised Learning"],"permalink":"/ai/review/2025-8-22-Mobile-Agent-v3_Foundamental_Agents_for_GUI_Automation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-LiveMCP-101_Stress_Testing_and_Diagnosing_MCP-enabled_Agents_on_Challenging_Queries","title":"[논문리뷰] LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries","excerpt":"huuuyeah이 [arXiv]에 게시한 'LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","AI Agents","Tool Use","Model Context Protocol (MCP)","Benchmarking","Large Language Models (LLMs)","Real-world Tasks","Evaluation","Error Analysis"],"permalink":"/ai/review/2025-8-22-LiveMCP-101_Stress_Testing_and_Diagnosing_MCP-enabled_Agents_on_Challenging_Queries/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-Intern-S1_A_Scientific_Multimodal_Foundation_Model","title":"[논문리뷰] Intern-S1: A Scientific Multimodal Foundation Model","excerpt":"xuhuang87이 [arXiv]에 게시한 'Intern-S1: A Scientific Multimodal Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Multimodal Foundation Model","Scientific AI","Reinforcement Learning","Mixture-of-Experts (MoE)","Dynamic Tokenizer","Data Curation","Low-Resource Learning"],"permalink":"/ai/review/2025-8-22-Intern-S1_A_Scientific_Multimodal_Foundation_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-INTIMA_A_Benchmark_for_Human-AI_Companionship_Behavior","title":"[논문리뷰] INTIMA: A Benchmark for Human-AI Companionship Behavior","excerpt":"Yacine Jernite이 [arXiv]에 게시한 'INTIMA: A Benchmark for Human-AI Companionship Behavior' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","AI Companionship","Benchmark","Language Models (LLMs)","Human-AI Interaction","Emotional AI","Boundary Setting","Psychological Frameworks","Evaluation Metrics"],"permalink":"/ai/review/2025-8-22-INTIMA_A_Benchmark_for_Human-AI_Companionship_Behavior/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-Fin-PRM_A_Domain-Specialized_Process_Reward_Model_for_Financial_Reasoning_in_Large_Language_Models","title":"[논문리뷰] Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models","excerpt":"Lifan Guo이 [arXiv]에 게시한 'Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Large Language Models","Process Reward Models","Financial Reasoning","Domain Specialization","RLHF","Best-of-N Selection","Data Curation"],"permalink":"/ai/review/2025-8-22-Fin-PRM_A_Domain-Specialized_Process_Reward_Model_for_Financial_Reasoning_in_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-Does_the_cafe_entrance_look_accessible_Where_is_the_door_Towards_Geospatial_AI_Agents_for_Visual_Inquiries","title":"[논문리뷰] 'Does the cafe entrance look accessible? Where is the door?' Towards Geospatial AI Agents for Visual Inquiries","excerpt":"Xia Su이 [arXiv]에 게시한 'Does the cafe entrance look accessible? Where is the door? Towards Geospatial AI Agents for Visual Inquiries' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Geospatial AI","Multimodal AI Agents","Visual Question Answering","Accessibility","Street View Imagery","Spatial Reasoning","Human-Computer Interaction"],"permalink":"/ai/review/2025-8-22-Does_the_cafe_entrance_look_accessible_Where_is_the_door_Towards_Geospatial_AI_Agents_for_Visual_Inquiries/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-Deep_Think_with_Confidence","title":"[논문리뷰] Deep Think with Confidence","excerpt":"Xuewei Wang이 [arXiv]에 게시한 'Deep Think with Confidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","LLM Reasoning","Confidence Filtering","Self-Consistency","Test-Time Optimization","Computational Efficiency","Adaptive Sampling","Early Stopping","Majority Voting"],"permalink":"/ai/review/2025-8-22-Deep_Think_with_Confidence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-A_Survey_on_Large_Language_Model_Benchmarks","title":"[논문리뷰] A Survey on Large Language Model Benchmarks","excerpt":"Siyi Li이 [arXiv]에 게시한 'A Survey on Large Language Model Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","LLM Benchmarks","Evaluation","Systematic Review","General Capabilities","Domain-Specific Benchmarks","Target-Specific Benchmarks","Data Contamination","AI Ethics"],"permalink":"/ai/review/2025-8-22-A_Survey_on_Large_Language_Model_Benchmarks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-22-ATLAS_Decoupling_Skeletal_and_Shape_Parameters_for_Expressive_Parametric_Human_Modeling","title":"[논문리뷰] ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling","excerpt":"Shunsuke Saito이 [arXiv]에 게시한 'ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","lastModifiedAt":"2025-08-22 13:10:52+0900","categories":["Review"],"tags":["Review","Parametric Human Model","3D Human Modeling","Shape-Skeleton Decoupling","Pose Correctives","Single Image Mesh Fitting","Expressive Modeling","Goliath Dataset"],"permalink":"/ai/review/2025-8-22-ATLAS_Decoupling_Skeletal_and_Shape_Parameters_for_Expressive_Parametric_Human_Modeling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-mSCoRe_a_Multilingual_and_Scalable_Benchmark_for_Skill-based_Commonsense_Reasoning","title":"[논문리뷰] mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning","excerpt":"anoperson이 [arXiv]에 게시한 'mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Multilingual Benchmark","Commonsense Reasoning","LLM Evaluation","Reasoning Taxonomy","Benchmark Scaling","Data Synthesis","Cultural Nuances"],"permalink":"/ai/review/2025-8-21-mSCoRe_a_Multilingual_and_Scalable_Benchmark_for_Skill-based_Commonsense_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-ViExam_Are_Vision_Language_Models_Better_than_Humans_on_Vietnamese_Multimodal_Exam_Questions","title":"[논문리뷰] ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?","excerpt":"Daeyoung Kim이 [arXiv]에 게시한 'ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Vision Language Models","Multimodal AI","Vietnamese Language","Educational Assessment","Low-Resource Languages","Cross-Lingual Reasoning","ViExam","Human-in-the-Loop"],"permalink":"/ai/review/2025-8-21-ViExam_Are_Vision_Language_Models_Better_than_Humans_on_Vietnamese_Multimodal_Exam_Questions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-Tinker_Diffusions_Gift_to_3D--Multi-View_Consistent_Editing_From_Sparse_Inputs_without_Per-Scene_Optimization","title":"[논문리뷰] Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization","excerpt":"Hao Chen이 [arXiv]에 게시한 'Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","3D Editing","Multi-View Consistency","Diffusion Models","Sparse Input","Zero-Shot Learning","Scene Completion","Gaussian Splatting"],"permalink":"/ai/review/2025-8-21-Tinker_Diffusions_Gift_to_3D--Multi-View_Consistent_Editing_From_Sparse_Inputs_without_Per-Scene_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-RynnEC_Bringing_MLLMs_into_Embodied_World","title":"[논문리뷰] RynnEC: Bringing MLLMs into Embodied World","excerpt":"jiangpinliu이 [arXiv]에 게시한 'RynnEC: Bringing MLLMs into Embodied World' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Multi-modal Large Language Models","Embodied AI","Embodied Cognition","Video Understanding","Instance Segmentation","Spatial Reasoning","Robotics"],"permalink":"/ai/review/2025-8-21-RynnEC_Bringing_MLLMs_into_Embodied_World/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-Refining_Contrastive_Learning_and_Homography_Relations_for_Multi-Modal_Recommendation","title":"[논문리뷰] Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation","excerpt":"Shiqing Wu이 [arXiv]에 게시한 'Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Multi-modal Recommendation","Contrastive Learning","Graph Neural Network","Homography Relations","Meta-network","Orthogonal Constraint","Data Sparsity"],"permalink":"/ai/review/2025-8-21-Refining_Contrastive_Learning_and_Homography_Relations_for_Multi-Modal_Recommendation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-Quantization_Meets_dLLMs_A_Systematic_Study_of_Post-training_Quantization_for_Diffusion_LLMs","title":"[논문리뷰] Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs","excerpt":"Haobo Xu이 [arXiv]에 게시한 'Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Post-training Quantization (PTQ)","Model Compression","Activation Outliers","Quantization Methods","Efficient Deployment","Large Language Models"],"permalink":"/ai/review/2025-8-21-Quantization_Meets_dLLMs_A_Systematic_Study_of_Post-training_Quantization_for_Diffusion_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-On-Policy_RL_Meets_Off-Policy_Experts_Harmonizing_Supervised_Fine-Tuning_and_Reinforcement_Learning_via_Dynamic_Weighting","title":"[논문리뷰] On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting","excerpt":"Guoyin Wang이 [arXiv]에 게시한 'On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Supervised Fine-Tuning","On-Policy RL","Off-Policy Experts","Dynamic Weighting","LLM Alignment","Reasoning"],"permalink":"/ai/review/2025-8-21-On-Policy_RL_Meets_Off-Policy_Experts_Harmonizing_Supervised_Fine-Tuning_and_Reinforcement_Learning_via_Dynamic_Weighting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-NVIDIA_Nemotron_Nano_2_An_Accurate_and_Efficient_Hybrid_Mamba-Transformer_Reasoning_Model","title":"[논문리뷰] NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model","excerpt":"abercovich이 [arXiv]에 게시한 'NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Hybrid Architecture","Mamba-Transformer","Reasoning LLM","Model Compression","Knowledge Distillation","Long Context","High Throughput","FP8 Training","Instruction Following"],"permalink":"/ai/review/2025-8-21-NVIDIA_Nemotron_Nano_2_An_Accurate_and_Efficient_Hybrid_Mamba-Transformer_Reasoning_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-MeshCoder_LLM-Powered_Structured_Mesh_Code_Generation_from_Point_Clouds","title":"[논문리뷰] MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds","excerpt":"Jiangmiao이 [arXiv]에 게시한 'MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","LLM","Point Clouds","3D Reconstruction","Structured Mesh","Blender Python","Shape Editing","Part-based Representation","Large Language Model"],"permalink":"/ai/review/2025-8-21-MeshCoder_LLM-Powered_Structured_Mesh_Code_Generation_from_Point_Clouds/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-MCP-Universe_Benchmarking_Large_Language_Models_with_Real-World_Model_Context_Protocol_Servers","title":"[논문리뷰] MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers","excerpt":"Prathyusha Jwalapuram이 [arXiv]에 게시한 'MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Large Language Models","Benchmarking","Model Context Protocol","Tool Use","Real-World Applications","Agent Evaluation","Long Context","Unknown Tools"],"permalink":"/ai/review/2025-8-21-MCP-Universe_Benchmarking_Large_Language_Models_with_Real-World_Model_Context_Protocol_Servers/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-Local_Scale_Equivariance_with_Latent_Deep_Equilibrium_Canonicalizer","title":"[논문리뷰] Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer","excerpt":"Jeremiah Jiang이 [arXiv]에 게시한 'Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Scale Equivariance","Deep Equilibrium Models","Canonicalization","Computer Vision","Image Classification","Semantic Segmentation","Latent Representation","Monotone Scaling"],"permalink":"/ai/review/2025-8-21-Local_Scale_Equivariance_with_Latent_Deep_Equilibrium_Canonicalizer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-Leuvenshtein_Efficient_FHE-based_Edit_Distance_Computation_with_Single_Bootstrap_per_Cell","title":"[논문리뷰] Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell","excerpt":"Ingrid Verbauwhede이 [arXiv]에 게시한 'Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Fully Homomorphic Encryption (FHE)","TFHE","Levenshtein Distance","Programmable Bootstrapping (PBS)","Privacy-Preserving Computation","String Similarity"],"permalink":"/ai/review/2025-8-21-Leuvenshtein_Efficient_FHE-based_Edit_Distance_Computation_with_Single_Bootstrap_per_Cell/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-FutureX_An_Advanced_Live_Benchmark_for_LLM_Agents_in_Future_Prediction","title":"[논문리뷰] FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction","excerpt":"tianlecai이 [arXiv]에 게시한 'FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","LLM Agents","Future Prediction","Live Benchmark","Dynamic Evaluation","Data Contamination","Tool Use","Web Search","Financial Forecasting","Misinformation"],"permalink":"/ai/review/2025-8-21-FutureX_An_Advanced_Live_Benchmark_for_LLM_Agents_in_Future_Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-From_Scores_to_Skills_A_Cognitive_Diagnosis_Framework_for_Evaluating_Financial_Large_Language_Models","title":"[논문리뷰] From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models","excerpt":"Ziyan Kuang이 [arXiv]에 게시한 'From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Financial LLMs","Cognitive Diagnosis Model","LLM Evaluation","Knowledge Assessment","Matrix Factorization","CPA-QKA","Interpretability"],"permalink":"/ai/review/2025-8-21-From_Scores_to_Skills_A_Cognitive_Diagnosis_Framework_for_Evaluating_Financial_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-From_AI_for_Science_to_Agentic_Science_A_Survey_on_Autonomous_Scientific_Discovery","title":"[논문리뷰] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery","excerpt":"zijieqiu이 [arXiv]에 게시한 'From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","Agentic AI","Autonomous Scientific Discovery","AI for Science","Large Language Models","Multi-agent Systems","Scientific Workflow Automation","Natural Sciences"],"permalink":"/ai/review/2025-8-21-From_AI_for_Science_to_Agentic_Science_A_Survey_on_Autonomous_Scientific_Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-21-DuPO_Enabling_Reliable_LLM_Self-Verification_via_Dual_Preference_Optimization","title":"[논문리뷰] DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization","excerpt":"Yu Lu이 [arXiv]에 게시한 'DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","lastModifiedAt":"2025-08-21 13:15:28+0900","categories":["Review"],"tags":["Review","LLM Optimization","Self-Verification","Dual Learning","Preference Optimization","Self-Supervised Learning","Mathematical Reasoning","Multilingual Translation","RLHF"],"permalink":"/ai/review/2025-8-21-DuPO_Enabling_Reliable_LLM_Self-Verification_via_Dual_Preference_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-ZARA_Zero-shot_Motion_Time-Series_Analysis_via_Knowledge_and_Retrieval_Driven_LLM_Agents","title":"[논문리뷰] ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents","excerpt":"Flora D. Salim이 [arXiv]에 게시한 'ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Zero-shot HAR","LLM Agents","Time-Series Analysis","Knowledge Base","Retrieval-Augmented Generation","Multi-sensor Fusion","Interpretability"],"permalink":"/ai/review/2025-8-20-ZARA_Zero-shot_Motion_Time-Series_Analysis_via_Knowledge_and_Retrieval_Driven_LLM_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Training-Free_Text-Guided_Color_Editing_with_Multi-Modal_Diffusion_Transformer","title":"[논문리뷰] Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer","excerpt":"Deyu Zhou이 [arXiv]에 게시한 'Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Text-Guided Editing","Color Editing","Diffusion Transformers","Training-Free","Multi-Modal AI","Attention Control","Image Manipulation"],"permalink":"/ai/review/2025-8-20-Training-Free_Text-Guided_Color_Editing_with_Multi-Modal_Diffusion_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-TempFlow-GRPO_When_Timing_Matters_for_GRPO_in_Flow_Models","title":"[논문리뷰] TempFlow-GRPO: When Timing Matters for GRPO in Flow Models","excerpt":"Jian Yang이 [arXiv]에 게시한 'TempFlow-GRPO: When Timing Matters for GRPO in Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Flow Matching","Reinforcement Learning","Human Preference Alignment","GRPO","Temporal Credit Assignment","Generative AI","Text-to-Image"],"permalink":"/ai/review/2025-8-20-TempFlow-GRPO_When_Timing_Matters_for_GRPO_in_Flow_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Semantic_IDs_for_Joint_Generative_Search_and_Recommendation","title":"[논문리뷰] Semantic IDs for Joint Generative Search and Recommendation","excerpt":"Enrico Palumbo이 [arXiv]에 게시한 'Semantic IDs for Joint Generative Search and Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Generative Models","Search and Recommendation","Semantic IDs","Bi-Encoder","Quantization","Multi-Task Learning","Retrieval Augmented Generation"],"permalink":"/ai/review/2025-8-20-Semantic_IDs_for_Joint_Generative_Search_and_Recommendation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Radiance_Fields_in_XR_A_Survey_on_How_Radiance_Fields_are_Envisioned_and_Addressed_for_XR_Research","title":"[논문리뷰] Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research","excerpt":"Susanne Schmidt이 [arXiv]에 게시한 'Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Radiance Fields","XR","NeRF","3D Gaussian Splatting","View Synthesis","Systematic Review","Immersive Technology"],"permalink":"/ai/review/2025-8-20-Radiance_Fields_in_XR_A_Survey_on_How_Radiance_Fields_are_Envisioned_and_Addressed_for_XR_Research/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Prompt_Orchestration_Markup_Language","title":"[논문리뷰] Prompt Orchestration Markup Language","excerpt":"Yuqing Yang이 [arXiv]에 게시한 'Prompt Orchestration Markup Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Prompt Engineering","Large Language Models","Markup Language","Structured Prompting","IDE Support","Multimodal Data","Styling System","Development Toolkit"],"permalink":"/ai/review/2025-8-20-Prompt_Orchestration_Markup_Language/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-OmniTry_Virtual_Try-On_Anything_without_Masks","title":"[논문리뷰] OmniTry: Virtual Try-On Anything without Masks","excerpt":"Xiaoduan Feng이 [arXiv]에 게시한 'OmniTry: Virtual Try-On Anything without Masks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Diffusion Model","Mask-Free","Image Inpainting","ID Consistency","Wearable Objects","Generative AI"],"permalink":"/ai/review/2025-8-20-OmniTry_Virtual_Try-On_Anything_without_Masks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-MultiRef_Controllable_Image_Generation_with_Multiple_Visual_References","title":"[논문리뷰] MultiRef: Controllable Image Generation with Multiple Visual References","excerpt":"Shiyun Lang이 [arXiv]에 게시한 'MultiRef: Controllable Image Generation with Multiple Visual References' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Controllable Image Generation","Multi-modal Generation","Visual References","Image-to-Image","Benchmark","Dataset","MLLM-as-a-Judge"],"permalink":"/ai/review/2025-8-20-MultiRef_Controllable_Image_Generation_with_Multiple_Visual_References/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Motion2Motion_Cross-topology_Motion_Transfer_with_Sparse_Correspondence","title":"[논문리뷰] Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence","excerpt":"Xin Chen이 [arXiv]에 게시한 'Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Motion Transfer","Cross-topology","Sparse Correspondence","Motion Matching","Animation","Training-free","Few-shot Learning"],"permalink":"/ai/review/2025-8-20-Motion2Motion_Cross-topology_Motion_Transfer_with_Sparse_Correspondence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Mind_the_Generation_Process_Fine-Grained_Confidence_Estimation_During_LLM_Generation","title":"[논문리뷰] Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation","excerpt":"Xinyi Wang이 [arXiv]에 게시한 'Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","LLMs","Confidence Estimation","Fine-Grained","Generation Process","Calibration","Monte Carlo Sampling","Backward Confidence Integration"],"permalink":"/ai/review/2025-8-20-Mind_the_Generation_Process_Fine-Grained_Confidence_Estimation_During_LLM_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-MedSAMix_A_Training-Free_Model_Merging_Approach_for_Medical_Image_Segmentation","title":"[논문리뷰] MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation","excerpt":"Jonas Geiping이 [arXiv]에 게시한 'MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Medical Image Segmentation","Model Merging","Training-Free","SAM","Generalization","Zero-Order Optimization","Bayesian Optimization"],"permalink":"/ai/review/2025-8-20-MedSAMix_A_Training-Free_Model_Merging_Approach_for_Medical_Image_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-MMAU-Pro_A_Challenging_and_Comprehensive_Benchmark_for_Holistic_Evaluation_of_Audio_General_Intelligence","title":"[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence","excerpt":"Fernando López이 [arXiv]에 게시한 'MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Audio Intelligence","Multimodal AI","Benchmark","Audio-Language Models","Holistic Evaluation","Reasoning","Long-Form Audio","Multicultural Music"],"permalink":"/ai/review/2025-8-20-MMAU-Pro_A_Challenging_and_Comprehensive_Benchmark_for_Holistic_Evaluation_of_Audio_General_Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-MM-BrowseComp_A_Comprehensive_Benchmark_for_Multimodal_Browsing_Agents","title":"[논문리뷰] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents","excerpt":"Jun Dong이 [arXiv]에 게시한 'MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Multimodal Browsing","AI Agents","Benchmark","Vision-Language Models","Reasoning","Tool Use","Deep Search"],"permalink":"/ai/review/2025-8-20-MM-BrowseComp_A_Comprehensive_Benchmark_for_Multimodal_Browsing_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-LongSplat_Robust_Unposed_3D_Gaussian_Splatting_for_Casual_Long_Videos","title":"[논문리뷰] LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos","excerpt":"Yen-Yu Lin이 [arXiv]에 게시한 'LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Novel View Synthesis","3D Gaussian Splatting","Unposed Reconstruction","Camera Pose Estimation","Incremental Optimization","Octree","Long Videos"],"permalink":"/ai/review/2025-8-20-LongSplat_Robust_Unposed_3D_Gaussian_Splatting_for_Casual_Long_Videos/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Leveraging_Large_Language_Models_for_Predictive_Analysis_of_Human_Misery","title":"[논문리뷰] Leveraging Large Language Models for Predictive Analysis of Human Misery","excerpt":"Abhilash Nandy이 [arXiv]에 게시한 'Leveraging Large Language Models for Predictive Analysis of Human Misery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Affective Computing","Misery Score Prediction","Prompt Engineering","Few-shot Learning","Gamified Evaluation","Feedback-driven Adaptation"],"permalink":"/ai/review/2025-8-20-Leveraging_Large_Language_Models_for_Predictive_Analysis_of_Human_Misery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Evaluating_Podcast_Recommendations_with_Profile-Aware_LLM-as-a-Judge","title":"[논문리뷰] Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge","excerpt":"Alice Wang이 [arXiv]에 게시한 'Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Podcast Recommendation","LLM-as-a-Judge","Offline Evaluation","User Profiling","Recommender Systems","Natural Language Processing"],"permalink":"/ai/review/2025-8-20-Evaluating_Podcast_Recommendations_with_Profile-Aware_LLM-as-a-Judge/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Embodied-R1_Reinforced_Embodied_Reasoning_for_General_Robotic_Manipulation","title":"[논문리뷰] Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation","excerpt":"Fei Ni이 [arXiv]에 게시한 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Embodied AI","Robotic Manipulation","Reinforcement Learning","Vision-Language Model","Pointing","Zero-shot Generalization"],"permalink":"/ai/review/2025-8-20-Embodied-R1_Reinforced_Embodied_Reasoning_for_General_Robotic_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Describe_What_You_See_with_Multimodal_Large_Language_Models_to_Enhance_Video_Recommendations","title":"[논문리뷰] Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations","excerpt":"Mounia Lalmas이 [arXiv]에 게시한 'Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Video Recommendation","Zero-Shot Learning","Content-Based Filtering","Natural Language Processing","Foundation Models"],"permalink":"/ai/review/2025-8-20-Describe_What_You_See_with_Multimodal_Large_Language_Models_to_Enhance_Video_Recommendations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-CorrSteer_Steering_Improves_Task_Performance_and_Safety_in_LLMs_through_Correlation-based_Sparse_Autoencoder_Feature_Selection","title":"[논문리뷰] CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection","excerpt":"Adriano Koshiyama이 [arXiv]에 게시한 'CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Sparse Autoencoders","LLM Steering","Feature Selection","Correlation Analysis","AI Safety","Bias Mitigation","Mechanistic Interpretability"],"permalink":"/ai/review/2025-8-20-CorrSteer_Steering_Improves_Task_Performance_and_Safety_in_LLMs_through_Correlation-based_Sparse_Autoencoder_Feature_Selection/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Copyright_Protection_for_Large_Language_Models_A_Survey_of_Methods_Challenges_and_Trends","title":"[논문리뷰] Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends","excerpt":"Xixiang Zhao이 [arXiv]에 게시한 'Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","LLM Copyright Protection","Model Fingerprinting","Text Watermarking","Invasive Fingerprinting","Intrinsic Fingerprinting","Intellectual Property","Digital Rights Management","Backdoor Watermarking"],"permalink":"/ai/review/2025-8-20-Copyright_Protection_for_Large_Language_Models_A_Survey_of_Methods_Challenges_and_Trends/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Chain-of-Agents_End-to-End_Agent_Foundation_Models_via_Multi-Agent_Distillation_and_Agentic_RL","title":"[논문리뷰] Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL","excerpt":"Liam-Liu이 [arXiv]에 게시한 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Chain-of-Agents","Agent Foundation Models","Multi-Agent Systems","Tool-Integrated Reasoning","Multi-agent Distillation","Agentic Reinforcement Learning","LLMs","End-to-End Learning"],"permalink":"/ai/review/2025-8-20-Chain-of-Agents_End-to-End_Agent_Foundation_Models_via_Multi-Agent_Distillation_and_Agentic_RL/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-CAMAR_Continuous_Actions_Multi-Agent_Routing","title":"[논문리뷰] CAMAR: Continuous Actions Multi-Agent Routing","excerpt":"Alexey Skrynnik이 [arXiv]에 게시한 'CAMAR: Continuous Actions Multi-Agent Routing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Multi-Agent Reinforcement Learning","Continuous Control","Pathfinding","MARL Benchmark","GPU Acceleration","Robotics Simulation","Scalability","Heterogeneous Agents"],"permalink":"/ai/review/2025-8-20-CAMAR_Continuous_Actions_Multi-Agent_Routing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Beyond_Human_Judgment_A_Bayesian_Evaluation_of_LLMs_Moral_Values_Understanding","title":"[논문리뷰] Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding","excerpt":"Alina Landowska이 [arXiv]에 게시한 'Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Large Language Models","Moral Reasoning","Bayesian Evaluation","Uncertainty Quantification","Natural Language Processing","Soft Labels"],"permalink":"/ai/review/2025-8-20-Beyond_Human_Judgment_A_Bayesian_Evaluation_of_LLMs_Moral_Values_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-Advances_in_Speech_Separation_Techniques_Challenges_and_Future_Trends","title":"[논문리뷰] Advances in Speech Separation: Techniques, Challenges, and Future Trends","excerpt":"Zhuo Chen이 [arXiv]에 게시한 'Advances in Speech Separation: Techniques, Challenges, and Future Trends' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Speech Separation","Deep Neural Networks","Cocktail Party Problem","Transformer Architecture","Unsupervised Learning","Supervised Learning","Evaluation Metrics","Datasets"],"permalink":"/ai/review/2025-8-20-Advances_in_Speech_Separation_Techniques_Challenges_and_Future_Trends/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-20-A_Stitch_in_Time_Saves_Nine_Proactive_Self-Refinement_for_Language_Models","title":"[논문리뷰] A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models","excerpt":"Zishang Jiang이 [arXiv]에 게시한 'A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","lastModifiedAt":"2025-08-20 13:26:54+0900","categories":["Review"],"tags":["Review","Self-Refinement","Language Models","Reinforcement Learning","Proactive AI","Generation Process","Markov Decision Process","Adaptive Learning","LLM Efficiency"],"permalink":"/ai/review/2025-8-20-A_Stitch_in_Time_Saves_Nine_Proactive_Self-Refinement_for_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-When_Punctuation_Matters_A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs","title":"[논문리뷰] When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs","excerpt":"Elena Tutubalina이 [arXiv]에 게시한 'When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","LLM Robustness","Prompt Sensitivity","In-Context Learning","Fine-Tuning","Batch Calibration","Template Ensembles","Distribution Shift"],"permalink":"/ai/review/2025-8-19-When_Punctuation_Matters_A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Speed_Always_Wins_A_Survey_on_Efficient_Architectures_for_Large_Language_Models","title":"[논문리뷰] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models","excerpt":"Jusen Du이 [arXiv]에 게시한 'Speed Always Wins: A Survey on Efficient Architectures for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Large Language Models","Efficient Architectures","Transformer Optimization","Linear Attention","State Space Models","Mixture-of-Experts","Sparse Attention","Diffusion LLMs"],"permalink":"/ai/review/2025-8-19-Speed_Always_Wins_A_Survey_on_Efficient_Architectures_for_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-S2-Guidance_Stochastic_Self_Guidance_for_Training-Free_Enhancement_of_Diffusion_Models","title":"[논문리뷰] S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models","excerpt":"Meiqi Wu이 [arXiv]에 게시한 'S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Classifier-free Guidance","Self-Guidance","Training-Free","Stochastic Block-Dropping","Generative Models","Text-to-Image"],"permalink":"/ai/review/2025-8-19-S2-Guidance_Stochastic_Self_Guidance_for_Training-Free_Enhancement_of_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Representing_Speech_Through_Autoregressive_Prediction_of_Cochlear_Tokens","title":"[논문리뷰] Representing Speech Through Autoregressive Prediction of Cochlear Tokens","excerpt":"Daniel L. K. Yamins이 [arXiv]에 게시한 'Representing Speech Through Autoregressive Prediction of Cochlear Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Speech Representation Learning","Autoregressive Models","Cochlear Tokens","Biologically Inspired AI","Self-Supervised Learning","Audio Processing","Transformer Networks"],"permalink":"/ai/review/2025-8-19-Representing_Speech_Through_Autoregressive_Prediction_of_Cochlear_Tokens/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Reinforcement_Learning_with_Rubric_Anchors","title":"[논문리뷰] Reinforcement Learning with Rubric Anchors","excerpt":"Haokai Xu이 [arXiv]에 게시한 'Reinforcement Learning with Rubric Anchors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Rubric-based Reward","RLVR Extension","Human-centric AI","Controllable Generation","Reward Hacking Mitigation"],"permalink":"/ai/review/2025-8-19-Reinforcement_Learning_with_Rubric_Anchors/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Precise_Action-to-Video_Generation_Through_Visual_Action_Prompts","title":"[논문리뷰] Precise Action-to-Video Generation Through Visual Action Prompts","excerpt":"Minghan Qin이 [arXiv]에 게시한 'Precise Action-to-Video Generation Through Visual Action Prompts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Action-to-Video Generation","Visual Action Prompts","Skeleton Representation","Human-Object Interaction","Robotic Manipulation","Cross-Domain Transfer","Diffusion Models"],"permalink":"/ai/review/2025-8-19-Precise_Action-to-Video_Generation_Through_Visual_Action_Prompts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Ovis2.5_Technical_Report","title":"[논문리뷰] Ovis2.5 Technical Report","excerpt":"Yang Li이 [arXiv]에 게시한 'Ovis2.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Native Resolution Vision","Deep Reasoning","Chart Analysis","OCR","Visual Grounding","Training Efficiency","Preference Optimization"],"permalink":"/ai/review/2025-8-19-Ovis2.5_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Next_Visual_Granularity_Generation","title":"[논문리뷰] Next Visual Granularity Generation","excerpt":"Kang Liao이 [arXiv]에 게시한 'Next Visual Granularity Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Image Generation","Granularity Control","Structured Representation","Hierarchical Generation","Coarse-to-fine","Visual Tokenization","Latent Space"],"permalink":"/ai/review/2025-8-19-Next_Visual_Granularity_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Matrix-Game_2.0_An_Open-Source_Real-Time_and_Streaming_Interactive_World_Model","title":"[논문리뷰] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model","excerpt":"Yifan Zhang이 [arXiv]에 게시한 'Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","World Model","Interactive Video Generation","Real-Time AI","Diffusion Models","Auto-Regressive Generation","Data Pipeline","Self-Forcing","KV Caching"],"permalink":"/ai/review/2025-8-19-Matrix-Game_2.0_An_Open-Source_Real-Time_and_Streaming_Interactive_World_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Lumen_Consistent_Video_Relighting_and_Harmonious_Background_Replacement_with_Video_Generative_Models","title":"[논문리뷰] Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models","excerpt":"Zixiang Gao이 [arXiv]에 게시한 'Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Video Relighting","Background Replacement","Generative Models","Diffusion Models","Temporal Consistency","Dataset Generation","Video Editing"],"permalink":"/ai/review/2025-8-19-Lumen_Consistent_Video_Relighting_and_Harmonious_Background_Replacement_with_Video_Generative_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Inverse-LLaVA_Eliminating_Alignment_Pre-training_Through_Text-to-Vision_Mapping","title":"[논문리뷰] Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping","excerpt":"Tyler Derr이 [arXiv]에 게시한 'Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Multimodal Learning","Vision-Language Models","Alignment Pre-training","Text-to-Vision Mapping","Continuous Representations","Computational Efficiency","LLM"],"permalink":"/ai/review/2025-8-19-Inverse-LLaVA_Eliminating_Alignment_Pre-training_Through_Text-to-Vision_Mapping/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-HeroBench_A_Benchmark_for_Long-Horizon_Planning_and_Structured_Reasoning_in_Virtual_Worlds","title":"[논문리뷰] HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds","excerpt":"Artyom Sorokin이 [arXiv]에 게시한 'HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Long-Horizon Planning","Structured Reasoning","LLM Evaluation","Virtual Worlds","RPG","Benchmark","Agent Systems","Combat Simulation"],"permalink":"/ai/review/2025-8-19-HeroBench_A_Benchmark_for_Long-Horizon_Planning_and_Structured_Reasoning_in_Virtual_Worlds/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Has_GPT-5_Achieved_Spatial_Intelligence_An_Empirical_Study","title":"[논문리뷰] Has GPT-5 Achieved Spatial Intelligence? An Empirical Study","excerpt":"Ruisi Wang이 [arXiv]에 게시한 'Has GPT-5 Achieved Spatial Intelligence? An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Spatial Intelligence","Multimodal LLMs","Benchmark Evaluation","GPT-5","Cognitive AI","AGI"],"permalink":"/ai/review/2025-8-19-Has_GPT-5_Achieved_Spatial_Intelligence_An_Empirical_Study/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-G-CUT3R_Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration","title":"[논문리뷰] G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration","excerpt":"Evgeny Burnaev이 [arXiv]에 게시한 'G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Deep Learning","Multi-Modal Fusion","Camera Pose Estimation","Depth Estimation","Transformer Networks","Prior Information"],"permalink":"/ai/review/2025-8-19-G-CUT3R_Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-ComoRAG_A_Cognitive-Inspired_Memory-Organized_RAG_for_Stateful_Long_Narrative_Reasoning","title":"[논문리뷰] ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning","excerpt":"Yufeng Wang이 [arXiv]에 게시한 'ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Cognitive-Inspired RAG","Stateful Reasoning","Long Narrative Comprehension","Dynamic Memory","Metacognitive Regulation","Multi-step Retrieval","Hierarchical Knowledge Source"],"permalink":"/ai/review/2025-8-19-ComoRAG_A_Cognitive-Inspired_Memory-Organized_RAG_for_Stateful_Long_Narrative_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-Beyond_Solving_Math_Quiz_Evaluating_the_Ability_of_Large_Reasoning_Models_to_Ask_for_Information","title":"[논문리뷰] Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information","excerpt":"Xi Yang이 [arXiv]에 게시한 'Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","Large Reasoning Models (LRMs)","Information Seeking","Incomplete Problems","Mathematical Reasoning","Supervised Fine-tuning (SFT)","Overthinking","Hallucination","CRITIC-math"],"permalink":"/ai/review/2025-8-19-Beyond_Solving_Math_Quiz_Evaluating_the_Ability_of_Large_Reasoning_Models_to_Ask_for_Information/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-19-4DNeX_Feed-Forward_4D_Generative_Modeling_Made_Easy","title":"[논문리뷰] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy","excerpt":"Zeng Tao이 [arXiv]에 게시한 '4DNeX: Feed-Forward 4D Generative Modeling Made Easy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","lastModifiedAt":"2025-08-19 13:15:01+0900","categories":["Review"],"tags":["Review","4D Generation","Dynamic 3D","Generative Models","Diffusion Models","Single Image Input","Video Synthesis","Point Clouds","Dataset"],"permalink":"/ai/review/2025-8-19-4DNeX_Feed-Forward_4D_Generative_Modeling_Made_Easy/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-X-Node_Self-Explanation_is_All_We_Need","title":"[논문리뷰] X-Node: Self-Explanation is All We Need","excerpt":"Islem Rekik이 [arXiv]에 게시한 'X-Node: Self-Explanation is All We Need' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Graph Neural Networks","Explainable AI","Self-Explanation","Node Classification","Medical Imaging","Natural Language Processing","Interpretability"],"permalink":"/ai/review/2025-8-18-X-Node_Self-Explanation_is_All_We_Need/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-Thyme_Think_Beyond_Images","title":"[논문리뷰] Thyme: Think Beyond Images","excerpt":"Wei Chen이 [arXiv]에 게시한 'Thyme: Think Beyond Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Code Generation","Image Processing","Reinforcement Learning","Supervised Fine-Tuning","Visual Reasoning","Sandbox"],"permalink":"/ai/review/2025-8-18-Thyme_Think_Beyond_Images/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-TexVerse_A_Universe_of_3D_Objects_with_High-Resolution_Textures","title":"[논문리뷰] TexVerse: A Universe of 3D Objects with High-Resolution Textures","excerpt":"Nan Cao이 [arXiv]에 게시한 'TexVerse: A Universe of 3D Objects with High-Resolution Textures' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","3D Dataset","High-Resolution Textures","Physically Based Rendering (PBR)","3D Animation","Data Curation","GPT-5 Annotations","Sketchfab"],"permalink":"/ai/review/2025-8-18-TexVerse_A_Universe_of_3D_Objects_with_High-Resolution_Textures/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-StyleMM_Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation","title":"[논문리뷰] StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation","excerpt":"Junyong Noh이 [arXiv]에 게시한 'StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","3D Morphable Model","Face Stylization","Text-to-Image Translation","Diffusion Model","Attribute Preservation","Generative AI","Computer Graphics"],"permalink":"/ai/review/2025-8-18-StyleMM_Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-SSRL_Self-Search_Reinforcement_Learning","title":"[논문리뷰] SSRL: Self-Search Reinforcement Learning","excerpt":"Yanxu Chen이 [arXiv]에 게시한 'SSRL: Self-Search Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Self-Search","Sim-to-Real Transfer","Agentic AI","Knowledge Retrieval","Reward Modeling"],"permalink":"/ai/review/2025-8-18-SSRL_Self-Search_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-SPARSE_Data_Rich_Results_Few-Shot_Semi-Supervised_Learning_via_Class-Conditioned_Image_Translation","title":"[논문리뷰] SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation","excerpt":"Paolo Soda이 [arXiv]에 게시한 'SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Semi-supervised Learning","Few-shot Learning","Medical Imaging","GAN-based Methods","Image-to-image Translation","Pseudo-labeling","Ensemble Learning"],"permalink":"/ai/review/2025-8-18-SPARSE_Data_Rich_Results_Few-Shot_Semi-Supervised_Learning_via_Class-Conditioned_Image_Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-PaperRegister_Boosting_Flexible-grained_Paper_Search_via_Hierarchical_Register_Indexing","title":"[논문리뷰] PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing","excerpt":"Xianpei Han이 [arXiv]에 게시한 'PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","논문 검색","계층적 인덱싱","유연한 검색","대규모 언어 모델","정보 추출","뷰 인식","강화 학습"],"permalink":"/ai/review/2025-8-18-PaperRegister_Boosting_Flexible-grained_Paper_Search_via_Hierarchical_Register_Indexing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-MAESTRO_Masked_AutoEncoders_for_Multimodal_Multitemporal_and_Multispectral_Earth_Observation_Data","title":"[논문리뷰] MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data","excerpt":"Nicolas Gonthier이 [arXiv]에 게시한 'MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Masked Autoencoder","Earth Observation","Multimodal","Multitemporal","Multispectral","Fusion Strategies","Target Normalization"],"permalink":"/ai/review/2025-8-18-MAESTRO_Masked_AutoEncoders_for_Multimodal_Multitemporal_and_Multispectral_Earth_Observation_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-FantasyTalking2_Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation","title":"[논문리뷰] FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation","excerpt":"Mu Xu이 [arXiv]에 게시한 'FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Audio-Driven Animation","Preference Optimization","Diffusion Models","Reward Modeling","Human Feedback","Multi-Objective Optimization","Timestep-Layer Adaptive"],"permalink":"/ai/review/2025-8-18-FantasyTalking2_Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-DINOv3","title":"[논문리뷰] DINOv3","excerpt":"Maxime Oquab이 [arXiv]에 게시한 'DINOv3' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Self-supervised Learning","Foundation Models","Vision Transformer","Dense Feature Maps","Gram Anchoring","Model Distillation","Geospatial AI"],"permalink":"/ai/review/2025-8-18-DINOv3/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-18-Controlling_Multimodal_LLMs_via_Reward-guided_Decoding","title":"[논문리뷰] Controlling Multimodal LLMs via Reward-guided Decoding","excerpt":"Michal Drozdzal이 [arXiv]에 게시한 'Controlling Multimodal LLMs via Reward-guided Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","lastModifiedAt":"2025-08-18 13:14:38+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Reward Models","Guided Decoding","Visual Grounding","Hallucination Mitigation","Object Precision","Object Recall","Inference-time Control"],"permalink":"/ai/review/2025-8-18-Controlling_Multimodal_LLMs_via_Reward-guided_Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-We-Math_2.0_A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning","title":"[논문리뷰] We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning","excerpt":"Xiaowan Wang이 [arXiv]에 게시한 'We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Visual Mathematical Reasoning","MLLMs","Knowledge System","Reinforcement Learning","Curriculum Learning","Dataset Construction","Mathematical Benchmark"],"permalink":"/ai/review/2025-8-15-We-Math_2.0_A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-UI-Venus_Technical_Report_Building_High-performance_UI_Agents_with_RFT","title":"[논문리뷰] UI-Venus Technical Report: Building High-performance UI Agents with RFT","excerpt":"Shuheng Shen이 [arXiv]에 게시한 'UI-Venus Technical Report: Building High-performance UI Agents with RFT' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","UI Agent","MLLM","RFT","UI Grounding","UI Navigation","GRPO","Data Cleaning","Self-Evolving Trajectory"],"permalink":"/ai/review/2025-8-15-UI-Venus_Technical_Report_Building_High-performance_UI_Agents_with_RFT/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-ToonComposer_Streamlining_Cartoon_Production_with_Generative_Post-Keyframing","title":"[논문리뷰] ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing","excerpt":"Xiaoyu Li이 [arXiv]에 게시한 'ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Cartoon Generation","Video Diffusion Models","DiT","Post-Keyframing","Low-Rank Adaptation","Sparse Control","Generative AI","Animation"],"permalink":"/ai/review/2025-8-15-ToonComposer_Streamlining_Cartoon_Production_with_Generative_Post-Keyframing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-STream3R_Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer","title":"[논문리뷰] STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer","excerpt":"Honghua Chen이 [arXiv]에 게시한 'STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Causal Transformer","Sequential Modeling","Streaming Data","Pointmap Prediction","Online Perception","KVCache"],"permalink":"/ai/review/2025-8-15-STream3R_Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_know_about_your_camera","title":"[논문리뷰] Processing and acquisition traces in visual encoders: What does CLIP know about your camera?","excerpt":"Giorgos Tolias이 [arXiv]에 게시한 'Processing and acquisition traces in visual encoders: What does CLIP know about your camera?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Visual Encoders","Metadata","Image Processing","Image Acquisition","Robustness","CLIP","Foundation Models","Distribution Shift"],"permalink":"/ai/review/2025-8-15-Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_know_about_your_camera/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-Passk_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models","title":"[논문리뷰] Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models","excerpt":"Qinghao Ye이 [arXiv]에 게시한 'Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Exploration-Exploitation","Reward Design","Reasoning Tasks","Pass@k","Policy Optimization"],"permalink":"/ai/review/2025-8-15-Passk_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-PRELUDE_A_Benchmark_Designed_to_Require_Global_Comprehension_and_Reasoning_over_Long_Contexts","title":"[논문리뷰] PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts","excerpt":"Rui Lu이 [arXiv]에 게시한 'PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Long-Context Understanding","Reasoning Benchmark","LLMs Evaluation","Natural Language Processing","Global Comprehension","Fluid Intelligence","Prequel Entailment","RAG"],"permalink":"/ai/review/2025-8-15-PRELUDE_A_Benchmark_Designed_to_Require_Global_Comprehension_and_Reasoning_over_Long_Contexts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-NextStep-1_Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale","title":"[논문리뷰] NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale","excerpt":"Quan Sun이 [arXiv]에 게시한 'NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Text-to-Image Generation","Continuous Latent Tokens","Flow Matching","Image Editing","Multimodal Learning","Transformer Architecture"],"permalink":"/ai/review/2025-8-15-NextStep-1_Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-HumanSense_From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs","title":"[논문리뷰] HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs","excerpt":"Yi Yuan이 [arXiv]에 게시한 'HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Multimodal LLMs","Human-Centered AI","Empathy","Context-Awareness","MLLM Benchmark","Reinforcement Learning","Reasoning"],"permalink":"/ai/review/2025-8-15-HumanSense_From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-From_Black_Box_to_Transparency_Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_College_Classrooms","title":"[논문리뷰] From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms","excerpt":"Ziyin Zhang이 [arXiv]에 게시한 'From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Automated Interpreting Assessment","Explainable AI","Data Augmentation","Variational Autoencoder","SHAP","Interpreting Quality","Natural Language Processing"],"permalink":"/ai/review/2025-8-15-From_Black_Box_to_Transparency_Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_College_Classrooms/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-A_Survey_on_Diffusion_Language_Models","title":"[논문리뷰] A Survey on Diffusion Language Models","excerpt":"Zhiqiang Shen이 [arXiv]에 게시한 'A Survey on Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Generative AI","Parallel Decoding","Text Generation","Multimodal AI","Model Compression","Reinforcement Learning from Human Feedback","Inference Optimization"],"permalink":"/ai/review/2025-8-15-A_Survey_on_Diffusion_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-15-2025-8-15-Explainability_and_Privacy_in_NLP","title":"[논문리뷰] When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing","excerpt":"Gjergji Kasneci이 [arXiv]에 게시한 'When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","lastModifiedAt":"2025-08-15 13:09:31+0900","categories":["Review"],"tags":["Review","Natural Language Processing (NLP)","Explainable AI (XAI)","Post-hoc Explainability","Differential Privacy (DP)","Privacy-Utility Trade-off","Model Faithfulness","Text Privatization"],"permalink":"/ai/review/2025-8-15-2025-8-15-Explainability_and_Privacy_in_NLP/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-VisCodex_Unified_Multimodal_Code_Generation_via_Merging_Vision_and_Coding_Models","title":"[논문리뷰] VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models","excerpt":"Dongdong Zhang이 [arXiv]에 게시한 'VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Code Generation","Model Merging","Task Vectors","Vision-Language Model","Coding LLM","Instruction Tuning","Benchmark"],"permalink":"/ai/review/2025-8-14-VisCodex_Unified_Multimodal_Code_Generation_via_Merging_Vision_and_Coding_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Story2Board_A_Training-Free_Approach_for_Expressive_Storyboard_Generation","title":"[논문리뷰] Story2Board: A Training-Free Approach for Expressive Storyboard Generation","excerpt":"Dani Lischinski이 [arXiv]에 게시한 'Story2Board: A Training-Free Approach for Expressive Storyboard Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Storyboard Generation","Text-to-Image","Diffusion Models","Training-Free","Character Consistency","Scene Diversity","Visual Storytelling"],"permalink":"/ai/review/2025-8-14-Story2Board_A_Training-Free_Approach_for_Expressive_Storyboard_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Stand-In_A_Lightweight_and_Plug-and-Play_Identity_Control_for_Video_Generation","title":"[논문리뷰] Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation","excerpt":"Chen Li이 [arXiv]에 게시한 'Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Video Generation","Identity Preservation","Plug-and-Play","Diffusion Models","Self-Attention","Lightweight AI","Conditional Image Branch"],"permalink":"/ai/review/2025-8-14-Stand-In_A_Lightweight_and_Plug-and-Play_Identity_Control_for_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Seeing_Listening_Remembering_and_Reasoning_A_Multimodal_Agent_with_Long-Term_Memory","title":"[논문리뷰] Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory","excerpt":"Yuan Lin이 [arXiv]에 게시한 'Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multimodal Agent","Long-Term Memory","Episodic Memory","Semantic Memory","Reinforcement Learning","Video Question Answering","Entity-Centric Memory"],"permalink":"/ai/review/2025-8-14-Seeing_Listening_Remembering_and_Reasoning_A_Multimodal_Agent_with_Long-Term_Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Noise_Hypernetworks_Amortizing_Test-Time_Compute_in_Diffusion_Models","title":"[논문리뷰] Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models","excerpt":"Zeynep Akata이 [arXiv]에 게시한 'Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Diffusion Models","Hypernetworks","Test-Time Optimization","Reward-Guided Generation","Latent Space Optimization","LoRA","Generative AI"],"permalink":"/ai/review/2025-8-14-Noise_Hypernetworks_Amortizing_Test-Time_Compute_in_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Mol-R1_Towards_Explicit_Long-CoT_Reasoning_in_Molecule_Discovery","title":"[논문리뷰] Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery","excerpt":"Di Zhang이 [arXiv]에 게시한 'Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Molecule Discovery","Chain-of-Thought","Large Language Models","Reinforcement Learning","Supervised Fine-tuning","Molecular Generation","Explainable AI"],"permalink":"/ai/review/2025-8-14-Mol-R1_Towards_Explicit_Long-CoT_Reasoning_in_Molecule_Discovery/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-MathReal_We_Keep_It_Real_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_Language_Models","title":"[논문리뷰] MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models","excerpt":"Zhihan Zhou이 [arXiv]에 게시한 'MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models (MLLMs)","Math Reasoning","Real-World Benchmark","Visual Perception","Robustness","K-12 Education","Dataset"],"permalink":"/ai/review/2025-8-14-MathReal_We_Keep_It_Real_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Learning_to_Align_Aligning_to_Learn_A_Unified_Approach_for_Self-Optimized_Alignment","title":"[논문리뷰] Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment","excerpt":"Lei Fan이 [arXiv]에 게시한 'Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","LLM Alignment","Reinforcement Learning from Human Feedback","Preference Learning","Group Relative Alignment Optimization","Self-Optimization","Mixture-of-Experts","Imitation Learning"],"permalink":"/ai/review/2025-8-14-Learning_to_Align_Aligning_to_Learn_A_Unified_Approach_for_Self-Optimized_Alignment/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-IAG_Input-aware_Backdoor_Attack_on_VLMs_for_Visual_Grounding","title":"[논문리뷰] IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding","excerpt":"Di Zhang이 [arXiv]에 게시한 'IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Backdoor Attack","Vision-Language Models (VLMs)","Visual Grounding","Input-aware Trigger","Adversarial Attack","Security","U-Net","Open-vocabulary"],"permalink":"/ai/review/2025-8-14-IAG_Input-aware_Backdoor_Attack_on_VLMs_for_Visual_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-GSFixer_Improving_3D_Gaussian_Splatting_with_Reference-Guided_Video_Diffusion_Priors","title":"[논문리뷰] GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors","excerpt":"Qingnan Fan이 [arXiv]에 게시한 'GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","3D Gaussian Splatting","Novel View Synthesis","Diffusion Model","Artifact Restoration","Sparse-view 3D Reconstruction","Reference-Guided"],"permalink":"/ai/review/2025-8-14-GSFixer_Improving_3D_Gaussian_Splatting_with_Reference-Guided_Video_Diffusion_Priors/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Echo-4o_Harnessing_the_Power_of_GPT-4o_Synthetic_Images_for_Improved_Image_Generation","title":"[논문리뷰] Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation","excerpt":"Zhenghao Hu이 [arXiv]에 게시한 'Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Synthetic Data","Image Generation","GPT-4o","Multimodal Models","Instruction Following","Surreal Image Generation","Dataset","Benchmarking"],"permalink":"/ai/review/2025-8-14-Echo-4o_Harnessing_the_Power_of_GPT-4o_Synthetic_Images_for_Improved_Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Diffusion_LLMs_Can_Do_Faster-Than-AR_Inference_via_Discrete_Diffusion_Forcing","title":"[논문리뷰] Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing","excerpt":"Hao Zhang이 [arXiv]에 게시한 'Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Diffusion LLMs","Faster Inference","Discrete Diffusion Forcing (D2F)","Autoregressive Generation","KV Cache Optimization","Parallel Decoding","Text Generation","Model Distillation"],"permalink":"/ai/review/2025-8-14-Diffusion_LLMs_Can_Do_Faster-Than-AR_Inference_via_Discrete_Diffusion_Forcing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Cooper_Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models","title":"[논문리뷰] Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models","excerpt":"Guiyang Hou이 [arXiv]에 게시한 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Reward Model","Policy Optimization","Reward Hacking","Hybrid Annotation","Mathematical Reasoning","Verifiable Rewards"],"permalink":"/ai/review/2025-8-14-Cooper_Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-Can_LLM-Generated_Textual_Explanations_Enhance_Model_Classification_Performance_An_Empirical_Study","title":"[논문리뷰] Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study","excerpt":"Gjergji Kasneci이 [arXiv]에 게시한 'Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Explainable NLP","Natural Language Explanations","Large Language Models","Pre-trained Language Models","Natural Language Inference","Model Performance Enhancement","Text Generation"],"permalink":"/ai/review/2025-8-14-Can_LLM-Generated_Textual_Explanations_Enhance_Model_Classification_Performance_An_Empirical_Study/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-AWorld_Dynamic_Multi-Agent_System_with_Stable_Maneuvering_for_Robust_GAIA_Problem_Solving","title":"[논문리뷰] AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving","excerpt":"Jinjie Gu이 [arXiv]에 게시한 'AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Agent Stability","LLM","Tool Use","GAIA Benchmark","Robustness","Dynamic Supervision","Maneuvering"],"permalink":"/ai/review/2025-8-14-AWorld_Dynamic_Multi-Agent_System_with_Stable_Maneuvering_for_Robust_GAIA_Problem_Solving/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-14-AMFT_Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance","title":"[논문리뷰] AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance","excerpt":"Yong Li이 [arXiv]에 게시한 'AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","lastModifiedAt":"2025-08-14 13:19:02+0900","categories":["Review"],"tags":["Review","Large Language Models","Fine-tuning","Reinforcement Learning","Meta-learning","Adaptive Control","Imitation Learning","Exploration","Reasoning"],"permalink":"/ai/review/2025-8-14-AMFT_Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-WGAST_Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_Spatio-Temporal_Fusion","title":"[논문리뷰] WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion","excerpt":"Rachid Nedjai이 [arXiv]에 게시한 'WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Spatio-Temporal Fusion","Land Surface Temperature","Generative Adversarial Network","Weakly-Supervised Learning","Remote Sensing","Deep Learning"],"permalink":"/ai/review/2025-8-13-WGAST_Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_Spatio-Temporal_Fusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-VertexRegen_Mesh_Generation_with_Continuous_Level_of_Detail","title":"[논문리뷰] VertexRegen: Mesh Generation with Continuous Level of Detail","excerpt":"Jakob Engel이 [arXiv]에 게시한 'VertexRegen: Mesh Generation with Continuous Level of Detail' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Mesh Generation","Level of Detail (LOD)","Progressive Meshes","Vertex Split","Autoregressive Models","Transformer","3D Graphics"],"permalink":"/ai/review/2025-8-13-VertexRegen_Mesh_Generation_with_Continuous_Level_of_Detail/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-UNCAGE_Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generation","title":"[논문리뷰] UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation","excerpt":"Kevin Galim이 [arXiv]에 게시한 'UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Masked Generative Transformers","Compositional Generation","Attention Guidance","Unmasking Strategy","Contrastive Learning","Training-Free","Attribute Binding"],"permalink":"/ai/review/2025-8-13-UNCAGE_Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Train_Long_Think_Short_Curriculum_Learning_for_Efficient_Reasoning","title":"[논문리뷰] Train Long, Think Short: Curriculum Learning for Efficient Reasoning","excerpt":"Marzyeh Ghassemi이 [arXiv]에 게시한 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Curriculum Learning","Reinforcement Learning","Large Language Models","Reasoning Efficiency","Token Budget Control","Group Relative Policy Optimization","Chain-of-Thought"],"permalink":"/ai/review/2025-8-13-Train_Long_Think_Short_Curriculum_Learning_for_Efficient_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors","title":"[논문리뷰] Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors","excerpt":"Haoran Xu이 [arXiv]에 게시한 'Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Robotic Dexterous Grasping","Affordance-Aware","Human-like Priors","Reinforcement Learning","Vision-Language Models","Two-Stage Training","Manipulation"],"permalink":"/ai/review/2025-8-13-Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-TopXGen_Topic-Diverse_Parallel_Data_Generation_for_Low-Resource_Machine_Translation","title":"[논문리뷰] TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation","excerpt":"Rachel Bawden이 [arXiv]에 게시한 'TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Low-Resource MT","Data Augmentation","Large Language Models (LLMs)","Back-Translation","In-Context Learning (ICL)","Fine-Tuning","Topic-Guided Generation","Parallel Data Synthesis"],"permalink":"/ai/review/2025-8-13-TopXGen_Topic-Diverse_Parallel_Data_Generation_for_Low-Resource_Machine_Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Time_Is_a_Feature_Exploiting_Temporal_Dynamics_in_Diffusion_Language_Models","title":"[논문리뷰] Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models","excerpt":"Chenchen Jing이 [arXiv]에 게시한 'Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Diffusion Language Models","Temporal Oscillation","Self-Consistency Voting","Reinforcement Learning","Temporal Semantic Entropy","Text Generation"],"permalink":"/ai/review/2025-8-13-Time_Is_a_Feature_Exploiting_Temporal_Dynamics_in_Diffusion_Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency","title":"[논문리뷰] Test-Time Reinforcement Learning for GUI Grounding via Region Consistency","excerpt":"Zhengxi Lu이 [arXiv]에 게시한 'Test-Time Reinforcement Learning for GUI Grounding via Region Consistency' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","GUI Grounding","Test-Time Scaling","Reinforcement Learning","Region Consistency","Spatial Voting","Self-Supervised Learning","Vision-Language Models"],"permalink":"/ai/review/2025-8-13-Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-OpenCUA_Open_Foundations_for_Computer-Use_Agents","title":"[논문리뷰] OpenCUA: Open Foundations for Computer-Use Agents","excerpt":"Tianbao Xie이 [arXiv]에 게시한 'OpenCUA: Open Foundations for Computer-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Computer-Use Agents","Vision-Language Models","Chain-of-Thought Reasoning","Large-scale Dataset","Open-source Framework","Desktop Automation","Agent Evaluation"],"permalink":"/ai/review/2025-8-13-OpenCUA_Open_Foundations_for_Computer-Use_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-NVSpeech_An_Integrated_and_Scalable_Pipeline_for_Human-Like_Speech_Modeling_with_Paralinguistic_Vocalizations","title":"[논문리뷰] NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations","excerpt":"Haoyue Zhan이 [arXiv]에 게시한 'NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Paralinguistic Vocalizations","Speech Recognition","Text-to-Speech","Speech Synthesis","Data Annotation","Mandarin Speech","Expressive Speech"],"permalink":"/ai/review/2025-8-13-NVSpeech_An_Integrated_and_Scalable_Pipeline_for_Human-Like_Speech_Modeling_with_Paralinguistic_Vocalizations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Matrix-3D_Omnidirectional_Explorable_3D_World_Generation","title":"[논문리뷰] Matrix-3D: Omnidirectional Explorable 3D World Generation","excerpt":"Yuqi Li이 [arXiv]에 게시한 'Matrix-3D: Omnidirectional Explorable 3D World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","3D World Generation","Panoramic Video Generation","3D Reconstruction","Diffusion Models","Gaussian Splatting","Dataset","Camera Control"],"permalink":"/ai/review/2025-8-13-Matrix-3D_Omnidirectional_Explorable_3D_World_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-HierSearch_A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches","title":"[논문리뷰] HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches","excerpt":"Qiang Ju이 [arXiv]에 게시한 'HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Hierarchical Reinforcement Learning","Deep Search","Multi-source RAG","Agentic AI","Knowledge Integration","Enterprise Search","Large Reasoning Models"],"permalink":"/ai/review/2025-8-13-HierSearch_A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-GeRe_Towards_Efficient_Anti-Forgetting_in_Continual_Learning_of_LLM_via_General_Samples_Replay","title":"[논문리뷰] GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay","excerpt":"Yang Fan이 [arXiv]에 게시한 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Continual Learning","Large Language Models (LLMs)","Catastrophic Forgetting","Replay","Knowledge Distillation","Activation States","Anti-forgetting","Threshold-based Margin Loss"],"permalink":"/ai/review/2025-8-13-GeRe_Towards_Efficient_Anti-Forgetting_in_Continual_Learning_of_LLM_via_General_Samples_Replay/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Feedback-Driven_Tool-Use_Improvements_in_Large_Language_Models_via_Automated_Build_Environments","title":"[논문리뷰] Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments","excerpt":"Xuesong Yao이 [arXiv]에 게시한 'Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Tool Use","Reinforcement Learning (RL)","Automated Environment Generation","Feedback-Driven Training","Reward Mechanism","Contextual Understanding"],"permalink":"/ai/review/2025-8-13-Feedback-Driven_Tool-Use_Improvements_in_Large_Language_Models_via_Automated_Build_Environments/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Democratizing_Diplomacy_A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy","title":"[논문리뷰] Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy","excerpt":"Elizabeth Karpinski이 [arXiv]에 게시한 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Large Language Models","Diplomacy Game","Multi-agent Systems","Strategic Reasoning","LLM Evaluation","Prompt Engineering","Behavioral Analysis","Game AI"],"permalink":"/ai/review/2025-8-13-Democratizing_Diplomacy_A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-DeCRED_Decoder-Centric_Regularization_for_Encoder-Decoder_Based_Speech_Recognition","title":"[논문리뷰] DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition","excerpt":"Lukáš Burget이 [arXiv]에 게시한 'DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Speech Recognition","Encoder-Decoder","Regularization","Decoder-Centric","Intermediate Supervision","Out-of-Domain Generalization","Internal Language Model"],"permalink":"/ai/review/2025-8-13-DeCRED_Decoder-Centric_Regularization_for_Encoder-Decoder_Based_Speech_Recognition/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Cut2Next_Generating_Next_Shot_via_In-Context_Tuning","title":"[논문리뷰] Cut2Next: Generating Next Shot via In-Context Tuning","excerpt":"Yu Qiao이 [arXiv]에 게시한 'Cut2Next: Generating Next Shot via In-Context Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Next Shot Generation","In-Context Tuning","Diffusion Transformer","Cinematic Continuity","Hierarchical Prompting","Video Generation","Shot Editing"],"permalink":"/ai/review/2025-8-13-Cut2Next_Generating_Next_Shot_via_In-Context_Tuning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-CharacterShot_Controllable_and_Consistent_4D_Character_Animation","title":"[논문리뷰] CharacterShot: Controllable and Consistent 4D Character Animation","excerpt":"Fei Shen이 [arXiv]에 게시한 'CharacterShot: Controllable and Consistent 4D Character Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","4D Character Animation","Diffusion Models","Gaussian Splatting","Pose Control","Multi-view Synthesis","Temporal Consistency","Character Dataset"],"permalink":"/ai/review/2025-8-13-CharacterShot_Controllable_and_Consistent_4D_Character_Animation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Bridging_Theory_and_Practice_in_Quantum_Game_Theory_Optimized_Implementation_of_the_Battle_of_the_Sexes_with_Error_Mitigation_on_NISQ_Hardware","title":"[논문리뷰] Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware","excerpt":"Jhon Alejandro Andrade이 [arXiv]에 게시한 'Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Quantum Game Theory","NISQ Hardware","Error Mitigation","Battle of the Sexes","Qiskit","Quantum Computing","Strategic Coordination","Payoff Maximization"],"permalink":"/ai/review/2025-8-13-Bridging_Theory_and_Practice_in_Quantum_Game_Theory_Optimized_Implementation_of_the_Battle_of_the_Sexes_with_Error_Mitigation_on_NISQ_Hardware/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-BiasGym_Fantastic_Biases_and_How_to_Find_and_Remove_Them","title":"[논문리뷰] BiasGym: Fantastic Biases and How to Find (and Remove) Them","excerpt":"Arnav Arora이 [arXiv]에 게시한 'BiasGym: Fantastic Biases and How to Find (and Remove) Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Bias Mitigation","LLMs","Mechanistic Interpretability","Fine-tuning","Attention Steering","Stereotype Analysis","Safety Alignment"],"permalink":"/ai/review/2025-8-13-BiasGym_Fantastic_Biases_and_How_to_Find_and_Remove_Them/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Beyond_Ten_Turns_Unlocking_Long-Horizon_Agentic_Search_with_Large-Scale_Asynchronous_RL","title":"[논문리뷰] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL","excerpt":"Chuyi He이 [arXiv]에 게시한 'Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","LLM Agents","Agentic Search","Asynchronous RL","Long-Horizon Planning","Tool Use","Data Synthesis"],"permalink":"/ai/review/2025-8-13-Beyond_Ten_Turns_Unlocking_Long-Horizon_Agentic_Search_with_Large-Scale_Asynchronous_RL/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-AutoCodeBench_Large_Language_Models_are_Automatic_Code_Benchmark_Generators","title":"[논문리뷰] AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators","excerpt":"Tao Zhang이 [arXiv]에 게시한 'AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","코드 생성","대규모 언어 모델","코드 벤치마크","다국어 프로그래밍","자동화된 데이터 생성","샌드박스 평가","멀티모달 AI"],"permalink":"/ai/review/2025-8-13-AutoCodeBench_Large_Language_Models_are_Automatic_Code_Benchmark_Generators/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Aryabhata_An_exam-focused_language_model_for_JEE_Math","title":"[논문리뷰] Aryabhata: An exam-focused language model for JEE Math","excerpt":"Sandeep Varma이 [arXiv]에 게시한 'Aryabhata: An exam-focused language model for JEE Math' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Language Model","Math Reasoning","JEE","Supervised Fine-Tuning","Reinforcement Learning","Model Merging","Chain-of-Thought","Curriculum Learning"],"permalink":"/ai/review/2025-8-13-Aryabhata_An_exam-focused_language_model_for_JEE_Math/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-13-Adversarial_Video_Promotion_Against_Text-to-Video_Retrieval","title":"[논문리뷰] Adversarial Video Promotion Against Text-to-Video Retrieval","excerpt":"Shuai Liu이 [arXiv]에 게시한 'Adversarial Video Promotion Against Text-to-Video Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","lastModifiedAt":"2025-08-13 13:29:23+0900","categories":["Review"],"tags":["Review","Adversarial Attack","Video Promotion","Text-to-Video Retrieval","Modality Refinement","Black-box Attack","Video Manipulation","Transferability"],"permalink":"/ai/review/2025-8-13-Adversarial_Video_Promotion_Against_Text-to-Video_Retrieval/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-WideSearch_Benchmarking_Agentic_Broad_Info-Seeking","title":"[논문리뷰] WideSearch: Benchmarking Agentic Broad Info-Seeking","excerpt":"Yan Gao이 [arXiv]에 게시한 'WideSearch: Benchmarking Agentic Broad Info-Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Agentic Search","LLM","Benchmark","Information Seeking","Structured Output","Evaluation Metrics","Multi-agent Systems"],"permalink":"/ai/review/2025-8-12-WideSearch_Benchmarking_Agentic_Broad_Info-Seeking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-When_Good_Sounds_Go_Adversarial_Jailbreaking_Audio-Language_Models_with_Benign_Inputs","title":"[논문리뷰] When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs","excerpt":"Dasol Choi이 [arXiv]에 게시한 'When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Audio-Language Models","Jailbreak Attack","Adversarial Audio","Reinforcement Learning","Projected Gradient Descent","Native Payload Discovery","Multimodal AI Safety"],"permalink":"/ai/review/2025-8-12-When_Good_Sounds_Go_Adversarial_Jailbreaking_Audio-Language_Models_with_Benign_Inputs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-VisR-Bench_An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Document_Understanding","title":"[논문리뷰] VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding","excerpt":"Tong Yu이 [arXiv]에 게시한 'VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Multimodal Retrieval","Retrieval-Augmented Generation","Long Document Understanding","Multilingual NLP","Visual QA","Benchmark","MLLMs","Table Understanding"],"permalink":"/ai/review/2025-8-12-VisR-Bench_An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Document_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-UserBench_An_Interactive_Gym_Environment_for_User-Centric_Agents","title":"[논문리뷰] UserBench: An Interactive Gym Environment for User-Centric Agents","excerpt":"Jianguo Zhang이 [arXiv]에 게시한 'UserBench: An Interactive Gym Environment for User-Centric Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","User-Centric AI","LLM Evaluation","Interactive Agents","Gym Environment","Preference Elicitation","Multi-turn Dialogue","Tool Use"],"permalink":"/ai/review/2025-8-12-UserBench_An_Interactive_Gym_Environment_for_User-Centric_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Temporal_Self-Rewarding_Language_Models_Decoupling_Chosen-Rejected_via_Past-Future","title":"[논문리뷰] Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future","excerpt":"Qiufeng Wang이 [arXiv]에 게시한 'Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Self-Rewarding LLMs","Direct Preference Optimization (DPO)","Preference Learning","Generative AI","Gradient Collapse","LLM Alignment","Iterative Optimization"],"permalink":"/ai/review/2025-8-12-Temporal_Self-Rewarding_Language_Models_Decoupling_Chosen-Rejected_via_Past-Future/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Speech-to-LaTeX_New_Models_and_Datasets_for_Converting_Spoken_Equations_and_Sentences","title":"[논문리뷰] Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences","excerpt":"Matvey Skripkin이 [arXiv]에 게시한 'Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Speech-to-LaTeX","ASR","Language Models","Multimodal AI","Dataset Creation","Mathematical Expression Recognition","LaTeX Generation"],"permalink":"/ai/review/2025-8-12-Speech-to-LaTeX_New_Models_and_Datasets_for_Converting_Spoken_Equations_and_Sentences/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Shortcut_Learning_in_Generalist_Robot_Policies_The_Role_of_Dataset_Diversity_and_Fragmentation","title":"[논문리뷰] Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation","excerpt":"Hengtao Shen이 [arXiv]에 게시한 'Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Robot Learning","Generalization","Shortcut Learning","Dataset Diversity","Dataset Fragmentation","Data Augmentation","Imitation Learning"],"permalink":"/ai/review/2025-8-12-Shortcut_Learning_in_Generalist_Robot_Policies_The_Role_of_Dataset_Diversity_and_Fragmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Reinforcement_Learning_in_Vision_A_Survey","title":"[논문리뷰] Reinforcement Learning in Vision: A Survey","excerpt":"Qingwei Meng이 [arXiv]에 게시한 'Reinforcement Learning in Vision: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Reinforcement Learning (RL)","Computer Vision (CV)","Multimodal Large Language Models (MLLMs)","Visual Generation","Vision-Language-Action (VLA) Models","Policy Optimization","Reward Modeling"],"permalink":"/ai/review/2025-8-12-Reinforcement_Learning_in_Vision_A_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-ReasonRank_Empowering_Passage_Ranking_with_Strong_Reasoning_Ability","title":"[논문리뷰] ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability","excerpt":"Yuchen Li이 [arXiv]에 게시한 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Passage Ranking","Reasoning Models","Large Language Models","Data Synthesis","Reinforcement Learning","Listwise Reranking","Information Retrieval"],"permalink":"/ai/review/2025-8-12-ReasonRank_Empowering_Passage_Ranking_with_Strong_Reasoning_Ability/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Part_I_Tricks_or_Traps_A_Deep_Dive_into_RL_for_LLM_Reasoning","title":"[논문리뷰] Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning","excerpt":"Jiaheng Liu이 [arXiv]에 게시한 'Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","LLM Reasoning","Policy Optimization","Normalization","Clipping","Loss Aggregation","Overlong Filtering"],"permalink":"/ai/review/2025-8-12-Part_I_Tricks_or_Traps_A_Deep_Dive_into_RL_for_LLM_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-OmniEAR_Benchmarking_Agent_Reasoning_in_Embodied_Tasks","title":"[논문리뷰] OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks","excerpt":"Hongxing Li이 [arXiv]에 게시한 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Embodied AI","Agent Reasoning","LLM","Benchmarking","Tool Use","Multi-Agent Systems","Physical Interaction","Constraint Reasoning"],"permalink":"/ai/review/2025-8-12-OmniEAR_Benchmarking_Agent_Reasoning_in_Embodied_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Omni-Effects_Unified_and_Spatially-Controllable_Visual_Effects_Generation","title":"[논문리뷰] Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation","excerpt":"Xiaokun Feng이 [arXiv]에 게시한 'Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Visual Effects","Video Generation","LoRA","Mixture of Experts","Spatial Control","Diffusion Models","Multi-VFX"],"permalink":"/ai/review/2025-8-12-Omni-Effects_Unified_and_Spatially-Controllable_Visual_Effects_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-MolmoAct_Action_Reasoning_Models_that_can_Reason_in_Space","title":"[논문리뷰] MolmoAct: Action Reasoning Models that can Reason in Space","excerpt":"Shuo Liu이 [arXiv]에 게시한 'MolmoAct: Action Reasoning Models that can Reason in Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Robotics","Action Reasoning","Vision-Language Models","Spatial Planning","Depth Perception","Trajectory Generation","Explainable AI"],"permalink":"/ai/review/2025-8-12-MolmoAct_Action_Reasoning_Models_that_can_Reason_in_Space/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-MoBE_Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs","title":"[논문리뷰] MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs","excerpt":"Jianguo Li이 [arXiv]에 게시한 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Mixture-of-Experts (MoE)","LLM Compression","Matrix Decomposition","Parameter Efficiency","Deep Learning","Memory Optimization"],"permalink":"/ai/review/2025-8-12-MoBE_Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Less_Is_More_Training-Free_Sparse_Attention_with_Global_Locality_for_Efficient_Reasoning","title":"[논문리뷰] Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning","excerpt":"Baihong Yuan이 [arXiv]에 게시한 'Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Sparse Attention","LLMs","Reasoning Tasks","Efficiency","Training-Free","Global Locality","KV Cache Optimization"],"permalink":"/ai/review/2025-8-12-Less_Is_More_Training-Free_Sparse_Attention_with_Global_Locality_for_Efficient_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Klear-Reasoner_Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization","title":"[논문리뷰] Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization","excerpt":"Guanting Dong이 [arXiv]에 게시한 'Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Reasoning LLMs","Reinforcement Learning","PPO","Gradient Clipping","Supervised Fine-tuning","Math Reasoning","Code Generation","Policy Optimization"],"permalink":"/ai/review/2025-8-12-Klear-Reasoner_Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Grove_MoE_Towards_Efficient_and_Superior_MoE_LLMs_with_Adjugate_Experts","title":"[논문리뷰] Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts","excerpt":"Tieyuan Chen이 [arXiv]에 게시한 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Mixture of Experts","LLMs","MoE Architecture","Dynamic Activation","Adjugate Experts","Upcycling Strategy","Load Balancing"],"permalink":"/ai/review/2025-8-12-Grove_MoE_Towards_Efficient_and_Superior_MoE_LLMs_with_Adjugate_Experts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-GLiClass_Generalist_Lightweight_Model_for_Sequence_Classification_Tasks","title":"[논문리뷰] GLiClass: Generalist Lightweight Model for Sequence Classification Tasks","excerpt":"Alexander Yavorskyi이 [arXiv]에 게시한 'GLiClass: Generalist Lightweight Model for Sequence Classification Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Sequence Classification","Zero-shot Learning","Few-shot Learning","Transformer","Multi-label Classification","PPO","GLiNER","Computational Efficiency"],"permalink":"/ai/review/2025-8-12-GLiClass_Generalist_Lightweight_Model_for_Sequence_Classification_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Follow-Your-Shape_Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control","title":"[논문리뷰] Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control","excerpt":"Hongyu Liu이 [arXiv]에 게시한 'Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Image Editing","Shape Transformation","Rectified Flow","Trajectory Divergence Map","Region Control","Generative Models","Diffusion Models"],"permalink":"/ai/review/2025-8-12-Follow-Your-Shape_Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Fact2Fiction_Targeted_Poisoning_Attack_to_Agentic_Fact-checking_System","title":"[논문리뷰] Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System","excerpt":"Reynold Cheng이 [arXiv]에 게시한 'Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Adversarial Attack","Poisoning Attack","Fact-checking","LLM Agent","Retrieval Augmented Generation","Misinformation","System Security"],"permalink":"/ai/review/2025-8-12-Fact2Fiction_Targeted_Poisoning_Attack_to_Agentic_Fact-checking_System/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Deep_Ignorance_Filtering_Pretraining_Data_Builds_Tamper-Resistant_Safeguards_into_Open-Weight_LLMs","title":"[논문리뷰] Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs","excerpt":"Robert Kirk이 [arXiv]에 게시한 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","LLMs","데이터 필터링","사전 학습","변조 저항성","바이오위협","AI 안전","서킷 브레이킹","머신 언러닝"],"permalink":"/ai/review/2025-8-12-Deep_Ignorance_Filtering_Pretraining_Data_Builds_Tamper-Resistant_Safeguards_into_Open-Weight_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Compressing_Chain-of-Thought_in_LLMs_via_Step_Entropy","title":"[논문리뷰] Compressing Chain-of-Thought in LLMs via Step Entropy","excerpt":"Zhijian Xu이 [arXiv]에 게시한 'Compressing Chain-of-Thought in LLMs via Step Entropy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","LLM","Chain-of-Thought","CoT Compression","Step Entropy","Reinforcement Learning","SFT","GRPO"],"permalink":"/ai/review/2025-8-12-Compressing_Chain-of-Thought_in_LLMs_via_Step_Entropy/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-BrowseComp-Plus_A_More_Fair_and_Transparent_Evaluation_Benchmark_of_Deep-Research_Agent","title":"[논문리뷰] BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent","excerpt":"Kai Zou이 [arXiv]에 게시한 'BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Benchmarking","Deep-Research Agents","LLMs","Retrieval","Curated Corpus","Evaluation","Fairness","Transparency","Reproducibility"],"permalink":"/ai/review/2025-8-12-BrowseComp-Plus_A_More_Fair_and_Transparent_Evaluation_Benchmark_of_Deep-Research_Agent/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-Bifrost-1_Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents","title":"[논문리뷰] Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents","excerpt":"Mohit Bansal이 [arXiv]에 게시한 'Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Multimodal LLM","Diffusion Model","CLIP Latent","Image Generation","Multimodal Understanding","ControlNet","Training Efficiency"],"permalink":"/ai/review/2025-8-12-Bifrost-1_Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-12-A_Comprehensive_Survey_of_Self-Evolving_AI_Agents_A_New_Paradigm_Bridging_Foundation_Models_and_Lifelong_Agentic_Systems","title":"[논문리뷰] A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems","excerpt":"Xinhao Yi이 [arXiv]에 게시한 'A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","lastModifiedAt":"2025-08-12 13:29:09+0900","categories":["Review"],"tags":["Review","Self-Evolving AI Agents","Lifelong Learning","Foundation Models","Multi-Agent Systems","Agent Optimization","Prompt Engineering","Tool Use","AI Safety","Survey"],"permalink":"/ai/review/2025-8-12-A_Comprehensive_Survey_of_Self-Evolving_AI_Agents_A_New_Paradigm_Bridging_Foundation_Models_and_Lifelong_Agentic_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-Voost_A_Unified_and_Scalable_Diffusion_Transformer_for_Bidirectional_Virtual_Try-On_and_Try-Off","title":"[논문리뷰] Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off","excerpt":"jgkwak이 [arXiv]에 게시한 'Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Virtual Try-On","Virtual Try-Off","Diffusion Transformer","Bidirectional Learning","Generative AI","Fashion Synthesis","Attention Mechanism","Self-Correction"],"permalink":"/ai/review/2025-8-11-Voost_A_Unified_and_Scalable_Diffusion_Transformer_for_Bidirectional_Virtual_Try-On_and_Try-Off/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-UI-AGILE_Advancing_GUI_Agents_with_Effective_Reinforcement_Learning_and_Precise_Inference-Time_Grounding","title":"[논문리뷰] UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding","excerpt":"Bingqi Chen이 [arXiv]에 게시한 'UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","GUI Agents","Reinforcement Learning","Grounding","MLLMs","Reward Function","Resampling","Visual Noise Reduction"],"permalink":"/ai/review/2025-8-11-UI-AGILE_Advancing_GUI_Agents_with_Effective_Reinforcement_Learning_and_Precise_Inference-Time_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-Pruning_the_Unsurprising_Efficient_Code_Reasoning_via_First-Token_Surprisal","title":"[논문리뷰] Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal","excerpt":"Chengcheng Wan이 [arXiv]에 게시한 'Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Code Reasoning","CoT Compression","LLMs","Efficiency","Surprisal","Pruning","Fine-tuning","Large Reasoning Models"],"permalink":"/ai/review/2025-8-11-Pruning_the_Unsurprising_Efficient_Code_Reasoning_via_First-Token_Surprisal/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate_3D_Mesh","title":"[논문리뷰] MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh","excerpt":"Yi Yang이 [arXiv]에 게시한 'MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","3D Mesh Generation","LLMs","Mesh Understanding","Text-to-3D","Primitive-Mesh Decomposition","Progressive Training","Multimodal AI"],"permalink":"/ai/review/2025-8-11-MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate_3D_Mesh/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-Memp_Exploring_Agent_Procedural_Memory","title":"[논문리뷰] Memp: Exploring Agent Procedural Memory","excerpt":"Shuofei Qiao이 [arXiv]에 게시한 'Memp: Exploring Agent Procedural Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Procedural Memory","LLM Agents","Memory Management","Task Automation","Lifelong Learning","Experience Replay","Agent Learning"],"permalink":"/ai/review/2025-8-11-Memp_Exploring_Agent_Procedural_Memory/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-MELLA_Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs","title":"[논문리뷰] MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs","excerpt":"Guohang Yan이 [arXiv]에 게시한 'MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Multimodal Large Language Models","Low-Resource Languages","Cultural Groundedness","Linguistic Capability","Dataset Creation","Multilingual AI"],"permalink":"/ai/review/2025-8-11-MELLA_Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-LightSwitch_Multi-view_Relighting_with_Material-guided_Diffusion","title":"[논문리뷰] LightSwitch: Multi-view Relighting with Material-guided Diffusion","excerpt":"Shubham Tulsiani이 [arXiv]에 게시한 'LightSwitch: Multi-view Relighting with Material-guided Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Multi-view Relighting","Diffusion Models","Material-guided","Inverse Rendering","3D Scene Reconstruction","Image Synthesis","Consistent Relighting"],"permalink":"/ai/review/2025-8-11-LightSwitch_Multi-view_Relighting_with_Material-guided_Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-InfiGUI-G1_Advancing_GUI_Grounding_with_Adaptive_Exploration_Policy_Optimization","title":"[논문리뷰] InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization","excerpt":"Pengxiang Li이 [arXiv]에 게시한 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","GUI Grounding","MLLMs","Reinforcement Learning","Policy Optimization","Exploration Strategy","Semantic Alignment","Adaptive Exploration Reward","Human-Computer Interaction"],"permalink":"/ai/review/2025-8-11-InfiGUI-G1_Advancing_GUI_Grounding_with_Adaptive_Exploration_Policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-GLM-4.5_Agentic_Reasoning_and_Coding_ARC_Foundation_Models","title":"[논문리뷰] GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models","excerpt":"GLM-4. 5 Team이 [arXiv]에 게시한 'GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Large Language Model","Mixture-of-Experts","Agentic AI","Reasoning","Code Generation","Reinforcement Learning","Foundation Model"],"permalink":"/ai/review/2025-8-11-GLM-4.5_Agentic_Reasoning_and_Coding_ARC_Foundation_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-GENIE_Gaussian_Encoding_for_Neural_Radiance_Fields_Interactive_Editing","title":"[논문리뷰] GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing","excerpt":"Przemysław Spurek이 [arXiv]에 게시한 'GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Neural Radiance Fields (NeRF)","Gaussian Splatting (GS)","Interactive Editing","3D Scene Representation","Physics Simulation","Hybrid Model","Real-time Rendering","Ray Tracing"],"permalink":"/ai/review/2025-8-11-GENIE_Gaussian_Encoding_for_Neural_Radiance_Fields_Interactive_Editing/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-11-Adapting_Vision-Language_Models_Without_Labels_A_Comprehensive_Survey","title":"[논문리뷰] Adapting Vision-Language Models Without Labels: A Comprehensive Survey","excerpt":"Eleni Chatzi이 [arXiv]에 게시한 'Adapting Vision-Language Models Without Labels: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","lastModifiedAt":"2025-08-11 13:13:28+0900","categories":["Review"],"tags":["Review","Vision-Language Models (VLMs)","Unsupervised Adaptation","Test-Time Adaptation (TTA)","Domain Transfer","Multimodal Learning","Label-Free Learning","Zero-Shot Learning"],"permalink":"/ai/review/2025-8-11-Adapting_Vision-Language_Models_Without_Labels_A_Comprehensive_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Visual_Document_Understanding_and_Question_Answering_A_Multi-Agent_Collaboration_Framework_with_Test-Time_Scaling","title":"[논문리뷰] Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling","excerpt":"Ruolin Shen이 [arXiv]에 게시한 'Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Visual Document Understanding","Visual Question Answering","Multi-Agent System","Test-Time Scaling","Self-Correction","Mixed Reward Modeling","Large Language Models"],"permalink":"/ai/review/2025-8-8-Visual_Document_Understanding_and_Question_Answering_A_Multi-Agent_Collaboration_Framework_with_Test-Time_Scaling/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-StrandDesigner_Towards_Practical_Strand_Generation_with_Sketch_Guidance","title":"[논문리뷰] StrandDesigner: Towards Practical Strand Generation with Sketch Guidance","excerpt":"Xiaobin Hu이 [arXiv]에 게시한 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Strand Generation","Sketch Guidance","Diffusion Models","Multi-scale Learning","Adaptive Conditioning","3D Hair Modeling","Computer Graphics"],"permalink":"/ai/review/2025-8-8-StrandDesigner_Towards_Practical_Strand_Generation_with_Sketch_Guidance/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Steering_One-Step_Diffusion_Model_with_Fidelity-Rich_Decoder_for_Fast_Image_Compression","title":"[논문리뷰] Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression","excerpt":"Yifei Ji이 [arXiv]에 게시한 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Image Compression","Diffusion Models","One-Step Decoding","Fidelity Guidance","Rate Annealing","VAE","Perceptual Quality"],"permalink":"/ai/review/2025-8-8-Steering_One-Step_Diffusion_Model_with_Fidelity-Rich_Decoder_for_Fast_Image_Compression/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-RPCANet_Deep_Interpretable_Robust_PCA_for_Sparse_Object_Segmentation","title":"[논문리뷰] RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation","excerpt":"Jian Yang이 [arXiv]에 게시한 'RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Robust PCA","Deep Unfolding","Sparse Segmentation","Interpretability","Image Decomposition","Computer Vision"],"permalink":"/ai/review/2025-8-8-RPCANet_Deep_Interpretable_Robust_PCA_for_Sparse_Object_Segmentation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-REINA_Regularized_Entropy_Information-Based_Loss_for_Efficient_Simultaneous_Speech_Translation","title":"[논문리뷰] REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation","excerpt":"Xiao Yu이 [arXiv]에 게시한 'REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Simultaneous Speech Translation","Adaptive Policy","Entropy-based Loss","Mutual Information","Latency-Quality Trade-off","Speech-to-Text Translation","REINA"],"permalink":"/ai/review/2025-8-8-REINA_Regularized_Entropy_Information-Based_Loss_for_Efficient_Simultaneous_Speech_Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-R-Zero_Self-Evolving_Reasoning_LLM_from_Zero_Data","title":"[논문리뷰] R-Zero: Self-Evolving Reasoning LLM from Zero Data","excerpt":"Zongxia Li이 [arXiv]에 게시한 'R-Zero: Self-Evolving Reasoning LLM from Zero Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Self-Evolving LLM","Reinforcement Learning","Curriculum Learning","Reasoning","Large Language Models","Self-Play","Zero-Data Training"],"permalink":"/ai/review/2025-8-8-R-Zero_Self-Evolving_Reasoning_LLM_from_Zero_Data/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-PRvL_Quantifying_the_Capabilities_and_Risks_of_Large_Language_Models_for_PII_Redaction","title":"[논문리뷰] PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction","excerpt":"Prajit Das이 [arXiv]에 게시한 'PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","PII Redaction","Large Language Models","Instruction Tuning","Retrieval-Augmented Generation","Privacy Preservation","Model Evaluation","Cross-Domain Generalization","Open-Source LLMs"],"permalink":"/ai/review/2025-8-8-PRvL_Quantifying_the_Capabilities_and_Risks_of_Large_Language_Models_for_PII_Redaction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-On_the_Generalization_of_SFT_A_Reinforcement_Learning_Perspective_with_Reward_Rectification","title":"[논문리뷰] On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification","excerpt":"Xinyu Ye이 [arXiv]에 게시한 'On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Generalization","Reward Rectification","Dynamic Fine-Tuning (DFT)","LLM","Policy Gradient","Mathematical Reasoning"],"permalink":"/ai/review/2025-8-8-On_the_Generalization_of_SFT_A_Reinforcement_Learning_Perspective_with_Reward_Rectification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Marco-Voice_Technical_Report","title":"[논문리뷰] Marco-Voice Technical Report","excerpt":"Qingjuan Li이 [arXiv]에 게시한 'Marco-Voice Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Speech Synthesis","Voice Cloning","Emotion Control","Text-to-Speech","Disentanglement","Contrastive Learning","Flow Matching","Emotional Speech Dataset"],"permalink":"/ai/review/2025-8-8-Marco-Voice_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-MOSEv2_A_More_Challenging_Dataset_for_Video_Object_Segmentation_in_Complex_Scenes","title":"[논문리뷰] MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes","excerpt":"Xudong Jiang이 [arXiv]에 게시한 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Video Object Segmentation","Dataset","Complex Scenes","Benchmark","Object Tracking","Computer Vision","Dataset Challenges"],"permalink":"/ai/review/2025-8-8-MOSEv2_A_More_Challenging_Dataset_for_Video_Object_Segmentation_in_Complex_Scenes/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-InfiAlign_A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabilities","title":"[논문리뷰] InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities","excerpt":"Zhijie Sang이 [arXiv]에 게시한 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","LLM Alignment","Reasoning","Data Curation","Supervised Fine-tuning (SFT)","Direct Preference Optimization (DPO)","Sample Efficiency","Scalability","Multi-dimensional Filtering"],"permalink":"/ai/review/2025-8-8-InfiAlign_A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabilities/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-I_Think_Therefore_I_Am_Under-Qualified_A_Benchmark_for_Evaluating_Linguistic_Shibboleth_Detection_in_LLM_Hiring_Evaluations","title":"[논문리뷰] I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations","excerpt":"Chirag Shah이 [arXiv]에 게시한 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","LLM Bias","Hiring Evaluation","Linguistic Shibboleth","Hedging Language","Fairness","Benchmarking","Sociolinguistics"],"permalink":"/ai/review/2025-8-8-I_Think_Therefore_I_Am_Under-Qualified_A_Benchmark_for_Evaluating_Linguistic_Shibboleth_Detection_in_LLM_Hiring_Evaluations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-I2CR_Intra-_and_Inter-modal_Collaborative_Reflections_for_Multimodal_Entity_Linking","title":"[논문리뷰] I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking","excerpt":"Chao Wang이 [arXiv]에 게시한 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Multimodal Entity Linking","Large Language Models","Collaborative Reflection","Iterative Reasoning","Visual Information","Text-centric"],"permalink":"/ai/review/2025-8-8-I2CR_Intra-_and_Inter-modal_Collaborative_Reflections_for_Multimodal_Entity_Linking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Hop_Skip_and_Overthink_Diagnosing_Why_Reasoning_Models_Fumble_during_Multi-Hop_Analysis","title":"[논문리뷰] Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis","excerpt":"Reshmi Ghosh이 [arXiv]에 게시한 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Multi-hop Question Answering","Large Language Models","Reasoning Errors","Error Taxonomy","Human Evaluation","Automated Evaluation","Overthinking"],"permalink":"/ai/review/2025-8-8-Hop_Skip_and_Overthink_Diagnosing_Why_Reasoning_Models_Fumble_during_Multi-Hop_Analysis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Hi3DEval_Advancing_3D_Generation_Evaluation_with_Hierarchical_Validity","title":"[논문리뷰] Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity","excerpt":"Zhibing Li이 [arXiv]에 게시한 'Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","3D Generation Evaluation","Hierarchical Evaluation","Material Properties","Multi-Agent Annotation","Hybrid Scoring System","Video-based Evaluation","Part-level Analysis"],"permalink":"/ai/review/2025-8-8-Hi3DEval_Advancing_3D_Generation_Evaluation_with_Hierarchical_Validity/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Genie_Envisioner_A_Unified_World_Foundation_Platform_for_Robotic_Manipulation","title":"[논문리뷰] Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation","excerpt":"Shengcong Chen이 [arXiv]에 게시한 'Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Robotic Manipulation","World Model","Video Generation","Diffusion Model","Embodied AI","Foundation Model","Robotics Simulation","Policy Learning"],"permalink":"/ai/review/2025-8-8-Genie_Envisioner_A_Unified_World_Foundation_Platform_for_Robotic_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Evaluating_Synthesizing_and_Enhancing_for_Customer_Support_Conversation","title":"[논문리뷰] Evaluating, Synthesizing, and Enhancing for Customer Support Conversation","excerpt":"Feng Chen이 [arXiv]에 게시한 'Evaluating, Synthesizing, and Enhancing for Customer Support Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Customer Support","Dialogue Generation","Large Language Models","Role-Playing","COPC Framework","Synthetic Data","Strategy Prediction","Empathetic AI"],"permalink":"/ai/review/2025-8-8-Evaluating_Synthesizing_and_Enhancing_for_Customer_Support_Conversation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Dont_Overthink_It_A_Survey_of_Efficient_R1-style_Large_Reasoning_Models","title":"[논문리뷰] Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models","excerpt":"Fangzhou Yao이 [arXiv]에 게시한 'Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Large Reasoning Models","Efficient Reasoning","Chain-of-Thought","Model Optimization","Model Collaboration","Overthinking Problem","LLM Efficiency"],"permalink":"/ai/review/2025-8-8-Dont_Overthink_It_A_Survey_of_Efficient_R1-style_Large_Reasoning_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-DeepPHY_Benchmarking_Agentic_VLMs_on_Physical_Reasoning","title":"[논문리뷰] DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning","excerpt":"Ziming Wang이 [arXiv]에 게시한 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Vision Language Models (VLMs)","Agentic AI","Physical Reasoning","Benchmark","Simulation Environments","Action Planning","Interactive AI"],"permalink":"/ai/review/2025-8-8-DeepPHY_Benchmarking_Agentic_VLMs_on_Physical_Reasoning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-CoAct-1_Computer-using_Agents_with_Coding_as_Actions","title":"[논문리뷰] CoAct-1: Computer-using Agents with Coding as Actions","excerpt":"Taiwei Shi이 [arXiv]에 게시한 'CoAct-1: Computer-using Agents with Coding as Actions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","AI Agent","Multi-agent System","GUI Automation","Programmatic Control","Code Generation","OSWorld Benchmark","Hybrid AI"],"permalink":"/ai/review/2025-8-8-CoAct-1_Computer-using_Agents_with_Coding_as_Actions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Can_Large_Multimodal_Models_Actively_Recognize_Faulty_Inputs_A_Systematic_Evaluation_Framework_of_Their_Input_Scrutiny_Ability","title":"[논문리뷰] Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability","excerpt":"Yuan Wu이 [arXiv]에 게시한 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Large Multimodal Models","Input Scrutiny","Error Detection","Faulty Inputs","Evaluation Framework","Modality Preference","Cross-Modal Inconsistency"],"permalink":"/ai/review/2025-8-8-Can_Large_Multimodal_Models_Actively_Recognize_Faulty_Inputs_A_Systematic_Evaluation_Framework_of_Their_Input_Scrutiny_Ability/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Are_We_on_the_Right_Way_for_Assessing_Document_Retrieval-Augmented_Generation","title":"[논문리뷰] Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?","excerpt":"Junjie Yang이 [arXiv]에 게시한 'Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Retrieval-Augmented Generation","Multimodal LLMs","Benchmark Evaluation","Document Understanding","Multi-hop Reasoning","Information Retrieval","Evaluation Dataset"],"permalink":"/ai/review/2025-8-8-Are_We_on_the_Right_Way_for_Assessing_Document_Retrieval-Augmented_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-8-Are_Todays_LLMs_Ready_to_Explain_Well-Being_Concepts","title":"[논문리뷰] Are Today's LLMs Ready to Explain Well-Being Concepts?","excerpt":"Huan Liu이 [arXiv]에 게시한 'Are Today's LLMs Ready to Explain Well-Being Concepts?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","lastModifiedAt":"2025-08-08 13:32:22+0900","categories":["Review"],"tags":["Review","Large Language Models","Well-being Concepts","LLM Evaluation","Principle-Guided Evaluation","LLM-as-a-Judge","Supervised Fine-Tuning (SFT)","Direct Preference Optimization (DPO)","Explanation Generation"],"permalink":"/ai/review/2025-8-8-Are_Todays_LLMs_Ready_to_Explain_Well-Being_Concepts/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Web-CogReasoner_Towards_Knowledge-Induced_Cognitive_Reasoning_for_Web_Agents","title":"[논문리뷰] Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents","excerpt":"Xinyu Yang이 [arXiv]에 게시한 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Web Agent","Cognitive Reasoning","Knowledge-Induced","Large Multimodal Models (LMMs)","Bloom's Taxonomy","Chain-of-Thought (CoT)","Web-CogDataset","Web-CogBench"],"permalink":"/ai/review/2025-8-7-Web-CogReasoner_Towards_Knowledge-Induced_Cognitive_Reasoning_for_Web_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Training_Long-Context_Multi-Turn_Software_Engineering_Agents_with_Reinforcement_Learning","title":"[논문리뷰] Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning","excerpt":"Maksim Nekrashevich이 [arXiv]에 게시한 'Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","Software Engineering","Multi-Turn Interaction","Long Context","DAPO","Autonomous Agents","SWE-BENCH"],"permalink":"/ai/review/2025-8-7-Training_Long-Context_Multi-Turn_Software_Engineering_Agents_with_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-The_Cow_of_Rembrandt_-_Analyzing_Artistic_Prompt_Interpretation_in_Text-to-Image_Models","title":"[논문리뷰] The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models","excerpt":"Elisabetta Rocchetti이 [arXiv]에 게시한 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Text-to-Image Generation","Diffusion Models","Cross-Attention Analysis","Content-Style Disentanglement","Artistic Style Transfer","Explainable AI","SDXL"],"permalink":"/ai/review/2025-8-7-The_Cow_of_Rembrandt_-_Analyzing_Artistic_Prompt_Interpretation_in_Text-to-Image_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Sotopia-RL_Reward_Design_for_Social_Intelligence","title":"[논문리뷰] Sotopia-RL: Reward Design for Social Intelligence","excerpt":"Keyang Xuan이 [arXiv]에 게시한 'Sotopia-RL: Reward Design for Social Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Social Intelligence","Reinforcement Learning","Reward Design","Large Language Models","Utterance-level Rewards","Multi-dimensional Rewards","Partial Observability","SOTOPIA"],"permalink":"/ai/review/2025-8-7-Sotopia-RL_Reward_Design_for_Social_Intelligence/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-SonicMaster_Towards_Controllable_All-in-One_Music_Restoration_and_Mastering","title":"[논문리뷰] SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering","excerpt":"Ambuj Mehrish이 [arXiv]에 게시한 'SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Music Restoration","Audio Mastering","Generative Models","Flow Matching","Text-to-Audio","Audio Quality Enhancement","Multi-task Learning","Dataset Creation"],"permalink":"/ai/review/2025-8-7-SonicMaster_Towards_Controllable_All-in-One_Music_Restoration_and_Mastering/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Sel3DCraft_Interactive_Visual_Prompts_for_User-Friendly_Text-to-3D_Generation","title":"[논문리뷰] Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation","excerpt":"Hao Huang이 [arXiv]에 게시한 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Text-to-3D Generation","Prompt Engineering","Visual Analytics","Human-Computer Interaction","Multi-modal Large Language Models","3D Model Evaluation"],"permalink":"/ai/review/2025-8-7-Sel3DCraft_Interactive_Visual_Prompts_for_User-Friendly_Text-to-3D_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Sculptor_Empowering_LLMs_with_Cognitive_Agency_via_Active_Context_Management","title":"[논문리뷰] Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management","excerpt":"Yunxin Liu이 [arXiv]에 게시한 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Large Language Models","Active Context Management","Proactive Interference","Tool Augmentation","Working Memory","Context Curation","Long Context"],"permalink":"/ai/review/2025-8-7-Sculptor_Empowering_LLMs_with_Cognitive_Agency_via_Active_Context_Management/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-SEAgent_Self-Evolving_Computer_Use_Agent_with_Autonomous_Learning_from_Experience","title":"[논문리뷰] SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience","excerpt":"Xiaoyi Dong이 [arXiv]에 게시한 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Computer Use Agent","Self-Evolving","Reinforcement Learning","Curriculum Learning","Vision-Language Models","Experiential Learning","Specialist-to-Generalist"],"permalink":"/ai/review/2025-8-7-SEAgent_Self-Evolving_Computer_Use_Agent_with_Autonomous_Learning_from_Experience/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Reasoning_Language_Models_for_Root_Cause_Analysis_in_5G_Wireless_Networks","title":"[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks","excerpt":"Haozhe Zhang이 [arXiv]에 게시한 'Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Root Cause Analysis","Large Language Models","5G Wireless Networks","Supervised Fine-Tuning","Reinforcement Learning","Chain-of-Thought","TeleLogs Dataset"],"permalink":"/ai/review/2025-8-7-Reasoning_Language_Models_for_Root_Cause_Analysis_in_5G_Wireless_Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-RL-PLUS_Countering_Capability_Boundary_Collapse_of_LLMs_in_Reinforcement_Learning_with_Hybrid-policy_Optimization","title":"[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization","excerpt":"Kechi Zhang이 [arXiv]에 게시한 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Large Language Models","Reinforcement Learning","Capability Collapse","Hybrid Policy Optimization","Multiple Importance Sampling","Exploration","Math Reasoning","Out-of-Distribution"],"permalink":"/ai/review/2025-8-7-RL-PLUS_Countering_Capability_Boundary_Collapse_of_LLMs_in_Reinforcement_Learning_with_Hybrid-policy_Optimization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Position_The_Current_AI_Conference_Model_is_Unsustainable_Diagnosing_the_Crisis_of_Centralized_AI_Conference","title":"[논문리뷰] Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference","excerpt":"Jiaying Wu이 [arXiv]에 게시한 'Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","AI Conferences","Sustainability","Peer Review","Community Building","Environmental Impact","Mental Health","Centralized Model","Decentralized Model"],"permalink":"/ai/review/2025-8-7-Position_The_Current_AI_Conference_Model_is_Unsustainable_Diagnosing_the_Crisis_of_Centralized_AI_Conference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-OpenMed_NER_Open-Source_Domain-Adapted_State-of-the-Art_Transformers_for_Biomedical_NER_Across_12_Public_Datasets","title":"[논문리뷰] OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets","excerpt":"MaziyarPanahi이 [arXiv]에 게시한 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Biomedical NER","Transformer","Domain Adaptation","LoRA","Open-Source","Named Entity Recognition","Healthcare AI"],"permalink":"/ai/review/2025-8-7-OpenMed_NER_Open-Source_Domain-Adapted_State-of-the-Art_Transformers_for_Biomedical_NER_Across_12_Public_Datasets/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-MiDashengLM_Efficient_Audio_Understanding_with_General_Audio_Captions","title":"[논문리뷰] MiDashengLM: Efficient Audio Understanding with General Audio Captions","excerpt":"Yadong Niu이 [arXiv]에 게시한 'MiDashengLM: Efficient Audio Understanding with General Audio Captions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Audio-Language Model","General Audio Captions","Audio Understanding","Speech Recognition","Efficient Inference","Public Datasets","Multimodality","Data Curation"],"permalink":"/ai/review/2025-8-7-MiDashengLM_Efficient_Audio_Understanding_with_General_Audio_Captions/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Light-IF_Endowing_LLMs_with_Generalizable_Reasoning_via_Preview_and_Self-Checking_for_Complex_Instruction_Following","title":"[논문리뷰] Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following","excerpt":"Liang Xu이 [arXiv]에 게시한 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","LLMs","Instruction Following","Reasoning","Reinforcement Learning","Supervised Fine-tuning","Entropy Regularization","Self-Checking","Previewing"],"permalink":"/ai/review/2025-8-7-Light-IF_Endowing_LLMs_with_Generalizable_Reasoning_via_Preview_and_Self-Checking_for_Complex_Instruction_Following/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-LeanK_Learnable_K_Cache_Channel_Pruning_for_Efficient_Decoding","title":"[논문리뷰] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding","excerpt":"Yuqing Yang이 [arXiv]에 게시한 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","LLM","KV Cache Optimization","Model Pruning","Efficient Decoding","Memory Optimization","Static Sparsity","Transformer"],"permalink":"/ai/review/2025-8-7-LeanK_Learnable_K_Cache_Channel_Pruning_for_Efficient_Decoding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-LaTCoder_Converting_Webpage_Design_to_Code_with_Layout-as-Thought","title":"[논문리뷰] LaTCoder: Converting Webpage Design to Code with Layout-as-Thought","excerpt":"Tianpeng Lv이 [arXiv]에 게시한 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Design-to-Code","Webpage Generation","Multimodal Large Language Models (MLLMs)","Layout Preservation","Chain-of-Thought (CoT)","UI Automation","Code Generation"],"permalink":"/ai/review/2025-8-7-LaTCoder_Converting_Webpage_Design_to_Code_with_Layout-as-Thought/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Is_Chain-of-Thought_Reasoning_of_LLMs_a_Mirage_A_Data_Distribution_Lens","title":"[논문리뷰] Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens","excerpt":"Zhen Tan이 [arXiv]에 게시한 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Chain-of-Thought","LLMs","OOD Generalization","Data Distribution Shift","Reasoning","Pattern Matching","DataAlchemy"],"permalink":"/ai/review/2025-8-7-Is_Chain-of-Thought_Reasoning_of_LLMs_a_Mirage_A_Data_Distribution_Lens/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-IFDECORATOR_Wrapping_Instruction_Following_Reinforcement_Learning_with_Verifiable_Rewards","title":"[논문리뷰] IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards","excerpt":"Ling-I Wu이 [arXiv]에 게시한 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Instruction Following","Reinforcement Learning","Reward Hacking","LLMs","Curriculum Learning","Data Flywheel","Verifiable Rewards"],"permalink":"/ai/review/2025-8-7-IFDECORATOR_Wrapping_Instruction_Following_Reinforcement_Learning_with_Verifiable_Rewards/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-IAUNet_Instance-Aware_U-Net","title":"[논문리뷰] IAUNet: Instance-Aware U-Net","excerpt":"Dmytro Fishman이 [arXiv]에 게시한 'IAUNet: Instance-Aware U-Net' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Instance Segmentation","U-Net","Query-based Model","Transformer Decoder","Biomedical Imaging","Cell Segmentation","Deep Learning"],"permalink":"/ai/review/2025-8-7-IAUNet_Instance-Aware_U-Net/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-HPSv3_Towards_Wide-Spectrum_Human_Preference_Score","title":"[논문리뷰] HPSv3: Towards Wide-Spectrum Human Preference Score","excerpt":"Hongsheng Li이 [arXiv]에 게시한 'HPSv3: Towards Wide-Spectrum Human Preference Score' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Human Preference Score","Text-to-Image Generation","Image Evaluation","Vision-Language Models (VLMs)","Uncertainty-Aware Ranking Loss","Dataset","Iterative Refinement","Chain-of-Thought"],"permalink":"/ai/review/2025-8-7-HPSv3_Towards_Wide-Spectrum_Human_Preference_Score/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Gaussian_Variation_Field_Diffusion_for_High-fidelity_Video-to-4D_Synthesis","title":"[논문리뷰] Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis","excerpt":"Feng Zhao이 [arXiv]에 게시한 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","4D Generation","Video-to-3D Synthesis","Gaussian Splatting","Diffusion Models","Latent Space Modeling","Variational Autoencoder","Temporal Coherence"],"permalink":"/ai/review/2025-8-7-Gaussian_Variation_Field_Diffusion_for_High-fidelity_Video-to-4D_Synthesis/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Enhancing_Vision-Language_Model_Training_with_Reinforcement_Learning_in_Synthetic_Worlds_for_Real-World_Success","title":"[논문리뷰] Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success","excerpt":"Ruslan Rakhimov이 [arXiv]에 게시한 'Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Vision-Language Models","Synthetic Worlds","Transfer Learning","PPO","Actor-Critic","Embodied AI"],"permalink":"/ai/review/2025-8-7-Enhancing_Vision-Language_Model_Training_with_Reinforcement_Learning_in_Synthetic_Worlds_for_Real-World_Success/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Efficient_Agents_Building_Effective_Agents_While_Reducing_Cost","title":"[논문리뷰] Efficient Agents: Building Effective Agents While Reducing Cost","excerpt":"Yue Hou이 [arXiv]에 게시한 'Efficient Agents: Building Effective Agents While Reducing Cost' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","LLM Agents","Cost Efficiency","Performance-Cost Trade-off","Agent Frameworks","GAIA Benchmark","Optimization","Resource Management"],"permalink":"/ai/review/2025-8-7-Efficient_Agents_Building_Effective_Agents_While_Reducing_Cost/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-EVOC2RUST_A_Skeleton-guided_Framework_for_Project-Level_C-to-Rust_Translation","title":"[논문리뷰] EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation","excerpt":"Dong Chen이 [arXiv]에 게시한 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","C-to-Rust Conversion","Project-Level Translation","Large Language Models","Code Synthesis","Memory Safety","Software Migration","Hybrid Translation"],"permalink":"/ai/review/2025-8-7-EVOC2RUST_A_Skeleton-guided_Framework_for_Project-Level_C-to-Rust_Translation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-DreamVVT_Mastering_Realistic_Video_Virtual_Try-On_in_the_Wild_via_a_Stage-Wise_Diffusion_Transformer_Framework","title":"[논문리뷰] DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework","excerpt":"Chao Liang이 [arXiv]에 게시한 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Video Virtual Try-On","Diffusion Transformers","Stage-Wise Framework","Vision-Language Models","LoRA","Temporal Consistency","Garment Preservation"],"permalink":"/ai/review/2025-8-7-DreamVVT_Mastering_Realistic_Video_Virtual_Try-On_in_the_Wild_via_a_Stage-Wise_Diffusion_Transformer_Framework/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-CoTox_Chain-of-Thought-Based_Molecular_Toxicity_Reasoning_and_Prediction","title":"[논문리뷰] CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction","excerpt":"Donghyeon Lee이 [arXiv]에 게시한 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Toxicity Prediction","Large Language Model","Chain-of-Thought","Drug Development","Cheminformatics","Interpretable AI","IUPAC Nomenclature"],"permalink":"/ai/review/2025-8-7-CoTox_Chain-of-Thought-Based_Molecular_Toxicity_Reasoning_and_Prediction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-C3D-AD_Toward_Continual_3D_Anomaly_Detection_via_Kernel_Attention_with_Learnable_Advisor","title":"[논문리뷰] C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor","excerpt":"Jinbao Wang이 [arXiv]에 게시한 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","3D Anomaly Detection","Continual Learning","Kernel Attention","Learnable Advisor","Parameter Perturbation","Point Cloud","Industrial AI"],"permalink":"/ai/review/2025-8-7-C3D-AD_Toward_Continual_3D_Anomaly_Detection_via_Kernel_Attention_with_Learnable_Advisor/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-Agent_Lightning_Train_ANY_AI_Agents_with_Reinforcement_Learning","title":"[논문리뷰] Agent Lightning: Train ANY AI Agents with Reinforcement Learning","excerpt":"Zilong Wang이 [arXiv]에 게시한 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Large Language Models","AI Agents","Framework","Markov Decision Process","Hierarchical RL","Training-Agent Disaggregation","Observability"],"permalink":"/ai/review/2025-8-7-Agent_Lightning_Train_ANY_AI_Agents_with_Reinforcement_Learning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-7-A_Coarse-to-Fine_Approach_to_Multi-Modality_3D_Occupancy_Grounding","title":"[논문리뷰] A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding","excerpt":"Jianke Zhu이 [arXiv]에 게시한 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","lastModifiedAt":"2025-08-07 13:38:21+0900","categories":["Review"],"tags":["Review","3D Occupancy Grounding","Multi-modal Learning","Natural Language Understanding","Autonomous Driving","Voxel-based Prediction","Benchmark Dataset","Coarse-to-Fine"],"permalink":"/ai/review/2025-8-7-A_Coarse-to-Fine_Approach_to_Multi-Modality_3D_Occupancy_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-Tool-integrated_Reinforcement_Learning_for_Repo_Deep_Search","title":"[논문리뷰] Tool-integrated Reinforcement Learning for Repo Deep Search","excerpt":"Yanzhen Zou이 [arXiv]에 게시한 'Tool-integrated Reinforcement Learning for Repo Deep Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Issue Localization","Large Language Models (LLMs)","Reinforcement Learning (RL)","Supervised Fine-tuning (SFT)","Tool-integrated Agents","Software Engineering","Code Search"],"permalink":"/ai/review/2025-8-6-Tool-integrated_Reinforcement_Learning_for_Repo_Deep_Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-TRACEALIGN_--_Tracing_the_Drift_Attributing_Alignment_Failures_to_Training-Time_Belief_Sources_in_LLMs","title":"[논문리뷰] TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs","excerpt":"Aman Chadha이 [arXiv]에 게시한 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","LLM Alignment","Alignment Drift","Training Data Provenance","Belief Conflict Index (BCI)","Suffix Array","Safety Interventions","Reinforcement Learning from Human Feedback","Explainable AI"],"permalink":"/ai/review/2025-8-6-TRACEALIGN_--_Tracing_the_Drift_Attributing_Alignment_Failures_to_Training-Time_Belief_Sources_in_LLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-Skywork_UniPic_Unified_Autoregressive_Modeling_for_Visual_Understanding_and_Generation","title":"[논문리뷰] Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation","excerpt":"Tianyidan Xie이 [arXiv]에 게시한 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Autoregressive Models","Multimodal AI","Image Generation","Image Editing","Visual Understanding","Unified Architecture","Parameter Efficiency"],"permalink":"/ai/review/2025-8-6-Skywork_UniPic_Unified_Autoregressive_Modeling_for_Visual_Understanding_and_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-Seed_Diffusion_A_Large-Scale_Diffusion_Language_Model_with_High-Speed_Inference","title":"[논문리뷰] Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference","excerpt":"Fan Xia이 [arXiv]에 게시한 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Diffusion Models","Language Models","Code Generation","Non-Autoregressive Inference","High-Speed Inference","Discrete Diffusion","LLM Inference"],"permalink":"/ai/review/2025-8-6-Seed_Diffusion_A_Large-Scale_Diffusion_Language_Model_with_High-Speed_Inference/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-Multi-human_Interactive_Talking_Dataset","title":"[논문리뷰] Multi-human Interactive Talking Dataset","excerpt":"Mike Zheng Shou이 [arXiv]에 게시한 'Multi-human Interactive Talking Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Multi-human Video Generation","Interactive Talking","Dataset","Audio-driven Animation","Pose Control","Speech Interaction","Diffusion Models"],"permalink":"/ai/review/2025-8-6-Multi-human_Interactive_Talking_Dataset/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-LongVie_Multimodal-Guided_Controllable_Ultra-Long_Video_Generation","title":"[논문리뷰] LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation","excerpt":"Chenyang Si이 [arXiv]에 게시한 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Ultra-long Video Generation","Multimodal Guidance","Controllable Video Generation","Diffusion Models","Temporal Consistency","Visual Quality","Autoregressive Generation","Degradation-aware Training"],"permalink":"/ai/review/2025-8-6-LongVie_Multimodal-Guided_Controllable_Ultra-Long_Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-LiveMCPBench_Can_Agents_Navigate_an_Ocean_of_MCP_Tools","title":"[논문리뷰] LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?","excerpt":"Yaojie Lu이 [arXiv]에 게시한 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","LLM Agent","Tool-use","MCP","Benchmark","Large-scale","Real-world tasks","Automated Evaluation","Meta-tool-learning"],"permalink":"/ai/review/2025-8-6-LiveMCPBench_Can_Agents_Navigate_an_Ocean_of_MCP_Tools/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-LAMIC_Layout-Aware_Multi-Image_Composition_via_Scalability_of_Multimodal_Diffusion_Transformer","title":"[논문리뷰] LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer","excerpt":"Shunyu Yao이 [arXiv]에 게시한 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Multi-Image Composition","Layout Control","Diffusion Models","Transformer","Attention Mechanisms","Training-Free","Zero-Shot Generalization"],"permalink":"/ai/review/2025-8-6-LAMIC_Layout-Aware_Multi-Image_Composition_via_Scalability_of_Multimodal_Diffusion_Transformer/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-Goedel-Prover-V2_Scaling_Formal_Theorem_Proving_with_Scaffolded_Data_Synthesis_and_Self-Correction","title":"[논문리뷰] Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction","excerpt":"Jui-Hui Chung이 [arXiv]에 게시한 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Formal Verification","Language Models","Self-Correction","Data Synthesis","Reinforcement Learning","Model Averaging","Lean"],"permalink":"/ai/review/2025-8-6-Goedel-Prover-V2_Scaling_Formal_Theorem_Proving_with_Scaffolded_Data_Synthesis_and_Self-Correction/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-CompassVerifier_A_Unified_and_Robust_Verifier_for_LLMs_Evaluation_and_Outcome_Reward","title":"[논문리뷰] CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward","excerpt":"Songyang Gao이 [arXiv]에 게시한 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","LLM Evaluation","Answer Verification","Reward Model","Benchmarking","Data Augmentation","Reinforcement Learning","Formula Verification","Hallucination Detection"],"permalink":"/ai/review/2025-8-6-CompassVerifier_A_Unified_and_Robust_Verifier_for_LLMs_Evaluation_and_Outcome_Reward/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning","title":"[논문리뷰] ChartCap: Mitigating Hallucination of Dense Chart Captioning","excerpt":"Gunhee Kim이 [arXiv]에 게시한 'ChartCap: Mitigating Hallucination of Dense Chart Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Chart Captioning","Hallucination Mitigation","Dataset Generation","Visual Language Models","Cycle Consistency","Reference-Free Metric","Data Visualization"],"permalink":"/ai/review/2025-8-6-ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-CRINN_Contrastive_Reinforcement_Learning_for_Approximate_Nearest_Neighbor_Search","title":"[논문리뷰] CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search","excerpt":"Jiwei Li이 [arXiv]에 게시한 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Approximate Nearest Neighbor Search","Reinforcement Learning","Large Language Models","Code Optimization","HNSW","Retrieval-Augmented Generation","Contrastive Learning"],"permalink":"/ai/review/2025-8-6-CRINN_Contrastive_Reinforcement_Learning_for_Approximate_Nearest_Neighbor_Search/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-6-AlignGuard-LoRA_Alignment-Preserving_Fine-Tuning_via_Fisher-Guided_Decomposition_and_Riemannian-Geodesic_Collision_Regularization","title":"[논문리뷰] AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization","excerpt":"Aman Chadha이 [arXiv]에 게시한 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","lastModifiedAt":"2025-08-06 13:46:36+0900","categories":["Review"],"tags":["Review","Alignment Preservation","Fine-Tuning","LoRA","Fisher Information Matrix","Catastrophic Forgetting","LLM Safety","Riemannian Geometry","Parameter-Efficient Learning"],"permalink":"/ai/review/2025-8-6-AlignGuard-LoRA_Alignment-Preserving_Fine-Tuning_via_Fisher-Guided_Decomposition_and_Riemannian-Geodesic_Collision_Regularization/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-VeOmni__Scaling_Any_Modality_Model_Training_with_Model-Centric __Distributed_Recipe_Zoo","title":"[논문리뷰] VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo","excerpt":"Bin Jia이 [arXiv]에 게시한 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Omni-modal LLMs","Distributed Training","Model-centric","Parallelism","FSDP","Sequence Parallelism","Expert Parallelism","Mixture-of-Experts"],"permalink":"/ai/review/2025-8-5-VeOmni__Scaling_Any_Modality_Model_Training_with_Model-Centric __Distributed_Recipe_Zoo/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-SitEmb-v1.5__Improved_Context-Aware_Dense_Retrieval_for_Semantic __Association_and_Long_Story_Comprehension","title":"[논문리뷰] SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension","excerpt":"Liyan Xu이 [arXiv]에 게시한 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Dense Retrieval","Context-Aware Embedding","RAG","Long Document Comprehension","Residual Learning","Semantic Association","Text Embedding"],"permalink":"/ai/review/2025-8-5-SitEmb-v1.5__Improved_Context-Aware_Dense_Retrieval_for_Semantic __Association_and_Long_Story_Comprehension/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-RoboMemory__A_Brain-inspired_Multi-memory_Agentic_Framework_for_Lifelong __Learning_in_Physical_Embodied_Systems","title":"[논문리뷰] RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems","excerpt":"Junkun Hong이 [arXiv]에 게시한 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Brain-inspired AI","Lifelong Learning","Embodied AI","Multi-memory Systems","Knowledge Graph","Robotics","Closed-Loop Planning"],"permalink":"/ai/review/2025-8-5-RoboMemory__A_Brain-inspired_Multi-memory_Agentic_Framework_for_Lifelong __Learning_in_Physical_Embodied_Systems/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-Qwen-Image_Technical_Report","title":"[논문리뷰] Qwen-Image Technical Report","excerpt":"Kaiyuan Gao이 [arXiv]에 게시한 'Qwen-Image Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Image Generation","Text-to-Image","Image Editing","Text Rendering","Multimodal Diffusion Transformer","Curriculum Learning","Reinforcement Learning","Foundation Model"],"permalink":"/ai/review/2025-8-5-Qwen-Image_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-Personalized_Safety_Alignment_for_Text-to-Image_Diffusion_Models","title":"[논문리뷰] Personalized Safety Alignment for Text-to-Image Diffusion Models","excerpt":"Kaidong Yu이 [arXiv]에 게시한 'Personalized Safety Alignment for Text-to-Image Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Personalized Safety Alignment","Text-to-Image Diffusion Models","DPO","User Preferences","Content Moderation","Generative AI","Cross-Attention","Safety Alignment"],"permalink":"/ai/review/2025-8-5-Personalized_Safety_Alignment_for_Text-to-Image_Diffusion_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct_Technical_Report","title":"[논문리뷰] Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report","excerpt":"Anu Vellore이 [arXiv]에 게시한 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Large Language Model","Cybersecurity","Instruction Tuning","Direct Preference Optimization","Cyber Threat Intelligence","Foundation Model","Chatbot"],"permalink":"/ai/review/2025-8-5-Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-InstructVLA__Vision-Language-Action_Instruction_Tuning_from __Understanding_to_Manipulation","title":"[논문리뷰] InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation","excerpt":"Yang Tian이 [arXiv]에 게시한 'InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Vision-Language-Action (VLA)","Instruction Tuning","Multimodal Reasoning","Robotic Manipulation","Catastrophic Forgetting","Mixture-of-Experts (MoE)","Flow Matching"],"permalink":"/ai/review/2025-8-5-InstructVLA__Vision-Language-Action_Instruction_Tuning_from __Understanding_to_Manipulation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-Exploitation_Is_All_You_Need..._for_Exploration","title":"[논문리뷰] Exploitation Is All You Need... for Exploration","excerpt":"Jesse Roberts이 [arXiv]에 게시한 'Exploitation Is All You Need... for Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Exploration-Exploitation","Meta-RL","Transformer Architecture","Emergent Behavior","Multi-Armed Bandits","Gridworlds","Pseudo-Thompson Sampling"],"permalink":"/ai/review/2025-8-5-Exploitation_Is_All_You_Need..._for_Exploration/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-Cyber-Zero__Training_Cybersecurity_Agents_without_Runtime","title":"[논문리뷰] Cyber-Zero: Training Cybersecurity Agents without Runtime","excerpt":"Zijian Wang이 [arXiv]에 게시한 'Cyber-Zero: Training Cybersecurity Agents without Runtime' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Cybersecurity Agents","LLM Training","Trajectory Synthesis","Runtime-Free Training","CTF Challenges","LLM Simulation"],"permalink":"/ai/review/2025-8-5-Cyber-Zero__Training_Cybersecurity_Agents_without_Runtime/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-CellForge__Agentic_Design_of_Virtual_Cell_Models","title":"[논문리뷰] CellForge: Agentic Design of Virtual Cell Models","excerpt":"Daniel Shao이 [arXiv]에 게시한 'CellForge: Agentic Design of Virtual Cell Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","AI Scientist","Multi-Agent System","Virtual Cell Modeling","Single-Cell Perturbation Prediction","Deep Learning","Automated Model Design","Code Generation","Retrieval-Augmented Generation"],"permalink":"/ai/review/2025-8-5-CellForge__Agentic_Design_of_Virtual_Cell_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-Beyond_the_Trade-off__Self-Supervised_Reinforcement_Learning_for __Reasoning_Models'_Instruction_Following","title":"[논문리뷰] Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following","excerpt":"Jiaqing Liang이 [arXiv]에 게시한 'Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Self-Supervised RL","Instruction Following","Reasoning Models","Large Language Models","Reward Modeling","Curriculum Learning"],"permalink":"/ai/review/2025-8-5-Beyond_the_Trade-off__Self-Supervised_Reinforcement_Learning_for __Reasoning_Models'_Instruction_Following/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-AgentTTS__Large_Language_Model_Agent_for_Test-time_Compute-optimal __Scaling_Strategy_in_Complex_Tasks","title":"[논문리뷰] AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks","excerpt":"Zhiwei Zhang이 [arXiv]에 게시한 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Large Language Models","LLM Agents","Test-time Scaling","Compute Optimization","Multi-stage Tasks","Resource Allocation","Search Efficiency"],"permalink":"/ai/review/2025-8-5-AgentTTS__Large_Language_Model_Agent_for_Test-time_Compute-optimal __Scaling_Strategy_in_Complex_Tasks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-5-A_Glimpse_to_Compress__Dynamic_Visual_Token_Pruning_for_Large __Vision-Language_Models","title":"[논문리뷰] A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models","excerpt":"Zuxuan Wu이 [arXiv]에 게시한 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","lastModifiedAt":"2025-08-05 11:40:52+0900","categories":["Review"],"tags":["Review","Large Vision-Language Models (LVLMs)","Visual Token Pruning","Dynamic Compression","GlimpsePrune","Computational Efficiency","VQA","Reinforcement Learning"],"permalink":"/ai/review/2025-8-5-A_Glimpse_to_Compress__Dynamic_Visual_Token_Pruning_for_Large __Vision-Language_Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-SpA2V__Harnessing_Spatial_Auditory_Cues_for_Audio-driven_Spatially-aware __Video_Generation","title":"[논문리뷰] SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation","excerpt":"Long Chen이 [arXiv]에 게시한 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Audio-driven Video Generation","Spatial Auditory Cues","Video Scene Layout","MLLM","Diffusion Models","Training-free"],"permalink":"/ai/review/2025-8-4-SpA2V__Harnessing_Spatial_Auditory_Cues_for_Audio-driven_Spatially-aware __Video_Generation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-SWE-Exp__Experience-Driven_Software_Issue_Resolution","title":"[논문리뷰] SWE-Exp: Experience-Driven Software Issue Resolution","excerpt":"Heng Lian이 [arXiv]에 게시한 'SWE-Exp: Experience-Driven Software Issue Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Software Issue Resolution","LLM Agents","Experience-Driven Learning","Automated Program Repair","Multi-Agent Systems","Knowledge Management","Continuous Learning"],"permalink":"/ai/review/2025-8-4-SWE-Exp__Experience-Driven_Software_Issue_Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-SWE-Debate__Competitive_Multi-Agent_Debate_for_Software_Issue_Resolution","title":"[논문리뷰] SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution","excerpt":"Heng Lian이 [arXiv]에 게시한 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Multi-Agent System","Software Engineering","Fault Localization","Issue Resolution","Large Language Models","Competitive Debate","Graph Traversal"],"permalink":"/ai/review/2025-8-4-SWE-Debate__Competitive_Multi-Agent_Debate_for_Software_Issue_Resolution/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-PixNerd__Pixel_Neural_Field_Diffusion","title":"[논문리뷰] PixNerd: Pixel Neural Field Diffusion","excerpt":"Limin Wang이 [arXiv]에 게시한 'PixNerd: Pixel Neural Field Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Diffusion Models","Neural Fields","Pixel Space","Generative Models","Image Synthesis","Transformer Architecture","End-to-End Learning"],"permalink":"/ai/review/2025-8-4-PixNerd__Pixel_Neural_Field_Diffusion/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-Multimodal_Referring_Segmentation__A_Survey","title":"[논문리뷰] Multimodal Referring Segmentation: A Survey","excerpt":"Zuxuan Wu이 [arXiv]에 게시한 'Multimodal Referring Segmentation: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Multimodal Learning","Referring Segmentation","Vision-Language Models","Image Segmentation","Video Segmentation","3D Vision","Survey"],"permalink":"/ai/review/2025-8-4-Multimodal_Referring_Segmentation__A_Survey/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-Learning_an_Efficient_Multi-Turn_Dialogue_Evaluator_from_Multiple_Judges","title":"[논문리뷰] Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges","excerpt":"Chengfei Lv이 [arXiv]에 게시한 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Multi-Turn Dialogue Evaluation","LLM-as-a-Judge","Multi-Judge Aggregation","Preference Learning","Dialogue Quality Assessment","Maximum Likelihood Estimation","Computational Efficiency"],"permalink":"/ai/review/2025-8-4-Learning_an_Efficient_Multi-Turn_Dialogue_Evaluator_from_Multiple_Judges/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-Investigating_Hallucination_in_Conversations_for_Low_Resource_Languages","title":"[논문리뷰] Investigating Hallucination in Conversations for Low Resource Languages","excerpt":"Fatemeh Jamshidi이 [arXiv]에 게시한 'Investigating Hallucination in Conversations for Low Resource Languages' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","LLM Hallucination","Low-resource Languages","Conversational AI","ROUGE Score","Cross-lingual Evaluation","Factual Consistency"],"permalink":"/ai/review/2025-8-4-Investigating_Hallucination_in_Conversations_for_Low_Resource_Languages/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-IGL-Nav__Incremental_3D_Gaussian_Localization_for_Image-goal_Navigation","title":"[논문리뷰] IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation","excerpt":"Jianjiang Feng이 [arXiv]에 게시한 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Image-goal Navigation","3D Gaussian Splatting (3DGS)","Incremental Scene Representation","Coarse-to-fine Localization","Embodied AI","Robotics","Differentiable Rendering"],"permalink":"/ai/review/2025-8-4-IGL-Nav__Incremental_3D_Gaussian_Localization_for_Image-goal_Navigation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-Beyond_Fixed__Variable-Length_Denoising_for_Diffusion_Large_Language __Models","title":"[논문리뷰] Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models","excerpt":"Jiaqi Wang이 [arXiv]에 게시한 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","Diffusion Large Language Models","Variable-Length Generation","Dynamic Length Adaptation","Denoising Strategy","Inference Optimization","Computational Efficiency"],"permalink":"/ai/review/2025-8-4-Beyond_Fixed__Variable-Length_Denoising_for_Diffusion_Large_Language __Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-4-3D-R1__Enhancing_Reasoning_in_3D_VLMs_for_Unified_Scene_Understanding","title":"[논문리뷰] 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding","excerpt":"Hao Tang이 [arXiv]에 게시한 '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","lastModifiedAt":"2025-08-04 12:17:01+0900","categories":["Review"],"tags":["Review","3D Vision-Language Models","Reasoning","Scene Understanding","Reinforcement Learning","Chain-of-Thought","Dynamic View Selection","Multi-task Learning"],"permalink":"/ai/review/2025-8-4-3D-R1__Enhancing_Reasoning_in_3D_VLMs_for_Unified_Scene_Understanding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-villa-X__Enhancing_Latent_Action_Modeling_in_Vision-Language-Action __Models","title":"[논문리뷰] villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models","excerpt":"Kaixin Wang이 [arXiv]에 게시한 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Vision-Language-Action Models","Latent Actions","Robot Manipulation","Pre-training","Diffusion Models","Proprioceptive Feedback","Foundation Models"],"permalink":"/ai/review/2025-8-3-villa-X__Enhancing_Latent_Action_Modeling_in_Vision-Language-Action__Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-iLRM__An_Iterative_Large_3D_Reconstruction_Model","title":"[논문리뷰] iLRM: An Iterative Large 3D Reconstruction Model","excerpt":"Abdelrahman Mohamed이 [arXiv]에 게시한 'iLRM: An Iterative Large 3D Reconstruction Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","3D Reconstruction","Gaussian Splatting","Iterative Refinement","Transformer Architecture","Multi-view Learning","Scalability","Feed-forward Models"],"permalink":"/ai/review/2025-8-3-iLRM__An_Iterative_Large_3D_Reconstruction_Model/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-TARS__MinMax_Token-Adaptive_Preference_Strategy_for_Hallucination __Reduction_in_MLLMs","title":"[논문리뷰] TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs","excerpt":"Jiasheng Tang이 [arXiv]에 게시한 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","MLLMs","Hallucination Reduction","Preference Optimization","Min-Max Optimization","Token-Adaptive Strategy","Spectral Regularization","Visual Grounding"],"permalink":"/ai/review/2025-8-3-TARS__MinMax_Token-Adaptive_Preference_Strategy_for_Hallucination__Reduction_in_MLLMs/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Seed-Prover__Deep_and_Broad_Reasoning_for_Automated_Theorem_Proving","title":"[논문리뷰] Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving","excerpt":"Zhicheng Jiang이 [arXiv]에 게시한 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Automated Theorem Proving","Large Language Models","Formal Verification","Reinforcement Learning","Lean","Geometry Reasoning","Chain-of-Thought","Lemma-Style Proving"],"permalink":"/ai/review/2025-8-3-Seed-Prover__Deep_and_Broad_Reasoning_for_Automated_Theorem_Proving/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Scalable_Multi-Task_Reinforcement_Learning_for_Generalizable_Spatial __Intelligence_in_Visuomotor_Agents","title":"[논문리뷰] Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents","excerpt":"Anji Liu이 [arXiv]에 게시한 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Reinforcement Learning","Multi-Task Learning","Visuomotor Agents","Spatial Reasoning","Generalization","Minecraft","Cross-View Goal Specification","Automated Task Synthesis"],"permalink":"/ai/review/2025-8-3-Scalable_Multi-Task_Reinforcement_Learning_for_Generalizable_Spatial__Intelligence_in_Visuomotor_Agents/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-RecGPT_Technical_Report","title":"[논문리뷰] RecGPT Technical Report","excerpt":"Jian Wu이 [arXiv]에 게시한 'RecGPT Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Recommender Systems","Large Language Models (LLMs)","User Intent Modeling","Multi-Stage Training","Human-in-the-Loop","E-commerce","Filter Bubble Mitigation","Matthew Effect"],"permalink":"/ai/review/2025-8-3-RecGPT_Technical_Report/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Phi-Ground_Tech_Report__Advancing_Perception_in_GUI_Grounding","title":"[논문리뷰] Phi-Ground Tech Report: Advancing Perception in GUI Grounding","excerpt":"Kai Qiu이 [arXiv]에 게시한 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","GUI grounding","AI agent","Large Multi-modal Model","Perception","Data Augmentation","Direct Preference Optimization","Computational Efficiency"],"permalink":"/ai/review/2025-8-3-Phi-Ground_Tech_Report__Advancing_Perception_in_GUI_Grounding/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Persona_Vectors__Monitoring_and_Controlling_Character_Traits_in_Language __Models","title":"[논문리뷰] Persona Vectors: Monitoring and Controlling Character Traits in Language Models","excerpt":"Jack Lindsey이 [arXiv]에 게시한 'Persona Vectors: Monitoring and Controlling Character Traits in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Large Language Models (LLMs)","Persona Control","Activation Steering","Finetuning","Behavioral Shift Detection","Interpretability","Data Filtering"],"permalink":"/ai/review/2025-8-3-Persona_Vectors__Monitoring_and_Controlling_Character_Traits_in_Language__Models/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-On_the_Expressiveness_of_Softmax_Attention__A_Recurrent_Neural_Network __Perspective","title":"[논문리뷰] On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective","excerpt":"Eric C. Larson이 [arXiv]에 게시한 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Softmax Attention","Linear Attention","Recurrent Neural Networks (RNNs)","Taylor Series Expansion","Attention Mechanisms","Expressiveness","Transformer Architectures"],"permalink":"/ai/review/2025-8-3-On_the_Expressiveness_of_Softmax_Attention__A_Recurrent_Neural_Network__Perspective/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-NeRF_Is_a_Valuable_Assistant_for_3D_Gaussian_Splatting","title":"[논문리뷰] NeRF Is a Valuable Assistant for 3D Gaussian Splatting","excerpt":"ZeSheng Wang이 [arXiv]에 게시한 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","NeRF","3D Gaussian Splatting","Hybrid Model","Joint Optimization","Scene Representation","Neural Rendering","Residual Learning","Sparse View"],"permalink":"/ai/review/2025-8-3-NeRF_Is_a_Valuable_Assistant_for_3D_Gaussian_Splatting/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Flow_Equivariant_Recurrent_Neural_Networks","title":"[논문리뷰] Flow Equivariant Recurrent Neural Networks","excerpt":"T. Anderson Keller이 [arXiv]에 게시한 'Flow Equivariant Recurrent Neural Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Flow Equivariance","Recurrent Neural Networks","Sequence Models","Group Equivariance","Lie Subgroups","Generalization","Time-Parameterized Symmetries"],"permalink":"/ai/review/2025-8-3-Flow_Equivariant_Recurrent_Neural_Networks/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Enhanced_Arabic_Text_Retrieval_with_Attentive_Relevance_Scoring","title":"[논문리뷰] Enhanced Arabic Text Retrieval with Attentive Relevance Scoring","excerpt":"Abdenour Hadid이 [arXiv]에 게시한 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Arabic NLP","Dense Passage Retrieval","Attentive Relevance Scoring","Information Retrieval","Question Answering","Transformer Models","Semantic Matching"],"permalink":"/ai/review/2025-8-3-Enhanced_Arabic_Text_Retrieval_with_Attentive_Relevance_Scoring/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Efficient_Machine_Unlearning_via_Influence_Approximation","title":"[논문리뷰] Efficient Machine Unlearning via Influence Approximation","excerpt":"Enhong Chen이 [arXiv]에 게시한 'Efficient Machine Unlearning via Influence Approximation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Machine Unlearning","Influence Function","Incremental Learning","Privacy Protection","Gradient Optimization","Model Editing","Computational Efficiency"],"permalink":"/ai/review/2025-8-3-Efficient_Machine_Unlearning_via_Influence_Approximation/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-C3__A_Bilingual_Benchmark_for_Spoken_Dialogue_Models_Exploring __Challenges_in_Complex_Conversations","title":"[논문리뷰] C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations","excerpt":"Yiwen Guo이 [arXiv]에 게시한 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Spoken Dialogue Models","Bilingual Benchmark","Complex Conversations","Ambiguity Resolution","Context Understanding","LLM Evaluation","Human-Computer Interaction"],"permalink":"/ai/review/2025-8-3-C3__A_Bilingual_Benchmark_for_Spoken_Dialogue_Models_Exploring__Challenges_in_Complex_Conversations/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-Beyond_Linear_Bottlenecks__Spline-Based_Knowledge_Distillation_for __Culturally_Diverse_Art_Style_Classification","title":"[논문리뷰] Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification","excerpt":"Abdelmalik Taleb-Ahmed이 [arXiv]에 게시한 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Kolmogorov-Arnold Networks","Knowledge Distillation","Art Style Classification","Self-Supervised Learning","Spline-Based Activation","Dual-Teacher","Gram Matrix"],"permalink":"/ai/review/2025-8-3-Beyond_Linear_Bottlenecks__Spline-Based_Knowledge_Distillation_for__Culturally_Diverse_Art_Style_Classification/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-8-3-AgroBench__Vision-Language_Model_Benchmark_in_Agriculture","title":"[논문리뷰] AgroBench: Vision-Language Model Benchmark in Agriculture","excerpt":"Yoshitaka Ushiku이 [arXiv]에 게시한 'AgroBench: Vision-Language Model Benchmark in Agriculture' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","lastModifiedAt":"2025-08-03 07:35:17+0900","categories":["Review"],"tags":["Review","Vision-Language Models","Agriculture","Benchmarking","Disease Identification","Pest Management","Crop Management","Agronomy"],"permalink":"/ai/review/2025-8-3-AgroBench__Vision-Language_Model_Benchmark_in_Agriculture/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-03-08-Pass-Certified-Solutions-Architect-Associate","title":"[AWS] AWS Solutions Architect Associate 자격증 취득 후기","excerpt":"AWS Solutions Architect Associate(SAA) 자격증 공부 방법, 시험 후기를 정리했습니다.","date":"2025-03-08 23:10:11+0900","lastModifiedAt":"2025-03-08 23:10:14+0900","categories":["Me"],"tags":[["Me"]],"permalink":"/me/pass-certified-solutions-architect-associate/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-25-brain-hack-productivity","title":"뇌과학적으로 게으름과 무기력을 극복하는 방법","excerpt":"게으름과 무기력은 단순한 의지 부족이 아니라, 뇌의 작동 방식과 깊은 관련이 있습니다. 전두엽을 활성화하고, 불필요한 정보를 차단하며, 긍정적인 자기 암시를 활용하는 방법을 통해 효율적으로 극복하는 법을 알아보세요.","date":"2025-02-25 23:10:11+0900","lastModifiedAt":"2025-02-25 23:10:14+0900","categories":["Me"],"tags":[["Me"]],"permalink":"/me/brain-hack-productivity/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-25-aws-lambda-eventbridge","title":"[AWS] AWS Lambda와 Notion API를 활용한 15분 단위 자동 기록 시스템","excerpt":"AWS Lambda, Notion API, Slack Webhook을 활용하여 15분마다 자동으로 시간을 기록하고 Slack 알림을 받는 시스템을 구축하는 방법을 설명합니다.","date":"2025-02-25 18:57:00+0900","lastModifiedAt":"2025-02-25 19:36:00+0900","categories":["AWS"],"tags":[["AWS","Lambda","EventBridge"]],"permalink":"/devops/aws/aws-lambda-eventbridge/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-25-block-youtube-extention","title":"[Chrome Extention] YouTube 차단 확장 프로그램 개발하기","excerpt":"YouTube 접속을 차단하고 생산성을 높이는 크롬 확장 프로그램을 만들어보세요. 개발 과정과 설치 방법을 자세히 설명합니다.","date":"2025-02-25 18:01:00+0900","lastModifiedAt":"2025-02-25 18:10:00+0900","categories":["Chrome Extension"],"tags":[["Chrome Extension"]],"permalink":"/etc/chrome-extension/block-youtube-with-chrome-extension/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-15-django-makefile","title":"[Django] Makefile 사용하기","excerpt":"Django 프로젝트에서 Makefile을 사용하는 방법을 공유합니다.","date":"2025-02-15 04:20:00+0900","lastModifiedAt":"2025-02-15 04:40:00+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/makefile/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-15-docker-fast-build","title":"[Docker] 빠른 빌드 방법","excerpt":"Docker 빌드 속도를 높이는 방법을 공유합니다.","date":"2025-02-14 11:30:11+0900","lastModifiedAt":"2025-02-15 00:11:11+0900","categories":["Docker"],"tags":[["Docker"]],"permalink":"/devops/docker/fast-build/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-14-django-optimistic-locking","title":"[Django] 낙관적 락","excerpt":"Django에서 낙관적 락에 대해 알아보자","date":"2025-02-14 08:33:11+0900","lastModifiedAt":"2025-02-14 08:49:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/optimistic-locking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-14-django-pessimistic-locking","title":"[Django] 비관적 락","excerpt":"Django에서 비관적 락에 대해 알아보자","date":"2025-02-14 08:03:11+0900","lastModifiedAt":"2025-02-14 08:31:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/pessimistic-locking/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-14-django-index","title":"[Django] index","excerpt":"Django에서 index의 사용법을 공유합니다.","date":"2025-02-14 07:30:00+0900","lastModifiedAt":"2025-02-14 07:30:00+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/index/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-14-django-prefetch_related","title":"[Django] prefetch_related","excerpt":"Django에서 prefetch_related의 사용법을 공유합니다.","date":"2025-02-14 06:03:11+0900","lastModifiedAt":"2025-02-14 06:03:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/prefetch_related/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-02-05-see-log-in-django","title":"[Django] DB 로그 확인하기","excerpt":"Django에서 ORM을 사용하여 데이터베이스에 접근하는 경우, 쿼리 로그를 확인하는 방법을 공유합니다.","date":"2025-02-05 19:03:11+0900","lastModifiedAt":"2025-02-05 19:03:11+0900","categories":["Django"],"tags":[["Django"]],"permalink":"/backend/django/see-log-in-django/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-18-zero-downtime-deploy-with-jenkins","title":"[Jenkins] 무중단 배포를 위한 파이프라인 구성","excerpt":"Jenkins를 이용한 무중단 배포 방법을 공유합니다.","date":"2025-01-18 19:03:11+0900","lastModifiedAt":"2025-01-18 19:03:11+0900","categories":["Jenkins"],"tags":[["Jenkins"]],"permalink":"/jenkins/zero-downtime-deploy-with-jenkins/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-17-get-ssl-auth-with-letsencrypt","title":"[Nginx] Let's Encrypt로 SSL 인증서 발급받기","excerpt":"Let's Encrypt로 SSL 인증서를 발급받는 방법을 공유합니다.","date":"2025-01-17 23:10:11+0900","lastModifiedAt":"2025-01-17 23:10:14+0900","categories":["Nginx"],"tags":[["Nginx"]],"permalink":"/nginx/get-ssl-auth-with-letsencrypt/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-14-automating-update-github-profile","title":"[GitHub Actions] 깃허브 프로필 자동 업데이트하기","excerpt":"GitHub Actions를 이용해 깃허브 프로필을 자동으로 업데이트하는 방법을 공유합니다.","date":"2025-01-14 23:10:11+0900","lastModifiedAt":"2025-01-14 23:10:14+0900","categories":["GitHub Actions"],"tags":[["GitHub Actions"]],"permalink":"/github-actions/automating-update-github-profile/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-07-create-pipeline-with-jenkins","title":"[Jenkins] Jenkins 파이프라인 생성","excerpt":"Jenkins 파이프라인 생성 방법을 공유합니다.","date":"2025-01-08 03:03:11+0900","lastModifiedAt":"2025-01-14 03:43:11+0900","categories":["Jenkins"],"tags":[["Jenkins"]],"permalink":"/jenkins/create-pipeline-with-jenkins/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-07-start-jenkins","title":"[Jenkins] Jenkins 초기 설정","excerpt":"Jenkins 초기 설정 방법을 공유합니다.","date":"2025-01-07 19:03:11+0900","lastModifiedAt":"2025-01-07 19:03:11+0900","categories":["Jenkins"],"tags":[["Jenkins"]],"permalink":"/jenkins/start-jenkins/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-03-code-review-with-llm","title":"[LLM] LLM을 활용한 코드 리뷰","excerpt":"github workflow를 활용해 코드 리뷰를 자동화하기","date":"2025-01-03 01:03:11+0900","lastModifiedAt":"2025-01-03 02:10:14+0900","categories":["LLM"],"tags":[["LLM"]],"permalink":"/llm/code-review-with-llm/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-02-loguru","title":"[Python] logging 사용법","excerpt":"Loguru 을 이용한 logging 간단하게 사용해보자","date":"2025-01-03 01:03:11+0900","lastModifiedAt":"2025-01-03 02:10:14+0900","categories":["Logging"],"tags":[["Logging"]],"permalink":"/logging/loguru/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-02-start-safeline","title":"[SafeLine] SafeLine 초기 설정","excerpt":"SafeLine 초기 설정 방법을 공유합니다.","date":"2025-01-02 13:03:11+0900","lastModifiedAt":"2025-01-02 14:10:14+0900","categories":["SafeLine"],"tags":[["SafeLine"]],"permalink":"/safeline/start-safeline/","toc":true,"tocSticky":true,"published":true,"contentHtml":""},{"id":"2025-01-01-first-post","title":"블로그 개발 완료","excerpt":"새해 첫날, 기술 블로그 작성을 시작하다","date":"2025-01-01 23:10:11+0900","lastModifiedAt":"2025-01-01 23:10:14+0900","categories":["Me"],"tags":[["Me"]],"permalink":"/me/first-post/","toc":true,"tocSticky":true,"published":true,"contentHtml":""}]