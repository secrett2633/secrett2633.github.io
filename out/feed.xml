<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Sun, 09 Nov 2025 15:13:23 GMT</lastBuildDate>
    <pubDate>Sun, 09 Nov 2025 15:13:23 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] V-Thinker: Interactive Thinking with Images</title>
      <description>Peiqing Yang이 [arXiv]에 게시한 &#39;V-Thinker: Interactive Thinking with Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-V-Thinker_Interactive_Thinking_with_Images/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-V-Thinker_Interactive_Thinking_with_Images/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Multimodal Models</category><category>Interactive Reasoning</category><category>Vision-Centric Thinking</category><category>Reinforcement Learning</category><category>Data Synthesis</category><category>Visual Tools</category><category>Curriculum Learning</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</title>
      <description>이 [arXiv]에 게시한 &#39;Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Thinking_with_Video_Video_Generation_as_a_Promising_Multimodal_Reasoning_Paradigm/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Thinking_with_Video_Video_Generation_as_a_Promising_Multimodal_Reasoning_Paradigm/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Multimodal Reasoning</category><category>Temporal Understanding</category><category>Spatial Reasoning</category><category>Foundation Models</category><category>AI Benchmarking</category><category>In-Context Learning</category><category>Self-Consistency</category>
    </item>
    <item>
      <title>[논문리뷰] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms</title>
      <description>Susumu Takeuchi이 [arXiv]에 게시한 &#39;The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-The_Strong_Lottery_Ticket_Hypothesis_for_Multi-Head_Attention_Mechanisms/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-The_Strong_Lottery_Ticket_Hypothesis_for_Multi-Head_Attention_Mechanisms/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Strong Lottery Ticket Hypothesis</category><category>Multi-Head Attention</category><category>Transformers</category><category>Neural Network Pruning</category><category>Overparameterization</category><category>Weight Initialization</category><category>Model Compression</category>
    </item>
    <item>
      <title>[논문리뷰] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-SIMS-V_Simulated_Instruction-Tuning_for_Spatial_Video_Understanding/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-SIMS-V_Simulated_Instruction-Tuning_for_Spatial_Video_Understanding/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Reasoning</category><category>Video Understanding</category><category>Simulated Data</category><category>Instruction Tuning</category><category>Multimodal LLMs</category><category>Sim-to-Real Transfer</category><category>AI2-THOR</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Agent Learning via Experience Synthesis</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Agent Learning via Experience Synthesis&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Scaling_Agent_Learning_via_Experience_Synthesis/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Scaling_Agent_Learning_via_Experience_Synthesis/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>LLM Agents</category><category>Experience Synthesis</category><category>World Models</category><category>Curriculum Learning</category><category>Sim-to-Real Transfer</category><category>Web Agents</category>
    </item>
    <item>
      <title>[논문리뷰] SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning</title>
      <description>이 [arXiv]에 게시한 &#39;SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-SAIL-RL_Guiding_MLLMs_in_When_and_How_to_Think_via_Dual-Reward_RL_Tuning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-SAIL-RL_Guiding_MLLMs_in_When_and_How_to_Think_via_Dual-Reward_RL_Tuning/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models</category><category>Reinforcement Learning</category><category>Post-training</category><category>Reasoning</category><category>Dual-Reward System</category><category>Thinking Reward</category><category>Judging Reward</category><category>Hallucination Reduction</category>
    </item>
    <item>
      <title>[논문리뷰] RDMA Point-to-Point Communication for LLM Systems</title>
      <description>이 [arXiv]에 게시한 &#39;RDMA Point-to-Point Communication for LLM Systems&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-RDMA_Point-to-Point_Communication_for_LLM_Systems/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-RDMA_Point-to-Point_Communication_for_LLM_Systems/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RDMA</category><category>LLM</category><category>Point-to-Point Communication</category><category>Disaggregated Inference</category><category>MoE Routing</category><category>KvCache</category><category>AWS EFA</category><category>NVIDIA ConnectX</category>
    </item>
    <item>
      <title>[논문리뷰] NVIDIA Nemotron Nano V2 VL</title>
      <description>이 [arXiv]에 게시한 &#39;NVIDIA Nemotron Nano V2 VL&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-NVIDIA_Nemotron_Nano_V2_VL/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-NVIDIA_Nemotron_Nano_V2_VL/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Model</category><category>Hybrid Architecture</category><category>Mamba-Transformer</category><category>Long-Context Understanding</category><category>Quantization</category><category>Efficient Inference</category><category>Document AI</category><category>Video AI</category>
    </item>
    <item>
      <title>[논문리뷰] Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</title>
      <description>이 [arXiv]에 게시한 &#39;Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Learning_Vision-Driven_Reactive_Soccer_Skills_for_Humanoid_Robots/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Learning_Vision-Driven_Reactive_Soccer_Skills_for_Humanoid_Robots/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Humanoid Robot</category><category>Reinforcement Learning</category><category>RoboCup</category><category>Soccer Skills</category><category>Vision-Driven Control</category><category>Adversarial Motion Priors</category><category>Sim-to-Real</category><category>Perception-Action Coordination</category>
    </item>
    <item>
      <title>[논문리뷰] How to Evaluate Speech Translation with Source-Aware Neural MT Metrics</title>
      <description>Luisa Bentivogli이 [arXiv]에 게시한 &#39;How to Evaluate Speech Translation with Source-Aware Neural MT Metrics&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-How_to_Evaluate_Speech_Translation_with_Source-Aware_Neural_MT_Metrics/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-How_to_Evaluate_Speech_Translation_with_Source-Aware_Neural_MT_Metrics/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Speech Translation</category><category>Neural MT Metrics</category><category>Source-Aware Evaluation</category><category>Automatic Speech Recognition (ASR)</category><category>Back-Translation (BT)</category><category>Cross-lingual Re-segmentation</category><category>COMET</category><category>MetricX</category>
    </item>
    <item>
      <title>[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</title>
      <description>이 [arXiv]에 게시한 &#39;GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-GUI-360_A_Comprehensive_Dataset_and_Benchmark_for_Computer-Using_Agents/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-GUI-360_A_Comprehensive_Dataset_and_Benchmark_for_Computer-Using_Agents/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Computer-Using Agents</category><category>GUI Grounding</category><category>Screen Parsing</category><category>Action Prediction</category><category>Desktop Automation</category><category>Dataset</category><category>Benchmark</category><category>Multimodal Learning</category><category>LLM-augmented Data</category>
    </item>
    <item>
      <title>[논문리뷰] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</title>
      <description>이 [arXiv]에 게시한 &#39;EVTAR: End-to-End Try on with Additional Unpaired Visual Reference&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-EVTAR_End-to-End_Try_on_with_Additional_Unpaired_Visual_Reference/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-EVTAR_End-to-End_Try_on_with_Additional_Unpaired_Visual_Reference/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Virtual Try-on</category><category>Diffusion Models</category><category>End-to-End Learning</category><category>Reference Images</category><category>Unpaired Data</category><category>Flow Matching</category><category>Transformer Architecture</category><category>Generative AI</category>
    </item>
    <item>
      <title>[논문리뷰] Contamination Detection for VLMs using Multi-Modal Semantic Perturbation</title>
      <description>이 [arXiv]에 게시한 &#39;Contamination Detection for VLMs using Multi-Modal Semantic Perturbation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Contamination_Detection_for_VLMs_using_Multi-Modal_Semantic_Perturbation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Contamination_Detection_for_VLMs_using_Multi-Modal_Semantic_Perturbation/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>VLM Contamination</category><category>Test-set Leakage</category><category>Multi-modal Perturbation</category><category>Generative Models</category><category>Generalization</category><category>Model Memorization</category><category>VLMs</category>
    </item>
    <item>
      <title>[논문리뷰] Cambrian-S: Towards Spatial Supersensing in Video</title>
      <description>Zihao Yang이 [arXiv]에 게시한 &#39;Cambrian-S: Towards Spatial Supersensing in Video&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Cambrian-S_Towards_Spatial_Supersensing_in_Video/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Cambrian-S_Towards_Spatial_Supersensing_in_Video/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Supersensing</category><category>Video Understanding</category><category>Multimodal LLMs</category><category>Predictive Sensing</category><category>Memory Management</category><category>Event Segmentation</category><category>VSI-SUPER</category><category>Instruction Tuning</category>
    </item>
    <item>
      <title>[논문리뷰] Benchmark Designers Should &#39;Train on the Test Set&#39; to Expose Exploitable Non-Visual Shortcuts</title>
      <description>이 [arXiv]에 게시한 &#39;Benchmark Designers Should &#39;Train on the Test Set&#39; to Expose Exploitable Non-Visual Shortcuts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Benchmark_Designers_Should_Train_on_the_Test_Set_to_Expose_Exploitable_Non-Visual_Shortcuts/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Benchmark_Designers_Should_Train_on_the_Test_Set_to_Expose_Exploitable_Non-Visual_Shortcuts/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>Benchmark Design</category><category>Non-Visual Shortcuts</category><category>Test-Set Stress-Test</category><category>Bias Mitigation</category><category>Model Evaluation</category><category>Benchmark Robustness</category>
    </item>
    <item>
      <title>[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</title>
      <description>이 [arXiv]에 게시한 &#39;UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-UniAVGen_Unified_Audio_and_Video_Generation_with_Asymmetric_Cross-Modal_Interactions/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-UniAVGen_Unified_Audio_and_Video_Generation_with_Asymmetric_Cross-Modal_Interactions/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Joint Audio-Video Generation</category><category>Cross-Modal Interaction</category><category>Diffusion Transformer</category><category>Face-Aware Modulation</category><category>Classifier-Free Guidance</category><category>Multimodal AI</category><category>Generative Models</category>
    </item>
    <item>
      <title>[논문리뷰] The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute</title>
      <description>이 [arXiv]에 게시한 &#39;The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-The_Sequential_Edge_Inverse-Entropy_Voting_Beats_Parallel_Self-Consistency_at_Matched_Compute/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-The_Sequential_Edge_Inverse-Entropy_Voting_Beats_Parallel_Self-Consistency_at_Matched_Compute/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Sequential Reasoning</category><category>Parallel Self-Consistency</category><category>Inverse-Entropy Voting</category><category>LLM Reasoning</category><category>Test-Time Scaling</category><category>Inference Optimization</category><category>Iterative Refinement</category><category>Error Correction</category>
    </item>
    <item>
      <title>[논문리뷰] TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-TabTune_A_Unified_Library_for_Inference_and_Fine-Tuning_Tabular_Foundation_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-TabTune_A_Unified_Library_for_Inference_and_Fine-Tuning_Tabular_Foundation_Models/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Tabular Foundation Models</category><category>Fine-Tuning</category><category>PEFT</category><category>Meta-Learning</category><category>Calibration</category><category>Fairness</category><category>Unified Library</category><category>Benchmarking</category>
    </item>
    <item>
      <title>[논문리뷰] Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</title>
      <description>이 [arXiv]에 게시한 &#39;Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-Orion-MSP_Multi-Scale_Sparse_Attention_for_Tabular_In-Context_Learning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-Orion-MSP_Multi-Scale_Sparse_Attention_for_Tabular_In-Context_Learning/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Tabular Data</category><category>In-Context Learning</category><category>Multi-Scale Attention</category><category>Sparse Attention</category><category>Foundation Models</category><category>Perceiver Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</title>
      <description>이 [arXiv]에 게시한 &#39;MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-MME-CC_A_Challenging_Multi-Modal_Evaluation_Benchmark_of_Cognitive_Capacity/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-MME-CC_A_Challenging_Multi-Modal_Evaluation_Benchmark_of_Cognitive_Capacity/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>Benchmark</category><category>Cognitive Capacity</category><category>Visual Reasoning</category><category>MLLM Evaluation</category><category>Error Analysis</category><category>Chain-of-Thought</category>
    </item>
    <item>
      <title>[논문리뷰] LiveTradeBench: Seeking Real-World Alpha with Large Language Models</title>
      <description>Jiaxuan You이 [arXiv]에 게시한 &#39;LiveTradeBench: Seeking Real-World Alpha with Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-LiveTradeBench_Seeking_Real-World_Alpha_with_Large_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-LiveTradeBench_Seeking_Real-World_Alpha_with_Large_Language_Models/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Evaluation</category><category>Live Trading</category><category>Portfolio Management</category><category>Financial AI</category><category>Prediction Markets</category><category>Real-World Uncertainty</category><category>Agent Benchmarking</category>
    </item>
    <item>
      <title>[논문리뷰] Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation</title>
      <description>Jaehyun Park이 [arXiv]에 게시한 &#39;Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-Let_Multimodal_Embedders_Learn_When_to_Augment_Query_via_Adaptive_Query_Augmentation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-Let_Multimodal_Embedders_Learn_When_to_Augment_Query_via_Adaptive_Query_Augmentation/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Embedders</category><category>Query Augmentation</category><category>Adaptive Learning</category><category>Multimodal LLM</category><category>Information Retrieval</category><category>Generative AI</category><category>Embedding Latency</category>
    </item>
    <item>
      <title>[논문리뷰] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation</title>
      <description>Soohyun Oh이 [arXiv]에 게시한 &#39;LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-LEGO-Eval_Towards_Fine-Grained_Evaluation_on_Synthesizing_3D_Embodied_Environments_with_Tool_Augmentation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-LEGO-Eval_Towards_Fine-Grained_Evaluation_on_Synthesizing_3D_Embodied_Environments_with_Tool_Augmentation/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Scene Synthesis</category><category>Fine-Grained Evaluation</category><category>Tool-Augmented LLMs</category><category>Embodied AI</category><category>Vision-Language Models</category><category>Benchmark</category><category>Multi-Hop Grounding</category>
    </item>
    <item>
      <title>[논문리뷰] Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects</title>
      <description>이 [arXiv]에 게시한 &#39;Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-Kinematify_Open-Vocabulary_Synthesis_of_High-DoF_Articulated_Objects/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-Kinematify_Open-Vocabulary_Synthesis_of_High-DoF_Articulated_Objects/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Articulated Objects</category><category>Kinematics Inference</category><category>High-DoF</category><category>Monte Carlo Tree Search</category><category>Joint Parameter Optimization</category><category>SDF</category><category>Open-Vocabulary Synthesis</category><category>Robot Self-Modeling</category>
    </item>
    <item>
      <title>[논문리뷰] Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
      <description>이 [arXiv]에 게시한 &#39;Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-Jr._AI_Scientist_and_Its_Risk_Report_Autonomous_Scientific_Exploration_from_a_Baseline_Paper/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-Jr._AI_Scientist_and_Its_Risk_Report_Autonomous_Scientific_Exploration_from_a_Baseline_Paper/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Scientist</category><category>Autonomous Research</category><category>Scientific Automation</category><category>LLM for Research</category><category>Code Generation</category><category>Experimental Design</category><category>Risk Assessment</category>
    </item>
    <item>
      <title>[논문리뷰] Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask</title>
      <description>이 [arXiv]에 게시한 &#39;Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-Grounded_Misunderstandings_in_Asymmetric_Dialogue_A_Perspectivist_Annotation_Scheme_for_MapTask/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-Grounded_Misunderstandings_in_Asymmetric_Dialogue_A_Perspectivist_Annotation_Scheme_for_MapTask/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Dialogue Systems</category><category>Common Ground</category><category>Misunderstanding</category><category>Annotation Scheme</category><category>MapTask Corpus</category><category>Large Language Models</category><category>Perspective Taking</category><category>Reference Resolution</category>
    </item>
    <item>
      <title>[논문리뷰] Diffusion Language Models are Super Data Learners</title>
      <description>이 [arXiv]에 게시한 &#39;Diffusion Language Models are Super Data Learners&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-Diffusion_Language_Models_are_Super_Data_Learners/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-Diffusion_Language_Models_are_Super_Data_Learners/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Language Models</category><category>Autoregressive Models</category><category>Data Efficiency</category><category>Scaling Laws</category><category>Data-Constrained Learning</category><category>Crossover Phenomenon</category><category>Pre-training</category><category>Masked Diffusion</category>
    </item>
    <item>
      <title>[논문리뷰] CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents</title>
      <description>Shijue Huang이 [arXiv]에 게시한 &#39;CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-CostBench_Evaluating_Multi-Turn_Cost-Optimal_Planning_and_Adaptation_in_Dynamic_Environments_for_LLM_Tool-Use_Agents/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-CostBench_Evaluating_Multi-Turn_Cost-Optimal_Planning_and_Adaptation_in_Dynamic_Environments_for_LLM_Tool-Use_Agents/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Tool Use</category><category>Cost-Optimal Planning</category><category>Dynamic Environments</category><category>Benchmarking</category><category>Multi-Turn Interaction</category><category>Economic Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought</title>
      <description>이 [arXiv]에 게시한 &#39;When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-5-When_Visualizing_is_the_First_Step_to_Reasoning_MIRA_a_Benchmark_for_Visual_Chain-of-Thought/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-5-When_Visualizing_is_the_First_Step_to_Reasoning_MIRA_a_Benchmark_for_Visual_Chain-of-Thought/</guid>
      <pubDate>Sun, 09 Nov 2025 10:35:02 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal AI</category><category>Visual Reasoning</category><category>Chain-of-Thought (CoT)</category><category>Benchmark</category><category>Image Generation</category><category>MLLMs</category><category>Visual-CoT</category>
    </item>
    <item>
      <title>[논문리뷰] When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs</title>
      <description>Haotian Wang이 [arXiv]에 게시한 &#39;When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-5-When_Modalities_Conflict_How_Unimodal_Reasoning_Uncertainty_Governs_Preference_Dynamics_in_MLLMs/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-5-When_Modalities_Conflict_How_Unimodal_Reasoning_Uncertainty_Governs_Preference_Dynamics_in_MLLMs/</guid>
      <pubDate>Sun, 09 Nov 2025 10:35:02 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models (MLLMs)</category><category>Modality Following</category><category>Unimodal Uncertainty</category><category>Modality Preference</category><category>Conflict Resolution</category><category>Internal Mechanism</category><category>Entropy</category><category>Controllable Dataset</category>
    </item>
    
  </channel>
</rss>