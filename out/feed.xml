<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.cloud</link>
    <atom:link href="https://blog.secrett2633.cloud/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Fri, 16 Jan 2026 10:46:58 GMT</lastBuildDate>
    <pubDate>Fri, 16 Jan 2026 10:46:58 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] TranslateGemma Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;TranslateGemma Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-TranslateGemma-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-TranslateGemma-Technical-Report/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Machine Translation</category><category>Large Language Models</category><category>Reinforcement Learning</category><category>Supervised Fine-tuning</category><category>Gemma 3</category><category>Multimodal AI</category><category>Synthetic Data</category>
    </item>
    <item>
      <title>[논문리뷰] The AI Hippocampus: How Far are We From Human Memory?</title>
      <description>Tong Wu이 [arXiv]에 게시한 &#39;The AI Hippocampus: How Far are We From Human Memory?&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models (LLMs)</category><category>Multi-Modal LLMs (MLLMs)</category><category>Memory Systems</category><category>Implicit Memory</category><category>Explicit Memory</category><category>Agentic Memory</category><category>Retrieval-Augmented Generation (RAG)</category><category>Contextual Understanding</category>
    </item>
    <item>
      <title>[논문리뷰] SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL</title>
      <description>이 [arXiv]에 게시한 &#39;SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Dermatological Diagnosis</category><category>Multimodal LLM</category><category>Reinforcement Learning</category><category>Dynamic Visual Encoding</category><category>Information Transmission</category><category>Clinically Grounded Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Scene Understanding</category><category>Open-Vocabulary Segmentation</category><category>Referring Expression Segmentation</category><category>Training-Free</category><category>Voxel Grouping</category><category>Vision-Language Models</category><category>Multi-modal Large Language Models</category><category>Sparse Voxel Rasterization</category>
    </item>
    <item>
      <title>[논문리뷰] Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models</title>
      <description>Wenjie Li이 [arXiv]에 게시한 &#39;Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>World Models</category><category>Adaptive Planning</category><category>Lookahead</category><category>Reinforcement Learning</category><category>POMDP</category><category>Task Planning</category><category>Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] Geometric Stability: The Missing Axis of Representations</title>
      <description>pcr2120이 [arXiv]에 게시한 &#39;Geometric Stability: The Missing Axis of Representations&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Geometric Stability</category><category>Representation Analysis</category><category>Similarity Metrics</category><category>Shesha Framework</category><category>Drift Detection</category><category>Transfer Learning</category><category>Neural Representations</category><category>CRISPR Screens</category>
    </item>
    <item>
      <title>[논문리뷰] FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection</title>
      <description>이 [arXiv]에 게시한 &#39;FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>UI Grounding</category><category>Visual Token Reduction</category><category>Position-Preserving</category><category>Vision-Language Models (VLMs)</category><category>Saliency Scoring</category><category>Computational Efficiency</category><category>Human-Computer Interaction</category>
    </item>
    <item>
      <title>[논문리뷰] Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models</title>
      <description>Xiao Yang이 [arXiv]에 게시한 &#39;Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Diffusion Models</category><category>Image-to-Video Generation</category><category>Diffusion Transformers (DiT)</category><category>Controllability</category><category>Semantic Alignment</category><category>Focal Guidance</category><category>Prompt Adherence</category>
    </item>
    <item>
      <title>[논문리뷰] Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</title>
      <description>이 [arXiv]에 게시한 &#39;Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action</category><category>Embodied AI</category><category>Latent Planning</category><category>Chain-of-Thought</category><category>Distillation</category><category>Inference Efficiency</category><category>Robotic Manipulation</category><category>Preference Learning</category>
    </item>
    <item>
      <title>[논문리뷰] ExpSeek: Self-Triggered Experience Seeking for Web Agents</title>
      <description>이 [arXiv]에 게시한 &#39;ExpSeek: Self-Triggered Experience Seeking for Web Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Web Agents</category><category>Experience Seeking</category><category>Self-Triggered</category><category>LLM Reasoning</category><category>Entropy</category><category>Proactive Guidance</category><category>Reinforcement Learning</category><category>Foundation Models</category>
    </item>
    <item>
      <title>[논문리뷰] EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines</title>
      <description>이 [arXiv]에 게시한 &#39;EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Self-Evolution</category><category>Finite State Machines</category><category>Deep Research</category><category>Multi-hop QA</category><category>Adaptive Workflow</category><category>Memory Mechanism</category><category>Controllable AI</category>
    </item>
    <item>
      <title>[논문리뷰] Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</title>
      <description>Ayush Tewari이 [arXiv]에 게시한 &#39;Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Diffusion Models</category><category>3D Reconstruction</category><category>3D Gaussian Splatting</category><category>Camera-Controlled</category><category>Sparse Keyframes</category><category>Real-time</category><category>Computational Efficiency</category>
    </item>
    <item>
      <title>[논문리뷰] Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Knowledge Distillation</category><category>Sequence-level Distillation</category><category>Chain-of-Thought Reasoning (CoT)</category><category>Large Language Models (LLMs)</category><category>Temperature-scheduled Learning</category><category>Divergence-aware Sampling</category><category>Mixed-policy Distillation</category><category>Open-source Models</category>
    </item>
    <item>
      <title>[논문리뷰] DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation</title>
      <description>이 [arXiv]에 게시한 &#39;DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Agentic AI</category><category>Deep Research Systems</category><category>Automated Evaluation</category><category>Task Construction</category><category>Fact-Checking</category><category>LLM Benchmarking</category><category>Adaptive Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] Controlled Self-Evolution for Algorithmic Code Optimization</title>
      <description>이 [arXiv]에 게시한 &#39;Controlled Self-Evolution for Algorithmic Code Optimization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-Evolution</category><category>Code Optimization</category><category>Large Language Models</category><category>Genetic Algorithms</category><category>Hierarchical Memory</category><category>Algorithmic Code Generation</category><category>Exploration Efficiency</category>
    </item>
    <item>
      <title>[논문리뷰] Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity</title>
      <description>Chi Zhang이 [arXiv]에 게시한 &#39;Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Preference Alignment</category><category>Preference-Undermining Attacks</category><category>Factorial Analysis</category><category>Sycophancy</category><category>Prompt Engineering</category><category>Truth-Deference Trade-off</category>
    </item>
    <item>
      <title>[논문리뷰] A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation</title>
      <description>Kai He이 [arXiv]에 게시한 &#39;A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Scientific Reasoning</category><category>Memory-Driven AI</category><category>Benchmarking</category><category>Large Language Models (LLMs)</category><category>Anchor-Attractor Activation</category><category>Episodic Memory</category><category>Knowledge Retrieval</category>
    </item>
    <item>
      <title>[논문리뷰] VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</title>
      <description>이 [arXiv]에 게시한 &#39;VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Embodied Navigation</category><category>VLA Model</category><category>Adaptive Reasoning</category><category>Chain-of-Thought (CoT)</category><category>Linguistic Memory</category><category>Reinforcement Learning</category><category>Sim-to-Real Transfer</category><category>Multi-task Learning</category>
    </item>
    <item>
      <title>[논문리뷰] User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale</title>
      <description>이 [arXiv]에 게시한 &#39;User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-Turn Dialogue Generation</category><category>Tool Use</category><category>Autonomous Agents</category><category>Large Reasoning Models</category><category>User Simulation</category><category>Synthetic Data Generation</category><category>SQL-based Tools</category><category>Agentic Benchmarks</category>
    </item>
    <item>
      <title>[논문리뷰] Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking</title>
      <description>Zhen Ye이 [arXiv]에 게시한 &#39;Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Fact-Checking</category><category>Large Language Models (LLMs)</category><category>Benchmarking</category><category>Multi-agent System</category><category>Stage-wise Evaluation</category><category>Claim Evolution</category><category>Trustworthy AI</category>
    </item>
    <item>
      <title>[논문리뷰] The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents</title>
      <description>Junjue Wang이 [arXiv]에 게시한 &#39;The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Calibration</category><category>Tool Use</category><category>Reinforcement Learning</category><category>Miscalibration</category><category>Overconfidence</category><category>Trustworthy AI</category>
    </item>
    <item>
      <title>[논문리뷰] Solar Open Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Solar Open Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-Solar-Open-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-Solar-Open-Technical-Report/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Mixture-of-Experts</category><category>Korean LLM</category><category>Synthetic Data Generation</category><category>Curriculum Learning</category><category>Reinforcement Learning</category><category>Tokenizer Optimization</category><category>Multilingual AI</category>
    </item>
    <item>
      <title>[논문리뷰] SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices</title>
      <description>이 [arXiv]에 게시한 &#39;SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Transformers</category><category>Edge AI</category><category>Efficient Image Generation</category><category>Sparse Attention</category><category>Elastic Training</category><category>Knowledge Distillation</category><category>Mobile AI</category><category>High-Fidelity</category>
    </item>
    <item>
      <title>[논문리뷰] ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands</title>
      <description>이 [arXiv]에 게시한 &#39;ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>GUI Automation</category><category>Flow-based Generative Models</category><category>Continuous Control</category><category>Vision-Language Models</category><category>Human-Computer Interaction</category><category>ScreenDrag Benchmark</category><category>Dexterous Manipulation</category>
    </item>
    <item>
      <title>[논문리뷰] Motion Attribution for Video Generation</title>
      <description>이 [arXiv]에 게시한 &#39;Motion Attribution for Video Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Motion Attribution</category><category>Video Generation</category><category>Diffusion Models</category><category>Gradient-based Attribution</category><category>Temporal Dynamics</category><category>Motion Masking</category><category>Fine-tuning</category><category>Data Curation</category>
    </item>
    <item>
      <title>[논문리뷰] Ministral 3</title>
      <description>이 [arXiv]에 게시한 &#39;Ministral 3&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-Ministral-3/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-Ministral-3/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Model Distillation</category><category>Pruning</category><category>Parameter-Efficient AI</category><category>Multimodal LLMs</category><category>Instruction Tuning</category><category>Reinforcement Learning from Human Feedback</category><category>Open-Source AI</category>
    </item>
    <item>
      <title>[논문리뷰] MemoBrain: Executive Memory as an Agentic Brain for Reasoning</title>
      <description>Zheng Liu이 [arXiv]에 게시한 &#39;MemoBrain: Executive Memory as an Agentic Brain for Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Executive Memory</category><category>LLM Agents</category><category>Reasoning</category><category>Context Management</category><category>Tool-Augmented Agents</category><category>Memory Management</category><category>Trajectory Folding</category><category>Preference Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences</title>
      <description>Rui Xu이 [arXiv]에 게시한 &#39;MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Code Agents</category><category>Software Engineering</category><category>Experiential Memory</category><category>GitHub Data</category><category>Experience Governance</category><category>Agentic Search</category><category>LLM Applications</category><category>Bug Fixing</category>
    </item>
    <item>
      <title>[논문리뷰] KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions</title>
      <description>Chenglong Li이 [arXiv]에 게시한 &#39;KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Person Understanding</category><category>Lifelong Digital Companions</category><category>Memory Benchmarking</category><category>Autobiographical Narratives</category><category>Cognitive Stream</category><category>Flashback Handling</category><category>LLM Evaluation</category><category>Hierarchical Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] JudgeRLVR: Judge First, Generate Second for Efficient Reasoning</title>
      <description>Sujian Li이 [arXiv]에 게시한 &#39;JudgeRLVR: Judge First, Generate Second for Efficient Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning/</guid>
      <pubDate>Wed, 14 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RLVR</category><category>LLMs</category><category>Reasoning</category><category>Judge-then-Generate</category><category>Quality-Efficiency</category><category>Discriminative Supervision</category><category>Mathematical Reasoning</category><category>Backtracking Reduction</category>
    </item>
    
  </channel>
</rss>