<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Mon, 22 Dec 2025 18:36:18 GMT</lastBuildDate>
    <pubDate>Mon, 22 Dec 2025 18:36:18 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</title>
      <description>이 [arXiv]에 게시한 &#39;VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>GUI Grounding</category><category>Multi-Platform</category><category>Benchmark</category><category>MLLM</category><category>Hierarchical Evaluation</category><category>Human-in-the-Loop Annotation</category><category>GUI Agents</category><category>Multilingual Dataset</category>
    </item>
    <item>
      <title>[논문리뷰] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</title>
      <description>이 [arXiv]에 게시한 &#39;The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Video Generation</category><category>Multimodal Control</category><category>Trajectory Guidance</category><category>Reference Images</category><category>Promptable Events</category><category>Cross-Attention</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</title>
      <description>이 [arXiv]에 게시한 &#39;StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Monocular-to-Stereo Conversion</category><category>Video Generation</category><category>Diffusion Models</category><category>Feed-Forward Architecture</category><category>Domain Switcher</category><category>Cycle Consistency</category><category>Unified Dataset</category><category>Depth Ambiguity</category>
    </item>
    <item>
      <title>[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
      <description>이 [arXiv]에 게시한 &#39;Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Audio-Visual Generation</category><category>Diffusion Transformer</category><category>Multimodal AI</category><category>Speech Synchronization</category><category>Video Generation</category><category>Reinforcement Learning from Human Feedback</category><category>Inference Acceleration</category>
    </item>
    <item>
      <title>[논문리뷰] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</title>
      <description>Yuqi Liu이 [arXiv]에 게시한 &#39;RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Editing</category><category>Vision-Language Models</category><category>Diffusion Models</category><category>Region-aligned Guidance</category><category>Reinforcement Learning</category><category>Instruction-Visual Complexity</category><category>Attention Mechanism</category>
    </item>
    <item>
      <title>[논문리뷰] REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</title>
      <description>Giorgos Sfikas이 [arXiv]에 게시한 &#39;REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Latent Diffusion Models</category><category>Vision Foundation Models</category><category>Semantic Compression</category><category>Global-Local Semantics</category><category>Image Generation</category><category>Representation Entanglement</category><category>Transformer Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] Next-Embedding Prediction Makes Strong Vision Learners</title>
      <description>이 [arXiv]에 게시한 &#39;Next-Embedding Prediction Makes Strong Vision Learners&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-supervised Learning</category><category>Generative Pretraining</category><category>Vision Transformer</category><category>Next-Embedding Prediction</category><category>Autoregressive Model</category><category>Image Classification</category><category>Semantic Segmentation</category><category>Causal Masking</category>
    </item>
    <item>
      <title>[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Grounding</category><category>Spatial Reasoning</category><category>Vision-Language Models</category><category>Depth Estimation</category><category>3D Object Detection</category><category>Chain-of-Thought</category><category>Data Generation</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title>
      <description>이 [arXiv]에 게시한 &#39;Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reward Models</category><category>Multimodal LLMs</category><category>Benchmark</category><category>Text-to-Image Generation</category><category>Image Editing</category><category>Interleaved Generation</category><category>Multimodal Reasoning</category><category>MLLM-as-a-judge</category>
    </item>
    <item>
      <title>[논문리뷰] Kling-Omni Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Kling-Omni Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Kling-Omni-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Kling-Omni-Technical-Report/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Multimodal Visual Language</category><category>Generative AI</category><category>Video Editing</category><category>Reasoning-enhanced Generation</category><category>Diffusion Transformer</category><category>Multi-modal World Simulators</category>
    </item>
    <item>
      <title>[논문리뷰] Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language</title>
      <description>이 [arXiv]에 게시한 &#39;Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Time Series Analysis</category><category>Multimodal Language Models</category><category>Natural Language Generation</category><category>Dataset Creation</category><category>Instruction Tuning</category><category>GPT-4</category><category>LLaVA</category><category>Cross-Domain Alignment</category>
    </item>
    <item>
      <title>[논문리뷰] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</title>
      <description>Carlos Escolano이 [arXiv]에 게시한 &#39;Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Speech-to-Text Translation</category><category>Multimodal LLMs</category><category>Speech Foundation Models</category><category>Cascaded Systems</category><category>Benchmarking</category><category>Speech Modality Integration</category><category>Robustness</category><category>Evaluation Metrics</category>
    </item>
    <item>
      <title>[논문리뷰] Generative Refocusing: Flexible Defocus Control from a Single Image</title>
      <description>Yu-Lun Liu이 [arXiv]에 게시한 &#39;Generative Refocusing: Flexible Defocus Control from a Single Image&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative AI</category><category>Image Refocusing</category><category>Defocus Deblurring</category><category>Bokeh Synthesis</category><category>Depth of Field Control</category><category>Semi-Supervised Learning</category><category>Diffusion Models</category><category>Aperture Shape Control</category>
    </item>
    <item>
      <title>[논문리뷰] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering</title>
      <description>Hendrik P. A. Lensch이 [arXiv]에 게시한 &#39;FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Neural Rendering</category><category>Diffusion Models</category><category>G-Buffer</category><category>Autoregressive Generation</category><category>Temporal Consistency</category><category>ControlNet</category><category>ControlLoRA</category><category>Interactive Applications</category>
    </item>
    <item>
      <title>[논문리뷰] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</title>
      <description>이 [arXiv]에 게시한 &#39;FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Portrait Animation</category><category>Diffusion Models</category><category>Inference Acceleration</category><category>Identity Preservation</category><category>Video Generation</category><category>Latent Prediction</category><category>Sliding Window</category>
    </item>
    <item>
      <title>[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title>
      <description>이 [arXiv]에 게시한 &#39;Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Large Language Models</category><category>Exploration-Exploitation</category><category>Clipping</category><category>Policy Entropy</category><category>Spurious Rewards</category><category>Mathematical Reasoning</category><category>RLVR</category>
    </item>
    <item>
      <title>[논문리뷰] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title>
      <description>이 [arXiv]에 게시한 &#39;Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>MLLM</category><category>Model Auditing</category><category>Capability Gaps</category><category>Failure Mode Discovery</category><category>Reinforcement Learning</category><category>Data Rectification</category><category>Counterfactual Generation</category><category>VQA</category>
    </item>
    <item>
      <title>[논문리뷰] Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</title>
      <description>Wenxuan Lu이 [arXiv]에 게시한 &#39;Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Panoramic Depth Estimation</category><category>Foundation Model</category><category>Semi-Supervised Learning</category><category>Pseudo-Labeling</category><category>Data-in-the-Loop</category><category>DINOv3</category><category>Metric Depth</category><category>360-degree Vision</category>
    </item>
    <item>
      <title>[논문리뷰] DeContext as Defense: Safe Image Editing in Diffusion Transformers</title>
      <description>이 [arXiv]에 게시한 &#39;DeContext as Defense: Safe Image Editing in Diffusion Transformers&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Transformers</category><category>Image Editing</category><category>Privacy Protection</category><category>Adversarial Attack</category><category>Attention Mechanism</category><category>Identity Preservation</category><category>Deepfake Defense</category><category>In-context Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</title>
      <description>Jiarong Ou이 [arXiv]에 게시한 &#39;Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image</category><category>Data Selection</category><category>Meta-Learning</category><category>Meta-Gradient</category><category>Data Efficiency</category><category>Generative Models</category><category>Coreset Selection</category><category>Data Pruning</category>
    </item>
    <item>
      <title>[논문리뷰] Adaptation of Agentic AI</title>
      <description>Zhiyi Shi이 [arXiv]에 게시한 &#39;Adaptation of Agentic AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Adaptation-of-Agentic-AI/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Adaptation-of-Agentic-AI/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Agentic AI</category><category>Adaptation</category><category>Agent Adaptation</category><category>Tool Adaptation</category><category>Reinforcement Learning</category><category>Fine-tuning</category><category>Modular AI</category>
    </item>
    <item>
      <title>[논문리뷰] AdaTooler-V: Adaptive Tool-Use for Images and Videos</title>
      <description>Zhixun Li이 [arXiv]에 게시한 &#39;AdaTooler-V: Adaptive Tool-Use for Images and Videos&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLM</category><category>Adaptive Tool-Use</category><category>Reinforcement Learning</category><category>Chain-of-Thought</category><category>Vision-Language Models</category><category>Visual Reasoning</category><category>AT-GRPO</category>
    </item>
    <item>
      <title>[논문리뷰] WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</title>
      <description>Sung Won Han이 [arXiv]에 게시한 &#39;WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AIS data</category><category>vessel destination estimation</category><category>deep learning</category><category>transformer</category><category>channel attention</category><category>trajectory analysis</category><category>Gradient Dropout</category><category>maritime surveillance</category>
    </item>
    <item>
      <title>[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</title>
      <description>이 [arXiv]에 게시한 &#39;VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Text Compression (VTC)</category><category>Long Context Understanding</category><category>Vision-Language Models (VLMs)</category><category>Benchmark</category><category>Information Retrieval</category><category>Associative Reasoning</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Universal Reasoning Model</title>
      <description>이 [arXiv]에 게시한 &#39;Universal Reasoning Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-Universal-Reasoning-Model/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-Universal-Reasoning-Model/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Universal Transformer</category><category>Recurrent Neural Networks</category><category>ARC-AGI</category><category>Reasoning Tasks</category><category>Nonlinearity</category><category>Convolutional Gating</category><category>Truncated Backpropagation</category><category>Model Efficiency</category>
    </item>
    <item>
      <title>[논문리뷰] Step-GUI Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Step-GUI Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-Step-GUI-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-Step-GUI-Technical-Report/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>GUI Automation</category><category>Self-Evolving Pipeline</category><category>Reinforcement Learning</category><category>Multimodal LLMs</category><category>Privacy-Preserving AI</category><category>Human-Computer Interaction</category><category>Model Context Protocol</category><category>Benchmarking</category>
    </item>
    <item>
      <title>[논문리뷰] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI-Generated Video Detection</category><category>Multimodal Large Language Model (MLLM)</category><category>Artifact Reasoning</category><category>Explainable AI</category><category>Supervised Fine-Tuning (SFT)</category><category>Reinforcement Learning (RL)</category><category>Video Forensics</category>
    </item>
    <item>
      <title>[논문리뷰] SCOPE: Prompt Evolution for Enhancing Agent Effectiveness</title>
      <description>Yunhe Wang이 [arXiv]에 게시한 &#39;SCOPE: Prompt Evolution for Enhancing Agent Effectiveness&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Prompt Optimization</category><category>Context Management</category><category>Online Learning</category><category>Agent Effectiveness</category><category>Self-Evolving Prompts</category><category>Trace-Based Learning</category><category>Dual-Stream Routing</category>
    </item>
    <item>
      <title>[논문리뷰] SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning</title>
      <description>이 [arXiv]에 게시한 &#39;SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Reasoning</category><category>Reinforcement Learning</category><category>Multi-Turn Reasoning</category><category>Agent System</category><category>Long Videos</category><category>Synthetic Data</category><category>Any-Horizon Reasoning</category><category>Large Language Models</category>
    </item>
    <item>
      <title>[논문리뷰] Robust and Calibrated Detection of Authentic Multimedia Content</title>
      <description>이 [arXiv]에 게시한 &#39;Robust and Calibrated Detection of Authentic Multimedia Content&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content/</guid>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Deepfake Detection</category><category>Content Authenticity</category><category>Generative Models</category><category>Adversarial Robustness</category><category>Image Inversion</category><category>Plausible Deniability</category><category>Diffusion Models</category><category>Multimedia Forensics</category>
    </item>
    
  </channel>
</rss>