<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Tue, 25 Nov 2025 02:41:02 GMT</lastBuildDate>
    <pubDate>Tue, 25 Nov 2025 02:41:02 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] WorldGen: From Text to Traversable and Interactive 3D Worlds</title>
      <description>이 [arXiv]에 게시한 &#39;WorldGen: From Text to Traversable and Interactive 3D Worlds&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D World Generation</category><category>Text-to-3D</category><category>Generative AI</category><category>Procedural Generation</category><category>Scene Decomposition</category><category>Navmesh</category><category>Game Engines</category><category>Interactive Environments</category>
    </item>
    <item>
      <title>[논문리뷰] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</title>
      <description>Yudong Zhang이 [arXiv]에 게시한 &#39;VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Latent Memory</category><category>Cognitive Memory</category><category>Visual Grounding</category><category>Short-term Memory</category><category>Long-term Memory</category><category>Reinforcement Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</title>
      <description>Jing Bi이 [arXiv]에 게시한 &#39;Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Reasoning</category><category>Large Multimodal Models</category><category>Reinforcement Learning</category><category>Visual Rumination</category><category>Text-Rich Video</category><category>Video Question Answering</category><category>Iterative Perception</category>
    </item>
    <item>
      <title>[논문리뷰] VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation</title>
      <description>Gim Hee Lee이 [arXiv]에 게시한 &#39;VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Models</category><category>Robotic Manipulation</category><category>SpatioTemporal Coherence</category><category>4D Awareness</category><category>Visual Representation</category><category>Action Representation</category><category>Cross-Attention</category>
    </item>
    <item>
      <title>[논문리뷰] Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story</title>
      <description>Kristian Kuznetsov이 [arXiv]에 게시한 &#39;Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Intrinsic Dimension</category><category>LLMs</category><category>Text Complexity</category><category>Sparse Autoencoders</category><category>Text Semantics</category><category>Genre Analysis</category><category>Embedding Space</category><category>Text Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Taming Generative Synthetic Data for X-ray Prohibited Item Detection</title>
      <description>Renshuai Tao이 [arXiv]에 게시한 &#39;Taming Generative Synthetic Data for X-ray Prohibited Item Detection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>X-ray Security</category><category>Synthetic Data Generation</category><category>Diffusion Models</category><category>Object Detection</category><category>Cross-Attention</category><category>Image Inpainting</category><category>Data Augmentation</category>
    </item>
    <item>
      <title>[논문리뷰] SAM 3: Segment Anything with Concepts</title>
      <description>이 [arXiv]에 게시한 &#39;SAM 3: Segment Anything with Concepts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-SAM-3-Segment-Anything-with-Concepts/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-SAM-3-Segment-Anything-with-Concepts/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Segment Anything Model</category><category>Open-Vocabulary Segmentation</category><category>Multimodal Foundation Model</category><category>Instance Segmentation</category><category>Video Object Tracking</category><category>Prompt Engineering</category><category>Data Engine</category><category>Human-in-the-loop</category>
    </item>
    <item>
      <title>[논문리뷰] RynnVLA-002: A Unified Vision-Language-Action and World Model</title>
      <description>이 [arXiv]에 게시한 &#39;RynnVLA-002: A Unified Vision-Language-Action and World Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action (VLA) Model</category><category>World Model</category><category>Robotics</category><category>Unified Framework</category><category>Multi-modal Learning</category><category>Action Generation</category><category>Attention Mask</category><category>Continuous Control</category>
    </item>
    <item>
      <title>[논문리뷰] Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations</title>
      <description>Noam Koenigstein이 [arXiv]에 게시한 &#39;Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Saliency Maps</category><category>Explainable AI (XAI)</category><category>Taxonomy</category><category>Evaluation Framework</category><category>Faithfulness Metrics</category><category>Contrastive Explanations</category><category>Granularity</category>
    </item>
    <item>
      <title>[논문리뷰] Planning with Sketch-Guided Verification for Physics-Aware Video Generation</title>
      <description>Shayegan Omidshafiei이 [arXiv]에 게시한 &#39;Planning with Sketch-Guided Verification for Physics-Aware Video Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Motion Planning</category><category>Physics-Aware AI</category><category>Multimodal Verification</category><category>Diffusion Models</category><category>Test-Time Optimization</category><category>Sketch-Guided</category>
    </item>
    <item>
      <title>[논문리뷰] Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Sycophancy</category><category>Model Robustness</category><category>AI Alignment</category><category>Benchmark</category><category>Confidence Calibration</category><category>Behavioral Taxonomy</category><category>Social Influence</category><category>Epistemic Collapse</category>
    </item>
    <item>
      <title>[논문리뷰] OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe</title>
      <description>이 [arXiv]에 게시한 &#39;OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Reasoning</category><category>Large Multimodal Models</category><category>Supervised Fine-tuning</category><category>Reinforcement Learning</category><category>Data Curation</category><category>Open-source</category><category>Multimodal Benchmarks</category>
    </item>
    <item>
      <title>[논문리뷰] OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists</title>
      <description>Weiquan Lin이 [arXiv]에 게시한 &#39;OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Scientist</category><category>Large Language Models (LLMs)</category><category>Human-AI Collaboration</category><category>Scientific Ecosystem</category><category>Research Automation</category><category>Omni Scientific Protocol (OSP)</category><category>ScienceArena</category><category>Knowledge Graph</category>
    </item>
    <item>
      <title>[논문리뷰] O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents</title>
      <description>이 [arXiv]에 게시한 &#39;O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Memory System</category><category>LLM Agents</category><category>Personalization</category><category>User Profiling</category><category>Hierarchical Retrieval</category><category>Long-Term Interaction</category><category>Self-Evolving Agents</category><category>Contextual Consistency</category>
    </item>
    <item>
      <title>[논문리뷰] Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models (VLMs)</category><category>Adversarial Attack</category><category>Jailbreaking</category><category>Reward Hacking</category><category>Content Moderation Bypass</category><category>Cross-Model Transferability</category><category>Safety Vulnerabilities</category>
    </item>
    <item>
      <title>[논문리뷰] MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging</title>
      <description>이 [arXiv]에 게시한 &#39;MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Genome Modeling</category><category>Dynamic Tokenization</category><category>Token Merging</category><category>Context-aware Learning</category><category>DNA Foundation Models</category><category>Transformer Architecture</category><category>Multi-omics</category>
    </item>
    <item>
      <title>[논문리뷰] Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight</title>
      <description>이 [arXiv]에 게시한 &#39;Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action (VLA) Models</category><category>Visual Foresight</category><category>Diffusion Transformer (DiT)</category><category>Robotics</category><category>Multimodal Learning</category><category>Adaptive Temporal Ensemble</category><category>Latent Actions</category>
    </item>
    <item>
      <title>[논문리뷰] Loomis Painter: Reconstructing the Painting Process</title>
      <description>이 [arXiv]에 게시한 &#39;Loomis Painter: Reconstructing the Painting Process&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Painting Process Generation</category><category>Video Diffusion Models</category><category>Media Transfer</category><category>Reverse Painting</category><category>Dataset Curation</category><category>Perceptual Distance Profile</category><category>Artistic Workflow</category><category>Generative AI</category>
    </item>
    <item>
      <title>[논문리뷰] Insights from the ICLR Peer Review and Rebuttal Process</title>
      <description>Nedjma Ousidhoum이 [arXiv]에 게시한 &#39;Insights from the ICLR Peer Review and Rebuttal Process&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Peer Review</category><category>Rebuttal Process</category><category>ICLR</category><category>Score Dynamics</category><category>LLM Analysis</category><category>Reviewer Engagement</category><category>Academic Publishing</category><category>OpenReview</category>
    </item>
    <item>
      <title>[논문리뷰] GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization</title>
      <description>이 [arXiv]에 게시한 &#39;GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Geolocalization</category><category>Agentic Models</category><category>Visual Reasoning</category><category>Web-Augmented</category><category>Multimodal LLMs</category><category>Reinforcement Learning</category><category>Tool Use</category><category>GeoBench</category>
    </item>
    <item>
      <title>[논문리뷰] Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models</title>
      <description>Serena Yeung-Levy이 [arXiv]에 게시한 &#39;Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Small Multimodal Models</category><category>LLM Downscaling</category><category>Perception Bottleneck</category><category>Reasoning Bottleneck</category><category>Visual Extraction Tuning</category><category>Chain-of-Thought Reasoning</category><category>Multimodal Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Diversity Has Always Been There in Your Visual Autoregressive Models</title>
      <description>Yaxing Wang이 [arXiv]에 게시한 &#39;Diversity Has Always Been There in Your Visual Autoregressive Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Autoregressive Models</category><category>Diversity Collapse</category><category>Generative Diversity</category><category>Soft-Suppression Regularization</category><category>Soft-Amplification Regularization</category><category>Training-Free</category><category>Image Generation</category><category>Singular Value Decomposition</category>
    </item>
    <item>
      <title>[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</title>
      <description>이 [arXiv]에 게시한 &#39;Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Next Event Prediction</category><category>Reinforcement Learning</category><category>Vision-Language Model</category><category>Video Diffusion Model</category><category>Joint Optimization</category><category>Multimodal AI</category><category>Procedural Learning</category>
    </item>
    <item>
      <title>[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</title>
      <description>Baijiong Lin이 [arXiv]에 게시한 &#39;V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Reasoning Benchmark</category><category>Chain-of-Frame</category><category>Evaluation</category><category>Multimodal AI</category><category>Physical Dynamics</category><category>Spatial Cognition</category><category>Pattern Inference</category>
    </item>
    <item>
      <title>[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval</title>
      <description>이 [arXiv]에 게시한 &#39;TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Information Retrieval</category><category>Turkish Language</category><category>Late-Interaction Models</category><category>ColBERT</category><category>Dense Retrieval</category><category>MUVERA</category><category>Benchmarking</category><category>Low-Resource NLP</category><category>Fine-tuning</category>
    </item>
    <item>
      <title>[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long Video Understanding</category><category>Hybrid Mamba-Transformer</category><category>Vision-Language Model</category><category>Token Compression</category><category>Vision-to-Text Aggregation</category><category>Efficient LLM</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</title>
      <description>Xinyan Chen이 [arXiv]에 게시한 &#39;Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Generation</category><category>Textual Reasoning</category><category>Interleaving</category><category>Large Multimodal Models (LMMs)</category><category>Chain-of-Thought (CoT)</category><category>Zero-shot Learning</category><category>Supervised Fine-tuning (SFT)</category><category>Reinforcement Learning (RL)</category>
    </item>
    <item>
      <title>[논문리뷰] Step-Audio-R1 Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Step-Audio-R1 Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Step-Audio-R1-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Step-Audio-R1-Technical-Report/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Audio Reasoning</category><category>Multimodal LLMs</category><category>Modality-Grounded Reasoning Distillation (MGRD)</category><category>Chain-of-Thought</category><category>Reinforcement Learning</category><category>Audio Understanding</category><category>Self-Distillation</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Spatial Intelligence with Multimodal Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Intelligence</category><category>Multimodal Foundation Models</category><category>Data Scaling</category><category>Perspective-taking</category><category>Visual Question Answering</category><category>Emergent Capabilities</category><category>Embodied AI</category><category>Benchmark Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
      <description>이 [arXiv]에 게시한 &#39;SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Vision-Language-Action Models</category><category>Reward Shaping</category><category>World Models</category><category>Self-Referential Learning</category><category>Robotics</category><category>Trajectory Optimization</category>
    </item>
    
  </channel>
</rss>