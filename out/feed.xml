<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Thu, 16 Oct 2025 09:15:11 GMT</lastBuildDate>
    <pubDate>Thu, 16 Oct 2025 09:15:11 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] What If : Understanding Motion Through Sparse Interactions</title>
      <description>이 [arXiv]에 게시한 &#39;What If : Understanding Motion Through Sparse Interactions&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-What_If_Understanding_Motion_Through_Sparse_Interactions/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-What_If_Understanding_Motion_Through_Sparse_Interactions/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Motion Understanding</category><category>Sparse Interactions</category><category>Multimodal Prediction</category><category>Flow Poke Transformer</category><category>Physical Scene Dynamics</category><category>Uncertainty Quantification</category><category>Generative Models</category><category>Computer Vision</category>
    </item>
    <item>
      <title>[논문리뷰] ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</title>
      <description>이 [arXiv]에 게시한 &#39;ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-ViCO_A_Training_Strategy_towards_Semantic_Aware_Dynamic_High-Resolution/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-ViCO_A_Training_Strategy_towards_Semantic_Aware_Dynamic_High-Resolution/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models (MLLMs)</category><category>Dynamic Resolution</category><category>Token Compression</category><category>Semantic Awareness</category><category>Visual Consistency Learning (ViCO)</category><category>Visual Resolution Router (ViR)</category><category>Inference Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] UniFusion: Vision-Language Model as Unified Encoder in Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;UniFusion: Vision-Language Model as Unified Encoder in Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-UniFusion_Vision-Language_Model_as_Unified_Encoder_in_Image_Generation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-UniFusion_Vision-Language_Model_as_Unified_Encoder_in_Image_Generation/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Model</category><category>Unified Encoder</category><category>Image Generation</category><category>Diffusion Models</category><category>Multimodal Learning</category><category>Text-to-Image</category><category>Image Editing</category><category>Zero-shot Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Tensor Logic: The Language of AI</title>
      <description>Pedro Domingos이 [arXiv]에 게시한 &#39;Tensor Logic: The Language of AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Tensor_Logic_The_Language_of_AI/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Tensor_Logic_The_Language_of_AI/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Tensor Logic</category><category>Neurosymbolic AI</category><category>Logic Programming</category><category>Tensor Algebra</category><category>Deep Learning</category><category>Automated Reasoning</category><category>Embedding Space</category>
    </item>
    <item>
      <title>[논문리뷰] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models</title>
      <description>이 [arXiv]에 게시한 &#39;Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Temporal_Alignment_Guidance_On-Manifold_Sampling_in_Diffusion_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Temporal_Alignment_Guidance_On-Manifold_Sampling_in_Diffusion_Models/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Models</category><category>Generative Models</category><category>Guidance</category><category>On-Manifold Sampling</category><category>Temporal Alignment</category><category>Score Approximation Error</category><category>Training-Free Guidance</category>
    </item>
    <item>
      <title>[논문리뷰] SynthID-Image: Image watermarking at internet scale</title>
      <description>이 [arXiv]에 게시한 &#39;SynthID-Image: Image watermarking at internet scale&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-SynthID-Image_Image_watermarking_at_internet_scale/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-SynthID-Image_Image_watermarking_at_internet_scale/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Watermarking</category><category>AI-Generated Content</category><category>Provenance</category><category>Robustness</category><category>Security</category><category>Deep Learning</category><category>Internet Scale</category><category>Post-hoc</category>
    </item>
    <item>
      <title>[논문리뷰] Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</title>
      <description>이 [arXiv]에 게시한 &#39;Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Spatial_Forcing_Implicit_Spatial_Representation_Alignment_for_Vision-language-action_Model/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Spatial_Forcing_Implicit_Spatial_Representation_Alignment_for_Vision-language-action_Model/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Models</category><category>Spatial Perception</category><category>Implicit Representation Alignment</category><category>3D Foundation Models</category><category>Robotics</category><category>Data Efficiency</category><category>Representation Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Language-Centric Omnimodal Representation Learning</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Language-Centric Omnimodal Representation Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Scaling_Language-Centric_Omnimodal_Representation_Learning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Scaling_Language-Centric_Omnimodal_Representation_Learning/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Embeddings</category><category>MLLMs</category><category>Contrastive Learning</category><category>Cross-modal Alignment</category><category>Generative Pretraining</category><category>Representation Learning</category><category>Scaling Laws</category>
    </item>
    <item>
      <title>[논문리뷰] SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models</title>
      <description>이 [arXiv]에 게시한 &#39;SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-SRUM_Fine-Grained_Self-Rewarding_for_Unified_Multimodal_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-SRUM_Fine-Grained_Self-Rewarding_for_Unified_Multimodal_Models/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Unified Multimodal Models</category><category>Self-Rewarding</category><category>Text-to-Image Generation</category><category>Image Understanding</category><category>Post-Training</category><category>Global-Local Reward</category><category>Compositional Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</title>
      <description>이 [arXiv]에 게시한 &#39;SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-SAIL-Embedding_Technical_Report_Omni-modal_Embedding_Foundation_Model/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-SAIL-Embedding_Technical_Report_Omni-modal_Embedding_Foundation_Model/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Omni-modal Embedding</category><category>Multimodal Learning</category><category>Recommendation Systems</category><category>Hard Negative Mining</category><category>Contrastive Learning</category><category>Large Language Models (LLMs)</category><category>Data Balancing</category><category>Multitask Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Robot Learning: A Tutorial</title>
      <description>이 [arXiv]에 게시한 &#39;Robot Learning: A Tutorial&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Robot_Learning_A_Tutorial/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Robot_Learning_A_Tutorial/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Robot Learning</category><category>Reinforcement Learning</category><category>Imitation Learning</category><category>Behavioral Cloning</category><category>Vision-Language-Action Models</category><category>Diffusion Models</category><category>Transformers</category><category>LeRobot</category>
    </item>
    <item>
      <title>[논문리뷰] ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability</title>
      <description>Tsui-Wei Weng이 [arXiv]에 게시한 &#39;ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-ReFIne_A_Framework_for_Trustworthy_Large_Reasoning_Models_with_Reliability_Faithfulness_and_Interpretability/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-ReFIne_A_Framework_for_Trustworthy_Large_Reasoning_Models_with_Reliability_Faithfulness_and_Interpretability/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Trustworthy AI</category><category>Large Reasoning Models (LRMs)</category><category>Interpretability</category><category>Faithfulness</category><category>Reliability</category><category>Chain-of-Thought (CoT)</category><category>Supervised Fine-tuning (SFT)</category><category>GRPO</category>
    </item>
    <item>
      <title>[논문리뷰] One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration</title>
      <description>Mohit Bansal이 [arXiv]에 게시한 &#39;One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-One_Life_to_Learn_Inferring_Symbolic_World_Models_for_Stochastic_Environments_from_Unguided_Exploration/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-One_Life_to_Learn_Inferring_Symbolic_World_Models_for_Stochastic_Environments_from_Unguided_Exploration/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Symbolic World Models</category><category>Stochastic Environments</category><category>Unguided Exploration</category><category>Probabilistic Programming</category><category>Law Synthesis</category><category>Crafter-OO</category><category>Program Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks</title>
      <description>Xueyuan Lin이 [arXiv]에 게시한 &#39;Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Memory_as_Action_Autonomous_Context_Curation_for_Long-Horizon_Agentic_Tasks/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Memory_as_Action_Autonomous_Context_Curation_for_Long-Horizon_Agentic_Tasks/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long-Horizon Tasks</category><category>Agentic AI</category><category>Context Curation</category><category>Working Memory</category><category>Reinforcement Learning</category><category>Policy Optimization</category><category>Large Language Models</category><category>Memory-as-Action</category>
    </item>
    <item>
      <title>[논문리뷰] MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces</title>
      <description>Sungchul Kim이 [arXiv]에 게시한 &#39;MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-MLLM_as_a_UI_Judge_Benchmarking_Multimodal_LLMs_for_Predicting_Human_Perception_of_User_Interfaces/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-MLLM_as_a_UI_Judge_Benchmarking_Multimodal_LLMs_for_Predicting_Human_Perception_of_User_Interfaces/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>UI Evaluation</category><category>Human Perception</category><category>Benchmarking</category><category>UX Research</category><category>MLLM-as-a-Judge</category><category>Cognitive Factors</category><category>Pairwise Comparison</category>
    </item>
    <item>
      <title>[논문리뷰] LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens</title>
      <description>이 [arXiv]에 게시한 &#39;LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-LLM_Reasoning_for_Machine_Translation_Synthetic_Data_Generation_over_Thinking_Tokens/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-LLM_Reasoning_for_Machine_Translation_Synthetic_Data_Generation_over_Thinking_Tokens/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models (LLMs)</category><category>Machine Translation (MT)</category><category>Chain-of-Thought (CoT)</category><category>Knowledge Distillation</category><category>Fine-tuning</category><category>Prompt Engineering</category><category>Synthetic Data</category>
    </item>
    <item>
      <title>[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation</title>
      <description>이 [arXiv]에 게시한 &#39;Information-Preserving Reformulation of Reasoning Traces for Antidistillation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Information-Preserving_Reformulation_of_Reasoning_Traces_for_Antidistillation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Information-Preserving_Reformulation_of_Reasoning_Traces_for_Antidistillation/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Antidistillation</category><category>Reasoning Traces</category><category>Large Language Models</category><category>Knowledge Distillation</category><category>Information Preservation</category><category>Trace Reformulation</category><category>Supervised Fine-Tuning</category>
    </item>
    <item>
      <title>[논문리뷰] HoneyBee: Data Recipes for Vision-Language Reasoners</title>
      <description>이 [arXiv]에 게시한 &#39;HoneyBee: Data Recipes for Vision-Language Reasoners&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-HoneyBee_Data_Recipes_for_Vision-Language_Reasoners/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-HoneyBee_Data_Recipes_for_Vision-Language_Reasoners/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Data Curation</category><category>Chain-of-Thought</category><category>VL Reasoning</category><category>Dataset Scaling</category><category>Supervised Finetuning</category><category>HONEYBEE</category><category>Test-Time Scaling</category>
    </item>
    <item>
      <title>[논문리뷰] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</title>
      <description>Yihao Liu이 [arXiv]에 게시한 &#39;FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-FlashVSR_Towards_Real-Time_Diffusion-Based_Streaming_Video_Super-Resolution/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-FlashVSR_Towards_Real-Time_Diffusion-Based_Streaming_Video_Super-Resolution/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Super-Resolution (VSR)</category><category>Diffusion Models</category><category>Real-time VSR</category><category>Streaming VSR</category><category>Sparse Attention</category><category>Distillation</category><category>Conditional Decoder</category><category>High-resolution</category>
    </item>
    <item>
      <title>[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-ExpVid_A_Benchmark_for_Experiment_Video_Understanding_Reasoning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-ExpVid_A_Benchmark_for_Experiment_Video_Understanding_Reasoning/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Experiment Video Understanding</category><category>Multimodal Large Language Models (MLLMs)</category><category>Scientific Reasoning</category><category>Benchmark</category><category>Wet-Lab Experiments</category><category>Procedural Understanding</category><category>Fine-grained Perception</category><category>Video QA</category>
    </item>
    <item>
      <title>[논문리뷰] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning</title>
      <description>이 [arXiv]에 게시한 &#39;ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-ERA_Transforming_VLMs_into_Embodied_Agents_via_Embodied_Prior_Learning_and_Online_Reinforcement_Learning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-ERA_Transforming_VLMs_into_Embodied_Agents_via_Embodied_Prior_Learning_and_Online_Reinforcement_Learning/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Embodied AI</category><category>Vision Language Models (VLMs)</category><category>Reinforcement Learning (RL)</category><category>Prior Learning</category><category>Supervised Fine-tuning (SFT)</category><category>Embodied Agents</category>
    </item>
    <item>
      <title>[논문리뷰] Dr.LLM: Dynamic Layer Routing in LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;Dr.LLM: Dynamic Layer Routing in LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Dr.LLM_Dynamic_Layer_Routing_in_LLMs/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Dr.LLM_Dynamic_Layer_Routing_in_LLMs/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Dynamic Routing</category><category>LLMs</category><category>Adaptive Depth</category><category>Computational Efficiency</category><category>Monte Carlo Tree Search (MCTS)</category><category>Retrofittable Framework</category><category>Supervised Learning</category><category>Accuracy Improvement</category>
    </item>
    <item>
      <title>[논문리뷰] Detect Anything via Next Point Prediction</title>
      <description>이 [arXiv]에 게시한 &#39;Detect Anything via Next Point Prediction&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Detect_Anything_via_Next_Point_Prediction/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Detect_Anything_via_Next_Point_Prediction/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models</category><category>Object Detection</category><category>Coordinate Prediction</category><category>Reinforcement Learning</category><category>Supervised Fine-tuning</category><category>Visual Perception</category><category>Zero-shot Learning</category><category>Spatial Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</title>
      <description>이 [arXiv]에 게시한 &#39;DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-DeepMMSearch-R1_Empowering_Multimodal_LLMs_in_Multimodal_Web_Search/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-DeepMMSearch-R1_Empowering_Multimodal_LLMs_in_Multimodal_Web_Search/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLM</category><category>Web Search</category><category>Visual Question Answering</category><category>Reinforcement Learning</category><category>Image Cropping</category><category>Self-Correction</category><category>Tool Use</category>
    </item>
    <item>
      <title>[논문리뷰] DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation</title>
      <description>이 [arXiv]에 게시한 &#39;DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-DITING_A_Multi-Agent_Evaluation_Framework_for_Benchmarking_Web_Novel_Translation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-DITING_A_Multi-Agent_Evaluation_Framework_for_Benchmarking_Web_Novel_Translation/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Machine Translation Evaluation</category><category>Large Language Models (LLMs)</category><category>Web Novel Translation</category><category>Multi-Agent Systems</category><category>Cultural Nuance</category><category>Benchmark Dataset</category><category>Natural Language Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Boundary-Guided_Policy_Optimization_for_Memory-efficient_RL_of_Diffusion_Large_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Boundary-Guided_Policy_Optimization_for_Memory-efficient_RL_of_Diffusion_Large_Language_Models/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Large Language Models</category><category>Reinforcement Learning</category><category>Memory Efficiency</category><category>Monte Carlo Sampling</category><category>Log-Likelihood Approximation</category><category>Policy Optimization</category><category>ELBO</category>
    </item>
    <item>
      <title>[논문리뷰] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training</title>
      <description>이 [arXiv]에 게시한 &#39;Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Advancing_End-to-End_Pixel_Space_Generative_Modeling_via_Self-supervised_Pre-training/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Advancing_End-to-End_Pixel_Space_Generative_Modeling_via_Self-supervised_Pre-training/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Pixel-space Generative Models</category><category>Diffusion Models</category><category>Consistency Models</category><category>Self-supervised Pre-training</category><category>End-to-end Training</category><category>Image Generation</category><category>FID</category><category>Representation Learning</category>
    </item>
    <item>
      <title>[논문리뷰] A Survey of Vibe Coding with Large Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;A Survey of Vibe Coding with Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-A_Survey_of_Vibe_Coding_with_Large_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-A_Survey_of_Vibe_Coding_with_Large_Language_Models/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vibe Coding</category><category>Large Language Models</category><category>Coding Agents</category><category>Human-AI Collaboration</category><category>Software Engineering</category><category>Development Models</category><category>Context Engineering</category>
    </item>
    <item>
      <title>[논문리뷰] Which Heads Matter for Reasoning? RL-Guided KV Cache Compression</title>
      <description>Huan Wang이 [arXiv]에 게시한 &#39;Which Heads Matter for Reasoning? RL-Guided KV Cache Compression&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-13-Which_Heads_Matter_for_Reasoning_RL-Guided_KV_Cache_Compression/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-13-Which_Heads_Matter_for_Reasoning_RL-Guided_KV_Cache_Compression/</guid>
      <pubDate>Mon, 13 Oct 2025 04:44:18 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>KV Cache Compression</category><category>Large Language Models (LLMs)</category><category>Reinforcement Learning (RL)</category><category>Reasoning Models</category><category>Attention Heads</category><category>Chain-of-Thought (CoT)</category><category>Memory Efficiency</category>
    </item>
    <item>
      <title>[논문리뷰] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels</title>
      <description>이 [arXiv]에 게시한 &#39;Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-13-Webscale-RL_Automated_Data_Pipeline_for_Scaling_RL_Data_to_Pretraining_Levels/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-13-Webscale-RL_Automated_Data_Pipeline_for_Scaling_RL_Data_to_Pretraining_Levels/</guid>
      <pubDate>Mon, 13 Oct 2025 04:44:18 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning (RL)</category><category>Large Language Models (LLMs)</category><category>Data Pipeline</category><category>Web-scale Data</category><category>Question-Answering (QA)</category><category>Data Generation</category><category>Data Diversity</category><category>Data Efficiency</category>
    </item>
    
  </channel>
</rss>