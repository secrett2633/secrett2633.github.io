<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Mon, 24 Nov 2025 11:05:39 GMT</lastBuildDate>
    <pubDate>Mon, 24 Nov 2025 11:05:39 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</title>
      <description>이 [arXiv]에 게시한 &#39;Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Next Event Prediction</category><category>Reinforcement Learning</category><category>Vision-Language Model</category><category>Video Diffusion Model</category><category>Joint Optimization</category><category>Multimodal AI</category><category>Procedural Learning</category>
    </item>
    <item>
      <title>[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</title>
      <description>Baijiong Lin이 [arXiv]에 게시한 &#39;V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Reasoning Benchmark</category><category>Chain-of-Frame</category><category>Evaluation</category><category>Multimodal AI</category><category>Physical Dynamics</category><category>Spatial Cognition</category><category>Pattern Inference</category>
    </item>
    <item>
      <title>[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval</title>
      <description>이 [arXiv]에 게시한 &#39;TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Information Retrieval</category><category>Turkish Language</category><category>Late-Interaction Models</category><category>ColBERT</category><category>Dense Retrieval</category><category>MUVERA</category><category>Benchmarking</category><category>Low-Resource NLP</category><category>Fine-tuning</category>
    </item>
    <item>
      <title>[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long Video Understanding</category><category>Hybrid Mamba-Transformer</category><category>Vision-Language Model</category><category>Token Compression</category><category>Vision-to-Text Aggregation</category><category>Efficient LLM</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</title>
      <description>Xinyan Chen이 [arXiv]에 게시한 &#39;Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Generation</category><category>Textual Reasoning</category><category>Interleaving</category><category>Large Multimodal Models (LMMs)</category><category>Chain-of-Thought (CoT)</category><category>Zero-shot Learning</category><category>Supervised Fine-tuning (SFT)</category><category>Reinforcement Learning (RL)</category>
    </item>
    <item>
      <title>[논문리뷰] Step-Audio-R1 Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Step-Audio-R1 Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Step-Audio-R1-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Step-Audio-R1-Technical-Report/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Audio Reasoning</category><category>Multimodal LLMs</category><category>Modality-Grounded Reasoning Distillation (MGRD)</category><category>Chain-of-Thought</category><category>Reinforcement Learning</category><category>Audio Understanding</category><category>Self-Distillation</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Spatial Intelligence with Multimodal Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Intelligence</category><category>Multimodal Foundation Models</category><category>Data Scaling</category><category>Perspective-taking</category><category>Visual Question Answering</category><category>Emergent Capabilities</category><category>Embodied AI</category><category>Benchmark Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
      <description>이 [arXiv]에 게시한 &#39;SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Vision-Language-Action Models</category><category>Reward Shaping</category><category>World Models</category><category>Self-Referential Learning</category><category>Robotics</category><category>Trajectory Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</title>
      <description>이 [arXiv]에 게시한 &#39;SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Surgical Video Segmentation</category><category>Interactive Video Object Segmentation</category><category>Long-term Tracking</category><category>Foundation Models</category><category>Domain Adaptation</category><category>Semantic Learning</category><category>Prompt-based Segmentation</category>
    </item>
    <item>
      <title>[논문리뷰] SAM 3D: 3Dfy Anything in Images</title>
      <description>이 [arXiv]에 게시한 &#39;SAM 3D: 3Dfy Anything in Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-SAM-3D-3Dfy-Anything-in-Images/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-SAM-3D-3Dfy-Anything-in-Images/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Reconstruction</category><category>Generative Models</category><category>Single Image 3D</category><category>Object Reconstruction</category><category>Scene Understanding</category><category>Data Engine</category><category>Model-in-the-Loop</category><category>Human Preference</category>
    </item>
    <item>
      <title>[논문리뷰] PartUV: Part-Based UV Unwrapping of 3D Meshes</title>
      <description>Hao Su이 [arXiv]에 게시한 &#39;PartUV: Part-Based UV Unwrapping of 3D Meshes&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>UV Unwrapping</category><category>3D Meshes</category><category>Part-Based Decomposition</category><category>Neural Fields</category><category>Geometric Heuristics</category><category>Parameterization</category><category>Texture Mapping</category>
    </item>
    <item>
      <title>[논문리뷰] Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Compression</category><category>Elastic Networks</category><category>Knowledge Distillation</category><category>Hybrid Mamba-Attention</category><category>Reasoning LLMs</category><category>Multi-Budget Training</category><category>Zero-Shot Deployment</category>
    </item>
    <item>
      <title>[논문리뷰] NaTex: Seamless Texture Generation as Latent Color Diffusion</title>
      <description>이 [arXiv]에 게시한 &#39;NaTex: Seamless Texture Generation as Latent Color Diffusion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Texture Generation</category><category>Latent Diffusion Model</category><category>Geometry-Aware VAE</category><category>Multi-Control DiT</category><category>Color Point Cloud</category><category>Texture Synthesis</category><category>3D Asset Creation</category>
    </item>
    <item>
      <title>[논문리뷰] MiMo-Embodied: X-Embodied Foundation Model Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;MiMo-Embodied: X-Embodied Foundation Model Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Model (VLM)</category><category>Embodied AI</category><category>Autonomous Driving</category><category>Foundation Model</category><category>Multimodal Learning</category><category>Task Planning</category><category>Affordance Prediction</category><category>Spatial Understanding</category><category>Reinforcement Learning</category>
    </item>
    <item>
      <title>[논문리뷰] First Frame Is the Place to Go for Video Content Customization</title>
      <description>이 [arXiv]에 게시한 &#39;First Frame Is the Place to Go for Video Content Customization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Content Customization</category><category>Few-shot Learning</category><category>LoRA</category><category>Vision-Language Models (VLMs)</category><category>First Frame Conditioning</category><category>Reference-based Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Draft and Refine with Visual Experts</title>
      <description>이 [arXiv]에 게시한 &#39;Draft and Refine with Visual Experts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Draft-and-Refine-with-Visual-Experts/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Draft-and-Refine-with-Visual-Experts/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Vision-Language Models (LVLMs)</category><category>Visual Grounding</category><category>Hallucination Mitigation</category><category>Agent Framework</category><category>Visual Question Answering (VQA)</category><category>Expert Coordination</category><category>Relevance Map</category><category>Multi-modal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity</title>
      <description>이 [arXiv]에 게시한 &#39;What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Research Agents</category><category>Ideation Diversity</category><category>MLE-bench</category><category>LLM Backbones</category><category>Agentic Scaffolds</category><category>Shannon Entropy</category><category>Machine Learning Engineering</category><category>Performance Metrics</category>
    </item>
    <item>
      <title>[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images</title>
      <description>이 [arXiv]에 게시한 &#39;VisPlay: Self-Evolving Vision-Language Models from Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-Evolving</category><category>Vision-Language Models</category><category>Reinforcement Learning</category><category>Self-Play</category><category>Unlabeled Data</category><category>Multimodal Reasoning</category><category>Group Relative Policy Optimization</category><category>Hallucination Mitigation</category>
    </item>
    <item>
      <title>[논문리뷰] Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks</title>
      <description>Yiran Peng이 [arXiv]에 게시한 &#39;Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Models</category><category>Spatial Reasoning</category><category>Maze Solving</category><category>Video Generation</category><category>Benchmark</category><category>Supervised Fine-tuning</category><category>Test-Time Scaling</category><category>Multimodal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</title>
      <description>이 [arXiv]에 게시한 &#39;Mixture of States: Routing Token-Level Dynamics for Multimodal Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Diffusion</category><category>Mixture of States (MoS)</category><category>Token-Level Routing</category><category>Dynamic Conditional Fusion</category><category>Text-to-Image Generation</category><category>Image Editing</category><category>Transformer Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] Medal S: Spatio-Textual Prompt Model for Medical Segmentation</title>
      <description>Tao Chen이 [arXiv]에 게시한 &#39;Medal S: Spatio-Textual Prompt Model for Medical Segmentation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Segmentation</category><category>Foundation Model</category><category>Spatio-Textual Prompts</category><category>3D Convolution</category><category>Multi-modal Imaging</category><category>Dynamic Resampling</category><category>Parallel Inference</category><category>Iterative Refinement</category>
    </item>
    <item>
      <title>[논문리뷰] MHR: Momentum Human Rig</title>
      <description>Chris Twigg이 [arXiv]에 게시한 &#39;MHR: Momentum Human Rig&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-MHR-Momentum-Human-Rig/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-MHR-Momentum-Human-Rig/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Parametric Body Model</category><category>Human Animation</category><category>Character Rigging</category><category>Pose Correctives</category><category>Skeletal Decoupling</category><category>Computer Graphics</category><category>AR/VR</category>
    </item>
    <item>
      <title>[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</title>
      <description>Vladimir Arkhipkin이 [arXiv]에 게시한 &#39;Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Generation</category><category>Video Generation</category><category>Diffusion Models</category><category>Flow Matching</category><category>Diffusion Transformer</category><category>NABLA</category><category>RLHF</category><category>Supervised Fine-tuning</category>
    </item>
    <item>
      <title>[논문리뷰] Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset</title>
      <description>이 [arXiv]에 게시한 &#39;Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Imaging</category><category>Chest X-ray</category><category>Lesion Segmentation</category><category>Vision-Language Models</category><category>Instruction Following</category><category>Data Generation</category><category>MIMIC-CXR</category>
    </item>
    <item>
      <title>[논문리뷰] FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</title>
      <description>Xinyu Yin이 [arXiv]에 게시한 &#39;FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Embodied AI</category><category>Vision-and-Language Navigation (VLN)</category><category>LLM-driven Simulation</category><category>Human-Agent Interaction</category><category>Closed-Loop</category><category>Benchmark Dataset</category><category>Social Cognition</category>
    </item>
    <item>
      <title>[논문리뷰] Aligning Generative Music AI with Human Preferences: Methods and Challenges</title>
      <description>Abhinaba Roy이 [arXiv]에 게시한 &#39;Aligning Generative Music AI with Human Preferences: Methods and Challenges&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative Music AI</category><category>Preference Alignment</category><category>Reinforcement Learning from Human Feedback (RLHF)</category><category>Direct Preference Optimization (DPO)</category><category>Inference-Time Optimization</category><category>Music Generation</category><category>Human-Computer Interaction</category>
    </item>
    <item>
      <title>[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries</title>
      <description>이 [arXiv]에 게시한 &#39;ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Chaptering</category><category>Long-form Video Understanding</category><category>Large Language Models</category><category>Multimodal Learning</category><category>Hierarchical Summarization</category><category>Video Segmentation</category><category>Reinforcement Learning</category><category>Dataset Creation</category>
    </item>
    <item>
      <title>[논문리뷰] Φeat: Physically-Grounded Feature Representation</title>
      <description>이 [arXiv]에 게시한 &#39;Φeat: Physically-Grounded Feature Representation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-19-eat-Physically-Grounded-Feature-Representation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-19-eat-Physically-Grounded-Feature-Representation/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-supervised Learning</category><category>Physically-Grounded Features</category><category>Material Representation</category><category>Intrinsic Scene Understanding</category><category>Vision Transformer</category><category>Synthetic Data</category><category>Contrastive Learning</category>
    </item>
    <item>
      <title>[논문리뷰] VIDEOP2R: Video Understanding from Perception to Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;VIDEOP2R: Video Understanding from Perception to Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Understanding</category><category>Reinforcement Fine-Tuning (RFT)</category><category>Large Video Language Models (LVLMs)</category><category>Perception and Reasoning</category><category>Chain-of-Thought (CoT)</category><category>Process-Aware Learning</category><category>Policy Optimization</category><category>Credit Assignment</category>
    </item>
    <item>
      <title>[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models</title>
      <description>Rong Zhao이 [arXiv]에 게시한 &#39;TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LVLM Evaluation</category><category>Global Visual Perception</category><category>Topological Properties</category><category>Shortcut-Free Benchmark</category><category>Visual Bottleneck</category><category>Multimodal AI</category><category>Synthetic Data</category>
    </item>
    
  </channel>
</rss>