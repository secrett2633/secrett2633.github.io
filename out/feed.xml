<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Mon, 24 Nov 2025 09:57:54 GMT</lastBuildDate>
    <pubDate>Mon, 24 Nov 2025 09:57:54 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</title>
      <description>이 [arXiv]에 게시한 &#39;Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Video-as-Answer_Predict_and_Generate_Next_Video_Event_with_Joint-GRPO/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Video-as-Answer_Predict_and_Generate_Next_Video_Event_with_Joint-GRPO/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Next Event Prediction</category><category>Reinforcement Learning</category><category>Vision-Language Model</category><category>Video Diffusion Model</category><category>Joint Optimization</category><category>Multimodal AI</category><category>Procedural Learning</category>
    </item>
    <item>
      <title>[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</title>
      <description>Baijiong Lin이 [arXiv]에 게시한 &#39;V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-V-ReasonBench_Toward_Unified_Reasoning_Benchmark_Suite_for_Video_Generation_Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-V-ReasonBench_Toward_Unified_Reasoning_Benchmark_Suite_for_Video_Generation_Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Reasoning Benchmark</category><category>Chain-of-Frame</category><category>Evaluation</category><category>Multimodal AI</category><category>Physical Dynamics</category><category>Spatial Cognition</category><category>Pattern Inference</category>
    </item>
    <item>
      <title>[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval</title>
      <description>이 [arXiv]에 게시한 &#39;TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-TurkColBERT_A_Benchmark_of_Dense_and_Late-Interaction_Models_for_Turkish_Information_Retrieval/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-TurkColBERT_A_Benchmark_of_Dense_and_Late-Interaction_Models_for_Turkish_Information_Retrieval/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Information Retrieval</category><category>Turkish Language</category><category>Late-Interaction Models</category><category>ColBERT</category><category>Dense Retrieval</category><category>MUVERA</category><category>Benchmarking</category><category>Low-Resource NLP</category><category>Fine-tuning</category>
    </item>
    <item>
      <title>[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-TimeViper_A_Hybrid_Mamba-Transformer_Vision-Language_Model_for_Efficient_Long_Video_Understanding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-TimeViper_A_Hybrid_Mamba-Transformer_Vision-Language_Model_for_Efficient_Long_Video_Understanding/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long Video Understanding</category><category>Hybrid Mamba-Transformer</category><category>Vision-Language Model</category><category>Token Compression</category><category>Vision-to-Text Aggregation</category><category>Efficient LLM</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</title>
      <description>Xinyan Chen이 [arXiv]에 게시한 &#39;Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Thinking-while-Generating_Interleaving_Textual_Reasoning_throughout_Visual_Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Thinking-while-Generating_Interleaving_Textual_Reasoning_throughout_Visual_Generation/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Generation</category><category>Textual Reasoning</category><category>Interleaving</category><category>Large Multimodal Models (LMMs)</category><category>Chain-of-Thought (CoT)</category><category>Zero-shot Learning</category><category>Supervised Fine-tuning (SFT)</category><category>Reinforcement Learning (RL)</category>
    </item>
    <item>
      <title>[논문리뷰] Step-Audio-R1 Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Step-Audio-R1 Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Step-Audio-R1_Technical_Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Step-Audio-R1_Technical_Report/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Audio Reasoning</category><category>Multimodal LLMs</category><category>Modality-Grounded Reasoning Distillation (MGRD)</category><category>Chain-of-Thought</category><category>Reinforcement Learning</category><category>Audio Understanding</category><category>Self-Distillation</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Spatial Intelligence with Multimodal Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Scaling_Spatial_Intelligence_with_Multimodal_Foundation_Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Scaling_Spatial_Intelligence_with_Multimodal_Foundation_Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Intelligence</category><category>Multimodal Foundation Models</category><category>Data Scaling</category><category>Perspective-taking</category><category>Visual Question Answering</category><category>Emergent Capabilities</category><category>Embodied AI</category><category>Benchmark Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
      <description>이 [arXiv]에 게시한 &#39;SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-SRPO_Self-Referential_Policy_Optimization_for_Vision-Language-Action_Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-SRPO_Self-Referential_Policy_Optimization_for_Vision-Language-Action_Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Vision-Language-Action Models</category><category>Reward Shaping</category><category>World Models</category><category>Self-Referential Learning</category><category>Robotics</category><category>Trajectory Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] SAM 3D: 3Dfy Anything in Images</title>
      <description>이 [arXiv]에 게시한 &#39;SAM 3D: 3Dfy Anything in Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-SAM_3D_3Dfy_Anything_in_Images/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-SAM_3D_3Dfy_Anything_in_Images/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Reconstruction</category><category>Generative Models</category><category>Single Image 3D</category><category>Object Reconstruction</category><category>Scene Understanding</category><category>Data Engine</category><category>Model-in-the-Loop</category><category>Human Preference</category>
    </item>
    <item>
      <title>[논문리뷰] SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</title>
      <description>이 [arXiv]에 게시한 &#39;SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-SAM2S_Segment_Anything_in_Surgical_Videos_via_Semantic_Long-term_Tracking/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-SAM2S_Segment_Anything_in_Surgical_Videos_via_Semantic_Long-term_Tracking/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Surgical Video Segmentation</category><category>Interactive Video Object Segmentation</category><category>Long-term Tracking</category><category>Foundation Models</category><category>Domain Adaptation</category><category>Semantic Learning</category><category>Prompt-based Segmentation</category>
    </item>
    <item>
      <title>[논문리뷰] PartUV: Part-Based UV Unwrapping of 3D Meshes</title>
      <description>Hao Su이 [arXiv]에 게시한 &#39;PartUV: Part-Based UV Unwrapping of 3D Meshes&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-PartUV_Part-Based_UV_Unwrapping_of_3D_Meshes/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-PartUV_Part-Based_UV_Unwrapping_of_3D_Meshes/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>UV Unwrapping</category><category>3D Meshes</category><category>Part-Based Decomposition</category><category>Neural Fields</category><category>Geometric Heuristics</category><category>Parameterization</category><category>Texture Mapping</category>
    </item>
    <item>
      <title>[논문리뷰] Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Nemotron_Elastic_Towards_Efficient_Many-in-One_Reasoning_LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Nemotron_Elastic_Towards_Efficient_Many-in-One_Reasoning_LLMs/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Compression</category><category>Elastic Networks</category><category>Knowledge Distillation</category><category>Hybrid Mamba-Attention</category><category>Reasoning LLMs</category><category>Multi-Budget Training</category><category>Zero-Shot Deployment</category>
    </item>
    <item>
      <title>[논문리뷰] NaTex: Seamless Texture Generation as Latent Color Diffusion</title>
      <description>이 [arXiv]에 게시한 &#39;NaTex: Seamless Texture Generation as Latent Color Diffusion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-NaTex_Seamless_Texture_Generation_as_Latent_Color_Diffusion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-NaTex_Seamless_Texture_Generation_as_Latent_Color_Diffusion/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Texture Generation</category><category>Latent Diffusion Model</category><category>Geometry-Aware VAE</category><category>Multi-Control DiT</category><category>Color Point Cloud</category><category>Texture Synthesis</category><category>3D Asset Creation</category>
    </item>
    <item>
      <title>[논문리뷰] MiMo-Embodied: X-Embodied Foundation Model Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;MiMo-Embodied: X-Embodied Foundation Model Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-MiMo-Embodied_X-Embodied_Foundation_Model_Technical_Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-MiMo-Embodied_X-Embodied_Foundation_Model_Technical_Report/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Model (VLM)</category><category>Embodied AI</category><category>Autonomous Driving</category><category>Foundation Model</category><category>Multimodal Learning</category><category>Task Planning</category><category>Affordance Prediction</category><category>Spatial Understanding</category><category>Reinforcement Learning</category>
    </item>
    <item>
      <title>[논문리뷰] First Frame Is the Place to Go for Video Content Customization</title>
      <description>이 [arXiv]에 게시한 &#39;First Frame Is the Place to Go for Video Content Customization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-First_Frame_Is_the_Place_to_Go_for_Video_Content_Customization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-First_Frame_Is_the_Place_to_Go_for_Video_Content_Customization/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Content Customization</category><category>Few-shot Learning</category><category>LoRA</category><category>Vision-Language Models (VLMs)</category><category>First Frame Conditioning</category><category>Reference-based Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Draft and Refine with Visual Experts</title>
      <description>이 [arXiv]에 게시한 &#39;Draft and Refine with Visual Experts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-21-Draft_and_Refine_with_Visual_Experts/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-21-Draft_and_Refine_with_Visual_Experts/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Vision-Language Models (LVLMs)</category><category>Visual Grounding</category><category>Hallucination Mitigation</category><category>Agent Framework</category><category>Visual Question Answering (VQA)</category><category>Expert Coordination</category><category>Relevance Map</category><category>Multi-modal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity</title>
      <description>이 [arXiv]에 게시한 &#39;What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Research Agents</category><category>Ideation Diversity</category><category>MLE-bench</category><category>LLM Backbones</category><category>Agentic Scaffolds</category><category>Shannon Entropy</category><category>Machine Learning Engineering</category><category>Performance Metrics</category>
    </item>
    <item>
      <title>[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images</title>
      <description>이 [arXiv]에 게시한 &#39;VisPlay: Self-Evolving Vision-Language Models from Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-Evolving</category><category>Vision-Language Models</category><category>Reinforcement Learning</category><category>Self-Play</category><category>Unlabeled Data</category><category>Multimodal Reasoning</category><category>Group Relative Policy Optimization</category><category>Hallucination Mitigation</category>
    </item>
    <item>
      <title>[논문리뷰] Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks</title>
      <description>Yiran Peng이 [arXiv]에 게시한 &#39;Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Models</category><category>Spatial Reasoning</category><category>Maze Solving</category><category>Video Generation</category><category>Benchmark</category><category>Supervised Fine-tuning</category><category>Test-Time Scaling</category><category>Multimodal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</title>
      <description>이 [arXiv]에 게시한 &#39;Mixture of States: Routing Token-Level Dynamics for Multimodal Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Diffusion</category><category>Mixture of States (MoS)</category><category>Token-Level Routing</category><category>Dynamic Conditional Fusion</category><category>Text-to-Image Generation</category><category>Image Editing</category><category>Transformer Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] Medal S: Spatio-Textual Prompt Model for Medical Segmentation</title>
      <description>Tao Chen이 [arXiv]에 게시한 &#39;Medal S: Spatio-Textual Prompt Model for Medical Segmentation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Medal_S_Spatio-Textual_Prompt_Model_for_Medical_Segmentation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Medal_S_Spatio-Textual_Prompt_Model_for_Medical_Segmentation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Segmentation</category><category>Foundation Model</category><category>Spatio-Textual Prompts</category><category>3D Convolution</category><category>Multi-modal Imaging</category><category>Dynamic Resampling</category><category>Parallel Inference</category><category>Iterative Refinement</category>
    </item>
    <item>
      <title>[논문리뷰] MHR: Momentum Human Rig</title>
      <description>Chris Twigg이 [arXiv]에 게시한 &#39;MHR: Momentum Human Rig&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-MHR_Momentum_Human_Rig/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-MHR_Momentum_Human_Rig/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Parametric Body Model</category><category>Human Animation</category><category>Character Rigging</category><category>Pose Correctives</category><category>Skeletal Decoupling</category><category>Computer Graphics</category><category>AR/VR</category>
    </item>
    <item>
      <title>[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</title>
      <description>Vladimir Arkhipkin이 [arXiv]에 게시한 &#39;Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Kandinsky_5.0_A_Family_of_Foundation_Models_for_Image_and_Video_Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Kandinsky_5.0_A_Family_of_Foundation_Models_for_Image_and_Video_Generation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Generation</category><category>Video Generation</category><category>Diffusion Models</category><category>Flow Matching</category><category>Diffusion Transformer</category><category>NABLA</category><category>RLHF</category><category>Supervised Fine-tuning</category>
    </item>
    <item>
      <title>[논문리뷰] Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset</title>
      <description>이 [arXiv]에 게시한 &#39;Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Instruction-Guided_Lesion_Segmentation_for_Chest_X-rays_with_Automatically_Generated_Large-Scale_Dataset/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Instruction-Guided_Lesion_Segmentation_for_Chest_X-rays_with_Automatically_Generated_Large-Scale_Dataset/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Imaging</category><category>Chest X-ray</category><category>Lesion Segmentation</category><category>Vision-Language Models</category><category>Instruction Following</category><category>Data Generation</category><category>MIMIC-CXR</category>
    </item>
    <item>
      <title>[논문리뷰] FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</title>
      <description>Xinyu Yin이 [arXiv]에 게시한 &#39;FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-FreeAskWorld_An_Interactive_and_Closed-Loop_Simulator_for_Human-Centric_Embodied_AI/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-FreeAskWorld_An_Interactive_and_Closed-Loop_Simulator_for_Human-Centric_Embodied_AI/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Embodied AI</category><category>Vision-and-Language Navigation (VLN)</category><category>LLM-driven Simulation</category><category>Human-Agent Interaction</category><category>Closed-Loop</category><category>Benchmark Dataset</category><category>Social Cognition</category>
    </item>
    <item>
      <title>[논문리뷰] Aligning Generative Music AI with Human Preferences: Methods and Challenges</title>
      <description>Abhinaba Roy이 [arXiv]에 게시한 &#39;Aligning Generative Music AI with Human Preferences: Methods and Challenges&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-Aligning_Generative_Music_AI_with_Human_Preferences_Methods_and_Challenges/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-Aligning_Generative_Music_AI_with_Human_Preferences_Methods_and_Challenges/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative Music AI</category><category>Preference Alignment</category><category>Reinforcement Learning from Human Feedback (RLHF)</category><category>Direct Preference Optimization (DPO)</category><category>Inference-Time Optimization</category><category>Music Generation</category><category>Human-Computer Interaction</category>
    </item>
    <item>
      <title>[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries</title>
      <description>이 [arXiv]에 게시한 &#39;ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-20-ARC-Chapter_Structuring_Hour-Long_Videos_into_Navigable_Chapters_and_Hierarchical_Summaries/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-20-ARC-Chapter_Structuring_Hour-Long_Videos_into_Navigable_Chapters_and_Hierarchical_Summaries/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Chaptering</category><category>Long-form Video Understanding</category><category>Large Language Models</category><category>Multimodal Learning</category><category>Hierarchical Summarization</category><category>Video Segmentation</category><category>Reinforcement Learning</category><category>Dataset Creation</category>
    </item>
    <item>
      <title>[논문리뷰] Φeat: Physically-Grounded Feature Representation</title>
      <description>이 [arXiv]에 게시한 &#39;Φeat: Physically-Grounded Feature Representation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-19-Φeat_Physically-Grounded_Feature_Representation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-19-Φeat_Physically-Grounded_Feature_Representation/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-supervised Learning</category><category>Physically-Grounded Features</category><category>Material Representation</category><category>Intrinsic Scene Understanding</category><category>Vision Transformer</category><category>Synthetic Data</category><category>Contrastive Learning</category>
    </item>
    <item>
      <title>[논문리뷰] VIDEOP2R: Video Understanding from Perception to Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;VIDEOP2R: Video Understanding from Perception to Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-19-VIDEOP2R_Video_Understanding_from_Perception_to_Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-19-VIDEOP2R_Video_Understanding_from_Perception_to_Reasoning/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Understanding</category><category>Reinforcement Fine-Tuning (RFT)</category><category>Large Video Language Models (LVLMs)</category><category>Perception and Reasoning</category><category>Chain-of-Thought (CoT)</category><category>Process-Aware Learning</category><category>Policy Optimization</category><category>Credit Assignment</category>
    </item>
    <item>
      <title>[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models</title>
      <description>Rong Zhao이 [arXiv]에 게시한 &#39;TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-19-TopoPerception_A_Shortcut-Free_Evaluation_of_Global_Visual_Perception_in_Large_Vision-Language_Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-19-TopoPerception_A_Shortcut-Free_Evaluation_of_Global_Visual_Perception_in_Large_Vision-Language_Models/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LVLM Evaluation</category><category>Global Visual Perception</category><category>Topological Properties</category><category>Shortcut-Free Benchmark</category><category>Visual Bottleneck</category><category>Multimodal AI</category><category>Synthetic Data</category>
    </item>
    
  </channel>
</rss>