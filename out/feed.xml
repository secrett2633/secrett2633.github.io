<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Sat, 25 Oct 2025 11:09:50 GMT</lastBuildDate>
    <pubDate>Sat, 25 Oct 2025 11:09:50 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] Thought Communication in Multiagent Collaboration</title>
      <description>Mingze Gao이 [arXiv]에 게시한 &#39;Thought Communication in Multiagent Collaboration&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Thought_Communication_in_Multiagent_Collaboration/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Thought_Communication_in_Multiagent_Collaboration/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multiagent Systems</category><category>LLM Communication</category><category>Latent Variable Models</category><category>Identifiability Theory</category><category>Thought Communication</category><category>Sparse Autoencoder</category><category>Prefix Tuning</category>
    </item>
    <item>
      <title>[논문리뷰] The Massive Legal Embedding Benchmark (MLEB)</title>
      <description>이 [arXiv]에 게시한 &#39;The Massive Legal Embedding Benchmark (MLEB)&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-The_Massive_Legal_Embedding_Benchmark_MLEB/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-The_Massive_Legal_Embedding_Benchmark_MLEB/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Legal Information Retrieval</category><category>Embedding Models</category><category>Benchmark Dataset</category><category>Natural Language Processing</category><category>Retrieval-Augmented Generation</category><category>Jurisdictional Diversity</category><category>Legal Tech</category>
    </item>
    <item>
      <title>[논문리뷰] Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets</title>
      <description>이 [arXiv]에 게시한 &#39;Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Seed3D_1.0_From_Images_to_High-Fidelity_Simulation-Ready_3D_Assets/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Seed3D_1.0_From_Images_to_High-Fidelity_Simulation-Ready_3D_Assets/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Asset Generation</category><category>Simulation-Ready Assets</category><category>Diffusion Models</category><category>Physically Based Rendering (PBR)</category><category>Embodied AI</category><category>Robotic Simulation</category><category>Image-to-3D</category><category>Foundation Model</category>
    </item>
    <item>
      <title>[논문리뷰] Search Self-play: Pushing the Frontier of Agent Capability without Supervision</title>
      <description>이 [arXiv]에 게시한 &#39;Search Self-play: Pushing the Frontier of Agent Capability without Supervision&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Search_Self-play_Pushing_the_Frontier_of_Agent_Capability_without_Supervision/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Search_Self-play_Pushing_the_Frontier_of_Agent_Capability_without_Supervision/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Self-play</category><category>Reinforcement Learning</category><category>Search Agents</category><category>Supervision-Free Training</category><category>Retrieval-Augmented Generation (RAG)</category><category>Task Generation</category><category>Curriculum Learning</category>
    </item>
    <item>
      <title>[논문리뷰] SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-SAKE_Towards_Editing_Auditory_Attribute_Knowledge_of_Large_Audio-Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-SAKE_Towards_Editing_Auditory_Attribute_Knowledge_of_Large_Audio-Language_Models/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Knowledge Editing</category><category>Audio-Language Models</category><category>Auditory Attributes</category><category>Benchmark</category><category>Reliability</category><category>Generality</category><category>Locality</category><category>Portability</category>
    </item>
    <item>
      <title>[논문리뷰] Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>
      <description>이 [arXiv]에 게시한 &#39;Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Open-o3_Video_Grounded_Video_Reasoning_with_Explicit_Spatio-Temporal_Evidence/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Open-o3_Video_Grounded_Video_Reasoning_with_Explicit_Spatio-Temporal_Evidence/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Reasoning</category><category>Spatio-Temporal Grounding</category><category>Large Multimodal Models</category><category>Reinforcement Learning</category><category>Chain-of-Thought</category><category>Visual Evidence</category><category>Dataset Curation</category>
    </item>
    <item>
      <title>[논문리뷰] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall</title>
      <description>Sungjin Ahn이 [arXiv]에 게시한 &#39;Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Loopholing_Discrete_Diffusion_Deterministic_Bypass_of_the_Sampling_Wall/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Loopholing_Discrete_Diffusion_Deterministic_Bypass_of_the_Sampling_Wall/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Discrete Diffusion Models</category><category>Sampling Wall</category><category>Loopholing</category><category>Self-Conditioning</category><category>Non-Autoregressive Generation</category><category>Text Generation</category><category>Language Modeling</category><category>Reasoning Tasks</category>
    </item>
    <item>
      <title>[논문리뷰] LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas</title>
      <description>이 [arXiv]에 게시한 &#39;LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-LayerComposer_Interactive_Personalized_T2I_via_Spatially-Aware_Layered_Canvas/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-LayerComposer_Interactive_Personalized_T2I_via_Spatially-Aware_Layered_Canvas/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image Generation</category><category>Personalization</category><category>Diffusion Models</category><category>Interactive Control</category><category>Multi-Subject Composition</category><category>Layered Canvas</category><category>Spatial Control</category><category>Image Editing</category>
    </item>
    <item>
      <title>[논문리뷰] Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations</title>
      <description>이 [arXiv]에 게시한 &#39;Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Investigating_Safety_Vulnerabilities_of_Large_Audio-Language_Models_Under_Speaker_Emotional_Variations/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Investigating_Safety_Vulnerabilities_of_Large_Audio-Language_Models_Under_Speaker_Emotional_Variations/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LALM Safety</category><category>Speaker Emotion</category><category>Safety Alignment</category><category>Jailbreaking</category><category>Audio-Language Models</category><category>Emotional Variation</category><category>Unsafe Rate</category><category>Non-refusal Rate</category>
    </item>
    <item>
      <title>[논문리뷰] ImpossibleBench: Measuring LLMs&#39; Propensity of Exploiting Test Cases</title>
      <description>Nicholas Carlini이 [arXiv]에 게시한 &#39;ImpossibleBench: Measuring LLMs&#39; Propensity of Exploiting Test Cases&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-ImpossibleBench_Measuring_LLMs_Propensity_of_Exploiting_Test_Cases/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-ImpossibleBench_Measuring_LLMs_Propensity_of_Exploiting_Test_Cases/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Evaluation</category><category>Reward Hacking</category><category>Benchmark Reliability</category><category>Test Exploitation</category><category>Prompt Engineering</category><category>LLM Safety</category><category>Code Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</title>
      <description>이 [arXiv]에 게시한 &#39;Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Human-Agent_Collaborative_Paper-to-Page_Crafting_for_Under_0.1/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Human-Agent_Collaborative_Paper-to-Page_Crafting_for_Under_0.1/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Human-Agent Collaboration</category><category>Project Page Generation</category><category>Multi-Agent System</category><category>LLM</category><category>VLM</category><category>Webpage Automation</category><category>PageBench</category><category>Scientific Communication</category><category>Cost-Effective AI</category>
    </item>
    <item>
      <title>[논문리뷰] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</title>
      <description>이 [arXiv]에 게시한 &#39;HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-HoloCine_Holistic_Generation_of_Cinematic_Multi-Shot_Long_Video_Narratives/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-HoloCine_Holistic_Generation_of_Cinematic_Multi-Shot_Long_Video_Narratives/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Video Generation</category><category>Multi-Shot Video</category><category>Narrative Coherence</category><category>Diffusion Models</category><category>Self-Attention</category><category>Cinematic AI</category><category>Video Consistency</category><category>Directorial Control</category>
    </item>
    <item>
      <title>[논문리뷰] From Masks to Worlds: A Hitchhiker&#39;s Guide to World Models</title>
      <description>Shufan Li이 [arXiv]에 게시한 &#39;From Masks to Worlds: A Hitchhiker&#39;s Guide to World Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-From_Masks_to_Worlds_A_Hitchhikers_Guide_to_World_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-From_Masks_to_Worlds_A_Hitchhikers_Guide_to_World_Models/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Generative AI</category><category>Multimodal Learning</category><category>Masked Modeling</category><category>Interactive AI</category><category>Memory Systems</category><category>Autonomous Agents</category><category>AI Roadmap</category>
    </item>
    <item>
      <title>[논문리뷰] Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values</title>
      <description>이 [arXiv]에 게시한 &#39;Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Every_Question_Has_Its_Own_Value_Reinforcement_Learning_with_Explicit_Human_Values/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Every_Question_Has_Its_Own_Value_Reinforcement_Learning_with_Explicit_Human_Values/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>LLM Alignment</category><category>Human Values</category><category>Reward Shaping</category><category>Value-Weighted Reward</category><category>Termination Policy</category><category>RLVR</category>
    </item>
    <item>
      <title>[논문리뷰] Emergence of Linear Truth Encodings in Language Models</title>
      <description>Alberto Bietti이 [arXiv]에 게시한 &#39;Emergence of Linear Truth Encodings in Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Emergence_of_Linear_Truth_Encodings_in_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Emergence_of_Linear_Truth_Encodings_in_Language_Models/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Language Models</category><category>Truth Encoding</category><category>Linear Subspaces</category><category>Mechanistic Interpretability</category><category>Transformer Models</category><category>Learning Dynamics</category><category>Truth Co-occurrence Hypothesis</category><category>Hallucinations</category>
    </item>
    <item>
      <title>[논문리뷰] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</title>
      <description>이 [arXiv]에 게시한 &#39;DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-DyPE_Dynamic_Position_Extrapolation_for_Ultra_High_Resolution_Diffusion/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-DyPE_Dynamic_Position_Extrapolation_for_Ultra_High_Resolution_Diffusion/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Models</category><category>Transformer Architecture</category><category>Positional Encoding</category><category>High-Resolution Image Generation</category><category>Extrapolation</category><category>Dynamic Adaptation</category><category>Training-Free</category>
    </item>
    <item>
      <title>[논문리뷰] Diff-XYZ: A Benchmark for Evaluating Diff Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;Diff-XYZ: A Benchmark for Evaluating Diff Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Diff-XYZ_A_Benchmark_for_Evaluating_Diff_Understanding/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Diff-XYZ_A_Benchmark_for_Evaluating_Diff_Understanding/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diff Understanding</category><category>Code Diff</category><category>Benchmark</category><category>LLMs</category><category>Code Editing</category><category>Software Engineering</category><category>Unified Diff Format</category><category>Search-Replace</category>
    </item>
    <item>
      <title>[논문리뷰] Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</title>
      <description>이 [arXiv]에 게시한 &#39;Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-Conan_Progressive_Learning_to_Reason_Like_a_Detective_over_Multi-Scale_Visual_Evidence/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-Conan_Progressive_Learning_to_Reason_Like_a_Detective_over_Multi-Scale_Visual_Evidence/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Reasoning</category><category>Multimodal Large Language Models (MLLMs)</category><category>Reinforcement Learning (RLVR)</category><category>Evidence Grounding</category><category>Multi-step Reasoning</category><category>Frame Retrieval</category><category>Dataset Construction</category><category>Progressive Learning</category>
    </item>
    <item>
      <title>[논문리뷰] ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature</title>
      <description>이 [arXiv]에 게시한 &#39;ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-ComProScanner_A_multi-agent_based_framework_for_composition-property_structured_data_extraction_from_scientific_literature/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-ComProScanner_A_multi-agent_based_framework_for_composition-property_structured_data_extraction_from_scientific_literature/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-agent Systems</category><category>Large Language Models (LLMs)</category><category>Information Extraction</category><category>Scientific Literature</category><category>Materials Science</category><category>Data Curation</category><category>Piezoelectric Materials</category><category>RAG (Retrieval-Augmented Generation)</category>
    </item>
    <item>
      <title>[논문리뷰] ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</title>
      <description>이 [arXiv]에 게시한 &#39;ARGenSeg: Image Segmentation with Autoregressive Image Generation Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-ARGenSeg_Image_Segmentation_with_Autoregressive_Image_Generation_Model/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-ARGenSeg_Image_Segmentation_with_Autoregressive_Image_Generation_Model/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Segmentation</category><category>Autoregressive Generation</category><category>Multimodal Large Language Models (MLLMs)</category><category>Visual Understanding</category><category>VQ-VAE</category><category>Multi-scale Prediction</category><category>Referring Expression Segmentation</category><category>Image Generation</category>
    </item>
    <item>
      <title>[논문리뷰] AlphaFlow: Understanding and Improving MeanFlow Models</title>
      <description>이 [arXiv]에 게시한 &#39;AlphaFlow: Understanding and Improving MeanFlow Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-AlphaFlow_Understanding_and_Improving_MeanFlow_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-AlphaFlow_Understanding_and_Improving_MeanFlow_Models/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative Models</category><category>Flow Matching</category><category>Consistency Models</category><category>MeanFlow</category><category>Curriculum Learning</category><category>Few-Step Generation</category><category>Image Generation</category>
    </item>
    <item>
      <title>[논문리뷰] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</title>
      <description>이 [arXiv]에 게시한 &#39;AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-24-AdaSPEC_Selective_Knowledge_Distillation_for_Efficient_Speculative_Decoders/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-24-AdaSPEC_Selective_Knowledge_Distillation_for_Efficient_Speculative_Decoders/</guid>
      <pubDate>Fri, 24 Oct 2025 04:04:16 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Speculative Decoding</category><category>Knowledge Distillation</category><category>LLM Inference</category><category>Model Acceleration</category><category>Token Filtering</category><category>Draft Model</category><category>Acceptance Rate</category>
    </item>
    <item>
      <title>[논문리뷰] VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos</title>
      <description>Xinyuan Wang이 [arXiv]에 게시한 &#39;VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-VideoAgentTrek_Computer_Use_Pretraining_from_Unlabeled_Videos/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-VideoAgentTrek_Computer_Use_Pretraining_from_Unlabeled_Videos/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>GUI Agents</category><category>Video Pretraining</category><category>Inverse Dynamics</category><category>Action Recognition</category><category>Computer Use Automation</category><category>Data Synthesis</category><category>Multimodal Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Unified Reinforcement and Imitation Learning for Vision-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;Unified Reinforcement and Imitation Learning for Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-Unified_Reinforcement_and_Imitation_Learning_for_Vision-Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-Unified_Reinforcement_and_Imitation_Learning_for_Vision-Language_Models/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Reinforcement Learning</category><category>Imitation Learning</category><category>Model Distillation</category><category>Lightweight VLMs</category><category>LLM-as-a-Judge</category><category>Multimodal Learning</category>
    </item>
    <item>
      <title>[논문리뷰] RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling</title>
      <description>Mandip Goswami이 [arXiv]에 게시한 &#39;RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-RIR-Mega_a_large-scale_simulated_room_impulse_response_dataset_for_machine_learning_and_room_acoustics_modeling/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-RIR-Mega_a_large-scale_simulated_room_impulse_response_dataset_for_machine_learning_and_room_acoustics_modeling/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Room Impulse Response</category><category>Dataset</category><category>Room Acoustics</category><category>Machine Learning</category><category>Dereverberation</category><category>Speech Recognition</category><category>Simulation</category><category>Hugging Face</category>
    </item>
    <item>
      <title>[논문리뷰] ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge</title>
      <description>이 [arXiv]에 게시한 &#39;ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-ProfBench_Multi-Domain_Rubrics_requiring_Professional_Knowledge_to_Answer_and_Judge/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-ProfBench_Multi-Domain_Rubrics_requiring_Professional_Knowledge_to_Answer_and_Judge/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Evaluation</category><category>Rubric-based Benchmark</category><category>Professional Knowledge</category><category>Multi-domain Tasks</category><category>LLM-Judge Bias Mitigation</category><category>Cost Reduction</category><category>Reasoning Assessment</category><category>Open-weight Models</category>
    </item>
    <item>
      <title>[논문리뷰] Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing</title>
      <description>이 [arXiv]에 게시한 &#39;Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-Pico-Banana-400K_A_Large-Scale_Dataset_for_Text-Guided_Image_Editing/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-Pico-Banana-400K_A_Large-Scale_Dataset_for_Text-Guided_Image_Editing/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-Guided Image Editing</category><category>Large-Scale Dataset</category><category>Multimodal Models</category><category>Dataset Curation</category><category>Quality Control</category><category>Prompt Engineering</category><category>Preference Learning</category><category>Multi-Turn Editing</category>
    </item>
    <item>
      <title>[논문리뷰] OmniNWM: Omniscient Driving Navigation World Models</title>
      <description>Zhujin Liang이 [arXiv]에 게시한 &#39;OmniNWM: Omniscient Driving Navigation World Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-OmniNWM_Omniscient_Driving_Navigation_World_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-OmniNWM_Omniscient_Driving_Navigation_World_Models/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Autonomous Driving</category><category>World Models</category><category>Multi-modal Generation</category><category>3D Occupancy</category><category>Plücker Ray-maps</category><category>Action Control</category><category>Dense Rewards</category><category>Long-term Forecasting</category>
    </item>
    <item>
      <title>[논문리뷰] olmOCR 2: Unit Test Rewards for Document OCR</title>
      <description>이 [arXiv]에 게시한 &#39;olmOCR 2: Unit Test Rewards for Document OCR&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-olmOCR_2_Unit_Test_Rewards_for_Document_OCR/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-olmOCR_2_Unit_Test_Rewards_for_Document_OCR/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Document OCR</category><category>Vision Language Model</category><category>Reinforcement Learning</category><category>Unit Tests</category><category>Synthetic Data Generation</category><category>RLVR</category><category>Document Parsing</category><category>State-of-the-Art OCR</category>
    </item>
    <item>
      <title>[논문리뷰] MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models</title>
      <description>Yifan Gao이 [arXiv]에 게시한 &#39;MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-23-MINED_Probing_and_Updating_with_Multimodal_Time-Sensitive_Knowledge_for_Large_Multimodal_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-23-MINED_Probing_and_Updating_with_Multimodal_Time-Sensitive_Knowledge_for_Large_Multimodal_Models/</guid>
      <pubDate>Thu, 23 Oct 2025 04:08:59 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Multimodal Models (LMMs)</category><category>Time-Sensitive Knowledge</category><category>Temporal Reasoning</category><category>Knowledge Editing</category><category>Multimodal Benchmarking</category><category>Temporal Awareness</category><category>Dynamic Knowledge</category>
    </item>
    
  </channel>
</rss>