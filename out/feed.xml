<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Wed, 26 Nov 2025 09:26:45 GMT</lastBuildDate>
    <pubDate>Wed, 26 Nov 2025 09:26:45 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios</title>
      <description>이 [arXiv]에 게시한 &#39;UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image Generation</category><category>Diffusion Transformers</category><category>4K Resolution</category><category>Aspect Ratio Extrapolation</category><category>Data-Model Co-Design</category><category>VAE Post-training</category><category>Positional Encoding</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</title>
      <description>Zhaowei Lu이 [arXiv]에 게시한 &#39;Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Mapless Navigation</category><category>Semantic Path Planning</category><category>Robot Learning</category><category>Video Prediction</category><category>Benchmark</category><category>Trajectory Generation</category>
    </item>
    <item>
      <title>[논문리뷰] SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis</title>
      <description>Hongwen Zhang이 [arXiv]에 게시한 &#39;SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Hand-Object Interaction</category><category>Multi-view Video Generation</category><category>4D Motion Synthesis</category><category>Diffusion Models</category><category>Spatio-temporal Consistency</category><category>Geometric Consistency</category><category>Appearance and Motion Joint Modeling</category>
    </item>
    <item>
      <title>[논문리뷰] Plan-X: Instruct Video Generation via Semantic Planning</title>
      <description>Chenxu Zhang이 [arXiv]에 게시한 &#39;Plan-X: Instruct Video Generation via Semantic Planning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Semantic Planning</category><category>Multimodal LLM</category><category>Diffusion Transformer</category><category>Spatio-temporal Guidance</category><category>Visual Hallucination</category><category>Prompt Alignment</category><category>Instruction Following</category>
    </item>
    <item>
      <title>[논문리뷰] Pillar-0: A New Frontier for Radiology Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;Pillar-0: A New Frontier for Radiology Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Radiology Foundation Model</category><category>Volumetric Imaging</category><category>Multi-window Tokenization</category><category>Multi-scale Attention</category><category>Contrastive Learning</category><category>Clinical Evaluation</category><category>Data Efficiency</category><category>Medical Imaging</category>
    </item>
    <item>
      <title>[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking</title>
      <description>Elias Stengel-Eskin이 [arXiv]에 게시한 &#39;PRInTS: Reward Modeling for Long-Horizon Information Seeking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reward Modeling</category><category>Long-Horizon Tasks</category><category>Information Seeking</category><category>Large Language Models</category><category>Trajectory Summarization</category><category>Reinforcement Learning</category><category>Tool Use</category><category>Process Reward Models</category>
    </item>
    <item>
      <title>[논문리뷰] Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO</title>
      <description>이 [arXiv]에 게시한 &#39;Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-Agent Systems</category><category>Reinforcement Learning</category><category>LLM Training</category><category>Hierarchical Credit Assignment</category><category>Trajectory Alignment</category><category>Group Relative Policy Optimization</category><category>Tool-Augmented Reasoning</category><category>Vertical Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] MIST: Mutual Information Via Supervised Training</title>
      <description>Kyunghyun Cho이 [arXiv]에 게시한 &#39;MIST: Mutual Information Via Supervised Training&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Mutual Information Estimation</category><category>Supervised Learning</category><category>Meta-Learning</category><category>Neural Networks</category><category>Uncertainty Quantification</category><category>SetTransformer</category><category>Quantile Regression</category>
    </item>
    <item>
      <title>[논문리뷰] MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Physics Reasoning</category><category>Motion Tracking</category><category>Spatial-Temporal Grounding</category><category>Video QA</category><category>AIGC Analysis</category><category>Reinforcement Learning</category>
    </item>
    <item>
      <title>[논문리뷰] M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark</title>
      <description>Bangwei Guo이 [arXiv]에 게시한 &#39;M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLM</category><category>Tool Use</category><category>Agent Benchmark</category><category>Model Context Protocol</category><category>Multi-Hop Reasoning</category><category>Multi-Threaded Execution</category><category>Evaluation Metrics</category><category>Similarity Alignment</category>
    </item>
    <item>
      <title>[논문리뷰] In-Video Instructions: Visual Signals as Generative Control</title>
      <description>이 [arXiv]에 게시한 &#39;In-Video Instructions: Visual Signals as Generative Control&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Controllable AI</category><category>Visual Instructions</category><category>Image-to-Video</category><category>Spatial Control</category><category>Zero-shot Learning</category><category>Generative Models</category>
    </item>
    <item>
      <title>[논문리뷰] HunyuanVideo 1.5 Technical Report</title>
      <description>Fang Yang이 [arXiv]에 게시한 &#39;HunyuanVideo 1.5 Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Diffusion Transformer</category><category>Sparse Attention</category><category>Super-Resolution</category><category>Open-Source</category><category>Multimodal Understanding</category><category>Training Optimization</category><category>Efficient Inference</category>
    </item>
    <item>
      <title>[논문리뷰] General Agentic Memory Via Deep Research</title>
      <description>이 [arXiv]에 게시한 &#39;General Agentic Memory Via Deep Research&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Agents</category><category>Memory Systems</category><category>Large Language Models (LLMs)</category><category>Just-in-Time (JIT) Compilation</category><category>Memorizer</category><category>Researcher</category><category>Reinforcement Learning</category><category>Context Management</category>
    </item>
    <item>
      <title>[논문리뷰] Flow Map Distillation Without Data</title>
      <description>Tommi Jaakkola이 [arXiv]에 게시한 &#39;Flow Map Distillation Without Data&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Flow Map Distillation</category><category>Data-Free Learning</category><category>Generative Models</category><category>Teacher-Student</category><category>Diffusion Acceleration</category><category>Teacher-Data Mismatch</category><category>One-Step Sampling</category>
    </item>
    <item>
      <title>[논문리뷰] Fidelity-Aware Recommendation Explanations via Stochastic Path Integration</title>
      <description>Oren Barkan이 [arXiv]에 게시한 &#39;Fidelity-Aware Recommendation Explanations via Stochastic Path Integration&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Recommender Systems</category><category>Explainable AI (XAI)</category><category>Explanation Fidelity</category><category>Path Integration</category><category>Stochastic Sampling</category><category>Counterfactual Explanations</category><category>Model-Agnostic</category><category>Sparse Data</category>
    </item>
    <item>
      <title>[논문리뷰] Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems</title>
      <description>Oren Barkan이 [arXiv]에 게시한 &#39;Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Recommender Systems</category><category>Sparse Autoencoder (SAE)</category><category>Monosemantic Neurons</category><category>Interpretability</category><category>Prediction-Aware Loss</category><category>User-Item Interactions</category><category>Post-hoc Control</category>
    </item>
    <item>
      <title>[논문리뷰] DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Pixel Diffusion</category><category>Image Generation</category><category>Frequency Decoupling</category><category>Diffusion Transformer (DiT)</category><category>Flow Matching</category><category>AdaLN</category><category>Text-to-Image Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research</title>
      <description>이 [arXiv]에 게시한 &#39;DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Evolving Rubrics</category><category>Deep Research</category><category>LLM Agents</category><category>Tool Use</category><category>Long-form QA</category><category>Open-source AI</category><category>Dynamic Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] Controllable Layer Decomposition for Reversible Multi-Layer Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;Controllable Layer Decomposition for Reversible Multi-Layer Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Controllable Layer Decomposition</category><category>Diffusion Models</category><category>Multi-Layer Image Generation</category><category>Layer Separation</category><category>Bounding Box Guidance</category><category>Generative AI</category><category>Image Editing</category>
    </item>
    <item>
      <title>[논문리뷰] Computer-Use Agents as Judges for Generative User Interface</title>
      <description>이 [arXiv]에 게시한 &#39;Computer-Use Agents as Judges for Generative User Interface&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Computer-Use Agents</category><category>Generative UI</category><category>AI-assisted Design</category><category>Human-Computer Interaction</category><category>LLM</category><category>AUI-Gym</category><category>Feedback Loop</category><category>Agent-centric Design</category>
    </item>
    <item>
      <title>[논문리뷰] Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</title>
      <description>Stephanie Fu이 [arXiv]에 게시한 &#39;Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models (VLMs)</category><category>Chain-of-Thought (CoT)</category><category>Continuous Visual Tokens</category><category>Multimodal Reasoning</category><category>Perceptual Grounding</category><category>Visual Thinking</category><category>Dense Prediction</category>
    </item>
    <item>
      <title>[논문리뷰] Budget-Aware Tool-Use Enables Effective Agent Scaling</title>
      <description>이 [arXiv]에 게시한 &#39;Budget-Aware Tool-Use Enables Effective Agent Scaling&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Tool Use</category><category>Budget Awareness</category><category>Test-time Scaling</category><category>Cost-Performance</category><category>Web Search Agents</category><category>Planning</category><category>Self-Verification</category>
    </item>
    <item>
      <title>[논문리뷰] AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning</title>
      <description>Alphamasterliu이 [arXiv]에 게시한 &#39;AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Automated Environment Generation</category><category>Cross-Environment Learning</category><category>Agent Learning</category><category>Language Models</category><category>Benchmark</category><category>Meta-Learning</category><category>Reinforcement Learning</category><category>Environment Design Language</category>
    </item>
    <item>
      <title>[논문리뷰] AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser</title>
      <description>이 [arXiv]에 게시한 &#39;AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>HTML Extraction</category><category>Web Corpus</category><category>Large Language Models</category><category>Data Curation</category><category>Structured Element Preservation</category><category>Sequence Labeling</category><category>Markdown Conversion</category><category>MainWebBench</category>
    </item>
    <item>
      <title>[논문리뷰] WorldGen: From Text to Traversable and Interactive 3D Worlds</title>
      <description>이 [arXiv]에 게시한 &#39;WorldGen: From Text to Traversable and Interactive 3D Worlds&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D World Generation</category><category>Text-to-3D</category><category>Generative AI</category><category>Procedural Generation</category><category>Scene Decomposition</category><category>Navmesh</category><category>Game Engines</category><category>Interactive Environments</category>
    </item>
    <item>
      <title>[논문리뷰] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</title>
      <description>Yudong Zhang이 [arXiv]에 게시한 &#39;VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Latent Memory</category><category>Cognitive Memory</category><category>Visual Grounding</category><category>Short-term Memory</category><category>Long-term Memory</category><category>Reinforcement Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</title>
      <description>Jing Bi이 [arXiv]에 게시한 &#39;Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Reasoning</category><category>Large Multimodal Models</category><category>Reinforcement Learning</category><category>Visual Rumination</category><category>Text-Rich Video</category><category>Video Question Answering</category><category>Iterative Perception</category>
    </item>
    <item>
      <title>[논문리뷰] VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation</title>
      <description>Gim Hee Lee이 [arXiv]에 게시한 &#39;VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Models</category><category>Robotic Manipulation</category><category>SpatioTemporal Coherence</category><category>4D Awareness</category><category>Visual Representation</category><category>Action Representation</category><category>Cross-Attention</category>
    </item>
    <item>
      <title>[논문리뷰] Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story</title>
      <description>Kristian Kuznetsov이 [arXiv]에 게시한 &#39;Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Intrinsic Dimension</category><category>LLMs</category><category>Text Complexity</category><category>Sparse Autoencoders</category><category>Text Semantics</category><category>Genre Analysis</category><category>Embedding Space</category><category>Text Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Taming Generative Synthetic Data for X-ray Prohibited Item Detection</title>
      <description>Renshuai Tao이 [arXiv]에 게시한 &#39;Taming Generative Synthetic Data for X-ray Prohibited Item Detection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection/</guid>
      <pubDate>Sun, 23 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>X-ray Security</category><category>Synthetic Data Generation</category><category>Diffusion Models</category><category>Object Detection</category><category>Cross-Attention</category><category>Image Inpainting</category><category>Data Augmentation</category>
    </item>
    
  </channel>
</rss>