<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Sat, 18 Oct 2025 12:52:52 GMT</lastBuildDate>
    <pubDate>Sat, 18 Oct 2025 12:52:52 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] WithAnyone: Towards Controllable and ID Consistent Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;WithAnyone: Towards Controllable and ID Consistent Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-WithAnyone_Towards_Controllable_and_ID_Consistent_Image_Generation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-WithAnyone_Towards_Controllable_and_ID_Consistent_Image_Generation/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Identity-Consistent Generation</category><category>Text-to-Image Diffusion</category><category>Copy-Paste Artifacts</category><category>Contrastive Learning</category><category>Multi-Identity Dataset</category><category>Controllable Generation</category><category>ID-Preservation</category>
    </item>
    <item>
      <title>[논문리뷰] When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA</title>
      <description>Artem Vazhentsev이 [arXiv]에 게시한 &#39;When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-When_Models_Lie_We_Learn_Multilingual_Span-Level_Hallucination_Detection_with_PsiloQA/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-When_Models_Lie_We_Learn_Multilingual_Span-Level_Hallucination_Detection_with_PsiloQA/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Hallucination Detection</category><category>Multilingual LLMs</category><category>Span-Level Annotation</category><category>Synthetic Data Generation</category><category>Question Answering (QA)</category><category>Encoder Models</category><category>Uncertainty Quantification</category><category>GPT-4o</category>
    </item>
    <item>
      <title>[논문리뷰] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-VR-Thinker_Boosting_Video_Reward_Models_through_Thinking-with-Image_Reasoning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-VR-Thinker_Boosting_Video_Reward_Models_through_Thinking-with-Image_Reasoning/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Reward Models</category><category>Multimodal Reasoning</category><category>Thinking-with-Image</category><category>Visual Reasoning</category><category>Reinforcement Learning</category><category>Chain-of-Thought</category><category>Context Management</category>
    </item>
    <item>
      <title>[논문리뷰] VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation</title>
      <description>이 [arXiv]에 게시한 &#39;VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-VLA2_Empowering_Vision-Language-Action_Models_with_an_Agentic_Framework_for_Unseen_Concept_Manipulation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-VLA2_Empowering_Vision-Language-Action_Models_with_an_Agentic_Framework_for_Unseen_Concept_Manipulation/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Models</category><category>Agentic Framework</category><category>Unseen Concept Manipulation</category><category>Out-of-Distribution Generalization</category><category>Tool Use</category><category>Web Retrieval</category><category>Object Detection</category><category>LIBERO Simulation</category>
    </item>
    <item>
      <title>[논문리뷰] VLA-0: Building State-of-the-Art VLAs with Zero Modification</title>
      <description>이 [arXiv]에 게시한 &#39;VLA-0: Building State-of-the-Art VLAs with Zero Modification&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-VLA-0_Building_State-of-the-Art_VLAs_with_Zero_Modification/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-VLA-0_Building_State-of-the-Art_VLAs_with_Zero_Modification/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Models</category><category>VLA-0</category><category>Zero Modification</category><category>Text-based Action Prediction</category><category>Robot Manipulation</category><category>Large Language Models</category><category>Fine-tuning</category><category>State-of-the-Art</category>
    </item>
    <item>
      <title>[논문리뷰] VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator</title>
      <description>Federico Tombari이 [arXiv]에 게시한 &#39;VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-VIST3A_Text-to-3D_by_Stitching_a_Multi-view_Reconstruction_Network_to_a_Video_Generator/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-VIST3A_Text-to-3D_by_Stitching_a_Multi-view_Reconstruction_Network_to_a_Video_Generator/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-3D</category><category>Model Stitching</category><category>Multi-view Reconstruction</category><category>Video Generation</category><category>Latent Diffusion Models</category><category>Gaussian Splats</category><category>Pointmaps</category><category>Reward Finetuning</category>
    </item>
    <item>
      <title>[논문리뷰] TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar</title>
      <description>이 [arXiv]에 게시한 &#39;TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-TokDrift_When_LLM_Speaks_in_Subwords_but_Code_Speaks_in_Grammar/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-TokDrift_When_LLM_Speaks_in_Subwords_but_Code_Speaks_in_Grammar/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Code LLMs</category><category>Subword Tokenization</category><category>Grammar-aware Tokenization</category><category>Semantic Preservation</category><category>Rewrite Rules</category><category>Model Robustness</category><category>Tokenization Misalignment</category>
    </item>
    <item>
      <title>[논문리뷰] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-The_German_Commons_-_154_Billion_Tokens_of_Openly_Licensed_Text_for_German_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-The_German_Commons_-_154_Billion_Tokens_of_Openly_Licensed_Text_for_German_Language_Models/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>German Commons</category><category>Large Language Models</category><category>Training Data</category><category>Openly Licensed Text</category><category>Data Curation</category><category>German NLP</category><category>Corpus Construction</category><category>Quality Filtering</category>
    </item>
    <item>
      <title>[논문리뷰] SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis</title>
      <description>이 [arXiv]에 게시한 &#39;SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-SCas4D_Structural_Cascaded_Optimization_for_Boosting_Persistent_4D_Novel_View_Synthesis/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-SCas4D_Structural_Cascaded_Optimization_for_Boosting_Persistent_4D_Novel_View_Synthesis/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>4D Novel View Synthesis</category><category>Dynamic Scenes</category><category>3D Gaussian Splatting</category><category>Cascaded Optimization</category><category>Deformation Modeling</category><category>Point Tracking</category><category>Object Segmentation</category>
    </item>
    <item>
      <title>[논문리뷰] RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-RefusalBench_Generative_Evaluation_of_Selective_Refusal_in_Grounded_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-RefusalBench_Generative_Evaluation_of_Selective_Refusal_in_Grounded_Language_Models/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RAG Systems</category><category>Selective Refusal</category><category>Generative Evaluation</category><category>Linguistic Perturbations</category><category>LLM Evaluation</category><category>Informational Uncertainty</category><category>Model Calibration</category><category>AI Safety</category>
    </item>
    <item>
      <title>[논문리뷰] RealDPO: Real or Not Real, that is the Preference</title>
      <description>Chenyang Si이 [arXiv]에 게시한 &#39;RealDPO: Real or Not Real, that is the Preference&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-RealDPO_Real_or_Not_Real_that_is_the_Preference/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-RealDPO_Real_or_Not_Real_that_is_the_Preference/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Diffusion Models</category><category>Direct Preference Optimization</category><category>Preference Learning</category><category>Real Data</category><category>Human Motion Synthesis</category><category>RealDPO</category><category>RealAction-5K</category>
    </item>
    <item>
      <title>[논문리뷰] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems</title>
      <description>이 [arXiv]에 게시한 &#39;RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-RAGCap-Bench_Benchmarking_Capabilities_of_LLMs_in_Agentic_Retrieval_Augmented_Generation_Systems/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-RAGCap-Bench_Benchmarking_Capabilities_of_LLMs_in_Agentic_Retrieval_Augmented_Generation_Systems/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Retrieval Augmented Generation</category><category>Agentic Systems</category><category>Benchmarking</category><category>Intermediate Tasks</category><category>Error Analysis</category><category>LLM Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] Qwen3Guard Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Qwen3Guard Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-Qwen3Guard_Technical_Report/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-Qwen3Guard_Technical_Report/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Safety</category><category>Guardrail Models</category><category>Multilingual AI</category><category>Real-time Moderation</category><category>Tri-class Classification</category><category>Instruction Tuning</category><category>Streaming Inference</category>
    </item>
    <item>
      <title>[논문리뷰] Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation</title>
      <description>이 [arXiv]에 게시한 &#39;Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-Ponimator_Unfolding_Interactive_Pose_for_Versatile_Human-human_Interaction_Animation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-Ponimator_Unfolding_Interactive_Pose_for_Versatile_Human-human_Interaction_Animation/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Human-human Interaction</category><category>Pose Animation</category><category>Diffusion Models</category><category>Generative AI</category><category>Motion Synthesis</category><category>Interactive Poses</category><category>Temporal Priors</category><category>Spatial Priors</category>
    </item>
    <item>
      <title>[논문리뷰] pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</title>
      <description>이 [arXiv]에 게시한 &#39;pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-pi-Flow_Policy-Based_Few-Step_Generation_via_Imitation_Distillation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-pi-Flow_Policy-Based_Few-Step_Generation_via_Imitation_Distillation/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Models</category><category>Flow Matching</category><category>Generative Models</category><category>Model Distillation</category><category>Imitation Learning</category><category>Few-Step Generation</category><category>Policy-Based AI</category><category>Text-to-Image</category>
    </item>
    <item>
      <title>[논문리뷰] PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model</title>
      <description>이 [arXiv]에 게시한 &#39;PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-PaddleOCR-VL_Boosting_Multilingual_Document_Parsing_via_a_0.9B_Ultra-Compact_Vision-Language_Model/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-PaddleOCR-VL_Boosting_Multilingual_Document_Parsing_via_a_0.9B_Ultra-Compact_Vision-Language_Model/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Document Parsing</category><category>Vision-Language Model</category><category>Multilingual OCR</category><category>Layout Analysis</category><category>Resource-Efficient AI</category><category>Table Recognition</category><category>Formula Recognition</category><category>Chart Recognition</category>
    </item>
    <item>
      <title>[논문리뷰] On Pretraining for Project-Level Code Completion</title>
      <description>이 [arXiv]에 게시한 &#39;On Pretraining for Project-Level Code Completion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-On_Pretraining_for_Project-Level_Code_Completion/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-On_Pretraining_for_Project-Level_Code_Completion/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Code LLMs</category><category>Project-level Context</category><category>Code Completion</category><category>Context Window Extension</category><category>RoPE Scaling</category><category>Repository Pretraining</category><category>Long Code Arena</category>
    </item>
    <item>
      <title>[논문리뷰] MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems</title>
      <description>Feiyu Xiong이 [arXiv]에 게시한 &#39;MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-MoM_Mixtures_of_Scenario-Aware_Document_Memories_for_Retrieval-Augmented_Generation_Systems/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-MoM_Mixtures_of_Scenario-Aware_Document_Memories_for_Retrieval-Augmented_Generation_Systems/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Retrieval-Augmented Generation (RAG)</category><category>Document Memory</category><category>Text Chunking</category><category>Small Language Models (SLMs)</category><category>Large Language Models (LLMs)</category><category>Scenario-Aware Processing</category><category>Multi-Layer Retrieval</category><category>Cognitive Simulation</category>
    </item>
    <item>
      <title>[논문리뷰] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</title>
      <description>Ke Wang이 [arXiv]에 게시한 &#39;MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-MathCanvas_Intrinsic_Visual_Chain-of-Thought_for_Multimodal_Mathematical_Reasoning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-MathCanvas_Intrinsic_Visual_Chain-of-Thought_for_Multimodal_Mathematical_Reasoning/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Reasoning</category><category>Visual Chain-of-Thought (VCoT)</category><category>Large Multimodal Models (LMMs)</category><category>Geometric Reasoning</category><category>Diagram Generation</category><category>Dataset</category><category>Benchmark</category>
    </item>
    <item>
      <title>[논문리뷰] LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training</title>
      <description>이 [arXiv]에 게시한 &#39;LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-LLMs_as_Scalable_General-Purpose_Simulators_For_Evolving_Digital_Agent_Training/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-LLMs_as_Scalable_General-Purpose_Simulators_For_Evolving_Digital_Agent_Training/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM</category><category>Digital Agents</category><category>UI Simulation</category><category>Synthetic Data Generation</category><category>Targeted Data Synthesis</category><category>World Models</category>
    </item>
    <item>
      <title>[논문리뷰] LLM-guided Hierarchical Retrieval</title>
      <description>이 [arXiv]에 게시한 &#39;LLM-guided Hierarchical Retrieval&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-LLM-guided_Hierarchical_Retrieval/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-LLM-guided_Hierarchical_Retrieval/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Information Retrieval</category><category>Large Language Models</category><category>Hierarchical Retrieval</category><category>Semantic Tree</category><category>Tree Traversal</category><category>Zero-shot Performance</category><category>Reasoning-based Retrieval</category><category>Computational Efficiency</category>
    </item>
    <item>
      <title>[논문리뷰] LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-LiteStage_Latency-aware_Layer_Skipping_for_Multi-stage_Reasoning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-LiteStage_Latency-aware_Layer_Skipping_for_Multi-stage_Reasoning/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Layer Skipping</category><category>Multi-stage Reasoning</category><category>Latency Optimization</category><category>Early Exit</category><category>Small Language Models (LLMs)</category><category>Adaptive Computation</category><category>Confidence-based Decoding</category>
    </item>
    <item>
      <title>[논문리뷰] Learning an Image Editing Model without Image Editing Pairs</title>
      <description>이 [arXiv]에 게시한 &#39;Learning an Image Editing Model without Image Editing Pairs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-Learning_an_Image_Editing_Model_without_Image_Editing_Pairs/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-Learning_an_Image_Editing_Model_without_Image_Editing_Pairs/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Editing</category><category>Diffusion Models</category><category>Vision-Language Models (VLMs)</category><category>No-Pair Training</category><category>Few-step Generation</category><category>Distribution Matching</category><category>Gradient-based Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</title>
      <description>이 [arXiv]에 게시한 &#39;LaSeR: Reinforcement Learning with Last-Token Self-Rewarding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-LaSeR_Reinforcement_Learning_with_Last-Token_Self-Rewarding/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-LaSeR_Reinforcement_Learning_with_Last-Token_Self-Rewarding/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>LLM</category><category>Self-Verification</category><category>Last-Token</category><category>Reward Modeling</category><category>Efficiency</category><category>Reasoning</category><category>RLVR</category>
    </item>
    <item>
      <title>[논문리뷰] Large Language Models Do NOT Really Know What They Don&#39;t Know</title>
      <description>이 [arXiv]에 게시한 &#39;Large Language Models Do NOT Really Know What They Don&#39;t Know&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-Large_Language_Models_Do_NOT_Really_Know_What_They_Dont_Know/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-Large_Language_Models_Do_NOT_Really_Know_What_They_Dont_Know/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLMs</category><category>Hallucination Detection</category><category>Mechanistic Interpretability</category><category>Internal States</category><category>Knowledge Recall</category><category>Refusal Tuning</category><category>Factual Associations</category><category>Associated Hallucinations</category>
    </item>
    <item>
      <title>[논문리뷰] Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents</title>
      <description>이 [arXiv]에 게시한 &#39;Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-Information_Gain-based_Policy_Optimization_A_Simple_and_Effective_Approach_for_Multi-Turn_LLM_Agents/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-Information_Gain-based_Policy_Optimization_A_Simple_and_Effective_Approach_for_Multi-Turn_LLM_Agents/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Reinforcement Learning</category><category>Multi-Turn Interactions</category><category>Reward Sparsity</category><category>Information Gain</category><category>Policy Optimization</category><category>Ground-Truth Awareness</category><category>Sample Efficiency</category>
    </item>
    <item>
      <title>[논문리뷰] ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints</title>
      <description>이 [arXiv]에 게시한 &#39;ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-ImagerySearch_Adaptive_Test-Time_Search_for_Video_Generation_Beyond_Semantic_Dependency_Constraints/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-ImagerySearch_Adaptive_Test-Time_Search_for_Video_Generation_Beyond_Semantic_Dependency_Constraints/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Test-Time Search</category><category>Diffusion Models</category><category>Semantic Dependency</category><category>Adaptive Reward</category><category>Evaluation Benchmark</category><category>Prompt-Guided</category>
    </item>
    <item>
      <title>[논문리뷰] From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</title>
      <description>이 [arXiv]에 게시한 &#39;From Pixels to Words -- Towards Native Vision-Language Primitives at Scale&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-From_Pixels_to_Words_--_Towards_Native_Vision-Language_Primitives_at_Scale/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-From_Pixels_to_Words_--_Towards_Native_Vision-Language_Primitives_at_Scale/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Native VLMs</category><category>Early Fusion</category><category>Multimodal Learning</category><category>Transformer Architecture</category><category>Rotary Position Embeddings</category><category>Pixel-Word Alignment</category><category>End-to-End Training</category>
    </item>
    <item>
      <title>[논문리뷰] Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report</title>
      <description>이 [arXiv]에 게시한 &#39;Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-Fantastic_small_Retrievers_and_How_to_Train_Them_mxbai-edge-colbert-v0_Tech_Report/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-Fantastic_small_Retrievers_and_How_to_Train_Them_mxbai-edge-colbert-v0_Tech_Report/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>ColBERT</category><category>Retrieval Models</category><category>Small Models</category><category>Distillation</category><category>Long Context</category><category>Edge AI</category><category>Information Retrieval</category><category>RAG</category>
    </item>
    <item>
      <title>[논문리뷰] Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning</title>
      <description>Sijia Gu이 [arXiv]에 게시한 &#39;Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-17-Expertise_need_not_monopolize_Action-Specialized_Mixture_of_Experts_for_Vision-Language-Action_Learning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-17-Expertise_need_not_monopolize_Action-Specialized_Mixture_of_Experts_for_Vision-Language-Action_Learning/</guid>
      <pubDate>Fri, 17 Oct 2025 04:09:57 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action (VLA)</category><category>Mixture of Experts (MoE)</category><category>Robotic Manipulation</category><category>Expert Specialization</category><category>Decoupled Routing</category><category>Load Balancing</category><category>Transfer Learning</category>
    </item>
    
  </channel>
</rss>