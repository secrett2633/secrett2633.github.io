<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Wed, 03 Dec 2025 18:44:49 GMT</lastBuildDate>
    <pubDate>Wed, 03 Dec 2025 18:44:49 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</title>
      <description>Wendong Bu이 [arXiv]에 게시한 &#39;WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Editing</category><category>Benchmarking</category><category>Cognitive AI</category><category>Creativity</category><category>Multimodal AI</category><category>Knowledge-based Reasoning</category><category>Diffusion Models</category><category>MLLMs</category>
    </item>
    <item>
      <title>[논문리뷰] Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</title>
      <description>Mikhail Burtsev이 [arXiv]에 게시한 &#39;Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Knowledge Graphs</category><category>Large Language Models</category><category>Information Extraction</category><category>Wikidata Ontology</category><category>Question Answering</category><category>Entity Normalization</category><category>Retrieval Augmented Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</title>
      <description>Wenhua Wu이 [arXiv]에 게시한 &#39;Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image Generation</category><category>Cultural Consistency</category><category>Multilingual AI</category><category>Neuron Activation</category><category>Cultural Probing</category><category>Fine-Tuning</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] What about gravity in video generation? Post-Training Newton&#39;s Laws with Verifiable Rewards</title>
      <description>이 [arXiv]에 게시한 &#39;What about gravity in video generation? Post-Training Newton&#39;s Laws with Verifiable Rewards&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Diffusion Models</category><category>Newtonian Dynamics</category><category>Physics-aware AI</category><category>Post-Training</category><category>Verifiable Rewards</category><category>Optical Flow</category><category>Mass Estimation</category>
    </item>
    <item>
      <title>[논문리뷰] VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference</title>
      <description>이 [arXiv]에 게시한 &#39;VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Models</category><category>Asynchronous Inference</category><category>Real-Time Robotics</category><category>Low-Latency Control</category><category>Future State Awareness</category><category>Action Quantization</category><category>Temporal Alignment</category>
    </item>
    <item>
      <title>[논문리뷰] The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment</title>
      <description>이 [arXiv]에 게시한 &#39;The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Generation</category><category>Image Editing</category><category>Diffusion Models</category><category>Consistency Correction</category><category>Attention Mechanism</category><category>Reference-Guided</category><category>Agent Framework</category><category>Data Curation</category>
    </item>
    <item>
      <title>[논문리뷰] The Art of Scaling Test-Time Compute for Large Language Models</title>
      <description>Tanmoy Chakraborty이 [arXiv]에 게시한 &#39;The Art of Scaling Test-Time Compute for Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Test-Time Scaling</category><category>LLMs</category><category>Reasoning</category><category>Compute Efficiency</category><category>Inference Optimization</category><category>Decoding Strategies</category><category>Model Behavior</category>
    </item>
    <item>
      <title>[논문리뷰] TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</title>
      <description>이 [arXiv]에 게시한 &#39;TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Unified Multimodal Models</category><category>Visual Representation</category><category>VAE</category><category>Flow Matching</category><category>Multimodal Understanding</category><category>Multimodal Generation</category><category>Image Editing</category><category>State-of-the-Art</category>
    </item>
    <item>
      <title>[논문리뷰] Structured Extraction from Business Process Diagrams Using Vision-Language Models</title>
      <description>Barry Devereux이 [arXiv]에 게시한 &#39;Structured Extraction from Business Process Diagrams Using Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>BPMN Extraction</category><category>Structured Information Extraction</category><category>OCR Enrichment</category><category>Prompt Engineering</category><category>Diagram Understanding</category><category>Business Process Management</category>
    </item>
    <item>
      <title>[논문리뷰] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</title>
      <description>이 [arXiv]에 게시한 &#39;StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Streaming Video Understanding</category><category>Gaze-Guided AI</category><category>Temporal Reasoning</category><category>Proactive AI</category><category>MLLMs</category><category>Eye Tracking</category><category>Benchmark</category><category>Human-Computer Interaction</category>
    </item>
    <item>
      <title>[논문리뷰] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</title>
      <description>이 [arXiv]에 게시한 &#39;Stabilizing Reinforcement Learning with LLMs: Formulation and Practices&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning (RL)</category><category>Large Language Models (LLMs)</category><category>Policy Gradient</category><category>REINFORCE</category><category>Mixture-of-Experts (MoE)</category><category>Training Stability</category><category>Importance Sampling</category><category>Routing Replay</category><category>Off-policy Learning</category>
    </item>
    <item>
      <title>[논문리뷰] SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLMs</category><category>Long-context Reasoning</category><category>KV Cache Optimization</category><category>Speculative Sparsity</category><category>Knowledge Distillation</category><category>Adaptive Memory Management</category><category>Throughput</category>
    </item>
    <item>
      <title>[논문리뷰] Seeing the Wind from a Falling Leaf</title>
      <description>Emily Yue-Ting Jia이 [arXiv]에 게시한 &#39;Seeing the Wind from a Falling Leaf&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Inverse Graphics</category><category>Differentiable Physics</category><category>Force Estimation</category><category>Video Generation</category><category>Material Point Method</category><category>3D Gaussians</category><category>Spatio-temporal Modeling</category><category>Vision-Language Models</category>
    </item>
    <item>
      <title>[논문리뷰] Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models (MLLMs)</category><category>Token Pruning</category><category>Graph-Structured Pruning (GSP)</category><category>Query-Conditioned Semantic Pruning (QCSP)</category><category>Determinantal Point Processes (DPP)</category><category>Model Efficiency</category><category>Visual Redundancy</category>
    </item>
    <item>
      <title>[논문리뷰] SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling</title>
      <description>이 [arXiv]에 게시한 &#39;SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Reasoning</category><category>Test-time Scaling</category><category>Resource Allocation</category><category>Dual-process Theory</category><category>Mathematical Reasoning</category><category>Adaptive Computation</category><category>Performance Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] Rectifying LLM Thought from Lens of Optimization</title>
      <description>Kai Chen이 [arXiv]에 게시한 &#39;Rectifying LLM Thought from Lens of Optimization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Reasoning</category><category>Chain-of-Thought</category><category>RLVR</category><category>Optimization Framework</category><category>Process-level Reward</category><category>Gradient Descent</category><category>Reasoning Efficiency</category><category>Suboptimal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] PromptBridge: Cross-Model Prompt Transfer for Large Language Models</title>
      <description>Wei Wei이 [arXiv]에 게시한 &#39;PromptBridge: Cross-Model Prompt Transfer for Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Prompt Engineering</category><category>Model Drifting</category><category>Prompt Transfer</category><category>Cross-Model Adaptation</category><category>Training-Free</category><category>Prompt Optimization</category><category>MAP-RPE</category>
    </item>
    <item>
      <title>[논문리뷰] OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic</title>
      <description>이 [arXiv]에 게시한 &#39;OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Autonomous Driving</category><category>Reinforcement Fine-tuning</category><category>LLM-as-Critic</category><category>Vision-Language Model</category><category>End-to-End Learning</category><category>Chain-of-Thought</category><category>Trajectory Planning</category>
    </item>
    <item>
      <title>[논문리뷰] OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion</title>
      <description>이 [arXiv]에 게시한 &#39;OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Translation</category><category>Speech Translation</category><category>Simultaneous Translation</category><category>Large Language Models</category><category>Multimodal Foundation Models</category><category>Modular Fusion</category><category>End-to-End</category><category>Gated Fusion</category><category>OCR</category>
    </item>
    <item>
      <title>[논문리뷰] Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</title>
      <description>Ying-Cong Chen이 [arXiv]에 게시한 &#39;Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Geometric Dense Prediction</category><category>Depth Estimation</category><category>Surface Normal Prediction</category><category>Diffusion Models</category><category>Rectified Flow</category><category>Generative Priors</category><category>Deterministic Inference</category><category>Two-Stage Framework</category>
    </item>
    <item>
      <title>[논문리뷰] LongVT: Incentivizing &#39;Thinking with Long Videos&#39; via Native Tool Calling</title>
      <description>이 [arXiv]에 게시한 &#39;LongVT: Incentivizing &#39;Thinking with Long Videos&#39; via Native Tool Calling&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long Video Understanding</category><category>Multimodal LLMs</category><category>Tool Calling</category><category>Reinforcement Learning</category><category>Chain-of-Thought</category><category>Temporal Grounding</category><category>Video Question Answering</category>
    </item>
    <item>
      <title>[논문리뷰] Learning Eigenstructures of Unstructured Data Manifolds</title>
      <description>이 [arXiv]에 게시한 &#39;Learning Eigenstructures of Unstructured Data Manifolds&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spectral Basis Learning</category><category>Unstructured Data</category><category>Manifold Learning</category><category>Laplacian Operator</category><category>Optimal Approximation Theory</category><category>Neural Networks</category><category>Eigenstructure</category><category>Point Cloud Processing</category>
    </item>
    <item>
      <title>[논문리뷰] LFM2 Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;LFM2 Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-LFM2-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-LFM2-Technical-Report/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Edge AI</category><category>Foundation Models</category><category>Hybrid Architecture</category><category>Knowledge Distillation</category><category>Multimodal AI</category><category>On-device Deployment</category><category>Efficient Inference</category><category>LLM Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</title>
      <description>이 [arXiv]에 게시한 &#39;InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Foundation Models</category><category>Self-Supervised Learning</category><category>Masked Video Modeling</category><category>Video-Text Supervision-Free</category><category>Encoder-Predictor-Decoder</category><category>Diffusion Decoder</category><category>Semantic Alignment</category><category>Latent World Model</category>
    </item>
    <item>
      <title>[논문리뷰] Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</title>
      <description>Pinar Yanardag이 [arXiv]에 게시한 &#39;Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Autoregressive Video Generation</category><category>Rotary Positional Embedding</category><category>Infinite Video Generation</category><category>Action Control</category><category>Cinematic Transitions</category><category>Video Diffusion Models</category><category>KV Cache</category>
    </item>
    <item>
      <title>[논문리뷰] IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages</title>
      <description>이 [arXiv]에 게시한 &#39;IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Low-resource Languages</category><category>Indic Languages</category><category>LLM Evaluation</category><category>Benchmark</category><category>Multilingual LLMs</category><category>Question Answering</category><category>Cross-lingual Transfer</category>
    </item>
    <item>
      <title>[논문리뷰] How Far Are We from Genuinely Useful Deep Research Agents?</title>
      <description>Xinran Zhou이 [arXiv]에 게시한 &#39;How Far Are We from Genuinely Useful Deep Research Agents?&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Deep Research Agents</category><category>Evaluation Benchmark</category><category>Failure Taxonomy</category><category>Report Generation</category><category>Information Retrieval</category><category>Reasoning Resilience</category><category>Content Fabrication</category><category>AI Agents</category>
    </item>
    <item>
      <title>[논문리뷰] HiconAgent: History Context-aware Policy Optimization for GUI Agents</title>
      <description>Kaiwen Zhou이 [arXiv]에 게시한 &#39;HiconAgent: History Context-aware Policy Optimization for GUI Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>GUI Agents</category><category>Reinforcement Learning</category><category>Context-aware</category><category>History Compression</category><category>Policy Optimization</category><category>Multimodal LLM</category><category>Dynamic Sampling</category>
    </item>
    <item>
      <title>[논문리뷰] Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks</title>
      <description>이 [arXiv]에 게시한 &#39;Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Clinical AI</category><category>Medical Benchmarks</category><category>AI Evaluation</category><category>Medical Decision Support</category><category>MedQA</category><category>HealthBench</category><category>Generalist AI</category>
    </item>
    <item>
      <title>[논문리뷰] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</title>
      <description>이 [arXiv]에 게시한 &#39;GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation/</guid>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Robotic Manipulation</category><category>Reinforcement Learning</category><category>Vision-Language-Action</category><category>Dexterous Control</category><category>Long-Horizon Tasks</category><category>Data Filtering</category><category>Data Augmentation</category><category>Foundation Models</category>
    </item>
    
  </channel>
</rss>