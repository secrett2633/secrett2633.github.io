<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.cloud</link>
    <atom:link href="https://blog.secrett2633.cloud/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Thu, 22 Jan 2026 12:29:12 GMT</lastBuildDate>
    <pubDate>Thu, 22 Jan 2026 12:29:12 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation</title>
      <description>이 [arXiv]에 게시한 &#39;UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Chest X-Ray</category><category>Medical Foundation Model</category><category>Autoregressive Model</category><category>Diffusion Model</category><category>Multimodal Learning</category><category>Image Understanding</category><category>Image Generation</category><category>Cross-Modal Attention</category>
    </item>
    <item>
      <title>[논문리뷰] Toward Efficient Agents: Memory, Tool learning, and Planning</title>
      <description>이 [arXiv]에 게시한 &#39;Toward Efficient Agents: Memory, Tool learning, and Planning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Agent Efficiency</category><category>Memory Management</category><category>Tool Learning</category><category>AI Planning</category><category>Resource Optimization</category><category>Cost-Performance Trade-off</category>
    </item>
    <item>
      <title>[논문리뷰] ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents</title>
      <description>이 [arXiv]에 게시한 &#39;ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Process Reward Models</category><category>Tool-using Agents</category><category>Benchmark</category><category>Reinforcement Learning</category><category>Large Language Models</category><category>Reward-guided Search</category><category>Agent Evaluation</category><category>Step-level Rewards</category>
    </item>
    <item>
      <title>[논문리뷰] Think3D: Thinking with Space for Spatial Reasoning</title>
      <description>Yuhan Wu이 [arXiv]에 게시한 &#39;Think3D: Thinking with Space for Spatial Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Reasoning</category><category>3D Reconstruction</category><category>VLM Agents</category><category>Tool Calling</category><category>Reinforcement Learning</category><category>Novel View Synthesis</category><category>Iterative Exploration</category>
    </item>
    <item>
      <title>[논문리뷰] SciCoQA: Quality Assurance for Scientific Paper--Code Alignment</title>
      <description>이 [arXiv]에 게시한 &#39;SciCoQA: Quality Assurance for Scientific Paper--Code Alignment&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reproducibility</category><category>Paper-Code Discrepancy</category><category>Code Alignment</category><category>LLM Evaluation</category><category>Synthetic Data Generation</category><category>Quality Assurance</category><category>Scientific Automation</category>
    </item>
    <item>
      <title>[논문리뷰] PRiSM: Benchmarking Phone Realization in Speech Models</title>
      <description>이 [arXiv]에 게시한 &#39;PRiSM: Benchmarking Phone Realization in Speech Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Phone Recognition</category><category>Speech Models</category><category>Benchmarking</category><category>Phonetic Analysis</category><category>Cross-lingual Speech</category><category>LALMs</category><category>Intrinsic Evaluation</category><category>Extrinsic Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] On the Evidentiary Limits of Membership Inference for Copyright Auditing</title>
      <description>Marten van Dijk이 [arXiv]에 게시한 &#39;On the Evidentiary Limits of Membership Inference for Copyright Auditing&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Membership Inference Attacks</category><category>Copyright Auditing</category><category>Large Language Models</category><category>Adversarial Robustness</category><category>Paraphrasing</category><category>Sparse Autoencoders</category><category>Semantic Preservation</category><category>LLM Security</category>
    </item>
    <item>
      <title>[논문리뷰] OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</title>
      <description>이 [arXiv]에 게시한 &#39;OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Transfer</category><category>Diffusion Models</category><category>Spatio-temporal Learning</category><category>Multimodal Alignment</category><category>Appearance Consistency</category><category>Temporal Control</category><category>Video Generation</category>
    </item>
    <item>
      <title>[논문리뷰] MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reward Models</category><category>LLM Memory Management</category><category>Benchmarking</category><category>Long Context</category><category>Evaluation Metrics</category><category>Generative RMs</category><category>Memory Management Patterns</category>
    </item>
    <item>
      <title>[논문리뷰] LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</title>
      <description>이 [arXiv]에 게시한 &#39;LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>OCR</category><category>Vision-Language Model</category><category>End-to-End Learning</category><category>Multilingual</category><category>Reinforcement Learning</category><category>Document Understanding</category><category>Bounding Box Prediction</category><category>Task Arithmetic Merging</category>
    </item>
    <item>
      <title>[논문리뷰] LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</title>
      <description>이 [arXiv]에 게시한 &#39;LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Explainability</category><category>Causal Inference</category><category>Structural Counterfactuals</category><category>Concept-Based Explanations</category><category>Evaluation Benchmark</category><category>Faithfulness</category><category>SCM</category>
    </item>
    <item>
      <title>[논문리뷰] KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning</title>
      <description>Aleksandr I. Panov이 [arXiv]에 게시한 &#39;KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Visual Generalization</category><category>Distribution Shift</category><category>Benchmarking</category><category>JAX</category><category>Controlled Environments</category><category>PPO</category>
    </item>
    <item>
      <title>[논문리뷰] FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>Future Forecasting</category><category>Audio-Visual Reasoning</category><category>Benchmark</category><category>Instruction Tuning</category><category>Omni-Modal</category><category>Causal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD</title>
      <description>이 [arXiv]에 게시한 &#39;Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Differential Privacy (DP)</category><category>DP-SGD</category><category>f-differential privacy</category><category>Privacy-Utility Trade-off</category><category>Shuffled Sampling</category><category>Poisson Subsampling</category><category>Gaussian Noise</category><category>Worst-Case Adversary</category>
    </item>
    <item>
      <title>[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation</title>
      <description>이 [arXiv]에 게시한 &#39;FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Navigation</category><category>Chain-of-Thought Reasoning</category><category>Multimodal AI</category><category>Implicit Reasoning</category><category>Visual AutoRegressor</category><category>Embodied AI</category><category>Long-Horizon Planning</category>
    </item>
    <item>
      <title>[논문리뷰] Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization</title>
      <description>이 [arXiv]에 게시한 &#39;Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Robot Learning</category><category>Cross-Embodiment Generalization</category><category>Vision-Language-Action Models</category><category>Human-Centric Learning</category><category>Unified Action Space</category><category>Mixture-of-Flow</category><category>Real-Time Deployment</category><category>Large-Scale Datasets</category>
    </item>
    <item>
      <title>[논문리뷰] Aligning Agentic World Models via Knowledgeable Experience Learning</title>
      <description>이 [arXiv]에 게시한 &#39;Aligning Agentic World Models via Knowledgeable Experience Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Agentic AI</category><category>World Models</category><category>Experience Learning</category><category>LLMs</category><category>Physical Hallucinations</category><category>Embodied AI</category><category>Predictive Coding</category><category>Knowledge Repository</category>
    </item>
    <item>
      <title>[논문리뷰] Agentic-R: Learning to Retrieve for Agentic Search</title>
      <description>Daiting Shi이 [arXiv]에 게시한 &#39;Agentic-R: Learning to Retrieve for Agentic Search&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Agentic Search</category><category>Retrieval-Augmented Generation</category><category>Retriever Training</category><category>Passage Utility Modeling</category><category>Iterative Optimization</category><category>Reinforcement Learning</category><category>Large Language Models</category>
    </item>
    <item>
      <title>[논문리뷰] Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey</title>
      <description>이 [arXiv]에 게시한 &#39;Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM-based Issue Resolution</category><category>Software Engineering</category><category>Autonomous Agents</category><category>Code Generation</category><category>Benchmarking</category><category>Reinforcement Learning</category><category>Supervised Fine-tuning</category><category>Multimodal LLMs</category>
    </item>
    <item>
      <title>[논문리뷰] A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus</title>
      <description>Özay Ezerceli이 [arXiv]에 게시한 &#39;A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Low-Resource NLP</category><category>Semantic Relations</category><category>Dataset Generation</category><category>Turkish Language</category><category>LLM</category><category>FastText Embeddings</category><category>Agglomerative Clustering</category><category>Synonyms</category><category>Antonyms</category><category>Co-hyponyms</category>
    </item>
    <item>
      <title>[논문리뷰] A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification</title>
      <description>이 [arXiv]에 게시한 &#39;A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification/</guid>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Orchestration</category><category>Lightweight Probes</category><category>Token-Layer Aggregation</category><category>Hidden States</category><category>Single-Pass Classification</category><category>Safety Moderation</category><category>Sentiment Analysis</category>
    </item>
    <item>
      <title>[논문리뷰] YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation</title>
      <description>이 [arXiv]에 게시한 &#39;YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models (LLMs)</category><category>Activation Steering</category><category>Sparse Autoencoders (SAEs)</category><category>Domain Adaptation</category><category>Cultural Alignment</category><category>Preference Optimization</category><category>Disentangled Representations</category><category>Fine-grained Control</category>
    </item>
    <item>
      <title>[논문리뷰] The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models</title>
      <description>Jack Lindsey이 [arXiv]에 게시한 &#39;The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Language Models</category><category>Persona Control</category><category>Activation Steering</category><category>Persona Drift</category><category>Alignment</category><category>Post-training</category><category>Interpretability</category><category>Safety</category>
    </item>
    <item>
      <title>[논문리뷰] Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title>
      <description>Lecheng Yan이 [arXiv]에 게시한 &#39;Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RLVR</category><category>LLMs</category><category>Mechanistic Interpretability</category><category>Memorization Shortcuts</category><category>Data Contamination</category><category>Anchor-Adapter Circuit</category><category>Path Patching</category><category>Logit Lens</category>
    </item>
    <item>
      <title>[논문리뷰] SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature</title>
      <description>이 [arXiv]에 게시한 &#39;SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long-Context Understanding</category><category>Multimodal AI</category><category>Scientific Literature</category><category>Evidence-based Reasoning</category><category>MLLM Evaluation</category><category>Benchmarking</category><category>Cross-modal Reasoning</category><category>Information Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge</title>
      <description>이 [arXiv]에 게시한 &#39;Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Reasoning</category><category>Chain-of-Thought</category><category>Reinforcement Learning</category><category>Stochastic Reasoning</category><category>Continuous Representation</category><category>Token Efficiency</category>
    </item>
    <item>
      <title>[논문리뷰] Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation</title>
      <description>Ziyang Yan이 [arXiv]에 게시한 &#39;Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Image Segmentation</category><category>Foundation Models</category><category>SAM3</category><category>Fine-tuning</category><category>Prompt-driven</category><category>Domain Adaptation</category><category>Text-guided Segmentation</category>
    </item>
    <item>
      <title>[논문리뷰] CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation</title>
      <description>Hengshuang이 [arXiv]에 게시한 &#39;CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-subject Animation</category><category>Pose-driven Animation</category><category>Diffusion Models</category><category>Spatial Misalignment</category><category>Unbind-Rebind Paradigm</category><category>Character Animation</category><category>Video Generation</category>
    </item>
    <item>
      <title>[논문리뷰] CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion</title>
      <description>이 [arXiv]에 게시한 &#39;CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Continual Learning</category><category>Vision-Language-Action Models</category><category>Adapter Learning</category><category>Catastrophic Forgetting</category><category>Autonomous Routing</category><category>Parameter-Efficient Learning</category><category>Robotics</category>
    </item>
    <item>
      <title>[논문리뷰] ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development</title>
      <description>이 [arXiv]에 게시한 &#39;ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development/</guid>
      <pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Backend Development</category><category>LLM Agents</category><category>Code Generation</category><category>Benchmarking</category><category>DevOps</category><category>Containerization</category><category>End-to-End Testing</category><category>Environment Configuration</category>
    </item>
    
  </channel>
</rss>