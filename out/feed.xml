<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Fri, 21 Nov 2025 03:12:02 GMT</lastBuildDate>
    <pubDate>Fri, 21 Nov 2025 03:12:02 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity</title>
      <description>이 [arXiv]에 게시한 &#39;What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Research Agents</category><category>Ideation Diversity</category><category>MLE-bench</category><category>LLM Backbones</category><category>Agentic Scaffolds</category><category>Shannon Entropy</category><category>Machine Learning Engineering</category><category>Performance Metrics</category>
    </item>
    <item>
      <title>[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images</title>
      <description>이 [arXiv]에 게시한 &#39;VisPlay: Self-Evolving Vision-Language Models from Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-Evolving</category><category>Vision-Language Models</category><category>Reinforcement Learning</category><category>Self-Play</category><category>Unlabeled Data</category><category>Multimodal Reasoning</category><category>Group Relative Policy Optimization</category><category>Hallucination Mitigation</category>
    </item>
    <item>
      <title>[논문리뷰] Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks</title>
      <description>Yiran Peng이 [arXiv]에 게시한 &#39;Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Models</category><category>Spatial Reasoning</category><category>Maze Solving</category><category>Video Generation</category><category>Benchmark</category><category>Supervised Fine-tuning</category><category>Test-Time Scaling</category><category>Multimodal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</title>
      <description>이 [arXiv]에 게시한 &#39;Mixture of States: Routing Token-Level Dynamics for Multimodal Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Diffusion</category><category>Mixture of States (MoS)</category><category>Token-Level Routing</category><category>Dynamic Conditional Fusion</category><category>Text-to-Image Generation</category><category>Image Editing</category><category>Transformer Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] Medal S: Spatio-Textual Prompt Model for Medical Segmentation</title>
      <description>Tao Chen이 [arXiv]에 게시한 &#39;Medal S: Spatio-Textual Prompt Model for Medical Segmentation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Medal_S_Spatio-Textual_Prompt_Model_for_Medical_Segmentation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Medal_S_Spatio-Textual_Prompt_Model_for_Medical_Segmentation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Segmentation</category><category>Foundation Model</category><category>Spatio-Textual Prompts</category><category>3D Convolution</category><category>Multi-modal Imaging</category><category>Dynamic Resampling</category><category>Parallel Inference</category><category>Iterative Refinement</category>
    </item>
    <item>
      <title>[논문리뷰] MHR: Momentum Human Rig</title>
      <description>Chris Twigg이 [arXiv]에 게시한 &#39;MHR: Momentum Human Rig&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-MHR_Momentum_Human_Rig/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-MHR_Momentum_Human_Rig/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Parametric Body Model</category><category>Human Animation</category><category>Character Rigging</category><category>Pose Correctives</category><category>Skeletal Decoupling</category><category>Computer Graphics</category><category>AR/VR</category>
    </item>
    <item>
      <title>[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</title>
      <description>Vladimir Arkhipkin이 [arXiv]에 게시한 &#39;Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Kandinsky_5.0_A_Family_of_Foundation_Models_for_Image_and_Video_Generation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Kandinsky_5.0_A_Family_of_Foundation_Models_for_Image_and_Video_Generation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Generation</category><category>Video Generation</category><category>Diffusion Models</category><category>Flow Matching</category><category>Diffusion Transformer</category><category>NABLA</category><category>RLHF</category><category>Supervised Fine-tuning</category>
    </item>
    <item>
      <title>[논문리뷰] Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset</title>
      <description>이 [arXiv]에 게시한 &#39;Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Instruction-Guided_Lesion_Segmentation_for_Chest_X-rays_with_Automatically_Generated_Large-Scale_Dataset/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Instruction-Guided_Lesion_Segmentation_for_Chest_X-rays_with_Automatically_Generated_Large-Scale_Dataset/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Imaging</category><category>Chest X-ray</category><category>Lesion Segmentation</category><category>Vision-Language Models</category><category>Instruction Following</category><category>Data Generation</category><category>MIMIC-CXR</category>
    </item>
    <item>
      <title>[논문리뷰] FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</title>
      <description>Xinyu Yin이 [arXiv]에 게시한 &#39;FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-FreeAskWorld_An_Interactive_and_Closed-Loop_Simulator_for_Human-Centric_Embodied_AI/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-FreeAskWorld_An_Interactive_and_Closed-Loop_Simulator_for_Human-Centric_Embodied_AI/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Embodied AI</category><category>Vision-and-Language Navigation (VLN)</category><category>LLM-driven Simulation</category><category>Human-Agent Interaction</category><category>Closed-Loop</category><category>Benchmark Dataset</category><category>Social Cognition</category>
    </item>
    <item>
      <title>[논문리뷰] Aligning Generative Music AI with Human Preferences: Methods and Challenges</title>
      <description>Abhinaba Roy이 [arXiv]에 게시한 &#39;Aligning Generative Music AI with Human Preferences: Methods and Challenges&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Aligning_Generative_Music_AI_with_Human_Preferences_Methods_and_Challenges/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Aligning_Generative_Music_AI_with_Human_Preferences_Methods_and_Challenges/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative Music AI</category><category>Preference Alignment</category><category>Reinforcement Learning from Human Feedback (RLHF)</category><category>Direct Preference Optimization (DPO)</category><category>Inference-Time Optimization</category><category>Music Generation</category><category>Human-Computer Interaction</category>
    </item>
    <item>
      <title>[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries</title>
      <description>이 [arXiv]에 게시한 &#39;ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-ARC-Chapter_Structuring_Hour-Long_Videos_into_Navigable_Chapters_and_Hierarchical_Summaries/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-ARC-Chapter_Structuring_Hour-Long_Videos_into_Navigable_Chapters_and_Hierarchical_Summaries/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Chaptering</category><category>Long-form Video Understanding</category><category>Large Language Models</category><category>Multimodal Learning</category><category>Hierarchical Summarization</category><category>Video Segmentation</category><category>Reinforcement Learning</category><category>Dataset Creation</category>
    </item>
    <item>
      <title>[논문리뷰] Φeat: Physically-Grounded Feature Representation</title>
      <description>이 [arXiv]에 게시한 &#39;Φeat: Physically-Grounded Feature Representation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Φeat_Physically-Grounded_Feature_Representation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Φeat_Physically-Grounded_Feature_Representation/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-supervised Learning</category><category>Physically-Grounded Features</category><category>Material Representation</category><category>Intrinsic Scene Understanding</category><category>Vision Transformer</category><category>Synthetic Data</category><category>Contrastive Learning</category>
    </item>
    <item>
      <title>[논문리뷰] VIDEOP2R: Video Understanding from Perception to Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;VIDEOP2R: Video Understanding from Perception to Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-VIDEOP2R_Video_Understanding_from_Perception_to_Reasoning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-VIDEOP2R_Video_Understanding_from_Perception_to_Reasoning/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Understanding</category><category>Reinforcement Fine-Tuning (RFT)</category><category>Large Video Language Models (LVLMs)</category><category>Perception and Reasoning</category><category>Chain-of-Thought (CoT)</category><category>Process-Aware Learning</category><category>Policy Optimization</category><category>Credit Assignment</category>
    </item>
    <item>
      <title>[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models</title>
      <description>Rong Zhao이 [arXiv]에 게시한 &#39;TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-TopoPerception_A_Shortcut-Free_Evaluation_of_Global_Visual_Perception_in_Large_Vision-Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-TopoPerception_A_Shortcut-Free_Evaluation_of_Global_Visual_Perception_in_Large_Vision-Language_Models/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LVLM Evaluation</category><category>Global Visual Perception</category><category>Topological Properties</category><category>Shortcut-Free Benchmark</category><category>Visual Bottleneck</category><category>Multimodal AI</category><category>Synthetic Data</category>
    </item>
    <item>
      <title>[논문리뷰] REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding</title>
      <description>Jingyang Chen이 [arXiv]에 게시한 &#39;REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-REVISOR_Beyond_Textual_Reflection_Towards_Multimodal_Introspective_Reasoning_in_Long-Form_Video_Understanding/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-REVISOR_Beyond_Textual_Reflection_Towards_Multimodal_Introspective_Reasoning_in_Long-Form_Video_Understanding/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Reasoning</category><category>Long-Form Video Understanding</category><category>Self-Reflection</category><category>Reinforcement Learning</category><category>Tool-Augmented MLLMs</category><category>Visual Rethinking</category><category>Video Question Answering</category><category>Causal Attribution</category>
    </item>
    <item>
      <title>[논문리뷰] Proactive Hearing Assistants that Isolate Egocentric Conversations</title>
      <description>이 [arXiv]에 게시한 &#39;Proactive Hearing Assistants that Isolate Egocentric Conversations&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Proactive_Hearing_Assistants_that_Isolate_Egocentric_Conversations/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Proactive_Hearing_Assistants_that_Isolate_Egocentric_Conversations/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Proactive Hearing Assistant</category><category>Egocentric Audio Processing</category><category>Speech Separation</category><category>Turn-taking Dynamics</category><category>Dual-Model Architecture</category><category>Real-time Inference</category><category>Wearable Devices</category><category>Dialogue Modeling</category>
    </item>
    <item>
      <title>[논문리뷰] Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution</title>
      <description>Sudeep Pillai이 [arXiv]에 게시한 &#39;Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Orion_A_Unified_Visual_Agent_for_Multimodal_Perception_Advanced_Visual_Reasoning_and_Execution/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Orion_A_Unified_Visual_Agent_for_Multimodal_Perception_Advanced_Visual_Reasoning_and_Execution/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Agent</category><category>Multimodal Perception</category><category>Tool-Augmented LLM</category><category>Agentic AI</category><category>Visual Reasoning</category><category>Computer Vision</category><category>Structured Outputs</category><category>ReAct Framework</category>
    </item>
    <item>
      <title>[논문리뷰] OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</title>
      <description>Jian liu이 [arXiv]에 게시한 &#39;OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-OmniZip_Audio-Guided_Dynamic_Token_Compression_for_Fast_Omnimodal_Large_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-OmniZip_Audio-Guided_Dynamic_Token_Compression_for_Fast_Omnimodal_Large_Language_Models/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Omnimodal LLMs</category><category>Token Compression</category><category>Audio-Video Understanding</category><category>Dynamic Pruning</category><category>Inference Acceleration</category><category>Spatio-Temporal Compression</category><category>Large Language Models</category>
    </item>
    <item>
      <title>[논문리뷰] Mitigating Label Length Bias in Large Language Models</title>
      <description>Katharina von der Wense이 [arXiv]에 게시한 &#39;Mitigating Label Length Bias in Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Mitigating_Label_Length_Bias_in_Large_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Mitigating_Label_Length_Bias_in_Large_Language_Models/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models</category><category>Label Bias</category><category>Calibration</category><category>In-Context Learning</category><category>Text Classification</category><category>Multi-token Labels</category><category>Label Length Bias</category><category>Multiple Choice QA</category>
    </item>
    <item>
      <title>[논문리뷰] MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs</title>
      <description>Kaijie Chen이 [arXiv]에 게시한 &#39;MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-MVI-Bench_A_Comprehensive_Benchmark_for_Evaluating_Robustness_to_Misleading_Visual_Inputs_in_LVLMs/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-MVI-Bench_A_Comprehensive_Benchmark_for_Evaluating_Robustness_to_Misleading_Visual_Inputs_in_LVLMs/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LVLM Robustness</category><category>Misleading Visual Inputs</category><category>VQA Benchmark</category><category>Visual Perception</category><category>Visual Reasoning</category><category>MVI-Sensitivity</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework</title>
      <description>이 [arXiv]에 게시한 &#39;Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Large_Language_Models_Meet_Extreme_Multi-label_Classification_Scaling_and_Multi-modal_Framework/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Large_Language_Models_Meet_Extreme_Multi-label_Classification_Scaling_and_Multi-modal_Framework/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Extreme Multi-label Classification (XMC)</category><category>Large Language Models (LLMs)</category><category>Multi-modal Learning</category><category>Dual-decoder Learning</category><category>Vision Transformers</category><category>Contrastive Learning</category><category>Prompt Engineering</category>
    </item>
    <item>
      <title>[논문리뷰] LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost</title>
      <description>Kengo Tajiri이 [arXiv]에 게시한 &#39;LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-LLM-Powered_Fully_Automated_Chaos_Engineering_Towards_Enabling_Anyone_to_Build_Resilient_Software_Systems_at_Low_Cost/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-LLM-Powered_Fully_Automated_Chaos_Engineering_Towards_Enabling_Anyone_to_Build_Resilient_Software_Systems_at_Low_Cost/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Chaos Engineering</category><category>Large Language Models</category><category>System Resilience</category><category>Kubernetes</category><category>Software Automation</category><category>AI Agents</category><category>Fault Injection</category>
    </item>
    <item>
      <title>[논문리뷰] Error-Driven Scene Editing for 3D Grounding in Large Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;Error-Driven Scene Editing for 3D Grounding in Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Error-Driven_Scene_Editing_for_3D_Grounding_in_Large_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Error-Driven_Scene_Editing_for_3D_Grounding_in_Large_Language_Models/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Grounding</category><category>3D-LLMs</category><category>Scene Editing</category><category>Counterfactual Augmentation</category><category>Error-Driven Learning</category><category>Spatial Reasoning</category><category>Visual Grounding</category>
    </item>
    <item>
      <title>[논문리뷰] Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark</title>
      <description>Yuzhang Shang이 [arXiv]에 게시한 &#39;Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Can_World_Simulators_Reason_Gen-ViRe_A_Generative_Visual_Reasoning_Benchmark/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Can_World_Simulators_Reason_Gen-ViRe_A_Generative_Visual_Reasoning_Benchmark/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative Visual Reasoning</category><category>Chain-of-Frames (CoF)</category><category>Video Generation Models</category><category>World Simulators</category><category>AI Benchmarking</category><category>Cognitive Reasoning</category><category>VLM Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-AraLingBench_A_Human-Annotated_Benchmark_for_Evaluating_Arabic_Linguistic_Capabilities_of_Large_Language_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-AraLingBench_A_Human-Annotated_Benchmark_for_Evaluating_Arabic_Linguistic_Capabilities_of_Large_Language_Models/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Arabic LLMs</category><category>Linguistic Benchmark</category><category>Human Annotation</category><category>Natural Language Understanding</category><category>Grammar Evaluation</category><category>Morphology Analysis</category><category>Syntax Assessment</category><category>Reading Comprehension</category>
    </item>
    <item>
      <title>[논문리뷰] Agent READMEs: An Empirical Study of Context Files for Agentic Coding</title>
      <description>Kundjanasith Thonglek이 [arXiv]에 게시한 &#39;Agent READMEs: An Empirical Study of Context Files for Agentic Coding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Agent_READMEs_An_Empirical_Study_of_Context_Files_for_Agentic_Coding/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Agent_READMEs_An_Empirical_Study_of_Context_Files_for_Agentic_Coding/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Agentic Coding</category><category>Context Files</category><category>READMEs for Agents</category><category>Empirical Study</category><category>Software Engineering</category><category>Documentation Maintenance</category><category>Non-functional Requirements</category><category>LLMs</category>
    </item>
    <item>
      <title>[논문리뷰] Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning</title>
      <description>Yucong Luo이 [arXiv]에 게시한 &#39;Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-Agent-R1_Training_Powerful_LLM_Agents_with_End-to-End_Reinforcement_Learning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-Agent-R1_Training_Powerful_LLM_Agents_with_End-to-End_Reinforcement_Learning/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Agents</category><category>Reinforcement Learning</category><category>Markov Decision Process</category><category>Tool Use</category><category>Multi-turn Interaction</category><category>Policy Optimization</category><category>Reward Shaping</category><category>Agent Framework</category>
    </item>
    <item>
      <title>[논문리뷰] A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space</title>
      <description>이 [arXiv]에 게시한 &#39;A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-A_Style_is_Worth_One_Code_Unlocking_Code-to-Style_Image_Generation_with_Discrete_Style_Space/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-A_Style_is_Worth_One_Code_Unlocking_Code-to-Style_Image_Generation_with_Discrete_Style_Space/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Code-to-Style Generation</category><category>Discrete Style Space</category><category>Style Codebook</category><category>Autoregressive Model</category><category>Diffusion Models</category><category>Visual Stylization</category><category>Generative AI</category>
    </item>
    <item>
      <title>[논문리뷰] A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition</title>
      <description>G. Maragatham이 [arXiv]에 게시한 &#39;A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-A_Brain_Wave_Encodes_a_Thousand_Tokens_Modeling_Inter-Cortical_Neural_Interactions_for_Effective_EEG-based_Emotion_Recognition/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-A_Brain_Wave_Encodes_a_Thousand_Tokens_Modeling_Inter-Cortical_Neural_Interactions_for_Effective_EEG-based_Emotion_Recognition/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>EEG</category><category>Emotion Recognition</category><category>Transformer Architecture</category><category>Inter-Cortical Neural Interactions</category><category>Multi-Head Attention</category><category>Brain-Computer Interface</category><category>Affective Computing</category>
    </item>
    <item>
      <title>[논문리뷰] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning</title>
      <description>Yuqiang Li이 [arXiv]에 게시한 &#39;ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-19-ATLAS_A_High-Difficulty_Multidisciplinary_Benchmark_for_Frontier_Scientific_Reasoning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-19-ATLAS_A_High-Difficulty_Multidisciplinary_Benchmark_for_Frontier_Scientific_Reasoning/</guid>
      <pubDate>Tue, 18 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Benchmark</category><category>LLMs</category><category>Scientific Reasoning</category><category>Multidisciplinary</category><category>AI4S</category><category>Data Contamination</category><category>Evaluation</category><category>LRM-as-Judge</category>
    </item>
    
  </channel>
</rss>