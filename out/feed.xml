<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/feed.xml" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Thu, 27 Nov 2025 05:42:15 GMT</lastBuildDate>
    <pubDate>Thu, 27 Nov 2025 05:42:15 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Generation</category><category>Video Models</category><category>Diffusion Models</category><category>Many-to-many</category><category>Unified Framework</category><category>Temporal Consistency</category><category>Image Editing</category><category>Positional Embedding</category>
    </item>
    <item>
      <title>[논문리뷰] Yo&#39;City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion</title>
      <description>Zhifei Yang이 [arXiv]에 게시한 &#39;Yo&#39;City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D City Generation</category><category>Generative AI</category><category>Large Language Models</category><category>Vision-Language Models</category><category>Multi-Agent Framework</category><category>Self-Critic Learning</category><category>Scene Graph</category><category>Text-to-3D</category>
    </item>
    <item>
      <title>[논문리뷰] VQ-VA World: Towards High-Quality Visual Question-Visual Answering</title>
      <description>Feng Li이 [arXiv]에 게시한 &#39;VQ-VA World: Towards High-Quality Visual Question-Visual Answering&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Question Answering (VQA)</category><category>Image Generation</category><category>Data-centric AI</category><category>Agentic Pipeline</category><category>Multimodal Models</category><category>Web-scale Data</category><category>Benchmark</category><category>LightFusion</category>
    </item>
    <item>
      <title>[논문리뷰] Unified all-atom molecule generation with neural fields</title>
      <description>이 [arXiv]에 게시한 &#39;Unified all-atom molecule generation with neural fields&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Molecule Generation</category><category>Neural Fields</category><category>Score-based Generative Models</category><category>Drug Design</category><category>Modality-agnostic</category><category>Antibody Design</category><category>Macrocyclic Peptides</category><category>All-atom</category>
    </item>
    <item>
      <title>[논문리뷰] UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers</title>
      <description>이 [arXiv]에 게시한 &#39;UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Diffusion Transformers</category><category>Length Extrapolation</category><category>Attention Mechanism</category><category>Attention Dispersion</category><category>Periodic Content Repetition</category><category>Quality Degradation</category><category>Training-free Method</category><category>Plug-and-play</category>
    </item>
    <item>
      <title>[논문리뷰] Soft Adaptive Policy Optimization</title>
      <description>이 [arXiv]에 게시한 &#39;Soft Adaptive Policy Optimization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Large Language Models</category><category>Policy Optimization</category><category>Importance Ratios</category><category>Soft Clipping</category><category>Trust Region</category><category>Mixture-of-Experts</category><category>Asymmetric Temperature</category>
    </item>
    <item>
      <title>[논문리뷰] SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System</title>
      <description>이 [arXiv]에 게시한 &#39;SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-Agent System</category><category>Video Understanding</category><category>Scientific Education</category><category>Deming Cycle</category><category>Large Language Models</category><category>Iterative Optimization</category><category>Knowledge Integration</category><category>Educational Content Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models (VLMs)</category><category>Reinforcement Learning (RL)</category><category>Tool-Integrated Reasoning (TIR)</category><category>Agentic AI</category><category>VQA</category><category>Training Environment</category><category>Behavioral Cloning</category><category>Policy Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space</title>
      <description>Yulan He이 [arXiv]에 게시한 &#39;SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Sparse Attention</category><category>Full Attention</category><category>Large Language Models (LLMs)</category><category>Context Length</category><category>Attention Sparsity</category><category>Alignment Loss</category><category>Long-Context Extrapolation</category>
    </item>
    <item>
      <title>[논문리뷰] ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding</title>
      <description>이 [arXiv]에 게시한 &#39;ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Retake Generation</category><category>Camera Control</category><category>Rotary Position Embedding (RoPE)</category><category>Rotary Camera Encoding (RoCE)</category><category>Geometric Consistency</category><category>Video Generative Models</category><category>Transformer Architecture</category><category>Multi-view Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding</title>
      <description>Hongzhi Zhang이 [arXiv]에 게시한 &#39;PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Physics Simulation</category><category>Controllable AI</category><category>Part-Aware</category><category>Semantic Grounding</category><category>Material Properties</category><category>Image-to-Video</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation</title>
      <description>이 [arXiv]에 게시한 &#39;OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RGBA Generation</category><category>Multi-Task Learning</category><category>Diffusion Transformers</category><category>Image Matting</category><category>Layer Decomposition</category><category>Object Removal</category><category>Alpha-aware VAE</category><category>MSROPE-BiL</category>
    </item>
    <item>
      <title>[논문리뷰] MedSAM3: Delving into Segment Anything with Medical Concepts</title>
      <description>Yi Lu이 [arXiv]에 게시한 &#39;MedSAM3: Delving into Segment Anything with Medical Concepts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Image Segmentation</category><category>Segment Anything Model (SAM)</category><category>Promptable Concept Segmentation (PCS)</category><category>Multimodal Large Language Models (MLLMs)</category><category>Agentic AI</category><category>Domain Adaptation</category><category>Text-guided Segmentation</category>
    </item>
    <item>
      <title>[논문리뷰] MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</title>
      <description>이 [arXiv]에 게시한 &#39;MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D City Generation</category><category>Natural Language Processing</category><category>Aesthetic Adaptation</category><category>Controllable Assets</category><category>Layout Generation</category><category>Interactive Editing</category><category>Diffusion Models</category><category>Multimodal Dataset</category>
    </item>
    <item>
      <title>[논문리뷰] HunyuanOCR Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;HunyuanOCR Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-HunyuanOCR-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-HunyuanOCR-Technical-Report/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Optical Character Recognition</category><category>Multimodal Large Language Model</category><category>End-to-End Learning</category><category>Reinforcement Learning</category><category>Document Parsing</category><category>Information Extraction</category><category>Text Spotting</category>
    </item>
    <item>
      <title>[논문리뷰] GigaWorld-0: World Models as Data Engine to Empower Embodied AI</title>
      <description>Chaojun Ni이 [arXiv]에 게시한 &#39;GigaWorld-0: World Models as Data Engine to Empower Embodied AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Embodied AI</category><category>Data Generation</category><category>Video Generation</category><category>3D Scene Reconstruction</category><category>Robotics</category><category>Vision-Language-Action</category>
    </item>
    <item>
      <title>[논문리뷰] GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</title>
      <description>이 [arXiv]에 게시한 &#39;GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM-driven Evolutionary Computation</category><category>Quality-Diversity</category><category>MAP-Elites</category><category>Program Synthesis</category><category>Open-source Framework</category><category>Algorithmic Discovery</category><category>Genetic Algorithms</category>
    </item>
    <item>
      <title>[논문리뷰] Fara-7B: An Efficient Agentic Model for Computer Use</title>
      <description>이 [arXiv]에 게시한 &#39;Fara-7B: An Efficient Agentic Model for Computer Use&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Computer Use Agents</category><category>Synthetic Data Generation</category><category>Multi-modal LLM</category><category>On-device AI</category><category>Web Automation</category><category>Pixel-in Action-out</category><category>Fara-7B</category><category>WebTailBench</category>
    </item>
    <item>
      <title>[논문리뷰] Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward</title>
      <description>이 [arXiv]에 게시한 &#39;Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Unified Multimodal Models</category><category>Understanding-Generation Gap</category><category>Reasoning</category><category>Knowledge Transfer</category><category>Chain-of-Thought</category><category>Self-Training</category><category>Synthetic Data</category><category>Evaluation Framework</category>
    </item>
    <item>
      <title>[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection</title>
      <description>Mike Zheng Shou이 [arXiv]에 게시한 &#39;DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AIGC Detection</category><category>Diffusion Models</category><category>Image Editing</category><category>Semantic Segmentation</category><category>Localization</category><category>Model Attribution</category><category>Benchmark</category><category>Multi-turn Editing</category>
    </item>
    <item>
      <title>[논문리뷰] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-Evolving Agent</category><category>Vision-Language Models</category><category>Tool-Integrated Reasoning</category><category>Reinforcement Learning</category><category>Self-Correction</category><category>Multimodal AI</category><category>Generative AI</category>
    </item>
    <item>
      <title>[논문리뷰] UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios</title>
      <description>이 [arXiv]에 게시한 &#39;UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image Generation</category><category>Diffusion Transformers</category><category>4K Resolution</category><category>Aspect Ratio Extrapolation</category><category>Data-Model Co-Design</category><category>VAE Post-training</category><category>Positional Encoding</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</title>
      <description>Zhaowei Lu이 [arXiv]에 게시한 &#39;Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Mapless Navigation</category><category>Semantic Path Planning</category><category>Robot Learning</category><category>Video Prediction</category><category>Benchmark</category><category>Trajectory Generation</category>
    </item>
    <item>
      <title>[논문리뷰] SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis</title>
      <description>Hongwen Zhang이 [arXiv]에 게시한 &#39;SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Hand-Object Interaction</category><category>Multi-view Video Generation</category><category>4D Motion Synthesis</category><category>Diffusion Models</category><category>Spatio-temporal Consistency</category><category>Geometric Consistency</category><category>Appearance and Motion Joint Modeling</category>
    </item>
    <item>
      <title>[논문리뷰] Plan-X: Instruct Video Generation via Semantic Planning</title>
      <description>Chenxu Zhang이 [arXiv]에 게시한 &#39;Plan-X: Instruct Video Generation via Semantic Planning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Semantic Planning</category><category>Multimodal LLM</category><category>Diffusion Transformer</category><category>Spatio-temporal Guidance</category><category>Visual Hallucination</category><category>Prompt Alignment</category><category>Instruction Following</category>
    </item>
    <item>
      <title>[논문리뷰] Pillar-0: A New Frontier for Radiology Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;Pillar-0: A New Frontier for Radiology Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Radiology Foundation Model</category><category>Volumetric Imaging</category><category>Multi-window Tokenization</category><category>Multi-scale Attention</category><category>Contrastive Learning</category><category>Clinical Evaluation</category><category>Data Efficiency</category><category>Medical Imaging</category>
    </item>
    <item>
      <title>[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking</title>
      <description>Elias Stengel-Eskin이 [arXiv]에 게시한 &#39;PRInTS: Reward Modeling for Long-Horizon Information Seeking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reward Modeling</category><category>Long-Horizon Tasks</category><category>Information Seeking</category><category>Large Language Models</category><category>Trajectory Summarization</category><category>Reinforcement Learning</category><category>Tool Use</category><category>Process Reward Models</category>
    </item>
    <item>
      <title>[논문리뷰] Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO</title>
      <description>이 [arXiv]에 게시한 &#39;Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-Agent Systems</category><category>Reinforcement Learning</category><category>LLM Training</category><category>Hierarchical Credit Assignment</category><category>Trajectory Alignment</category><category>Group Relative Policy Optimization</category><category>Tool-Augmented Reasoning</category><category>Vertical Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] MIST: Mutual Information Via Supervised Training</title>
      <description>Kyunghyun Cho이 [arXiv]에 게시한 &#39;MIST: Mutual Information Via Supervised Training&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Mutual Information Estimation</category><category>Supervised Learning</category><category>Meta-Learning</category><category>Neural Networks</category><category>Uncertainty Quantification</category><category>SetTransformer</category><category>Quantile Regression</category>
    </item>
    <item>
      <title>[논문리뷰] MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Physics Reasoning</category><category>Motion Tracking</category><category>Spatial-Temporal Grounding</category><category>Video QA</category><category>AIGC Analysis</category><category>Reinforcement Learning</category>
    </item>
    
  </channel>
</rss>