<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633&apos;s blog</title>
    <description></description>
    <link>https://secrett2633.github.io/</link>
    <atom:link href="https://secrett2633.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 25 Sep 2025 17:34:08 +0900</pubDate>
    <lastBuildDate>Thu, 25 Sep 2025 17:34:08 +0900</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>[논문리뷰] Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.19087&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Genady Beryozkin, Maxim Neumann, Dahun Kim, Yotam Gigi, Ganesh Mallya, Tomer Shekel, Anelia Angelova&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 &lt;strong&gt;RGB 전용 이미지로 훈련된 범용 대규모 멀티모달 모델(LMM)&lt;/strong&gt;이 원격 감지 분야에서 널리 사용되는 &lt;strong&gt;다중 스펙트럼(multi-spectral) 입력&lt;/strong&gt;을 추가 훈련 없이 &lt;strong&gt;Zero-Shot 방식으로 이해하고 활용&lt;/strong&gt;할 수 있도록 하는 새로운 접근 방식을 제안합니다. 이는 기존 LMM의 입력 제약을 극복하고 원격 감지 애플리케이션에 대한 적용 가능성을 확장하는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;제안된 접근 방식은 훈련이 필요 없는 방법으로, 다중 스펙트럼 데이터를 &lt;strong&gt;의사 이미지(pseudo-image) 형태&lt;/strong&gt;로 변환하여 범용 멀티모달 모델에 입력합니다. 각 의사 이미지에 대해 모델의 프롬프트 내에서 &lt;strong&gt;상세한 텍스트 설명(prompt engineering)&lt;/strong&gt;을 제공하여 해당 이미지가 어떤 대역으로 구성되었고 물리적으로 무엇을 나타내는지(예: ‘수분 존재 지표’) 문맥화합니다. 이 과정은 &lt;strong&gt;Gemini 2.5 모델&lt;/strong&gt;의 시각적 이해 능력과 지시 해석 능력을 활용합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BigEarthNet 데이터셋(43개 클래스)&lt;/strong&gt;에서 다중 스펙트럼 입력은 RGB 전용 입력 대비 &lt;strong&gt;F1 점수를 0.388에서 0.429로 +0.04 향상&lt;/strong&gt;시켰습니다. 또한, &lt;strong&gt;BigEarthNet(19개 클래스)&lt;/strong&gt;에서는 F1 점수를 &lt;strong&gt;0.414에서 0.453으로 +0.04 향상&lt;/strong&gt;시키며, 기존 SOTA Zero-Shot 방법론 대비 &lt;strong&gt;+0.053 F1 이득&lt;/strong&gt;을 달성했습니다. &lt;strong&gt;EuroSat 데이터셋&lt;/strong&gt;에서는 RGB 전용 대비 &lt;strong&gt;+3%의 정확도(66.3%에서 69.1%)&lt;/strong&gt; 향상을 보이며 SOTA 귀납적 Zero-Shot 방법론을 능가했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;AI 실무자들은 이 방법을 통해 &lt;strong&gt;Gemini 2.5와 같은 강력한 범용 LMM&lt;/strong&gt;을 원격 감지의 특수 다중 스펙트럼 데이터에 &lt;strong&gt;추가 훈련 없이 즉시 적용&lt;/strong&gt;할 수 있습니다. 이는 모델 개발 및 지원에 필요한 &lt;strong&gt;비용과 시간을 대폭 절감&lt;/strong&gt;하며, 새로운 센서 유형이나 데이터 형식에 대한 &lt;strong&gt;빠른 적응성&lt;/strong&gt;을 제공합니다. 특히, 복잡한 프롬프트 엔지니어링을 통해 &lt;strong&gt;모델의 추론 능력&lt;/strong&gt;을 활용하는 것이 다양한 지구 관측 데이터 분석에 중요한 이점을 제공합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-Zero-Shot_Multi-Spectral_Learning_Reimagining_a_Generalist_Multimodal_Gemini_2.5_Model_for_Remote_Sensing_Applications/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-Zero-Shot_Multi-Spectral_Learning_Reimagining_a_Generalist_Multimodal_Gemini_2.5_Model_for_Remote_Sensing_Applications/</guid>
        
        <category>Review</category>
        
        <category>Remote Sensing</category>
        
        <category>Zero-Shot Learning</category>
        
        <category>Multimodal Models</category>
        
        <category>Multi-spectral Imagery</category>
        
        <category>Gemini 2.5</category>
        
        <category>Prompt Engineering</category>
        
        <category>Land Cover Classification</category>
        
        <category>Pseudo-Image</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.19284&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 대규모 추론 모델(LRMs)에서 효과적인 CoT(Chain-of-Thought) 추론의 특성을 규명하는 것을 목표로 합니다. 특히, 기존의 “길수록 좋다”는 CoT 길이 및 검토(review) 증가 경향에 의문을 제기하고, 추론 과정의 어휘적, 구조적 특성이 정확도에 미치는 영향을 체계적으로 분석하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;연구팀은 &lt;strong&gt;HARP&lt;/strong&gt; 및 &lt;strong&gt;GPQA-Diamond&lt;/strong&gt; 데이터셋에 걸쳐 10개 &lt;strong&gt;LRM&lt;/strong&gt;의 CoT 추론을 평가했습니다. CoT 길이와 &lt;strong&gt;Review Ratio&lt;/strong&gt; 외에, CoT에서 자동으로 추출된 &lt;strong&gt;추론 그래프&lt;/strong&gt;를 기반으로 &lt;strong&gt;Failed-Step Fraction (FSF)&lt;/strong&gt;이라는 새로운 구조적 메트릭을 도입했습니다. 인과관계를 검증하기 위해 &lt;strong&gt;테스트 시점 선택(test-time selection)&lt;/strong&gt; 및 &lt;strong&gt;CoT 편집(controlled CoT editing)&lt;/strong&gt;이라는 두 가지 개입 실험을 수행했습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;기존의 통념과 달리, 대부분의 모델에서 &lt;strong&gt;CoT 길이&lt;/strong&gt;와 &lt;strong&gt;Review Ratio&lt;/strong&gt;가 길어질수록 &lt;strong&gt;정확도가 낮아지는 일관된 음의 상관관계&lt;/strong&gt;를 발견했습니다. 특히, &lt;strong&gt;Failed-Step Fraction (FSF)&lt;/strong&gt;은 모든 모델과 데이터셋에서 정확도를 예측하는 &lt;strong&gt;가장 강력하고 일관된 지표&lt;/strong&gt;로 나타났으며, &lt;strong&gt;FSF가 낮을수록 정확도가 높았습니다&lt;/strong&gt;. &lt;strong&gt;AIME 2025&lt;/strong&gt;에서 &lt;strong&gt;FSF 기반 테스트 시점 선택&lt;/strong&gt;은 무작위 기준선 대비 &lt;strong&gt;5-13%의 정확도 향상&lt;/strong&gt;을 보였고, 오답 CoT에서 &lt;strong&gt;실패한 브랜치를 제거&lt;/strong&gt;했을 때 정확도가 &lt;strong&gt;8-14% 유의미하게 향상&lt;/strong&gt;되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;AI 실무자들은 CoT 추론의 성능을 향상시키기 위해 단순히 &lt;strong&gt;CoT의 길이를 늘리거나 검토 횟수를 증가&lt;/strong&gt;시키는 양적 접근보다는 &lt;strong&gt;CoT의 구조적 품질&lt;/strong&gt;에 더 집중해야 합니다. 특히, 모델이 생성하는 추론 과정에서 &lt;strong&gt;실패한 탐색 경로의 비율(FSF)&lt;/strong&gt;을 줄이는 것이 중요하며, &lt;strong&gt;실패한 브랜치를 식별하고 제거&lt;/strong&gt;하는 &lt;strong&gt;구조 인지적 전략&lt;/strong&gt;이 추론 정확도를 크게 개선할 수 있음을 시사합니다. 이는 모델이 이전의 잘못된 시도에 편향되는 것을 방지하고, &lt;strong&gt;더 효율적이고 신뢰성 높은 AI 추론 시스템&lt;/strong&gt;을 개발하는 데 기여할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-What_Characterizes_Effective_Reasoning_Revisiting_Length_Review_and_Structure_of_CoT/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-What_Characterizes_Effective_Reasoning_Revisiting_Length_Review_and_Structure_of_CoT/</guid>
        
        <category>Review</category>
        
        <category>Chain-of-Thought</category>
        
        <category>Reasoning Effectiveness</category>
        
        <category>Large Reasoning Models</category>
        
        <category>Failed-Step Fraction</category>
        
        <category>Test-time Scaling</category>
        
        <category>Reasoning Graph</category>
        
        <category>Model Evaluation</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.19297&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;기존 Feed-Forward 3D Gaussian Splatting (3DGS) 방식의 문제점인 픽셀 정렬(pixel alignment) 의존성, 뷰 편향된 밀도 분포, 그리고 정렬 오류를 해결하는 것을 목표로 합니다. 특히 입력 뷰 수에 대한 의존성과 저텍스처 또는 폐색 영역에서의 한계를 극복하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;본 논문은 픽셀 정렬 대신 &lt;strong&gt;볼륨 정렬(voxel-aligned)&lt;/strong&gt; 기반의 예측을 수행하는 새로운 프레임워크인 &lt;strong&gt;VolSplat&lt;/strong&gt;을 제안합니다. 이 방법은 입력 이미지에서 추출된 &lt;strong&gt;2D 특징&lt;/strong&gt;을 &lt;strong&gt;평면 스위핑(plane sweeping)&lt;/strong&gt; 기반의 깊이 예측을 통해 3D 공간의 &lt;strong&gt;스파스 복셀 특징 그리드(sparse voxel feature grid)&lt;/strong&gt;로 변환합니다. 이후, &lt;strong&gt;스파스 3D U-Net 디코더&lt;/strong&gt;를 사용하여 이 특징을 정제하고, 각 점유 복셀로부터 &lt;strong&gt;3D 가우시안 파라미터(위치, 공분산, 불투명도, 색상)&lt;/strong&gt;를 직접 예측하여 뷰 독립적인 표현을 생성합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;VolSplat&lt;/strong&gt;은 &lt;strong&gt;RealEstate10K&lt;/strong&gt; 및 &lt;strong&gt;ScanNet&lt;/strong&gt; 벤치마크에서 최첨단 성능을 달성했습니다. RealEstate10K에서는 &lt;strong&gt;PSNR 31.30, SSIM 0.941, LPIPS 0.075&lt;/strong&gt;를 기록하여 기존 DepthSplat (PSNR 27.47)과 같은 픽셀 정렬 기반 모델을 크게 능가했습니다. 또한, &lt;strong&gt;ACID 데이터셋&lt;/strong&gt;에서의 교차 데이터셋 일반화 실험에서도 &lt;strong&gt;PSNR 32.65&lt;/strong&gt;로 탁월한 성능을 보여주며, 플로터(floaters)와 아티팩트가 없는 더욱 사실적이고 일관된 가우시안 재구성을 제공합니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;VolSplat&lt;/strong&gt;은 3D 재구성에서 &lt;strong&gt;뷰 일관성&lt;/strong&gt;과 &lt;strong&gt;기하학적 정확도&lt;/strong&gt;를 향상시키는 실용적인 방법을 제시합니다. &lt;strong&gt;복셀 정렬 기반 예측&lt;/strong&gt;은 씬 복잡도에 따라 가우시안 밀도를 적응적으로 제어할 수 있어, 리소스 효율성을 높이면서도 세밀한 기하학적 표현이 가능합니다. 이는 &lt;strong&gt;자율 주행, 로봇 공학, 가상현실/증강현실&lt;/strong&gt;과 같이 고품질 3D 씬 재구성이 필수적인 실시간 애플리케이션 개발에 중요한 기여를 할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-VolSplat_Rethinking_Feed-Forward_3D_Gaussian_Splatting_with_Voxel-Aligned_Prediction/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-VolSplat_Rethinking_Feed-Forward_3D_Gaussian_Splatting_with_Voxel-Aligned_Prediction/</guid>
        
        <category>Review</category>
        
        <category>3D Gaussian Splatting</category>
        
        <category>Novel View Synthesis</category>
        
        <category>Voxel-Aligned Prediction</category>
        
        <category>Feed-Forward Reconstruction</category>
        
        <category>Multi-View Consistency</category>
        
        <category>Scene Representation</category>
        
        <category>Computer Vision</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.19002&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia Chen, Hiroyuki Otomo, Hiroki Ouchi, Daisuke Kawahara&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 연구는 기존 비디오 벤치마크들이 장거리 이동 및 다일(multi-day) 활동과 같은 &lt;strong&gt;거시적 규모의 지리 공간-시간적 시나리오&lt;/strong&gt;를 충분히 다루지 못한다는 한계를 지적하며, MLLM(Multimodal Large Language Models)의 &lt;strong&gt;확장된 지리 공간 및 시간적 이해 능력&lt;/strong&gt;을 평가하는 새로운 벤치마크 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VIR-Bench&lt;/code&gt;를 제시합니다. 궁극적으로 MLLM이 실제 환경에서의 계획 및 내비게이션과 같은 작업에 필요한 능력을 갖추도록 촉진하는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VIR-Bench&lt;/code&gt;는 200개의 여행 비디오를 기반으로 &lt;strong&gt;방문 순서 그래프(visiting order graph)&lt;/strong&gt;를 재구성하는 과제를 정의합니다. 이 그래프는 &lt;strong&gt;노드 예측&lt;/strong&gt; (방문한 장소 식별: 행정 구역, 도시, POI 및 카테고리) 및 &lt;strong&gt;엣지 예측&lt;/strong&gt; (공간적 포함 관계 및 시간적 전환 관계 추론)의 두 가지 하위 작업으로 분해됩니다. 또한, &lt;strong&gt;Gemini-2.5-Pro&lt;/strong&gt;를 백본으로 하는 프로토타입 &lt;strong&gt;여행 계획 에이전트&lt;/strong&gt;를 개발하여, POI 목록과 비디오를 모두 활용한 여행 계획 생성 능력을 평가했습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;실험 결과, 최신 MLLM들은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VIR-Bench&lt;/code&gt;에서 &lt;strong&gt;높은 점수를 달성하는 데 어려움&lt;/strong&gt;을 겪으며, 특히 &lt;strong&gt;POI 노드 예측&lt;/strong&gt; 및 &lt;strong&gt;전환 엣지 예측&lt;/strong&gt;에서 약점을 드러냈습니다. &lt;strong&gt;오픈소스 모델&lt;/strong&gt;은 &lt;strong&gt;소유 모델&lt;/strong&gt;에 비해 성능이 현저히 낮았고, 일부 오픈소스 모델의 &lt;strong&gt;전환 엣지 예측 F1 점수&lt;/strong&gt;는 &lt;strong&gt;약 1점&lt;/strong&gt;에 불과했습니다. 반면, &lt;strong&gt;Gemini-2.5-Pro&lt;/strong&gt;와 같은 소유 모델은 &lt;strong&gt;POI 노드 및 전환 엣지 예측 F1 점수&lt;/strong&gt;가 &lt;strong&gt;약 60점대&lt;/strong&gt;를 기록했습니다. 추가적으로, &lt;strong&gt;더 많은 프레임&lt;/strong&gt;, &lt;strong&gt;더 많은 추론 노력&lt;/strong&gt;, &lt;strong&gt;오디오 입력&lt;/strong&gt;이 일관된 성능 향상을 가져왔으며, 특히 &lt;strong&gt;오디오 입력&lt;/strong&gt;은 &lt;strong&gt;전환 엣지 예측 성능을 거의 50% 향상&lt;/strong&gt;시켰습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;본 연구는 MLLM이 &lt;strong&gt;장거리 지리 공간 및 시간적 추론 능력&lt;/strong&gt;을 개선해야 할 필요성을 명확히 보여줍니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VIR-Bench&lt;/code&gt;는 이러한 능력을 평가하기 위한 &lt;strong&gt;도전적인 벤치마크&lt;/strong&gt;를 제공하여, AI 엔지니어들이 &lt;strong&gt;고차원적인 비디오 이해 모델&lt;/strong&gt;을 개발하는 데 중요한 기반이 될 것입니다. 특히, &lt;strong&gt;여행 계획 에이전트&lt;/strong&gt; 사례 연구는 &lt;strong&gt;정확한 여정 재구성&lt;/strong&gt;과 &lt;strong&gt;비디오의 풍부한 시각적/청각적 문맥&lt;/strong&gt;을 결합하는 것이 &lt;strong&gt;사용자 친화적인 애플리케이션&lt;/strong&gt; 개발에 필수적임을 시사합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-VIR-Bench_Evaluating_Geospatial_and_Temporal_Understanding_of_MLLMs_via_Travel_Video_Itinerary_Reconstruction/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-VIR-Bench_Evaluating_Geospatial_and_Temporal_Understanding_of_MLLMs_via_Travel_Video_Itinerary_Reconstruction/</guid>
        
        <category>Review</category>
        
        <category>Multimodal LLMs</category>
        
        <category>Video Understanding</category>
        
        <category>Geospatial Reasoning</category>
        
        <category>Temporal Reasoning</category>
        
        <category>Travel Itinerary Reconstruction</category>
        
        <category>Benchmark</category>
        
        <category>Agent System</category>
        
        <category>VLOG</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Reinforcement Learning on Pre-Training Data</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.19249&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;논문은 대규모 언어 모델(LLM)의 훈련 시 발생하는 컴퓨팅 자원의 기하급수적 증가와 고품질 텍스트 데이터의 유한한 성장 사이의 불균형 문제를 해결하고자 합니다. 인간의 어노테이션에 의존하지 않고 사전 훈련 데이터에서 직접 보상 신호를 도출하는 &lt;strong&gt;RLPT(Reinforcement Learning on Pre-Training data)&lt;/strong&gt;라는 새로운 훈련 시간 스케일링 패러다임을 제안하여 LLM의 역량과 일반화된 추론 능력을 향상시키는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;RLPT&lt;/strong&gt;는 &lt;strong&gt;다음 세그먼트 추론(next-segment reasoning)&lt;/strong&gt;을 RL 목표로 사용하며, 정책이 선행 컨텍스트를 기반으로 후속 텍스트 세그먼트를 정확하게 예측하도록 보상합니다. 이를 위해 &lt;strong&gt;ASR(Autoregressive Segment Reasoning)&lt;/strong&gt; 및 &lt;strong&gt;MSR(Middle Segment Reasoning)&lt;/strong&gt; 두 가지 세그먼트 수준 훈련 목표를 도입하고, 예측된 세그먼트와 참조 세그먼트 간의 &lt;strong&gt;의미론적 일관성(semantic consistency)&lt;/strong&gt;을 평가하는 &lt;strong&gt;생성형 보상 모델(generative reward model) Grm&lt;/strong&gt;을 활용합니다. 특히 &lt;strong&gt;Grm&lt;/strong&gt;은 예측 세그먼트가 참조 콘텐츠의 유효한 접두사인지 확인하여 엄격한 단어 일치 대신 유연한 보상 구조를 제공합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Qwen3-4B-Base&lt;/strong&gt; 모델에 적용했을 때, &lt;strong&gt;RLPT&lt;/strong&gt;는 MMLU에서 &lt;strong&gt;3.0%p&lt;/strong&gt;, MMLU-Pro에서 &lt;strong&gt;5.1%p&lt;/strong&gt;, GPQA-Diamond에서 &lt;strong&gt;8.1%p&lt;/strong&gt;, KOR-Bench에서 &lt;strong&gt;6.0%p&lt;/strong&gt;, AIME24에서 &lt;strong&gt;6.6%p&lt;/strong&gt;, AIME25에서 &lt;strong&gt;5.3%p&lt;/strong&gt;의 절대 성능 향상을 달성했습니다. 또한, &lt;strong&gt;RLVR&lt;/strong&gt;와 함께 사용될 경우 AIME24에서 &lt;strong&gt;2.3%p&lt;/strong&gt;, AIME25에서 &lt;strong&gt;1.3%p&lt;/strong&gt;의 추가적인 Pass@1 성능 개선을 보였습니다. &lt;strong&gt;RLPT&lt;/strong&gt;의 성능은 훈련 토큰 수에 따라 &lt;strong&gt;멱법칙(power-law decay)&lt;/strong&gt; 스케일링 특성을 따르며, 이는 지속적인 성능 향상 가능성을 시사합니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;RLPT&lt;/strong&gt;는 대규모 &lt;strong&gt;사전 훈련 데이터&lt;/strong&gt;에서 &lt;strong&gt;RL&lt;/strong&gt;을 직접 적용할 수 있는 길을 열어, 기존 &lt;strong&gt;RLHF&lt;/strong&gt;나 &lt;strong&gt;RLVR&lt;/strong&gt;의 &lt;strong&gt;인간 어노테이션 의존성&lt;/strong&gt;이라는 주요 한계를 극복합니다. 이 방법론은 LLM이 더 깊은 추론 능력을 탐색하고 일반화 능력을 향상시키는 데 기여하며, 특히 &lt;strong&gt;수학적 추론&lt;/strong&gt;과 같은 복잡한 도메인에서 효과적임을 보여줍니다. &lt;strong&gt;멱법칙 스케일링&lt;/strong&gt;은 컴퓨팅 자원을 추가함에 따라 &lt;strong&gt;RLPT&lt;/strong&gt;의 성능이 지속적으로 개선될 수 있음을 의미하므로, AI 모델 개발자들에게 LLM의 잠재력을 최대한 활용할 수 있는 중요한 방향을 제시합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-Reinforcement_Learning_on_Pre-Training_Data/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-Reinforcement_Learning_on_Pre-Training_Data/</guid>
        
        <category>Review</category>
        
        <category>Reinforcement Learning</category>
        
        <category>Pre-training</category>
        
        <category>Large Language Models</category>
        
        <category>Self-supervised Learning</category>
        
        <category>Scaling Laws</category>
        
        <category>Next-segment Reasoning</category>
        
        <category>Reward Modeling</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] OpenGVL - Benchmarking Visual Temporal Progress for Data Curation</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17321&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Paweł Budzianowski, Emilia Wiśnios, Gracjan Góral, Igor Kulakov, Viktor Petrenko, Krzysztof Walas&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;로봇 공학 분야의 데이터 부족 문제를 해결하고, 대규모 로봇 데이터셋을 자동으로 주석 및 큐레이션할 수 있는 도구의 필요성을 강조합니다. 이를 위해 시각적 관측을 통한 로봇 작업 진행도 예측을 위한 벤치마크인 &lt;strong&gt;OpenGVL&lt;/strong&gt;을 제안하고, 데이터 큐레이션 도구로서의 활용 가능성을 입증하는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;본 연구는 &lt;strong&gt;Generative Value Learning (GVL)&lt;/strong&gt; 접근 방식을 기반으로, &lt;strong&gt;Vision-Language Models (VLMs)&lt;/strong&gt;이 시각적 관측으로부터 작업 진행도를 예측하도록 합니다. &lt;strong&gt;OpenGVL 벤치마크&lt;/strong&gt;는 다양한 조작 작업(로봇 및 인간 수행)에 걸쳐 공개 소스 및 독점 기반 모델의 성능을 평가하며, 예측 품질은 &lt;strong&gt;Value-Order Correlation (VOC)&lt;/strong&gt; 지표(예측 값과 셔플된 프레임 간의 순위 상관관계)를 통해 측정합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;공개 소스 &lt;strong&gt;VLM&lt;/strong&gt;은 작업 진행도 예측에서 독점 모델 성능의 약 &lt;strong&gt;70%&lt;/strong&gt; 수준에 머물러 상당한 성능 격차를 보였으며, &lt;strong&gt;Gemma&lt;/strong&gt; 및 &lt;strong&gt;Qwen&lt;/strong&gt; 계열 모델의 경우 &lt;strong&gt;VLM&lt;/strong&gt; 규모가 &lt;strong&gt;VOC 점수&lt;/strong&gt; 향상으로 이어졌습니다. &lt;strong&gt;MiMo-VL-7B-RL-2508&lt;/strong&gt; 및 &lt;strong&gt;GLM-4.1V-9B-Thinking&lt;/strong&gt;은 공개 소스 추론 모델 중 우수한 성능을 나타냈으나, &lt;strong&gt;Kimi-VL-A3B&lt;/strong&gt;는 부진했습니다. &lt;strong&gt;OpenGVL&lt;/strong&gt;은 또한 불분명한 작업 정의, 레이블 모호성, OOD(Out-Of-Distribution) 예시 등 데이터 품질 문제를 효과적으로 식별할 수 있음을 보여주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;OpenGVL&lt;/strong&gt;은 로봇 공학 대규모 데이터셋의 &lt;strong&gt;자동화된 큐레이션 및 필터링&lt;/strong&gt;을 위한 실용적인 도구로 활용될 수 있으며, 효율적인 데이터 품질 평가를 가능하게 합니다. 공개 소스 &lt;strong&gt;VLM&lt;/strong&gt;과 독점 모델 간의 성능 차이는 로봇 작업에 필요한 &lt;strong&gt;정교한 공간 추론 능력&lt;/strong&gt;을 갖춘 공개 소스 모델 개발의 시급성을 시사합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-OpenGVL_-_Benchmarking_Visual_Temporal_Progress_for_Data_Curation/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-OpenGVL_-_Benchmarking_Visual_Temporal_Progress_for_Data_Curation/</guid>
        
        <category>Review</category>
        
        <category>Robotics Data Curation</category>
        
        <category>Visual Temporal Progress</category>
        
        <category>Generative Value Learning (GVL)</category>
        
        <category>Vision-Language Models (VLMs)</category>
        
        <category>Benchmark</category>
        
        <category>Task Progress Prediction</category>
        
        <category>Value-Order Correlation (VOC)</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18154&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, et al.&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 급속히 발전하는 Multimodal Large Language Models (MLLMs)의 고질적인 훈련 및 추론 효율성 문제를 해결하는 것을 목표로 합니다. 특히, 시각 토큰 수 증가로 인한 연산 오버헤드, 문서 지식 학습의 데이터 엔지니어링 복잡성, 그리고 강화 학습 기반 추론 모델의 과도한 응답 길이라는 세 가지 주요 병목 현상에 집중하여, &lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;를 통해 고성능과 고효율을 동시에 달성하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;연구진은 세 가지 핵심 개선 사항을 제시합니다. 첫째, 이미지와 비디오를 고도로 압축 인코딩하는 &lt;strong&gt;Unified 3D-Resampler&lt;/strong&gt; 아키텍처를 도입하여 비디오 토큰 비용을 12~24배 절감합니다. 둘째, 문서 이미지에서 직접 지식과 텍스트 인식을 학습하는 &lt;strong&gt;통합 학습 패러다임&lt;/strong&gt;을 제안하며, 동적 시각 손상 기법으로 외부 파서 의존성을 제거하고 OCR 및 맥락 기반 추론을 통합합니다. 셋째, 효율적인 짧은 추론 모드와 복잡한 긴 추론 모드를 모두 지원하는 &lt;strong&gt;하이브리드 강화 학습(RL) 전략&lt;/strong&gt;을 통해 훈련 및 추론 효율성을 향상시킵니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;는 &lt;strong&gt;OpenCompass&lt;/strong&gt; 종합 평가에서 &lt;strong&gt;GPT-4o-latest&lt;/strong&gt; 및 &lt;strong&gt;Qwen2.5-VL 72B&lt;/strong&gt;와 같은 대형 모델들을 능가하는 &lt;strong&gt;77.0+&lt;/strong&gt;의 뛰어난 성능을 보였습니다. 특히, &lt;strong&gt;VideoMME&lt;/strong&gt; 벤치마크에서는 30B 미만 모델 중 최고 성능을 달성하면서 &lt;strong&gt;Qwen2.5-VL 7B&lt;/strong&gt; 대비 &lt;strong&gt;46.7%의 GPU 메모리&lt;/strong&gt;와 &lt;strong&gt;8.7%의 추론 시간&lt;/strong&gt;만을 사용하며 독보적인 효율성을 입증했습니다. 이는 &lt;strong&gt;3D-Resampler&lt;/strong&gt;의 효율성과 하이브리드 RL 전략의 효과를 명확히 보여줍니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;는 MLLM의 효율성 병목 현상 해결을 위한 실용적인 “레시피”를 제공합니다. &lt;strong&gt;Unified 3D-Resampler&lt;/strong&gt;는 고해상도 이미지 및 비디오 처리를 위한 GPU 메모리 및 추론 시간 절감에 크게 기여하여 MLLM의 접근성과 확장성을 높일 수 있습니다. 또한, 문서 지식 및 OCR의 &lt;strong&gt;통합 학습 패러다임&lt;/strong&gt;은 데이터 전처리 복잡성을 줄이고 모델의 견고성을 향상시키며, &lt;strong&gt;하이브리드 RL 전략&lt;/strong&gt;은 다양한 추론 요구사항에 유연하게 대응할 수 있는 모델 개발 방향을 제시합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-MiniCPM-V_4.5_Cooking_Efficient_MLLMs_via_Architecture_Data_and_Training_Recipe/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-MiniCPM-V_4.5_Cooking_Efficient_MLLMs_via_Architecture_Data_and_Training_Recipe/</guid>
        
        <category>Review</category>
        
        <category>MLLM Efficiency</category>
        
        <category>Multimodal Transformer</category>
        
        <category>3D-Resampler</category>
        
        <category>Document AI</category>
        
        <category>Hybrid Reinforcement Learning</category>
        
        <category>Video Understanding</category>
        
        <category>Efficient Inference</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] MAPO: Mixed Advantage Policy Optimization</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18849&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, et al.&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 연구는 파운데이션 모델의 추론 성능 향상을 위한 기존 강화 학습(RL) 방법론, 특히 &lt;strong&gt;Group Relative Policy Optimization (GRPO)&lt;/strong&gt;이 겪는 “advantage reversion” 및 “advantage mirror” 문제 해결을 목표로 합니다. 이러한 문제는 고정된 어드밴티지 함수 공식으로 인해 궤적 확실성(trajectory certainty)이 다양한 샘플 간 어드밴티지 할당을 저해하여 모델 학습을 불안정하게 만듭니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;저자들은 &lt;strong&gt;Mixed Advantage Policy Optimization (MAPO)&lt;/strong&gt;을 제안하며, 궤적 결과를 &lt;strong&gt;베르누이 분포&lt;/strong&gt;로 모델링하여 궤적 확실성 &lt;strong&gt;p&lt;/strong&gt;를 추정합니다. 높은 확실성을 가진 궤적에 대해서는 기존 &lt;strong&gt;z-점수 정규화&lt;/strong&gt; 대신 &lt;strong&gt;Advantage Percent Deviation (APD)&lt;/strong&gt;(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r_i - \mu / \mu&lt;/code&gt;)을 도입하여 어드밴티지 문제를 해결합니다. 또한, &lt;strong&gt;Trajectory Certainty Reweight (TCR)&lt;/strong&gt;를 통해 궤적 확실성 &lt;strong&gt;p&lt;/strong&gt;에 따라 &lt;strong&gt;APD&lt;/strong&gt;와 기존의 표준편차 기반 어드밴티지(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r_i - \mu / \sigma&lt;/code&gt;)를 &lt;strong&gt;동적으로 가중 평균&lt;/strong&gt;(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\hat{A}_i = (1 - \lambda(p)) \cdot \frac{r_i - \mu}{\sigma} + \lambda(p) \cdot \frac{r_i - \mu}{\mu}&lt;/code&gt;)하여 최종 어드밴티지 함수를 구성합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MAPO&lt;/strong&gt;는 &lt;strong&gt;Qwen2.5-VL-7B-Instruct&lt;/strong&gt; 모델을 기반으로 수학 및 감성 추론 태스크에서 기존 &lt;strong&gt;GRPO&lt;/strong&gt; 및 &lt;strong&gt;DAPO&lt;/strong&gt;보다 일관되게 우수한 성능을 보였습니다. 롤아웃 수 &lt;strong&gt;G=12&lt;/strong&gt;에서 수학 추론 태스크에서 &lt;strong&gt;51.26%&lt;/strong&gt;, 감성 추론 태스크에서 &lt;strong&gt;66.77%&lt;/strong&gt;의 가장 높은 전체 정확도를 달성했습니다. 이는 &lt;strong&gt;MAPO&lt;/strong&gt;가 어드밴티지 역전 및 미러 문제를 효과적으로 완화하고 파운데이션 모델의 안정적이고 정확한 추론 성능을 보장함을 입증합니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MAPO&lt;/strong&gt;는 추가적인 모델 아키텍처나 복잡한 하이퍼파라미터 튜닝 없이 &lt;strong&gt;파운데이션 모델&lt;/strong&gt;의 추론 능력을 강화하는 실용적인 방법을 제시합니다. 특히, &lt;strong&gt;다양한 궤적 확실성 수준&lt;/strong&gt;을 가진 샘플에 대한 어드밴티지 할당을 최적화함으로써 모델이 더 신뢰성 있는 방향으로 학습되도록 유도합니다. 이는 &lt;strong&gt;대규모 언어 모델(LLM)&lt;/strong&gt; 및 &lt;strong&gt;멀티모달 LLM(MLLM)&lt;/strong&gt;의 후처리 강화 학습 단계에서 추론 성능과 안정성을 효과적으로 개선할 수 있는 강력한 전략으로 활용될 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-MAPO_Mixed_Advantage_Policy_Optimization/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-MAPO_Mixed_Advantage_Policy_Optimization/</guid>
        
        <category>Review</category>
        
        <category>Reinforcement Learning</category>
        
        <category>Foundation Models</category>
        
        <category>Policy Optimization</category>
        
        <category>Advantage Function</category>
        
        <category>Trajectory Certainty</category>
        
        <category>Multimodal Reasoning</category>
        
        <category>GRPO</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.19296&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문의 핵심 목표는 &lt;strong&gt;실세계 다중 뷰 데이터 없이&lt;/strong&gt; 단일 이미지 또는 비디오 입력으로부터 고품질의 3D 및 4D 장면을 생성하는 것입니다. 이를 위해 비디오 확산 모델에 내재된 암묵적인 3D 지식을 명시적인 &lt;strong&gt;3D Gaussian Splatting (3DGS)&lt;/strong&gt; 표현으로 증류하여 기존 3D 재구성 방법의 데이터 부족 문제를 해결하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;Lyra는 &lt;strong&gt;self-distillation 프레임워크&lt;/strong&gt;를 제안하며, 여기서 사전 훈련된 &lt;strong&gt;카메라 제어 비디오 확산 모델 (GEN3C)&lt;/strong&gt;이 교사 역할을 하고, &lt;strong&gt;3DGS 디코더&lt;/strong&gt;가 학생 역할을 합니다. 이 &lt;strong&gt;3DGS 디코더&lt;/strong&gt;는 비디오 확산 모델의 잠재 공간에서 작동하며, 교사 모델의 RGB 디코더 출력과 렌더링된 3DGS 뷰 간의 &lt;strong&gt;image-based reconstruction loss (MSE + LPIPS)&lt;/strong&gt; 및 &lt;strong&gt;ViPE&lt;/strong&gt;로 추정된 깊이 맵을 활용하는 &lt;strong&gt;depth loss&lt;/strong&gt;로 학습됩니다. 또한, 다이내믹 씬을 위해 &lt;strong&gt;시간-조건부 3DGS&lt;/strong&gt;와 &lt;strong&gt;동적 데이터 증강 기법&lt;/strong&gt;을 사용하여 시간적 일관성을 확보합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;Lyra는 정적 3D 및 동적 4D 장면 생성에서 모두 최첨단 성능을 달성했습니다. 정적 3D 재구성 벤치마크인 &lt;strong&gt;RealEstate10K&lt;/strong&gt;에서 &lt;strong&gt;PSNR 21.79, SSIM 0.752, LPIPS 0.219&lt;/strong&gt;를 기록하여 이전 모델인 &lt;strong&gt;Bolt3D (PSNR 21.54)&lt;/strong&gt;를 능가했습니다. 동적 4D 재구성에서는 &lt;strong&gt;PSNR 23.07, SSIM 0.779, LPIPS 0.231&lt;/strong&gt;을 달성하며 &lt;strong&gt;BTimer (GEN3C)&lt;/strong&gt; 대비 우수한 성능을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;이 연구는 고비용의 &lt;strong&gt;다중 뷰 실세계 데이터&lt;/strong&gt; 수집 필요성을 제거하여 3D/4D 장면 생성의 접근성을 크게 높입니다. 생성된 &lt;strong&gt;명시적인 3DGS 표현&lt;/strong&gt;은 실시간 렌더링을 지원하며 &lt;strong&gt;로봇 시뮬레이션&lt;/strong&gt; 및 &lt;strong&gt;물리 기반 AI&lt;/strong&gt;와 같은 다운스트림 애플리케이션에 직접 적용될 수 있습니다. &lt;strong&gt;비디오 확산 모델의 잠재 공간&lt;/strong&gt;에서 작동하는 방식은 여러 뷰를 효율적으로 처리하며 메모리 오버헤드를 줄여 실용적인 AI 시스템 개발에 기여할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-Lyra_Generative_3D_Scene_Reconstruction_via_Video_Diffusion_Model_Self-Distillation/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-Lyra_Generative_3D_Scene_Reconstruction_via_Video_Diffusion_Model_Self-Distillation/</guid>
        
        <category>Review</category>
        
        <category>Generative AI</category>
        
        <category>3D Scene Reconstruction</category>
        
        <category>Video Diffusion Models</category>
        
        <category>Self-Distillation</category>
        
        <category>3D Gaussian Splatting</category>
        
        <category>Dynamic 4D Generation</category>
        
        <category>Monocular Input</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Large Language Models Discriminate Against Speakers of German Dialects</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.13835&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Minh Duc Bui, Carolin Holtermann, Valentin Hofmann, Anne Lauscher, Katharina von der Wense&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 대규모 언어 모델(LLMs)이 독일 방언 사용자에 대한 사회적 고정관념을 반영하고 강화하는지 탐구하는 것을 목표로 합니다. 특히, 독일 인구의 &lt;strong&gt;40% 이상&lt;/strong&gt;이 지역 방언을 사용하는 상황에서, LLM의 편향이 실제 세계에 미칠 수 있는 차별적 영향을 분석하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;연구는 &lt;strong&gt;연관성 작업&lt;/strong&gt;과 &lt;strong&gt;의사결정 작업&lt;/strong&gt;의 두 가지 방식으로 LLM의 편향을 평가합니다. &lt;strong&gt;방언 명명 편향&lt;/strong&gt;은 사용자의 언어적 배경을 명시적으로 언급하여 측정하고, &lt;strong&gt;방언 사용 편향&lt;/strong&gt;은 표준 독일어와 7개 지역 독일어 방언(예: &lt;strong&gt;알레만어&lt;/strong&gt;, &lt;strong&gt;바이에른어&lt;/strong&gt;)으로 작성된 텍스트를 통해 간접적으로 유도합니다. 이를 위해 &lt;strong&gt;새로운 병렬 평가 코퍼스&lt;/strong&gt;가 구축되었으며, &lt;strong&gt;Llama-3.1&lt;/strong&gt;, &lt;strong&gt;Qwen 2.5&lt;/strong&gt;, &lt;strong&gt;Gemma 3&lt;/strong&gt;, &lt;strong&gt;GPT-5 Mini&lt;/strong&gt; 등 9개 이상의 LLM이 평가되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;모든 LLM은 독일 방언 사용자에 대한 &lt;strong&gt;현저한 방언 명명 및 방언 사용 편향&lt;/strong&gt;을 보였으며, 이는 부정적인 형용사 연관성(예: &lt;strong&gt;“무교육적”&lt;/strong&gt;)으로 나타났습니다. 특히, &lt;strong&gt;Llama-3.1 70B&lt;/strong&gt;는 방언 사용자를 “무교육적” 특성과 유의미하게 연결했으며, 의사결정 작업에서 낮은 교육 수준의 직업을 할당했습니다. 흥미롭게도, &lt;strong&gt;명시적인 언어적 배경 언급이 암묵적인 단서보다 편향을 증폭&lt;/strong&gt;시켰으며, 동일 계열 내 &lt;strong&gt;더 큰 LLM일수록 더 강력한 편향&lt;/strong&gt;을 나타냈습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;본 연구 결과는 LLM이 독일 방언 사용자에게 &lt;strong&gt;심각한 편향&lt;/strong&gt;을 보이며, 이는 인사 선발과 같은 실세계 응용에서 차별적인 결과를 초래할 수 있음을 시사합니다. AI 개발자들은 특히 다국어 및 방언 사용자들을 위한 LLM 배포 시 이러한 &lt;strong&gt;언어적 차별 편향&lt;/strong&gt;을 적극적으로 인식하고 완화하기 위한 노력을 기울여야 합니다. 이는 LLM의 공정성과 형평성을 확보하기 위한 중요한 고려 사항입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-Large_Language_Models_Discriminate_Against_Speakers_of_German_Dialects/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-Large_Language_Models_Discriminate_Against_Speakers_of_German_Dialects/</guid>
        
        <category>Review</category>
        
        <category>Large Language Models</category>
        
        <category>Bias</category>
        
        <category>German Dialects</category>
        
        <category>Sociolinguistics</category>
        
        <category>Stereotypes</category>
        
        <category>Implicit Association Test</category>
        
        <category>Decision Making</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18824&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Yanzuo Lu, Xin Xia, Manlin Zhang, Huafeng Kuang, Jianbin Zheng, Yuxi Ren, Xuefeng Xiao&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;통합 멀티모달 모델에서 확산 디노이징과 자기회귀 디코딩의 반복적인 프로세스로 발생하는 상당한 &lt;strong&gt;계산 오버헤드&lt;/strong&gt;를 해결하는 것이 주 목표입니다. &lt;strong&gt;Hyper-Bagel&lt;/strong&gt;이라는 통합 가속 프레임워크를 제안하여 멀티모달 이해 및 생성 작업을 동시에 가속화하면서 원본 모델의 고품질 출력을 유지하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Hyper-Bagel&lt;/strong&gt;은 이해 작업을 위한 &lt;strong&gt;스펙큘레이티브 디코딩&lt;/strong&gt;과 생성 작업을 위한 &lt;strong&gt;다단계 확산 증류(multi-stage diffusion distillation)&lt;/strong&gt; 전략을 사용합니다. 스펙큘레이티브 디코딩은 &lt;strong&gt;새로운 중간 계층 아키텍처&lt;/strong&gt;와 &lt;strong&gt;제로-초기화 기법&lt;/strong&gt;으로 개선되었으며, 확산 증류는 &lt;strong&gt;CFG 증류&lt;/strong&gt;, &lt;strong&gt;TSCD(Trajectory Segmented Consistency Distillation)&lt;/strong&gt;, 그리고 &lt;strong&gt;DMDO(Distribution Matching Distillation via ODE)&lt;/strong&gt;로 구성됩니다. 또한, &lt;strong&gt;1-NFE 모델&lt;/strong&gt;은 &lt;strong&gt;ADP(Adversarial Diffusion Pre-training)&lt;/strong&gt; 및 &lt;strong&gt;ReFL(Reward Feedback Learning)&lt;/strong&gt;을 통해 미세 조정되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;이 프레임워크는 멀티모달 이해에서 &lt;strong&gt;2배 이상의 속도 향상&lt;/strong&gt;을 달성했습니다. 생성 작업의 경우, 손실 없는 &lt;strong&gt;6-NFE 모델&lt;/strong&gt;은 텍스트-투-이미지 생성에서 &lt;strong&gt;16.67배&lt;/strong&gt;, 이미지 편집에서 &lt;strong&gt;22배&lt;/strong&gt;의 속도 향상을 이루었으며, 원본 모델과 동등하거나 우수한 품질을 유지했습니다. 특히, &lt;strong&gt;6-NFE Hyper-BAGEL&lt;/strong&gt;은 GenEval에서 &lt;strong&gt;0.8647&lt;/strong&gt;점을 기록하여 기준 모델을 능가했으며, GEdit-Bench에서도 &lt;strong&gt;6.612(영어)&lt;/strong&gt; 및 &lt;strong&gt;6.671(중국어)&lt;/strong&gt;로 뛰어난 성능을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Hyper-Bagel&lt;/strong&gt;은 대규모 멀티모달 모델의 &lt;strong&gt;추론 속도와 비용 효율성을 혁신적&lt;/strong&gt;으로 개선하여 실무 배포 가능성을 크게 높였습니다. &lt;strong&gt;스펙큘레이티브 디코딩&lt;/strong&gt;과 &lt;strong&gt;다단계 증류&lt;/strong&gt;는 복잡한 AI 모델의 지연 시간을 줄이고 사용자 경험을 향상시키는 데 중요한 기술적 통찰을 제공합니다. 특히 &lt;strong&gt;1-NFE 모델&lt;/strong&gt;은 실시간 상호작용이 필수적인 대화형 이미지 편집 및 생성 애플리케이션에서 &lt;strong&gt;즉각적이고 끊김 없는 사용자 경험&lt;/strong&gt;을 구현하는 데 핵심적인 역할을 할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-Hyper-Bagel_A_Unified_Acceleration_Framework_for_Multimodal_Understanding_and_Generation/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-Hyper-Bagel_A_Unified_Acceleration_Framework_for_Multimodal_Understanding_and_Generation/</guid>
        
        <category>Review</category>
        
        <category>Multimodal AI</category>
        
        <category>Acceleration Framework</category>
        
        <category>Speculative Decoding</category>
        
        <category>Diffusion Distillation</category>
        
        <category>Unified Models</category>
        
        <category>Text-to-Image Generation</category>
        
        <category>Image Editing</category>
        
        <category>Computational Efficiency</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17083&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Zipeng Wang, Dan Xu&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;3D Gaussian Splatting (3DGS)&lt;/strong&gt;의 실시간 고품질 렌더링 장점은 유지하면서, 뷰-의존적 효과 및 이방성 모양 모델링으로 인한 &lt;strong&gt;막대한 메모리 오버헤드&lt;/strong&gt;를 해결하는 것을 목표로 합니다. 기존 &lt;strong&gt;Neural Field&lt;/strong&gt; 기반 압축 방식이 고주파 공간 변화를 포착하는 데 어려움을 겪는 한계를 극복하고, 고주파 디테일을 보존하면서도 메모리 효율적인 &lt;strong&gt;하이브리드 장면 표현&lt;/strong&gt;을 제시하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;장면을 &lt;strong&gt;명시적 컴팩트 가우시안&lt;/strong&gt;과 &lt;strong&gt;그리드 기반 Neural Fields&lt;/strong&gt;의 두 가지 상호 보완적인 구성 요소로 분해합니다. &lt;strong&gt;명시적 가우시안&lt;/strong&gt;은 3D 위치, 등방성 스케일, 불투명도, 확산 색상 등 필수 고주파 파라미터만 저장합니다. &lt;strong&gt;Neural Fields&lt;/strong&gt;는 기하학적 속성(&lt;strong&gt;스케일, 불투명도, 회전&lt;/strong&gt;)과 뷰-의존적 색상(&lt;strong&gt;view-dependent color&lt;/strong&gt;)을 각각 별도의 &lt;strong&gt;decoupled neural field architecture&lt;/strong&gt;를 통해 예측합니다. 또한, &lt;strong&gt;visibility pre-culling&lt;/strong&gt;과 &lt;strong&gt;Neural Field&lt;/strong&gt;로 예측된 배경 맵을 가우시안 렌더링과 합성하는 &lt;strong&gt;하이브리드 렌더링 파이프라인&lt;/strong&gt;을 제안합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;제안하는 &lt;strong&gt;HyRF&lt;/strong&gt;는 &lt;strong&gt;3DGS&lt;/strong&gt; 대비 모델 크기를 &lt;strong&gt;20배 이상&lt;/strong&gt; (평균 &lt;strong&gt;34MB&lt;/strong&gt; vs &lt;strong&gt;676MB&lt;/strong&gt;), &lt;strong&gt;Scaffold-GS&lt;/strong&gt; 대비 &lt;strong&gt;1.5배에서 5배&lt;/strong&gt;까지 줄이면서도 &lt;strong&gt;state-of-the-art 렌더링 품질&lt;/strong&gt;을 달성했습니다. 특히, &lt;strong&gt;Deep Blending&lt;/strong&gt; 데이터셋에서 &lt;strong&gt;PSNR 30.37, SSIM 0.910&lt;/strong&gt;의 높은 점수를 기록했습니다. 렌더링 속도 면에서도 &lt;strong&gt;114 FPS&lt;/strong&gt;로 &lt;strong&gt;3DGS&lt;/strong&gt;와 유사한 &lt;strong&gt;실시간 성능&lt;/strong&gt;을 유지했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;3DGS&lt;/strong&gt;의 주요 단점인 과도한 메모리 사용량을 획기적으로 개선하여, &lt;strong&gt;제한된 컴퓨팅 자원&lt;/strong&gt;에서도 고품질 &lt;strong&gt;Novel View Synthesis&lt;/strong&gt;를 가능하게 합니다. 기하학과 외관 속성을 분리 학습하는 &lt;strong&gt;decoupled neural field&lt;/strong&gt; 설계는 모델의 &lt;strong&gt;표현 능력과 파라미터 효율성&lt;/strong&gt;을 동시에 높여 다양한 응용 분야에 적용될 수 있습니다. 또한, &lt;strong&gt;Neural Field&lt;/strong&gt; 기반의 배경 맵은 원거리 객체 렌더링 품질을 향상시켜 대규모 장면 모델링에 대한 실용적인 솔루션을 제공합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-HyRF_Hybrid_Radiance_Fields_for_Memory-efficient_and_High-quality_Novel_View_Synthesis/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-HyRF_Hybrid_Radiance_Fields_for_Memory-efficient_and_High-quality_Novel_View_Synthesis/</guid>
        
        <category>Review</category>
        
        <category>Novel View Synthesis</category>
        
        <category>3D Gaussian Splatting (3DGS)</category>
        
        <category>Neural Radiance Fields (NeRF)</category>
        
        <category>Memory Efficiency</category>
        
        <category>High-Quality Rendering</category>
        
        <category>Hybrid Representation</category>
        
        <category>Real-time Rendering</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18090&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Jiahe Li, Jiawei Zhang, Youmin Zhang, Xiao Bai, Jin Zheng, Xiaohan Yu, Lin Gu&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 기존 &lt;strong&gt;3D Gaussian Splatting (3DGS)&lt;/strong&gt; 기반 표면 재구성 방법론의 한계, 즉 초기화 시 &lt;strong&gt;점군(point clouds)&lt;/strong&gt;에 대한 의존성, 불완전한 커버리지, 모호한 기하학적 표현 등의 문제를 해결하는 것을 목표로 합니다. &lt;strong&gt;명시적 희소 복셀(sparse voxels)&lt;/strong&gt;을 활용하여 기하학적으로 정확하고 상세하며 완전한 표면 재구성을 달성하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;제안하는 &lt;strong&gt;GeoSVR&lt;/strong&gt;은 &lt;strong&gt;SVRaster&lt;/strong&gt;를 기반으로 희소 복셀을 사용하여 장면을 표현하고 최적화합니다. 불확실한 기하학적 영역을 식별하고 외부 단안 깊이 정보를 신뢰도에 따라 활용하기 위해 &lt;strong&gt;Voxel-Uncertainty Depth Constraint&lt;/strong&gt;를 도입합니다. 또한, 희소 복셀의 국소적 한계를 극복하고 날카로운 표면 형성을 위해 &lt;strong&gt;Voxel Dropout&lt;/strong&gt;을 통한 &lt;strong&gt;Sparse Voxel Surface Regularization&lt;/strong&gt;, 그리고 표면을 복셀 밀도 필드에 정렬하는 &lt;strong&gt;Surface Rectification&lt;/strong&gt;, 부정확한 대형 복셀의 기여를 제한하는 &lt;strong&gt;Scaling Penalty&lt;/strong&gt;를 적용합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;DTU 데이터셋&lt;/strong&gt;에서 기존 SOTA 방법들을 능가하는 &lt;strong&gt;0.47&lt;/strong&gt;의 Chamfer distance를 달성하며 최고의 재구성 품질을 보였습니다. &lt;strong&gt;Tanks and Temples (TnT) 데이터셋&lt;/strong&gt;에서는 &lt;strong&gt;0.56&lt;/strong&gt;의 F1-score로 우수한 성능을 입증했습니다. &lt;strong&gt;Mip-NeRF 360 데이터셋&lt;/strong&gt;에서도 &lt;strong&gt;24.83 PSNR&lt;/strong&gt;의 경쟁력 있는 렌더링 품질을 유지하며, 높은 효율성으로 상세하고 완전한 표면 재구성을 제공합니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;명시적 희소 복셀 기반 프레임워크&lt;/strong&gt;가 &lt;strong&gt;3DGS&lt;/strong&gt;의 초기화 및 기하학적 모호성 한계를 극복하는 유망한 대안임을 제시합니다. &lt;strong&gt;Voxel-Uncertainty Depth Constraint&lt;/strong&gt;는 외부 &lt;strong&gt;단안 깊이 추정&lt;/strong&gt;과 같은 불확실한 보조 정보를 효과적이고 유연하게 활용하는 실용적인 접근 방식을 제공합니다. &lt;strong&gt;GeoSVR&lt;/strong&gt;은 높은 기하학적 정확도와 상세도를 유지하면서 &lt;strong&gt;빠른 추론 속도&lt;/strong&gt;를 제공하여, 실시간 3D 재구성 및 디지털 트윈, 가상 현실 등의 응용 분야에서 잠재적 활용 가치가 높습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-GeoSVR_Taming_Sparse_Voxels_for_Geometrically_Accurate_Surface_Reconstruction/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-GeoSVR_Taming_Sparse_Voxels_for_Geometrically_Accurate_Surface_Reconstruction/</guid>
        
        <category>Review</category>
        
        <category>Surface Reconstruction</category>
        
        <category>Sparse Voxels</category>
        
        <category>Geometric Accuracy</category>
        
        <category>Neural Radiance Fields</category>
        
        <category>3D Gaussian Splatting</category>
        
        <category>Monocular Depth</category>
        
        <category>Voxel Uncertainty</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Do You Need Proprioceptive States in Visuomotor Policies?</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18644&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Juntu Zhao, Wenbo Lu, Di Zhang, Yufeng Liu, Yushen Liang&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 연구는 로봇의 시각-운동 정책(visuomotor policies)에서 고유 수용성 상태(proprioceptive states)의 필요성을 재평가하고, 기존 상태 기반 정책이 학습 궤적에 과적합되어 공간 일반화 능력이 저해되는 문제를 해결하고자 합니다. 궁극적으로 고유 수용성 상태를 제거한 “State-free Policies”를 제안하여 실세계 로봇 애플리케이션의 실용성을 높이는 것이 목표입니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;제안하는 &lt;strong&gt;State-free Policy&lt;/strong&gt;는 고유 수용성 상태 입력을 완전히 제거하고, 오직 시각 관측에만 기반하여 행동을 예측합니다. 이를 위해 &lt;strong&gt;상대 End-Effector (EEF) 액션 공간&lt;/strong&gt;을 사용하고, &lt;strong&gt;듀얼 광각 손목 카메라&lt;/strong&gt;를 통해 충분한 “전체 태스크 관련 시각 정보(full task observation)”를 확보합니다. 다양한 로봇 기종과 태스크(예: &lt;strong&gt;π0, ACT, Diffusion Policy&lt;/strong&gt; 등)에서 &lt;strong&gt;실세계 환경 및 시뮬레이션&lt;/strong&gt;을 통해 정책의 성능을 평가했습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;State-free Policy는 상태 기반 정책 대비 현저히 향상된 공간 일반화 능력을 보였습니다. &lt;strong&gt;Pick Pen 태스크&lt;/strong&gt;의 경우, 높이 일반화 성공률은 &lt;strong&gt;0%에서 98%로&lt;/strong&gt;, 수평 일반화는 &lt;strong&gt;0%에서 58%로&lt;/strong&gt; 개선되었습니다. 다양한 실세계 태스크에서 평균 성공률이 높이 일반화는 &lt;strong&gt;0%에서 85%로&lt;/strong&gt;, 수평 일반화는 &lt;strong&gt;6%에서 64%로&lt;/strong&gt; 향상되었습니다. 또한, &lt;strong&gt;데이터 효율성&lt;/strong&gt;과 &lt;strong&gt;기종 간 적응 능력&lt;/strong&gt;에서도 상당한 이점을 입증하였으며, &lt;strong&gt;오버헤드 카메라 제거&lt;/strong&gt;가 공간 일반화 능력을 추가적으로 향상시킬 수 있음도 발견했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;이 연구는 로봇 제어 정책의 &lt;strong&gt;공간 일반화 문제&lt;/strong&gt;를 해결하는 실용적인 방법을 제시하며, &lt;strong&gt;고유 수용성 상태 제거&lt;/strong&gt;가 데이터 수집 비용 절감과 &lt;strong&gt;다양한 로봇 시스템으로의 전이 학습&lt;/strong&gt; 효율 증대에 기여함을 보여줍니다. &lt;strong&gt;듀얼 광각 손목 카메라&lt;/strong&gt;와 같은 센서 구성은 태스크 관련 시각 정보를 충분히 제공하여 정책의 견고성을 높이며, 이는 &lt;strong&gt;더욱 일반화되고 유연한 로봇 시스템 개발&lt;/strong&gt;에 중요한 방향을 제시합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-Do_You_Need_Proprioceptive_States_in_Visuomotor_Policies/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-Do_You_Need_Proprioceptive_States_in_Visuomotor_Policies/</guid>
        
        <category>Review</category>
        
        <category>Visuomotor Policies</category>
        
        <category>Spatial Generalization</category>
        
        <category>Imitation Learning</category>
        
        <category>Proprioception</category>
        
        <category>State-free Policies</category>
        
        <category>Robot Manipulation</category>
        
        <category>End-Effector Control</category>
        
        <category>Data Efficiency</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.19300&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Chen Chen, Pengsheng Guo, Liangchen Song, Jiasen Lu, Rui Qian, Xinze Wang, Tsu-Jui Fu, Wei Liu, Yinfei Yang, Alex Schwing&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;조건부 생성 모델에서 속도 네트워크가 데이터 분포의 &lt;strong&gt;질량 이동(mass transport)&lt;/strong&gt;과 &lt;strong&gt;조건 정보 인코딩(conditional injection)&lt;/strong&gt;이라는 두 가지 과제를 동시에 처리해야 하는 부담을 완화하는 것이 주요 목표입니다. 이를 통해 모델 학습을 가속화하고 생성 품질을 향상시키고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;논문은 &lt;strong&gt;CAR-Flow (Condition-Aware Reparameterization for Flow Matching)&lt;/strong&gt;를 제안합니다. 이는 &lt;strong&gt;학습 가능한 경량 시프트 맵&lt;/strong&gt;(&lt;strong&gt;f(x0, y) = x0 + μ0(y)&lt;/strong&gt; 및 &lt;strong&gt;g(x1, y) = x1 + μ1(y)&lt;/strong&gt;)을 사용하여 소스, 타겟 또는 두 분포 모두를 조건에 따라 재매개변수화하는 방법입니다. 특히, 무제한 재매개변수화가 &lt;strong&gt;모드 붕괴(mode collapse)&lt;/strong&gt;를 유발하는 &lt;strong&gt;영비용 해(zero-cost solutions)&lt;/strong&gt;를 유도함을 이론적으로 분석하고, 이를 방지하기 위해 &lt;strong&gt;시프트 전용(shift-only)&lt;/strong&gt; 제약을 가합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;ImageNet-256&lt;/strong&gt; 데이터셋에서 &lt;strong&gt;SiT-XL/2&lt;/strong&gt; 모델에 &lt;strong&gt;CAR-Flow Joint&lt;/strong&gt; 버전을 적용한 결과, &lt;strong&gt;FID(Fréchet Inception Distance)&lt;/strong&gt;를 &lt;strong&gt;2.07에서 1.68로 감소&lt;/strong&gt;시켰으며, 이는 &lt;strong&gt;0.6% 미만의 추가 매개변수&lt;/strong&gt;만으로 달성되었습니다. 합성 데이터 실험에서는 &lt;strong&gt;CAR-Flow&lt;/strong&gt;가 Wasserstein 거리를 크게 줄여 더 빠른 수렴과 향상된 정렬을 보였으며, &lt;strong&gt;무제한 재매개변수화&lt;/strong&gt; 시 모드 붕괴가 실제로 발생함을 입증했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;CAR-Flow&lt;/strong&gt;는 조건부 생성 모델의 성능을 효율적으로 개선할 수 있는 강력한 기법으로, &lt;strong&gt;최소한의 추가 비용&lt;/strong&gt;으로 &lt;strong&gt;SiT-XL/2&lt;/strong&gt;와 같은 대규모 모델에 통합될 수 있습니다. 특히 &lt;strong&gt;소스 및 타겟 분포에 모두 조건부 시프트&lt;/strong&gt;를 적용하는 &lt;strong&gt;Joint variant&lt;/strong&gt;가 가장 우수한 결과를 보여주므로, 실제 애플리케이션에서 이를 우선적으로 고려할 수 있습니다. 생성 모델 설계 시 &lt;strong&gt;무제한 재매개변수화&lt;/strong&gt;의 위험성과 &lt;strong&gt;shift-only 제약&lt;/strong&gt;의 중요성을 이해하는 것이 중요합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-CAR-Flow_Condition-Aware_Reparameterization_Aligns_Source_and_Target_for_Better_Flow_Matching/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-CAR-Flow_Condition-Aware_Reparameterization_Aligns_Source_and_Target_for_Better_Flow_Matching/</guid>
        
        <category>Review</category>
        
        <category>Flow Matching</category>
        
        <category>Conditional Generative Models</category>
        
        <category>Reparameterization</category>
        
        <category>Mode Collapse</category>
        
        <category>Image Generation</category>
        
        <category>Latent Space Alignment</category>
        
        <category>Diffusion Models</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18174&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Khalil Hennara, Muhammad Hreden, Mohamed Motasim Hamed, Ahmad Bastati, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 필기체 스크립트, 다양한 글꼴, 발음 기호, 우-좌향 텍스트 방향성으로 인해 어려운 아랍어 문서 OCR의 과제를 해결하고자 합니다. 기존 멀티모달 대규모 언어 모델(MLLM)의 아랍어 문서 이해 능력 한계를 극복하고, 아랍어 문서 OCR을 위한 &lt;strong&gt;State-of-the-Art 성능&lt;/strong&gt;을 달성하는 &lt;strong&gt;Baseer&lt;/strong&gt; 모델을 개발하는 것이 목표입니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Qwen2.5-VL-3B-Instruct&lt;/strong&gt; 모델을 기반으로, &lt;strong&gt;500k 쌍의 하이브리드 데이터셋&lt;/strong&gt;(300k 합성 문서 및 200k 실제 문서)을 활용하여 미세 조정했습니다. 특히, &lt;strong&gt;vision encoder는 동결&lt;/strong&gt;하고 &lt;strong&gt;language decoder만 업데이트&lt;/strong&gt;하는 &lt;strong&gt;decoder-only fine-tuning 전략&lt;/strong&gt;을 채택하여 일반적인 시각적 특징을 유지하면서 언어 모델을 아랍어 문서에 최적화했습니다. 또한, 최적의 컨텍스트 길이는 &lt;strong&gt;4096 토큰&lt;/strong&gt;으로 설정되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;Baseer는 새로 제안된 &lt;strong&gt;Misraj-DocOCR 벤치마크&lt;/strong&gt;에서 &lt;strong&gt;WER 0.25&lt;/strong&gt;를 달성하여 기존 오픈 소스 및 상용 솔루션보다 현저히 뛰어난 성능을 보였습니다. 수정된 &lt;strong&gt;KITAB-Bench PDF-to-Markdown 벤치마크&lt;/strong&gt;에서도 &lt;strong&gt;TEDS 56&lt;/strong&gt;과 &lt;strong&gt;MARS 68.13&lt;/strong&gt;을 기록하며 구조적 이해 능력에서 우수성을 입증했습니다. &lt;strong&gt;Decoder-only fine-tuning 전략&lt;/strong&gt;이 &lt;strong&gt;ChrF 89.79&lt;/strong&gt;로 가장 좋은 성능을 나타냈습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;본 연구는 일반적인 MLLM을 아랍어 OCR과 같이 &lt;strong&gt;도메인 특화된 복잡한 언어 처리&lt;/strong&gt;에 성공적으로 적용하는 효과적인 전략을 제시합니다. &lt;strong&gt;고품질의 대규모 하이브리드 데이터셋 구축&lt;/strong&gt;과 &lt;strong&gt;효율적인 fine-tuning 방법론&lt;/strong&gt;은 형태학적으로 풍부한 언어의 OCR 성능을 크게 향상시킬 수 있음을 보여줍니다. 공개된 &lt;strong&gt;Misraj-DocOCR 벤치마크&lt;/strong&gt;는 향후 아랍어 OCR 연구 및 시스템 평가에 중요한 표준 자료로 활용될 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 24 Sep 2025 13:14:19 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-24-Baseer_A_Vision-Language_Model_for_Arabic_Document-to-Markdown_OCR/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-24-Baseer_A_Vision-Language_Model_for_Arabic_Document-to-Markdown_OCR/</guid>
        
        <category>Review</category>
        
        <category>Arabic OCR</category>
        
        <category>Vision-Language Model</category>
        
        <category>Fine-tuning</category>
        
        <category>Document Understanding</category>
        
        <category>Markdown Conversion</category>
        
        <category>Benchmark</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.16633&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Abhirama Subramanyam Penamakuri, Navlika Singh, Piyush Arora, Anand Mishra&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 시각 질문 답변(VQA) 태스크에서 &lt;strong&gt;Small Vision-Language Models (S-VLMs)&lt;/strong&gt;의 성능을 향상시키는 것을 목표로 합니다. 이는 &lt;strong&gt;Large Vision-Language Models (L-VLMs)&lt;/strong&gt;의 높은 계산 비용과 성능 격차 문제를 해결하기 위해, &lt;strong&gt;레이블이 없는 이미지&lt;/strong&gt;와 효과적인 &lt;strong&gt;지식 전이&lt;/strong&gt;를 활용하여 S-VLMs를 개선하는 데 중점을 둡니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;제안하는 &lt;strong&gt;Model Parity Aligner (MPA)&lt;/strong&gt;는 세 가지 모듈로 구성됩니다. 먼저 &lt;strong&gt;Pseudo Annotator (PA)&lt;/strong&gt;는 L-VLM을 사용하여 레이블이 없는 이미지에 대한 질문-답변 쌍을 생성합니다. 다음으로 &lt;strong&gt;Parity Identifier (PI)&lt;/strong&gt;는 S-VLM과 L-VLM의 답변을 비교하여 L-VLM은 정답이지만 S-VLM은 오답인 ‘지식 격차’ 샘플을 식별하고 노이즈를 필터링합니다. 마지막으로 &lt;strong&gt;Parity Leveler (PL)&lt;/strong&gt;는 PI에서 식별된 지식 격차 샘플을 사용하여 S-VLM을 파인튜닝하여 L-VLM의 추론 능력을 모방하도록 합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;MPA는 &lt;strong&gt;TextVQA, ST-VQA, ChartQA, OKVQA&lt;/strong&gt; 등 4가지 VQA 벤치마크에서 S-VLMs의 성능을 일관되게 향상시켰으며, 최대 &lt;strong&gt;15.2%&lt;/strong&gt; (ChartQA의 TinyLLaVA-2B)의 절대 성능 향상과 평균 &lt;strong&gt;3.4%&lt;/strong&gt;의 개선을 보였습니다. 특히 MPA-정렬된 &lt;strong&gt;Qwen2VL-2B (75.4%)&lt;/strong&gt;는 더 큰 모델인 &lt;strong&gt;Qwen2VL-7B (74.7%)&lt;/strong&gt;를 능가하는 성능을 달성했으며, OCR 정확도를 &lt;strong&gt;+4.5%&lt;/strong&gt; 개선하는 등 VQA 외의 기본적인 역량도 전이됨을 입증했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;이 연구는 고비용의 레이블링된 데이터 없이도 &lt;strong&gt;S-VLMs&lt;/strong&gt;의 성능을 크게 향상시킬 수 있는 효과적인 방법을 제시합니다. 이는 자원 제약이 있는 환경이나 추론 중심 애플리케이션에서 &lt;strong&gt;대규모 VLM&lt;/strong&gt;의 접근성을 높여 &lt;strong&gt;AI 기술의 민주화&lt;/strong&gt;에 기여합니다. 특히, &lt;strong&gt;폐쇄형 L-VLM&lt;/strong&gt;을 가이드 모델로 활용하여 지식 전이가 가능하다는 점은 실제 산업 응용에 있어 큰 잠재력을 가집니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-When_Big_Models_Train_Small_Ones_Label-Free_Model_Parity_Alignment_for_Efficient_Visual_Question_Answering_using_Small_VLMs/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-When_Big_Models_Train_Small_Ones_Label-Free_Model_Parity_Alignment_for_Efficient_Visual_Question_Answering_using_Small_VLMs/</guid>
        
        <category>Review</category>
        
        <category>VQA</category>
        
        <category>Small VLMs</category>
        
        <category>Large VLMs</category>
        
        <category>Knowledge Transfer</category>
        
        <category>Pseudo-labeling</category>
        
        <category>Label-Free Learning</category>
        
        <category>Model Parity Alignment</category>
        
        <category>Computational Efficiency</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17985&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Sunghyun Cho, Janghyeok Han, Geonung Kim&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 조잡한(coarse) 3D 지오메트리, 카메라 궤적, 그리고 참조 이미지를 사용하여 고품질 3D 장면 비디오를 생성하는 문제를 해결하고자 합니다. 기존 비디오 확산 모델이 복잡한 장면에서 시각적 품질, 움직임, 시간적 일관성을 공동으로 모델링하는 데 겪는 어려움을 극복하고, 3D 그래픽 디자인 워크플로우를 간소화하는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;제안하는 &lt;strong&gt;VideoFrom3D&lt;/strong&gt; 프레임워크는 이미지 및 비디오 확산 모델의 상호보완적인 강점을 활용하는 2단계 접근 방식을 사용합니다. 첫째, &lt;strong&gt;Sparse Anchor-view Generation (SAG)&lt;/strong&gt; 모듈은 &lt;strong&gt;FLUX-dev 이미지 확산 모델&lt;/strong&gt;을 기반으로 &lt;strong&gt;ControlNet (HED 엣지)&lt;/strong&gt; 및 &lt;strong&gt;LoRA 기반 스타일 정렬&lt;/strong&gt;을 통해 고품질의 다중 뷰 일관성 앵커 뷰를 생성합니다. 둘째, &lt;strong&gt;Geometry-guided Generative Inbetweening (GGI)&lt;/strong&gt; 모듈은 &lt;strong&gt;CogVideoX-5B-1.0 비디오 확산 모델&lt;/strong&gt;을 활용하여 앵커 뷰 사이의 중간 프레임을 보간하며, &lt;strong&gt;Go-with-the-Flow&lt;/strong&gt;의 광학 흐름 기반 카메라 제어 및 &lt;strong&gt;VAE 인코딩된 HED 엣지 맵&lt;/strong&gt;을 통한 구조적 가이던스로 정확하고 일관된 보간을 보장합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;정량적 평가에서 &lt;strong&gt;VideoFrom3D (SAG + GGI)&lt;/strong&gt;는 &lt;strong&gt;PSNR 16.739, SSIM 0.554, LPIPS 0.236, MUSIQ 68.615&lt;/strong&gt; 등 대부분의 시각적 품질, 구조적 충실도, 스타일 및 시간적 일관성 지표에서 최신 기준 모델들을 뛰어넘는 성능을 달성했습니다. 특히, &lt;strong&gt;PSNR-D는 19.754&lt;/strong&gt;로 구조적 정확도에서 우수함을 보였으며, 다양한 시나리오에서 일관되고 고품질의 스타일 유지 비디오를 성공적으로 생성할 수 있음을 입증했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;이 연구는 &lt;strong&gt;이미지 확산 모델&lt;/strong&gt;과 &lt;strong&gt;비디오 확산 모델&lt;/strong&gt;을 효과적으로 결합함으로써 3D 장면 비디오 생성의 품질과 효율성을 동시에 높일 수 있는 실용적인 방안을 제시합니다. &lt;strong&gt;코스(coarse)한 3D 지오메트리&lt;/strong&gt;로부터 고품질 비디오를 생성하여 3D 디자인 초기 단계에서의 &lt;strong&gt;빠른 시안 생성 및 반복 작업&lt;/strong&gt;에 유용하게 활용될 수 있습니다. 특히, 3D 모델과 자연 이미지가 쌍을 이루는 &lt;strong&gt;방대한 데이터셋 없이도 학습 및 추론이 가능&lt;/strong&gt;하다는 점은 데이터 확보가 어려운 환경에서 AI/ML 엔지니어들에게 큰 이점으로 작용할 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-VideoFrom3D_3D_Scene_Video_Generation_via_Complementary_Image_and_Video_Diffusion_Models/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-VideoFrom3D_3D_Scene_Video_Generation_via_Complementary_Image_and_Video_Diffusion_Models/</guid>
        
        <category>Review</category>
        
        <category>3D Scene Generation</category>
        
        <category>Video Diffusion</category>
        
        <category>Image Diffusion</category>
        
        <category>Generative Models</category>
        
        <category>Computer Graphics</category>
        
        <category>Temporal Consistency</category>
        
        <category>Sparse Anchor Views</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17191&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Jinchao Ge, Tengfei Cheng, Biao Wu, Zeyu Zhang, Shiya Huang&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 연구는 고대 그리스 도자기에 대한 전문가 수준의 추론 능력을 갖춘 &lt;strong&gt;MLLM(Multimodal Large Language Models)&lt;/strong&gt; 에이전트를 개발하는 것을 목표로 합니다. 일반적인 MLLM이 부족한 도메인 전문성과 &lt;strong&gt;SFT(Supervised Fine-Tuning)&lt;/strong&gt; 모델의 피상적인 패턴 학습 문제를 극복하여, 유물의 진위 확인 및 역사적 귀속에 대한 강력하고 신뢰할 수 있는 추론 능력을 확보하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;저자들은 &lt;strong&gt;VaseVL&lt;/strong&gt;이라는 &lt;strong&gt;SFT-then-RL&lt;/strong&gt; 시스템을 제안합니다. 이는 &lt;strong&gt;7가지 질문 유형(Fabric, Technique, Shape, Provenance, Attribution, Date, Decoration) 분류 체계&lt;/strong&gt;를 구축하고, &lt;strong&gt;SFT 모델의 유형별 성능 격차를 진단&lt;/strong&gt;한 후, 이를 목표로 하는 &lt;strong&gt;진단 기반, 분류 체계 조건부 보상&lt;/strong&gt;으로 &lt;strong&gt;GRPO(Group Relative Policy Optimization) 강화 학습&lt;/strong&gt;을 수행합니다. 보상 함수는 &lt;strong&gt;키워드 중첩&lt;/strong&gt;과 &lt;strong&gt;의미적 유사성&lt;/strong&gt;을 결합하며, 부족한 부분에 더 높은 가중치를 부여합니다. 또한, &lt;strong&gt;31,773개 이미지&lt;/strong&gt;와 &lt;strong&gt;93,544개 QA 쌍&lt;/strong&gt;으로 구성된 &lt;strong&gt;VaseVQA 벤치마크&lt;/strong&gt;와 &lt;strong&gt;ANLS-기반 정확도&lt;/strong&gt;, &lt;strong&gt;BLEU@1&lt;/strong&gt; 등의 유형별 평가 스크립트도 함께 공개됩니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;VaseVL&lt;/strong&gt;은 &lt;strong&gt;SFT-only 베이스라인&lt;/strong&gt; 대비 &lt;strong&gt;Attribution 스코어를 56.96%에서 60.83%로 향상&lt;/strong&gt;시키고, &lt;strong&gt;Decoration 질문의 BLEU@1 스코어를 2.57에서 9.82로 대폭 개선&lt;/strong&gt;했습니다. 이는 보상 엔지니어링이 목표로 삼았던 영역에서 상당한 성능 향상을 입증합니다. 특히, 제로샷 MLLM(&lt;strong&gt;Qwen-2.5-VL&lt;/strong&gt;)의 저조한 성능(Attribution 11.50%)에 비해 &lt;strong&gt;VaseVL&lt;/strong&gt;은 &lt;strong&gt;최신 기술 수준의 결과&lt;/strong&gt;를 달성하며, 문화유산 도메인에서의 &lt;strong&gt;합성적 견고성&lt;/strong&gt;과 &lt;strong&gt;사실적 정확성&lt;/strong&gt;을 크게 향상시켰습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;문화유산과 같이 &lt;strong&gt;특정 도메인의 심층적인 전문 지식&lt;/strong&gt;이 요구되는 분야에서 &lt;strong&gt;MLLM의 추론 능력&lt;/strong&gt;을 효과적으로 향상시키는 방법론을 제시합니다. &lt;strong&gt;진단 기반의 보상 설계&lt;/strong&gt;와 &lt;strong&gt;강화 학습(RL)&lt;/strong&gt;의 조합은 단순 &lt;strong&gt;SFT&lt;/strong&gt;의 한계를 극복하고, 모델이 &lt;strong&gt;피상적인 패턴을 넘어 심층적인 의미를 학습&lt;/strong&gt;하도록 유도하는 강력한 접근법임을 보여줍니다. &lt;strong&gt;VaseVQA 벤치마크&lt;/strong&gt;는 향후 문화유산 분석을 위한 &lt;strong&gt;도메인 특화 AI 모델 개발 및 평가&lt;/strong&gt;에 있어 중요한 자원이 될 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-VaseVQA_Multimodal_Agent_and_Benchmark_for_Ancient_Greek_Pottery/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-VaseVQA_Multimodal_Agent_and_Benchmark_for_Ancient_Greek_Pottery/</guid>
        
        <category>Review</category>
        
        <category>Multimodal Large Language Models</category>
        
        <category>Visual Question Answering</category>
        
        <category>Reinforcement Learning</category>
        
        <category>Cultural Heritage</category>
        
        <category>Ancient Greek Pottery</category>
        
        <category>Supervised Fine-Tuning</category>
        
        <category>Benchmark</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Understanding Embedding Scaling in Collaborative Filtering</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.15709&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Zhuangzhuang He, Kaiyu Zhou, Haoyue Bai, Fengbin Zhu, Yonghui Yang&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;협업 필터링 모델에서 임베딩 차원을 확장할 때 발생하는 성능 변화를 이해하고, 기존에 알려진 ‘단일 봉우리(single-peak)’ 현상을 넘어서는 새로운 스케일링 패턴을 발견하는 것이 목표입니다. 또한, 이러한 현상의 근본적인 원인을 밝히고 특히 데이터 내 &lt;strong&gt;노이즈 상호작용&lt;/strong&gt;의 역할을 규명하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BPR, NeuMF, LightGCN, SGL&lt;/strong&gt; 네 가지 대표적인 협업 필터링 모델을 사용하여 &lt;strong&gt;10개의 데이터셋&lt;/strong&gt;에 걸쳐 대규모 실험을 수행했습니다. 임베딩 차원을 &lt;strong&gt;2의 거듭제곱&lt;/strong&gt;으로 점진적으로 늘려가며 &lt;strong&gt;NDCG@20&lt;/strong&gt; 지표의 성능 변화를 관찰하고, 모델 아키텍처별 노이즈 강건성을 &lt;strong&gt;이론적 분석&lt;/strong&gt;과 &lt;strong&gt;샘플 드롭 전략(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BPR_Drop&lt;/code&gt;)&lt;/strong&gt;을 통한 실험으로 검증했습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;임베딩 스케일링 시 &lt;strong&gt;‘이중 봉우리(double-peak)’&lt;/strong&gt;와 &lt;strong&gt;‘로그(logarithmic)’&lt;/strong&gt;라는 두 가지 새로운 현상을 발견했습니다. &lt;strong&gt;BPR&lt;/strong&gt;과 &lt;strong&gt;NeuMF&lt;/strong&gt;는 노이즈에 취약하여 ‘이중 봉우리’ 현상을 자주 보인 반면, &lt;strong&gt;LightGCN&lt;/strong&gt;과 &lt;strong&gt;SGL&lt;/strong&gt;은 노이즈에 강건하여 ‘로그’ 패턴으로 지속적인 성능 향상을 보였으며, 특히 &lt;strong&gt;SGL&lt;/strong&gt;은 일부 데이터셋에서 &lt;strong&gt;NDCG@20&lt;/strong&gt; 기준 최대 &lt;strong&gt;25.57%&lt;/strong&gt;의 성능 향상을 달성했습니다. 이론적 분석은 &lt;strong&gt;BPR&lt;/strong&gt;의 높은 그라디언트 민감도와 &lt;strong&gt;NeuMF&lt;/strong&gt;의 그라디언트 증폭이 노이즈 취약성의 원인임을 보여주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;임베딩 차원 스케일링 시 모델의 &lt;strong&gt;노이즈 강건성&lt;/strong&gt;이 성능에 결정적인 영향을 미친다는 점은 추천 시스템 설계에 중요한 시사점을 제공합니다. &lt;strong&gt;SGL&lt;/strong&gt;과 같이 &lt;strong&gt;대조 학습&lt;/strong&gt; 및 &lt;strong&gt;그래프 컨볼루션&lt;/strong&gt;을 통해 노이즈를 효과적으로 필터링하는 모델은 대규모 임베딩에서도 안정적이고 지속적인 성능 향상을 기대할 수 있습니다. 따라서 AI 실무자는 단순히 임베딩 크기를 늘리기보다 &lt;strong&gt;데이터 노이즈 처리 능력&lt;/strong&gt;이 뛰어난 모델 아키텍처 선택과 &lt;strong&gt;데이터 품질 향상&lt;/strong&gt;에 집중해야 합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-Understanding_Embedding_Scaling_in_Collaborative_Filtering/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-Understanding_Embedding_Scaling_in_Collaborative_Filtering/</guid>
        
        <category>Review</category>
        
        <category>Collaborative Filtering</category>
        
        <category>Embedding Scaling</category>
        
        <category>Noise Robustness</category>
        
        <category>Recommender Systems</category>
        
        <category>Graph Neural Networks</category>
        
        <category>Self-supervised Learning</category>
        
        <category>Performance Degradation</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17671&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Selva Taş, Mahmut El Huseyni, Özay Ezerceli, Reyhan Bayraktar, Fatma Betül Terzioğlu&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;대규모 언어 모델(LLMs)의 &lt;strong&gt;환각(hallucination)&lt;/strong&gt; 문제를 해결하고, 특히 형태학적으로 복잡한 &lt;strong&gt;터키어 RAG(Retrieval-Augmented Generation) 애플리케이션&lt;/strong&gt;을 위한 효과적인 환각 탐지 모델을 개발하는 것이 목표입니다. 기존 탐지 방법의 계산 비효율성과 제한된 컨텍스트 길이라는 한계를 극복하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;환각 탐지 작업을 &lt;strong&gt;토큰-수준 분류(token-level classification)&lt;/strong&gt;로 정형화하고, &lt;strong&gt;LettuceDetect 프레임워크&lt;/strong&gt;를 확장했습니다. &lt;strong&gt;ModernBERT-base-tr&lt;/strong&gt;, &lt;strong&gt;TurkEmbed4STS&lt;/strong&gt;, &lt;strong&gt;lettucedect-210m-eurobert-tr-v1&lt;/strong&gt; 세 가지 인코더 아키텍처를 &lt;strong&gt;기계 번역된 RAGTruth 벤치마크 데이터셋(17,790개 인스턴스)&lt;/strong&gt;에 파인튜닝했습니다. 모델들은 &lt;strong&gt;8,192 토큰&lt;/strong&gt;까지의 긴 컨텍스트를 지원하도록 설계되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;ModernBERT-base-tr&lt;/strong&gt; 모델은 전체 테스트 세트에서 &lt;strong&gt;0.7266 F1-score&lt;/strong&gt;를 달성했으며, 특히 질문 답변(QA)과 같은 구조화된 작업에서 &lt;strong&gt;0.7588 F1-score&lt;/strong&gt;로 강력한 성능을 보였습니다. 최신 LLM들은 높은 재현율(최대 &lt;strong&gt;0.9938&lt;/strong&gt;)을 보였으나, 정밀도가 낮아 환각 콘텐츠를 과도하게 생성하는 경향이 있어 전문화된 탐지 메커니즘의 필요성이 부각되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;이 연구는 &lt;strong&gt;터키어 RAG 시스템&lt;/strong&gt;의 신뢰성을 향상시키는 데 필수적인 &lt;strong&gt;환각 탐지 모델&lt;/strong&gt;을 제공합니다. &lt;strong&gt;ModernBERT&lt;/strong&gt;와 같은 최신 인코더 아키텍처를 활용하여 &lt;strong&gt;8,192 토큰&lt;/strong&gt;에 이르는 긴 컨텍스트를 효율적으로 처리하면서도, &lt;strong&gt;형태학적으로 복잡한 언어&lt;/strong&gt;에 대한 성능을 유지할 수 있음을 입증했습니다. 공개된 &lt;strong&gt;Turk-LettuceDetect 모델&lt;/strong&gt;과 &lt;strong&gt;번역된 RAGTruth 데이터셋&lt;/strong&gt;은 터키어를 포함한 저자원 언어의 RAG 애플리케이션 개발을 가속화할 중요한 기반을 마련했습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-Turk-LettuceDetect_A_Hallucination_Detection_Models_for_Turkish_RAG_Applications/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-Turk-LettuceDetect_A_Hallucination_Detection_Models_for_Turkish_RAG_Applications/</guid>
        
        <category>Review</category>
        
        <category>Hallucination Detection</category>
        
        <category>Retrieval Augmented Generation</category>
        
        <category>Large Language Models</category>
        
        <category>Turkish NLP</category>
        
        <category>Token Classification</category>
        
        <category>ModernBERT</category>
        
        <category>Low-Resource Languages</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18056&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;이 논문은 비디오 시간적 접지(temporal grounding) 작업에서 &lt;strong&gt;멀티모달 대규모 언어 모델(MLLMs)&lt;/strong&gt;의 효율성을 개선하는 것을 목표로 합니다. 기존 강화 학습(&lt;strong&gt;RL&lt;/strong&gt;) 방법론, 특히 &lt;strong&gt;GRPO&lt;/strong&gt;가 큰 시간 검색 공간에서 비효율적인 탐색과 불안정한 정책 업데이트를 겪는 문제를 해결하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TempSamp-R1&lt;/strong&gt;이라는 새로운 강화 미세 조정 프레임워크를 제안하며, &lt;strong&gt;온-정책 샘플링(on-policy sampling)&lt;/strong&gt;과 &lt;strong&gt;오프-정책 지도(off-policy guidance)&lt;/strong&gt;(예: 정답 주석)를 결합하여 정확한 시간적 감독을 제공합니다. 훈련 안정성을 위해 보상 피드백을 비대칭 변환을 통해 동적으로 재구성하는 &lt;strong&gt;비선형 소프트 이점 계산(non-linear soft advantage computation)&lt;/strong&gt; 방법을 도입합니다. 또한, &lt;strong&gt;하이브리드 CoT(Chain-of-Thought) 훈련 패러다임&lt;/strong&gt;을 사용하여 CoT 및 비-CoT 추론 모드를 모두 지원하는 단일 모델을 최적화합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TempSamp-R1&lt;/strong&gt;은 기존 &lt;strong&gt;GRPO 기반&lt;/strong&gt; 모델들을 뛰어넘는 최첨단 성능을 달성했습니다. &lt;strong&gt;Charades-STA&lt;/strong&gt;에서 &lt;strong&gt;R1@0.7 52.9%(+2.7%)&lt;/strong&gt;, &lt;strong&gt;ActivityNet Captions&lt;/strong&gt;에서 &lt;strong&gt;R1@0.5 56.0%(+5.3%)&lt;/strong&gt;, &lt;strong&gt;QVHighlights&lt;/strong&gt;에서 &lt;strong&gt;mAP 30.0%(+3.0%)&lt;/strong&gt;를 기록했습니다. 특히 &lt;strong&gt;제한된 데이터&lt;/strong&gt; 환경에서도 강력한 &lt;strong&gt;few-shot 일반화 능력&lt;/strong&gt;을 보여주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;이 연구는 &lt;strong&gt;비디오 LLM&lt;/strong&gt;을 위한 더 안정적이고 데이터 효율적인 &lt;strong&gt;강화 학습 미세 조정 패러다임&lt;/strong&gt;을 제시합니다. &lt;strong&gt;정답 주석을 오프-정책 감독으로 활용&lt;/strong&gt;하고 &lt;strong&gt;적응형 보상 쉐이핑&lt;/strong&gt;을 적용하는 방식은 비디오 이해 작업의 정확도를 크게 향상시킬 수 있음을 시사합니다. &lt;strong&gt;하이브리드 CoT 접근 방식&lt;/strong&gt;은 다양한 추론 복잡성을 가진 쿼리에 유연하게 대응할 수 있어 실제 응용 분야에서 매우 유용할 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-TempSamp-R1_Effective_Temporal_Sampling_with_Reinforcement_Fine-Tuning_for_Video_LLMs/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-TempSamp-R1_Effective_Temporal_Sampling_with_Reinforcement_Fine-Tuning_for_Video_LLMs/</guid>
        
        <category>Review</category>
        
        <category>Video LLMs</category>
        
        <category>Temporal Grounding</category>
        
        <category>Reinforcement Learning</category>
        
        <category>Off-policy Learning</category>
        
        <category>Reward Shaping</category>
        
        <category>Chain-of-Thought</category>
        
        <category>Multimodal LLMs</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Synthetic bootstrapped pretraining</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.15248&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Zitong Yang, Aonan Zhang, Hong Liu, Tatsunori Hashimoto, Emmanuel Candès, Chong Wang, Ruoming Pang&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 대규모 언어 모델(LM) 사전 훈련 시 고품질 텍스트 데이터 고갈 문제를 해결하고, 표준 사전 훈련에서 간과되는 &lt;strong&gt;문서 간 풍부한 상관관계&lt;/strong&gt;를 효과적으로 모델링하여 LM 성능을 개선하는 것을 목표로 합니다. 기존 데이터의 활용도를 극대화하여 새로운 데이터 수집 없이 모델의 성능을 향상시키는 방법론을 제안합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;저자들은 &lt;strong&gt;Synthetic Bootstrapped Pretraining (SBP)&lt;/strong&gt;이라는 3단계 절차를 제안합니다. 첫째, &lt;strong&gt;Qwen3-Embedding-0.6B&lt;/strong&gt;를 사용하여 문서 임베딩을 생성하고 &lt;strong&gt;ScaNN&lt;/strong&gt;으로 유사한 문서 쌍을 식별합니다. 둘째, &lt;strong&gt;Llama 3 기반 3B-파라미터 LM&lt;/strong&gt;을 데이터 합성기(&lt;strong&gt;po(d2|d1)&lt;/strong&gt;)로 튜닝하여 주어진 문서(d1)로부터 관련 문서(d2)를 생성하는 방법을 학습합니다. 셋째, 이 합성기를 사용하여 방대한 합성 데이터셋(&lt;strong&gt;Spretrain&lt;/strong&gt;)을 생성하고, 원본 데이터셋(&lt;strong&gt;Dpretrain&lt;/strong&gt;)과 합성 데이터셋을 &lt;strong&gt;결합하여 최종 LM을 공동 훈련&lt;/strong&gt;합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;200B 토큰 규모&lt;/strong&gt;에서 SBP는 강력한 반복 베이스라인 대비 평균 &lt;strong&gt;QA 정확도를 +2.17%&lt;/strong&gt; 향상시켰으며, 이는 &lt;strong&gt;20배 더 많은 고유 데이터&lt;/strong&gt;에 접근 가능한 오라클 성능 개선량(&lt;strong&gt;+5.09%&lt;/strong&gt;)의 &lt;strong&gt;42%&lt;/strong&gt;에 해당합니다. &lt;strong&gt;1T 토큰 규모&lt;/strong&gt;에서는 평균 &lt;strong&gt;QA 정확도를 +0.74%&lt;/strong&gt; 개선하여 오라클 성능 개선량(&lt;strong&gt;+1.50%&lt;/strong&gt;)의 &lt;strong&gt;49%&lt;/strong&gt;를 달성했습니다. 합성된 데이터는 단순한 의역을 넘어 추상화된 개념을 기반으로 새로운 서술을 생성하는 질적 특성을 보였으며, 1T-scale 합성 데이터의 &lt;strong&gt;비사실성(Non-factual) 비율은 8.65%&lt;/strong&gt;로 200B-scale의 &lt;strong&gt;15.09%&lt;/strong&gt;보다 크게 낮아졌습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;SBP는 데이터 제약이 있는 환경에서 LM의 사전 훈련 성능을 향상시킬 수 있는 &lt;strong&gt;실용적이고 확장 가능한 방법&lt;/strong&gt;을 제시합니다. 외부 “교사 LM” 없이 기존 데이터를 활용하여 모델 스스로 학습 능력을 부트스트랩하는 것은 &lt;strong&gt;데이터 수집 비용&lt;/strong&gt;을 줄이고 LM 개발의 지속 가능성을 높이는 데 기여할 수 있습니다. 그러나 합성 데이터의 &lt;strong&gt;사실성 및 관련성&lt;/strong&gt;을 지속적으로 모니터링하고 평가하는 체계적인 접근 방식이 중요합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-Synthetic_bootstrapped_pretraining/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-Synthetic_bootstrapped_pretraining/</guid>
        
        <category>Review</category>
        
        <category>Language Model Pretraining</category>
        
        <category>Synthetic Data</category>
        
        <category>Inter-document Correlation</category>
        
        <category>Data Augmentation</category>
        
        <category>Transformer</category>
        
        <category>Bootstrapping</category>
        
        <category>Concept Learning</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.16941&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Xiang Deng&lt;em&gt;, Jeff Da&lt;/em&gt;, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Chen Bo Calvin Zhang, Noah Jacobson, Bing Liu, Brad Kenstler&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 기존의 &lt;strong&gt;SWE-Bench&lt;/strong&gt;와 같은 코드 생성 벤치마크의 한계를 지적하며, 현실적인 &lt;strong&gt;엔터프라이즈 수준&lt;/strong&gt;의 복잡성과 &lt;strong&gt;장기적 관점(long-horizon)&lt;/strong&gt;을 지닌 소프트웨어 엔지니어링 문제 해결 능력을 평가하기 위한 새로운 벤치마크 &lt;strong&gt;SWE-BENCH PRO&lt;/strong&gt;를 제시합니다. 이는 기존 벤치마크의 &lt;strong&gt;데이터 오염 문제(data contamination)&lt;/strong&gt;를 완화하고, &lt;strong&gt;대규모 언어 모델(LLM) 에이전트&lt;/strong&gt;가 실제 소프트웨어 개발 환경에서 직면하는 도전 과제를 더 정확하게 반영하는 것을 목표로 합니다. 궁극적으로 자율적인 소프트웨어 엔지니어링 에이전트 개발을 촉진하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SWE-BENCH PRO&lt;/strong&gt;는 &lt;strong&gt;41개&lt;/strong&gt;의 활발히 유지보수되는 리포지토리에서 &lt;strong&gt;1,865개&lt;/strong&gt;의 문제를 수집했으며, &lt;strong&gt;강력한 카피레프트 라이선스(GPL)&lt;/strong&gt;를 가진 공개 리포지토리와 &lt;strong&gt;상업용 스타트업 코드베이스&lt;/strong&gt;를 활용하여 데이터 오염 위험을 줄였습니다. 벤치마크 문제는 평균 &lt;strong&gt;107.4 라인의 코드&lt;/strong&gt; 수정과 &lt;strong&gt;4.1개의 파일&lt;/strong&gt;을 아우르는 &lt;strong&gt;다중 파일 수정&lt;/strong&gt;을 요구하는 &lt;strong&gt;복잡한 태스크&lt;/strong&gt;로만 구성되어 난이도를 높였습니다. 또한, &lt;strong&gt;3단계의 인간 개입(human-in-the-loop) 프로세스&lt;/strong&gt;를 통해 문제 설명의 모호성을 제거하고 충분한 맥락을 제공하며, 테스트 케이스가 유효하고 해결 가능한지 검증합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SWE-BENCH PRO&lt;/strong&gt; 평가에서 최신 &lt;strong&gt;LLM 에이전트&lt;/strong&gt;들의 성능은 기대치를 크게 밑돌았으며, &lt;strong&gt;GPT-5&lt;/strong&gt;가 &lt;strong&gt;23.3%&lt;/strong&gt;의 &lt;strong&gt;Pass@1&lt;/strong&gt;로 가장 높은 해결률을 보였고, &lt;strong&gt;Claude Opus 4.1&lt;/strong&gt;은 &lt;strong&gt;22.7%&lt;/strong&gt;를 기록했습니다. 이는 기존 &lt;strong&gt;SWE-Bench Verified&lt;/strong&gt;의 &lt;strong&gt;70% 이상&lt;/strong&gt; 해결률과 비교할 때 현저히 낮은 수치입니다. 상업용 데이터셋에서는 모델들의 해결률이 &lt;strong&gt;20% 미만&lt;/strong&gt;으로 더욱 낮게 나타나 &lt;strong&gt;엔터프라이즈 코드베이스&lt;/strong&gt;의 복잡성이 반영되었습니다. &lt;strong&gt;LLM-as-a-judge&lt;/strong&gt;를 통한 실패 모드 분석 결과, &lt;strong&gt;Opus 4.1&lt;/strong&gt;은 &lt;strong&gt;의미론적/알고리즘적 부정확성(wrong solution 35.9%)&lt;/strong&gt;에서, 소형 모델들은 &lt;strong&gt;구문 오류(syntax error)&lt;/strong&gt;, &lt;strong&gt;도구 사용 오류(tool error)&lt;/strong&gt;, &lt;strong&gt;컨텍스트 오버플로우(context overflow)&lt;/strong&gt;에서 주요 실패 원인을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SWE-BENCH PRO&lt;/strong&gt;는 &lt;strong&gt;LLM 에이전트&lt;/strong&gt;의 실제 소프트웨어 엔지니어링 역량을 평가하는 데 있어 &lt;strong&gt;더욱 현실적이고 엄격한 기준&lt;/strong&gt;을 제시합니다. 현재 &lt;strong&gt;최고 성능 모델&lt;/strong&gt;들도 실제 산업 환경의 복잡한 작업에는 크게 미치지 못함을 보여주므로, 에이전트의 &lt;strong&gt;문제 이해력, 장기 계획 수립, 다중 파일 변경 관리, 그리고 견고한 도구 사용 능력&lt;/strong&gt;을 향상시키는 연구가 중요함을 강조합니다. 이 벤치마크는 &lt;strong&gt;데이터 오염 방지 기술&lt;/strong&gt;의 중요성을 부각하고, &lt;strong&gt;엔터프라이즈 코드베이스&lt;/strong&gt;를 다룰 수 있는 &lt;strong&gt;차세대 AI 에이전트&lt;/strong&gt; 개발을 위한 중요한 이정표 역할을 할 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-SWE-Bench_Pro_Can_AI_Agents_Solve_Long-Horizon_Software_Engineering_Tasks/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-SWE-Bench_Pro_Can_AI_Agents_Solve_Long-Horizon_Software_Engineering_Tasks/</guid>
        
        <category>Review</category>
        
        <category>AI Agents</category>
        
        <category>Software Engineering</category>
        
        <category>LLMs</category>
        
        <category>Code Generation</category>
        
        <category>Benchmark</category>
        
        <category>Contamination Resistance</category>
        
        <category>Long-Horizon Tasks</category>
        
        <category>Enterprise Software</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.16548&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Yuyang Ding, Xinyu Shi, Juntao Li, Xiaobo Liang, Zhaopeng Tu, Min Zhang&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 대규모 언어 모델(LLMs)의 추론 과정을 평가하는 &lt;strong&gt;Process Reward Models (PRMs)&lt;/strong&gt; 개발의 핵심 난제인 &lt;strong&gt;높은 비용의 사람 주석 데이터&lt;/strong&gt;와 &lt;strong&gt;Monte Carlo (MC) 추정 데이터의 높은 노이즈&lt;/strong&gt; 문제를 해결하고자 합니다. 외부의 강력한 감독 없이 &lt;strong&gt;MC 추정 자체의 노이즈 제거 잠재력&lt;/strong&gt;과 &lt;strong&gt;PRM의 강건한 학습&lt;/strong&gt; 방법을 탐구하여, 비용 효율적이고 확장 가능한 PRM 학습 프레임워크를 제안하는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;본 논문은 &lt;strong&gt;Self-Denoising Monte Carlo Annotation (SCAN)&lt;/strong&gt; 프레임워크를 제안합니다. 이는 MC 추정 노이즈 분포 분석을 통해 어노테이션 모델의 과소/과대 추정 경향을 파악한 데 기반하며, &lt;strong&gt;self-confidence metric&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SCo(q)&lt;/code&gt;를 도입하여 어노테이션 신뢰도를 측정합니다. 이 신뢰도를 바탕으로 &lt;strong&gt;효율적인 데이터 합성 모듈&lt;/strong&gt;은 정보성 샘플에만 MC 어노테이션을 선택적으로 적용하여 기존 MC 추정 대비 &lt;strong&gt;6%의 추론 비용&lt;/strong&gt;만으로 데이터를 생성합니다. 또한, 노이즈에 강건한 학습을 위해 &lt;strong&gt;노이즈 내성 라벨링 전략&lt;/strong&gt; (오류 이전 단계에 &lt;strong&gt;tolerance distance d=2&lt;/strong&gt;의 소프트 라벨 적용)과 &lt;strong&gt;신뢰도 기반 재가중치&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min(ci/SCπ(q), 1)&lt;/code&gt;를 통해 모델 편향을 완화합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SCAN-Base&lt;/strong&gt; (101K 합성 샘플, 1.5B 모델 생성)는 &lt;strong&gt;Best-of-8에서 평균 정확도 69.1%&lt;/strong&gt;, &lt;strong&gt;ProcessBench에서 F1 점수 56.8%&lt;/strong&gt;를 달성하여 더 큰 합성 데이터셋으로 훈련된 PRM들을 능가했습니다. &lt;strong&gt;SCAN-Pro&lt;/strong&gt; (197K 합성 샘플, 7B 모델 통합)는 &lt;strong&gt;평균 정확도 70.1%&lt;/strong&gt;, &lt;strong&gt;F1 점수 59.1%&lt;/strong&gt;로 성능을 더욱 향상시켜, &lt;strong&gt;인간 주석 PRM800K 데이터셋 (평균 정확도 69.3%, F1 점수 56.5%)&lt;/strong&gt;의 성능을 뛰어넘었습니다. 또한, 제안된 방법은 합성 데이터의 노이즈 비율을 크게 줄여, &lt;strong&gt;Llama-3.1-8B-Ins&lt;/strong&gt;의 경우 &lt;strong&gt;56.2%에서 19.1% (37.1%↓)&lt;/strong&gt;, &lt;strong&gt;Qwen2.5-Math-7B-Ins&lt;/strong&gt;의 경우 &lt;strong&gt;51.8%에서 29.4% (22.4%↓)&lt;/strong&gt;로 개선했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;본 연구는 경량 모델로도 &lt;strong&gt;자체 denoising 전략&lt;/strong&gt;을 통해 고품질 PRM 훈련 데이터를 효율적으로 생성할 수 있음을 입증하며, 비용 효율적인 AI 모델 개발 가능성을 제시합니다. 제안된 &lt;strong&gt;강력한 학습 전략&lt;/strong&gt;은 노이즈가 많은 약한 감독 환경에서도 PRM이 효과적으로 학습하여 &lt;strong&gt;ProcessBench에서 F1 점수 39.2%p 개선&lt;/strong&gt;이라는 실질적인 성능 향상을 달성할 수 있음을 보여줍니다. 이는 &lt;strong&gt;대규모 데이터 없이도 강력한 PRM을 구축&lt;/strong&gt;할 수 있는 기반을 제공하며, LLM 기반의 복잡한 추론 태스크에서 &lt;strong&gt;강건하고 확장 가능한 평가 시스템&lt;/strong&gt;을 구축하는 데 중요한 통찰을 제공합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-SCAN_Self-Denoising_Monte_Carlo_Annotation_for_Robust_Process_Reward_Learning/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-SCAN_Self-Denoising_Monte_Carlo_Annotation_for_Robust_Process_Reward_Learning/</guid>
        
        <category>Review</category>
        
        <category>Process Reward Models</category>
        
        <category>Monte Carlo Annotation</category>
        
        <category>Noise Denoising</category>
        
        <category>Robust Learning</category>
        
        <category>Self-Supervision</category>
        
        <category>Mathematical Reasoning</category>
        
        <category>Large Language Models</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18083&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Valentin Lacombe, Valentin Quesnel, and Damien Sileo&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 연구는 LLM의 기초적인 기호 추론 능력을 향상시키기 위한 확장 가능한 &lt;strong&gt;RLVR (Reinforcement Learning with Verifiable Rewards) 환경인 Reasoning Core&lt;/strong&gt;를 소개합니다. 기존 벤치마크의 고정된 특성이나 제한적인 데이터 다양성으로 인한 확장성 병목 현상을 극복하고, LLM이 일반적이고 견고한 추론 능력을 학습할 수 있도록 하는 새로운 훈련 데이터를 제공하는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Reasoning Core&lt;/strong&gt;는 &lt;strong&gt;PDDL 플래닝, 1차 논리, 문맥 자유 문법 파싱, 인과 추론, 시스템 방정식 풀이&lt;/strong&gt; 등 핵심 형식 도메인에서 문제를 절차적으로 생성합니다. 이 환경은 &lt;strong&gt;고일반성 문제 분포, 외부 전문 도구(예: 정리 증명기, 플래닝 엔진)를 통한 솔루션 검증, 연속적인 난이도 제어&lt;/strong&gt;라는 세 가지 핵심 원칙을 기반으로 설계되었습니다. 특히, &lt;strong&gt;“난이도 조절 노브”&lt;/strong&gt;를 통해 생성되는 문제의 복잡성을 정밀하게 조절하여 모델 성능에 맞춘 적응형 커리큘럼 생성을 지원합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GPT-5 (nano, mini, base)&lt;/strong&gt; 모델들을 대상으로 한 초기 제로샷 평가 결과, &lt;strong&gt;Reasoning Core의 모든 태스크가 높은 난이도를 보이며 GPT-5에 충분히 도전적임&lt;/strong&gt;이 확인되었습니다. 또한, &lt;strong&gt;난이도 제어 메커니즘이 대부분의 태스크에서 의도대로 작동&lt;/strong&gt;하여, 쉬운 모드(knob level 0)보다 어려운 모드(knob level 5)에서 일관되게 &lt;strong&gt;더 높은 실패율&lt;/strong&gt;을 기록했습니다. 이는 환경이 LLM의 추론 능력 향상을 위한 유효한 벤치마크 및 훈련 자원임을 입증합니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Reasoning Core&lt;/strong&gt;는 LLM의 &lt;strong&gt;복잡한 기호 추론 능력&lt;/strong&gt;을 훈련하기 위한 &lt;strong&gt;확장 가능하고 검증 가능한 고품질 데이터&lt;/strong&gt;를 제공함으로써 데이터 부족 문제를 해결합니다. &lt;strong&gt;연속적인 난이도 제어&lt;/strong&gt; 기능은 모델의 학습 진행도에 따라 동적으로 조절되는 &lt;strong&gt;적응형 학습 커리큘럼&lt;/strong&gt;을 구현할 수 있게 하여, 보다 효율적이고 견고한 모델 훈련이 가능합니다. 외부 전문 솔버와의 통합은 추론 결과의 &lt;strong&gt;높은 정확성과 신뢰성&lt;/strong&gt;을 보장하며, 이는 &lt;strong&gt;일반화되고 강력한 LLM 추론 시스템 개발&lt;/strong&gt;에 필수적입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-Reasoning_Core_A_Scalable_RL_Environment_for_LLM_Symbolic_Reasoning/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-Reasoning_Core_A_Scalable_RL_Environment_for_LLM_Symbolic_Reasoning/</guid>
        
        <category>Review</category>
        
        <category>LLM Reasoning</category>
        
        <category>Symbolic AI</category>
        
        <category>Reinforcement Learning</category>
        
        <category>Procedural Content Generation</category>
        
        <category>Verifiable Rewards</category>
        
        <category>Adaptive Curricula</category>
        
        <category>First-Order Logic</category>
        
        <category>PDDL Planning</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] Qwen3-Omni Technical Report</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17765&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Qwen Team&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티 전반에 걸쳐 &lt;strong&gt;단일 멀티모달 모델(Qwen3-Omni)&lt;/strong&gt;이 기존 단일 모달 모델과 비교하여 성능 저하 없이 &lt;strong&gt;최첨단 성능을 유지&lt;/strong&gt;하는 것을 목표로 합니다. 또한, &lt;strong&gt;교차 모달 추론 능력&lt;/strong&gt;과 &lt;strong&gt;실시간 시청각 상호작용&lt;/strong&gt;을 향상시키는 것을 주된 연구 목적으로 삼습니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;Qwen3-Omni는 인식과 생성 과정을 통합하는 &lt;strong&gt;Thinker–Talker Mixture-of-Experts (MoE) 아키텍처&lt;/strong&gt;를 채택합니다. 이 모델은 &lt;strong&gt;AuT (Audio Transformer) 인코더&lt;/strong&gt;를 통해 강력한 오디오 표현을 학습하며, &lt;strong&gt;멀티-코드북 기반 음성 생성&lt;/strong&gt; 및 &lt;strong&gt;경량화된 인과 ConvNet (Code2Wav)&lt;/strong&gt;을 사용하여 실시간 음성 합성을 구현합니다. &lt;strong&gt;Time-aligned Multimodal Rotary Position Embedding (TM-RoPE)&lt;/strong&gt;을 통해 다양한 모달리티의 시간 정보를 효과적으로 통합하고, &lt;strong&gt;청크 단위 사전 채우기(chunked prefilling) 메커니즘&lt;/strong&gt;을 통해 스트리밍 성능을 최적화했습니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;Qwen3-Omni는 36개의 오디오 및 시청각 벤치마크 중 &lt;strong&gt;32개에서 오픈소스 최첨단(SOTA)&lt;/strong&gt;을 달성했고, &lt;strong&gt;22개에서 전체 SOTA&lt;/strong&gt;를 기록하며 Gemini-2.5-Pro와 같은 강력한 폐쇄형 모델을 능가했습니다. 특히, 오디오의 경우 이론적인 종단 간 &lt;strong&gt;첫 패킷 지연 시간(first-packet latency) 234ms&lt;/strong&gt;를 달성했습니다. 또한, 텍스트 및 시각 모달리티에서도 동일 크기 단일 모달 Qwen 모델과 동등하거나 우수한 성능을 보여주며 성능 저하가 없음을 입증했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;Qwen3-Omni는 &lt;strong&gt;통합 멀티모달 훈련&lt;/strong&gt;이 모달리티별 성능 저하 없이 모든 모달리티에서 동등한 성능을 달성할 수 있음을 보여주어, 복잡한 AI 시스템 구축을 위한 다재다능한 기반을 제공합니다. &lt;strong&gt;234ms의 낮은 첫 패킷 지연 시간&lt;/strong&gt;과 &lt;strong&gt;고동시성 MoE 아키텍처&lt;/strong&gt;는 음성 비서 및 비디오 대화 시스템과 같은 반응성이 뛰어나고 확장 가능한 실시간 AI 애플리케이션 개발에 중요한 이점을 제공합니다. &lt;strong&gt;Apache 2.0 라이선스&lt;/strong&gt;로 공개된 Qwen3-Omni 모델은 특히 오디오 작업에서 강력한 성능을 발휘하여 고급 멀티모달 에이전트 개발을 위한 강력한 오픈소스 도구입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-Qwen3-Omni_Technical_Report/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-Qwen3-Omni_Technical_Report/</guid>
        
        <category>Review</category>
        
        <category>Multimodal Model</category>
        
        <category>Thinker-Talker Architecture</category>
        
        <category>Mixture-of-Experts</category>
        
        <category>Low-latency</category>
        
        <category>Audio Understanding</category>
        
        <category>Cross-modal Reasoning</category>
        
        <category>State-of-the-Art</category>
        
        <category>Real-time Interaction</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17428&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 대규모 언어 모델(LLM)의 효율적인 배포를 위해 &lt;strong&gt;양자화-인식(Quantization-Aware) PEFT&lt;/strong&gt; (Parameter-Efficient Fine-Tuning) 방법을 개발하여, 양자화된 모델의 낮은 비트 환경에서 &lt;strong&gt;정확도를 높이고&lt;/strong&gt; 동시에 &lt;strong&gt;훈련 효율성을 개선&lt;/strong&gt;하는 것을 목표로 합니다. 특히, 기존의 저랭크 어댑터의 제한된 표현력과 푸리에 관련 변환(FT) 기반 어댑터의 비효율적인 양자화 오류 감소 및 높은 연산 오버헤드 문제를 해결하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;저자들은 &lt;strong&gt;Walsh-Hadamard Transform (WHT)&lt;/strong&gt;을 변환 커널로 사용하는 새로운 어댑터(&lt;strong&gt;WHA&lt;/strong&gt;)를 제안하여 FT 기반 어댑터를 양자화된 모델에 통합했습니다. 이 방법론은 양자화-인식 어댑터 초기화 방식을 포함하며, &lt;strong&gt;AdaAlloc&lt;/strong&gt;을 통해 출력 채널별 양자화 오류 크기에 비례하여 매개변수 예산을 적응적으로 할당하고, 가장 큰 계수를 선택하여 효율적으로 양자화 오류를 줄입니다. 또한, &lt;strong&gt;값 정제(Value Refinement)&lt;/strong&gt;를 통해 선택된 매개변수 값을 최적화하며, 기존 FT 기반 어댑터와 달리 &lt;strong&gt;단일 변환&lt;/strong&gt;을 사용하여 연산 비용을 크게 줄입니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;QWHA&lt;/strong&gt;는 &lt;strong&gt;2-비트 양자화&lt;/strong&gt; 설정에서 기존 베이스라인 대비 &lt;strong&gt;최소 2-3% 더 높은 정확도&lt;/strong&gt;를 달성하며 낮은 비트 양자화에서 일관되게 우수한 성능을 보였습니다. 특히, LLaMA-3.2-3B 모델의 2-bit GSM8k 벤치마크에서 &lt;strong&gt;60.98%&lt;/strong&gt;의 정확도를 기록하여 CLOQ의 &lt;strong&gt;54.89%&lt;/strong&gt;보다 크게 앞섰습니다. 또한, &lt;strong&gt;Alpaca 데이터셋&lt;/strong&gt;에서 기존 FT 기반 어댑터 대비 &lt;strong&gt;상당한 훈련 속도 향상&lt;/strong&gt;을 보였는데, 예를 들어 배치 크기 16에서 &lt;strong&gt;CLOQ (8.3시간)&lt;/strong&gt;, &lt;strong&gt;SHIRA (9.8시간)&lt;/strong&gt;와 유사한 &lt;strong&gt;3.9시간&lt;/strong&gt;을 기록하며 기존 FT 기반 어댑터(LoCA/SSH)의 &lt;strong&gt;수십 시간&lt;/strong&gt; 대비 압도적인 효율성을 입증했습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;QWHA&lt;/strong&gt;는 &lt;strong&gt;저비트 양자화된 LLM&lt;/strong&gt;의 정확성과 효율성을 동시에 향상시키는 실용적인 솔루션을 제공합니다. &lt;strong&gt;WHT 기반 어댑터&lt;/strong&gt;와 &lt;strong&gt;적응형 초기화(AdaAlloc)&lt;/strong&gt;는 기존 &lt;strong&gt;LoRA&lt;/strong&gt; 및 다른 &lt;strong&gt;FT 기반 어댑터&lt;/strong&gt;의 한계를 극복하며, &lt;strong&gt;제한된 컴퓨팅 자원&lt;/strong&gt;에서도 대규모 모델의 &lt;strong&gt;정확한 미세 조정&lt;/strong&gt;을 가능하게 합니다. 이는 AI 엔지니어들이 &lt;strong&gt;엣지 디바이스&lt;/strong&gt;나 &lt;strong&gt;저전력 환경&lt;/strong&gt;에 LLM을 배포하거나, 개발 주기에서 &lt;strong&gt;빠른 실험 반복&lt;/strong&gt;을 수행하는 데 큰 도움이 될 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-QWHA_Quantization-Aware_Walsh-Hadamard_Adaptation_for_Parameter-Efficient_Fine-Tuning_on_Large_Language_Models/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-QWHA_Quantization-Aware_Walsh-Hadamard_Adaptation_for_Parameter-Efficient_Fine-Tuning_on_Large_Language_Models/</guid>
        
        <category>Review</category>
        
        <category>LLM Fine-tuning</category>
        
        <category>Quantization-Aware PEFT</category>
        
        <category>Walsh-Hadamard Transform</category>
        
        <category>Sparse Adaptation</category>
        
        <category>Low-bit Quantization</category>
        
        <category>Parameter-Efficient Learning</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.17627&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Jinshu Chen, Xinghui Li, Xu Bai, Tianxiang Ma, Pengze Zhang, Zhuowei Chen, Gen Li, Lijie Liu, Songtao Zhao, Bingchuan Li, Qian He&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;본 논문은 기존 비디오 삽입 모델의 복잡한 제어 신호(예: 마스크, 포인트) 의존성, 주제 일관성 부족, 그리고 데이터 희소성 문제를 해결하여 &lt;strong&gt;Mask-free Video Insertion (MVI)&lt;/strong&gt;의 실용성을 높이는 것을 목표로 합니다. 특히, 단일 및 다중 참조 주제를 원본 비디오에 자연스럽게 삽입하는 통합 프레임워크를 개발하는 데 중점을 둡니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;데이터 희소성을 해결하기 위해 &lt;strong&gt;InsertPipe&lt;/strong&gt;라는 새로운 데이터 파이프라인을 제안하여 다양한 쌍 데이터를 자동으로 구축합니다. 이 파이프라인 위에 &lt;strong&gt;OmniInsert&lt;/strong&gt; 프레임워크를 개발했으며, &lt;strong&gt;Condition-Specific Feature Injection (CFI)&lt;/strong&gt; 메커니즘을 통해 다중 소스 조건을 효과적으로 주입합니다. 또한, &lt;strong&gt;Progressive Training (PT)&lt;/strong&gt; 전략과 &lt;strong&gt;Subject-Focused Loss (SL)&lt;/strong&gt;를 도입하여 주제 일관성과 상세한 외관을 개선하고, &lt;strong&gt;Insertive Preference Optimization (IPO)&lt;/strong&gt; 및 &lt;strong&gt;Context-Aware Rephraser (CAR)&lt;/strong&gt; 모듈로 삽입 조화를 극대화합니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;제안된 &lt;strong&gt;OmniInsert&lt;/strong&gt;는 새로운 벤치마크인 &lt;strong&gt;InsertBench&lt;/strong&gt;에서 최첨단 상용 솔루션을 능가하는 성능을 보였습니다. 정량적 평가에서 &lt;strong&gt;Ours&lt;/strong&gt;는 사용자 연구 기준 Subject Consistency &lt;strong&gt;65.50%&lt;/strong&gt;, Text-Video Alignment (ViCLIP-T) &lt;strong&gt;25.945&lt;/strong&gt;를 달성하여 &lt;strong&gt;Pika-Pro&lt;/strong&gt;와 &lt;strong&gt;Kling&lt;/strong&gt; 대비 우월함을 입증했습니다. 이는 마스크 없이도 높은 주제 일관성과 장면 통합 능력을 보여줍니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;OmniInsert&lt;/strong&gt;는 복잡한 제어 신호 없이도 고품질 비디오 삽입이 가능함을 보여주어 AI 기반 콘텐츠 생성 및 편집 도구 개발에 중요한 발전입니다. &lt;strong&gt;InsertPipe&lt;/strong&gt;는 데이터 부족 문제를 해결하는 효과적인 전략을 제시하며, &lt;strong&gt;Progressive Training&lt;/strong&gt;과 &lt;strong&gt;Preference Optimization&lt;/strong&gt; 기법은 복잡한 생성 모델의 안정성과 품질을 향상시키는 데 적용될 수 있는 실용적인 방법론입니다. 새롭게 제안된 &lt;strong&gt;InsertBench&lt;/strong&gt;는 향후 MVI 연구를 위한 표준 벤치마크로 활용될 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-OmniInsert_Mask-Free_Video_Insertion_of_Any_Reference_via_Diffusion_Transformer_Models/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-OmniInsert_Mask-Free_Video_Insertion_of_Any_Reference_via_Diffusion_Transformer_Models/</guid>
        
        <category>Review</category>
        
        <category>Video Insertion</category>
        
        <category>Diffusion Models</category>
        
        <category>Diffusion Transformers</category>
        
        <category>Mask-Free</category>
        
        <category>Data Augmentation</category>
        
        <category>Progressive Training</category>
        
        <category>Preference Optimization</category>
        
        <category>Video Generation</category>
        
        
        <category>Review</category>
        
      </item>
    
      <item>
        <title>[논문리뷰] MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;링크:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2509.18095&quot;&gt;논문 PDF로 바로 열기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;저자:&lt;/strong&gt; Zilin Xiao, Qi Ma, Mengting Gu, Jason Chen, Xintao Chen, Vicente Ordonez, Vijai Mohan&lt;/p&gt;

&lt;h2 id=&quot;핵심-연구-목표&quot;&gt;핵심 연구 목표&lt;/h2&gt;
&lt;p&gt;기존 멀티모달 검색 방법론들이 단일 벡터 임베딩의 표현력 한계에 부딪히거나, 다수의 토큰으로 인한 다중 벡터 방식의 계산 비용 문제로 확장성에 제약을 받는 문제를 해결하고자 합니다. 유연한 테스트 시간 임베딩 세분화 제어를 통해 확장 가능하며 높은 정확도를 유지하는 멀티모달 검색 패러다임을 개발하는 것이 주 목표입니다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-방법론&quot;&gt;핵심 방법론&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MetaEmbed&lt;/strong&gt;는 쿼리 및 후보 입력 시퀀스에 소수의 &lt;strong&gt;학습 가능한 Meta Tokens&lt;/strong&gt;를 추가합니다. 이 토큰들은 기저 &lt;strong&gt;Vision-Language Model (VLM)&lt;/strong&gt;에 의해 처리되며, 최종 은닉 상태는 &lt;strong&gt;Meta Embeddings&lt;/strong&gt;로 활용되어 압축적이고 표현력 있는 멀티 벡터 표현을 제공합니다. 또한, &lt;strong&gt;Matryoshka Multi-Vector Retrieval (MMR)&lt;/strong&gt; 모듈을 통해 이 임베딩들을 접두사 중첩 그룹으로 구조화하여, 추론 시 &lt;strong&gt;거친-세밀(coarse-to-fine) 임베딩&lt;/strong&gt;과 유연한 후기 상호작용을 가능하게 합니다. 훈련은 모든 그룹에 걸쳐 &lt;strong&gt;대조 학습 목적 함수&lt;/strong&gt;를 사용하여 진행됩니다.&lt;/p&gt;

&lt;h2 id=&quot;주요-결과&quot;&gt;주요 결과&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MetaEmbed&lt;/strong&gt;는 다양한 시나리오에서 최첨단 검색 성능을 달성했습니다. &lt;strong&gt;Massive Multimodal Embedding Benchmark (MMEB)&lt;/strong&gt;에서 &lt;strong&gt;MetaEmbed-7B&lt;/strong&gt; 모델은 &lt;strong&gt;전체 Precision@1 76.6%&lt;/strong&gt;를 기록하여 MoCa-7B (71.5%) 및 mmE5 (69.8%)와 같은 강력한 기준선을 능가했습니다. &lt;strong&gt;MetaEmbed-32B&lt;/strong&gt;로 확장 시 성능은 &lt;strong&gt;78.7%&lt;/strong&gt;까지 향상되었습니다. &lt;strong&gt;ViDoRe v2&lt;/strong&gt; 벤치마크에서는 &lt;strong&gt;MetaEmbed-7B&lt;/strong&gt;가 평균 &lt;strong&gt;NDCG@5 61.3%&lt;/strong&gt;를 달성했습니다. 특히, 테스트 시간 확장성은 효과적이며, 모델 크기 증가에 따라 단일 벡터 방식 대비 &lt;strong&gt;3B에서 3.3점&lt;/strong&gt;, &lt;strong&gt;32B에서 6.6점&lt;/strong&gt;의 성능 향상을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;ai-실무자를-위한-시사점&quot;&gt;AI 실무자를 위한 시사점&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MetaEmbed&lt;/strong&gt;는 기존 멀티 벡터 방식의 계산 문제를 해결하여 확장 가능한 멀티모달 검색 시스템을 구축하기 위한 매우 효율적이고 유연한 프레임워크를 제공합니다. &lt;strong&gt;Matryoshka 설계&lt;/strong&gt;는 개발자가 테스트 시간에 검색 정확도와 계산 예산(인덱스 크기 및 지연 시간)을 조절할 수 있는 실용적인 제어 기능을 제공하여 다양한 배포 환경에 유용합니다. 이 접근 방식은 다양한 &lt;strong&gt;VLM 백본&lt;/strong&gt;(예: &lt;strong&gt;Qwen2.5-VL&lt;/strong&gt;, &lt;strong&gt;Llama-3.2-Vision&lt;/strong&gt;)과 도메인 유형 전반에 걸쳐 강력한 일반화 능력을 보여주어 견고한 멀티모달 검색 솔루션 개발에 기여할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ &lt;strong&gt;알림:&lt;/strong&gt; 이 리뷰는 AI로 작성되었습니다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 23 Sep 2025 13:36:03 +0900</pubDate>
        <link>https://secrett2633.github.io/ai/review/2025-9-23-MetaEmbed_Scaling_Multimodal_Retrieval_at_Test-Time_with_Flexible_Late_Interaction/</link>
        <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-9-23-MetaEmbed_Scaling_Multimodal_Retrieval_at_Test-Time_with_Flexible_Late_Interaction/</guid>
        
        <category>Review</category>
        
        <category>Multimodal Retrieval</category>
        
        <category>Late Interaction</category>
        
        <category>Meta Tokens</category>
        
        <category>Matryoshka Representation Learning</category>
        
        <category>Test-Time Scaling</category>
        
        <category>Vision-Language Models</category>
        
        <category>Dense Retrieval</category>
        
        <category>Efficiency</category>
        
        
        <category>Review</category>
        
      </item>
    
  </channel>
</rss> 