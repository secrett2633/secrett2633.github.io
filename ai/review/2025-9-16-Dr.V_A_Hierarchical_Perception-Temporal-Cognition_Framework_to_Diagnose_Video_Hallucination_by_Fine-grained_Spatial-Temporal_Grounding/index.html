<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/secrett2633.github.io/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/secrett2633.github.io/_next/static/css/b9d6ec750ad82add.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/secrett2633.github.io/_next/static/chunks/webpack-a04af954a21fa650.js"/><script src="/secrett2633.github.io/_next/static/chunks/fd9d1056-62aaf4b921c84028.js" async=""></script><script src="/secrett2633.github.io/_next/static/chunks/23-ca4408d024135d8d.js" async=""></script><script src="/secrett2633.github.io/_next/static/chunks/main-app-fa660020ba1e0b6e.js" async=""></script><script src="/secrett2633.github.io/_next/static/chunks/231-c4b666723e6aae68.js" async=""></script><script src="/secrett2633.github.io/_next/static/chunks/app/layout-8808afda01b7a1b7.js" async=""></script><script src="/secrett2633.github.io/_next/static/chunks/app/%5B...slug%5D/page-01b66e77b48ed573.js" async=""></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"></script><title>secrett2633&#x27;s blog</title><meta name="description" content="기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"/><meta name="author" content="secrett2633"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://secrett2633.github.io/"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="secrett2633&#x27;s blog"/><meta property="og:description" content="기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"/><meta property="og:url" content="https://secrett2633.github.io/"/><meta property="og:site_name" content="secrett2633&#x27;s blog"/><meta property="og:locale" content="ko_KR"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="secrett2633&#x27;s blog"/><meta name="twitter:description" content="기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"/><meta name="next-size-adjust"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());
              gtag('config', 'G-NE2W3CFPNY');
            </script><script src="/secrett2633.github.io/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_9012cf layout--default"><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav"><a class="site-title" href="/secrett2633.github.io">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button><button class="greedy-nav__toggle" type="button"><div class="navicon"></div></button></div><ul class="hidden-links hidden md:hidden"><li><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer" class="block py-2">GitHub</a></li></ul></nav></div></div></div><main class="initial-content"><!--$--><div class="flex flex-col lg:flex-row gap-8"><main class="flex-1"><article class="page"><header class="mb-8"><h1 class="page__title">[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</h1><div class="page__meta"><time dateTime="2025-09-16 13:16:41+0900">2025년 9월 16일</time><span class="ml-4">수정: <!-- -->2025년 9월 16일</span></div></header><div class="page__content"><div><blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2509.11866">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Li Zheng, Tianjie Ju, Liqiang Jing, Shengqiong Wu, Meng Luo 외</p>
<h2>핵심 연구 목표</h2>
<p>본 논문은 대규모 비디오 모델(LVM)이 입력 비디오와 불일치하는 내용을 생성하는 "환각(hallucination)" 문제를 해결하는 것을 목표로 합니다. 기존 환각 평가 벤치마크의 단편적인 분류 체계와 세분화된 어노테이션 부족이라는 한계를 극복하여, 비디오 환각을 진단하기 위한 포괄적이고 계층적인 프레임워크를 제안합니다.</p>
<h2>핵심 방법론</h2>
<p>제안하는 <strong>Dr.V</strong> 프레임워크는 두 가지 핵심 요소로 구성됩니다. 첫째, <strong>Dr.V-Bench</strong>는 <strong>4,974개 비디오</strong>에서 추출한 <strong>10,000개 인스턴스</strong>를 포함하는 새로운 벤치마크 데이터셋으로, <strong>미세한 시공간 어노테이션</strong>과 지각, 시간, 인지 수준의 <strong>14가지 환각 유형</strong>을 포함하는 계층적 분류 체계를 제공합니다. 둘째, <strong>Dr.V-Agent</strong>는 인간의 비디오 이해 방식을 모방한 <strong>계층적 추론 메커니즘</strong>("From-Perception-to-Temporal-to-Cognition")을 사용하여 환각을 진단합니다. 이는 <strong>Grounded SAM 2, YOLO-World, CG-STVG</strong> 등 <strong>최첨단 외부 도구</strong>를 동적으로 활용하여 시공간 증거를 검증하고, LVM의 응답을 개선하기 위한 구조화된 피드백을 제공합니다.</p>
<h2>주요 결과</h2>
<p><strong>Dr.V-Bench</strong>는 모든 테스트된 LVM이 상당한 환각을 보이는 매우 도전적인 벤치마크임을 입증했습니다. 모델들은 지각 작업에서 가장 우수한 성능을 보인 반면, 시간 및 인지 작업에서는 정확도가 현저히 감소했습니다. 예를 들어, 최상위 오픈소스 모델인 <strong>Qwen2-VL</strong>의 정확도는 지각 작업에서 **78.75%**에서 시간 작업에서 **65.61%**로 하락했습니다. <strong>Dr.V-Agent</strong>는 <strong>Self-PEP</strong> 전략 대비 모든 대표 모델과 환각 유형에서 일관되게 우수한 성능을 보여주었으며, 특히 <strong>VideoChat2</strong>의 경우 **18.60%**의 큰 성능 향상을 기록했습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p>본 연구는 LVM의 환각 문제를 해결하기 위해 <strong>미세한 시공간 그라운딩</strong>과 <strong>계층적 추론</strong>의 중요성을 강조합니다. <strong>Dr.V-Agent</strong>의 모듈식, 학습 없는 접근 방식은 외부 최첨단 도구를 활용하여 LVM의 신뢰성을 높이는 실용적인 청사진을 제공합니다. 또한, <strong>Dr.V-Bench</strong> 데이터셋은 복잡한 비디오 이해 작업에서 LVM의 취약점을 평가하고 진단하는 포괄적인 도구로서, 향후 LVM 연구 및 개발 방향에 중요한 기여를 할 것입니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
</div></div><footer class="page__meta mt-8"><div class="page__taxonomy mb-4"><h4 class="text-sm font-medium text-gray-900 mb-2">카테고리</h4><span class="page__taxonomy-item">Review</span></div><div class="page__taxonomy"><h4 class="text-sm font-medium text-gray-900 mb-2">태그</h4><span class="page__taxonomy-item">#<!-- -->Review</span><span class="page__taxonomy-item">#<!-- -->Video Hallucination</span><span class="page__taxonomy-item">#<!-- -->Large Video Models (LVMs)</span><span class="page__taxonomy-item">#<!-- -->Hierarchical Reasoning</span><span class="page__taxonomy-item">#<!-- -->Spatial-Temporal Grounding</span><span class="page__taxonomy-item">#<!-- -->Diagnostic Framework</span><span class="page__taxonomy-item">#<!-- -->Benchmark Dataset</span><span class="page__taxonomy-item">#<!-- -->Multimodal AI</span></div></footer></article></main></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© 2025 secrett2633. All rights reserved.</p></div></footer></div></div><script src="/secrett2633.github.io/_next/static/chunks/webpack-a04af954a21fa650.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/secrett2633.github.io/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/secrett2633.github.io/_next/static/css/b9d6ec750ad82add.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[4281,[\"231\",\"static/chunks/231-c4b666723e6aae68.js\",\"185\",\"static/chunks/app/layout-8808afda01b7a1b7.js\"],\"default\"]\na:I[231,[\"231\",\"static/chunks/231-c4b666723e6aae68.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-01b66e77b48ed573.js\"],\"\"]\nc:I[6130,[],\"\"]\n7:[\"slug\",\"ai/review/2025-9-16-Dr.V_A_Hierarchical_Perception-Temporal-Cognition_Framework_to_Diagnose_Video_Hallucination_by_Fine-grained_Spatial-Temporal_Grounding\",\"c\"]\nd:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/secrett2633.github.io/_next/static/css/b9d6ec750ad82add.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"iV6XySbMHIJ3imQdvgy3I\",\"assetPrefix\":\"/secrett2633.github.io\",\"initialCanonicalUrl\":\"/ai/review/2025-9-16-Dr.V_A_Hierarchical_Perception-Temporal-Cognition_Framework_to_Diagnose_Video_Hallucination_by_Fine-grained_Spatial-Temporal_Grounding/\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-9-16-Dr.V_A_Hierarchical_Perception-Temporal-Cognition_Framework_to_Diagnose_Video_Hallucination_by_Fine-grained_Spatial-Temporal_Grounding\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"review\\\",\\\"2025-9-16-Dr.V_A_Hierarchical_Perception-Temporal-Cognition_Framework_to_Diagnose_Video_Hallucination_by_Fine-grained_Spatial-Temporal_Grounding\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-9-16-Dr.V_A_Hierarchical_Perception-Temporal-Cognition_Framework_to_Diagnose_Video_Hallucination_by_Fine-grained_Spatial-Temporal_Grounding\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\"],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"32x32\",\"href\":\"/favicon-32x32.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"16x16\",\"href\":\"/favicon-16x16.png\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"script\",null,{\"async\":true,\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              window.dataLayer = window.dataLayer || [];\\n              function gtag(){dataLayer.push(arguments);}\\n              gtag('js', new Date());\\n              gtag('config', 'G-NE2W3CFPNY');\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_9012cf layout--default\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"main\",null,{\"className\":\"initial-content\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":\"© 2025 secrett2633. All rights reserved.\"}]}]}]}]]}]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lb\"],\"globalErrorComponent\":\"$c\",\"missingSlots\":\"$Wd\"}]]\n"])</script><script>self.__next_f.push([1,"e:Tc8c,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e링크:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/2509.11866\"\u003e논문 PDF로 바로 열기\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e저자:\u003c/strong\u003e Li Zheng, Tianjie Ju, Liqiang Jing, Shengqiong Wu, Meng Luo 외\u003c/p\u003e\n\u003ch2\u003e핵심 연구 목표\u003c/h2\u003e\n\u003cp\u003e본 논문은 대규모 비디오 모델(LVM)이 입력 비디오와 불일치하는 내용을 생성하는 \"환각(hallucination)\" 문제를 해결하는 것을 목표로 합니다. 기존 환각 평가 벤치마크의 단편적인 분류 체계와 세분화된 어노테이션 부족이라는 한계를 극복하여, 비디오 환각을 진단하기 위한 포괄적이고 계층적인 프레임워크를 제안합니다.\u003c/p\u003e\n\u003ch2\u003e핵심 방법론\u003c/h2\u003e\n\u003cp\u003e제안하는 \u003cstrong\u003eDr.V\u003c/strong\u003e 프레임워크는 두 가지 핵심 요소로 구성됩니다. 첫째, \u003cstrong\u003eDr.V-Bench\u003c/strong\u003e는 \u003cstrong\u003e4,974개 비디오\u003c/strong\u003e에서 추출한 \u003cstrong\u003e10,000개 인스턴스\u003c/strong\u003e를 포함하는 새로운 벤치마크 데이터셋으로, \u003cstrong\u003e미세한 시공간 어노테이션\u003c/strong\u003e과 지각, 시간, 인지 수준의 \u003cstrong\u003e14가지 환각 유형\u003c/strong\u003e을 포함하는 계층적 분류 체계를 제공합니다. 둘째, \u003cstrong\u003eDr.V-Agent\u003c/strong\u003e는 인간의 비디오 이해 방식을 모방한 \u003cstrong\u003e계층적 추론 메커니즘\u003c/strong\u003e(\"From-Perception-to-Temporal-to-Cognition\")을 사용하여 환각을 진단합니다. 이는 \u003cstrong\u003eGrounded SAM 2, YOLO-World, CG-STVG\u003c/strong\u003e 등 \u003cstrong\u003e최첨단 외부 도구\u003c/strong\u003e를 동적으로 활용하여 시공간 증거를 검증하고, LVM의 응답을 개선하기 위한 구조화된 피드백을 제공합니다.\u003c/p\u003e\n\u003ch2\u003e주요 결과\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDr.V-Bench\u003c/strong\u003e는 모든 테스트된 LVM이 상당한 환각을 보이는 매우 도전적인 벤치마크임을 입증했습니다. 모델들은 지각 작업에서 가장 우수한 성능을 보인 반면, 시간 및 인지 작업에서는 정확도가 현저히 감소했습니다. 예를 들어, 최상위 오픈소스 모델인 \u003cstrong\u003eQwen2-VL\u003c/strong\u003e의 정확도는 지각 작업에서 **78.75%**에서 시간 작업에서 **65.61%**로 하락했습니다. \u003cstrong\u003eDr.V-Agent\u003c/strong\u003e는 \u003cstrong\u003eSelf-PEP\u003c/strong\u003e 전략 대비 모든 대표 모델과 환각 유형에서 일관되게 우수한 성능을 보여주었으며, 특히 \u003cstrong\u003eVideoChat2\u003c/strong\u003e의 경우 **18.60%**의 큰 성능 향상을 기록했습니다.\u003c/p\u003e\n\u003ch2\u003eAI 실무자를 위한 시사점\u003c/h2\u003e\n\u003cp\u003e본 연구는 LVM의 환각 문제를 해결하기 위해 \u003cstrong\u003e미세한 시공간 그라운딩\u003c/strong\u003e과 \u003cstrong\u003e계층적 추론\u003c/strong\u003e의 중요성을 강조합니다. \u003cstrong\u003eDr.V-Agent\u003c/strong\u003e의 모듈식, 학습 없는 접근 방식은 외부 최첨단 도구를 활용하여 LVM의 신뢰성을 높이는 실용적인 청사진을 제공합니다. 또한, \u003cstrong\u003eDr.V-Bench\u003c/strong\u003e 데이터셋은 복잡한 비디오 이해 작업에서 LVM의 취약점을 평가하고 진단하는 포괄적인 도구로서, 향후 LVM 연구 및 개발 방향에 중요한 기여를 할 것입니다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e⚠️ \u003cstrong\u003e알림:\u003c/strong\u003e 이 리뷰는 AI로 작성되었습니다.\u003c/p\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"article\",null,{\"className\":\"page\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"page__title\",\"children\":\"[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding\"}],[\"$\",\"div\",null,{\"className\":\"page__meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-16 13:16:41+0900\",\"children\":\"2025년 9월 16일\"}],[\"$\",\"span\",null,{\"className\":\"ml-4\",\"children\":[\"수정: \",\"2025년 9월 16일\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"page__content\",\"children\":[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$e\"}}]}],[\"$\",\"footer\",null,{\"className\":\"page__meta mt-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"page__taxonomy mb-4\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"text-sm font-medium text-gray-900 mb-2\",\"children\":\"카테고리\"}],[[\"$\",\"span\",\"Review\",{\"className\":\"page__taxonomy-item\",\"children\":\"Review\"}]]]}],[\"$\",\"div\",null,{\"className\":\"page__taxonomy\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"text-sm font-medium text-gray-900 mb-2\",\"children\":\"태그\"}],[[\"$\",\"span\",\"Review\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Review\"]}],[\"$\",\"span\",\"Video Hallucination\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Video Hallucination\"]}],[\"$\",\"span\",\"Large Video Models (LVMs)\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Large Video Models (LVMs)\"]}],[\"$\",\"span\",\"Hierarchical Reasoning\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Hierarchical Reasoning\"]}],[\"$\",\"span\",\"Spatial-Temporal Grounding\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Spatial-Temporal Grounding\"]}],[\"$\",\"span\",\"Diagnostic Framework\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Diagnostic Framework\"]}],[\"$\",\"span\",\"Benchmark Dataset\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Benchmark Dataset\"]}],[\"$\",\"span\",\"Multimodal AI\",{\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Multimodal AI\"]}]]]}]]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"5\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"6\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"7\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"9\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"10\",{\"rel\":\"canonical\",\"href\":\"https://secrett2633.github.io/\"}],[\"$\",\"meta\",\"11\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:title\",\"content\":\"secrett2633's blog\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:description\",\"content\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:url\",\"content\":\"https://secrett2633.github.io/\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:site_name\",\"content\":\"secrett2633's blog\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:locale\",\"content\":\"ko_KR\"}],[\"$\",\"meta\",\"17\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:title\",\"content\":\"secrett2633's blog\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:description\",\"content\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\"}],[\"$\",\"meta\",\"21\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>